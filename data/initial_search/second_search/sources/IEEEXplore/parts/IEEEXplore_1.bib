@INPROCEEDINGS{10386611,
  author={Liu, Jiehui and Zhan, Jieyu},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Constructing Knowledge Graph from Cyber Threat Intelligence Using Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={516-521},
  abstract={Cyber Threat Intelligence (CTI) reports are valuable resources in various applications but manually extracting information from them is time-consuming. Existing approaches for automating extraction require specialized models trained on a substantial corpus. In this paper, we present an efficient methodology for constructing knowledge graphs from CTI by leveraging the Large Language Model (LLM), using ChatGPT for instance. Our approach automatically extracts attack-related entities and their relationships, organizing them within a CTI knowledge graph. We evaluate our approach on 13 CTIs, demonstrating better performance compared to AttacKG and REBEL while requiring less manual intervention and computational resources. This proves the feasibility and suitability of our method in low-resource scenarios, specifically within the domain of cyber threat intelligence.},
  keywords={Computational modeling;Knowledge graphs;Manuals;Ontologies;Information retrieval;Data models;Cognition;knowledge graph;threat intelligence;large language model;ChatGPT},
  doi={10.1109/BigData59044.2023.10386611},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10475639,
  author={Vizcarra, Julio and Haruta, Shuichiro and Kurokawa, Mori},
  booktitle={2024 IEEE 18th International Conference on Semantic Computing (ICSC)}, 
  title={Representing the Interaction between Users and Products via LLM-assisted Knowledge Graph Construction}, 
  year={2024},
  volume={},
  number={},
  pages={231-232},
  abstract={To understand user behavior, representing the semantic knowledge of user-product interaction is essential. In this paper, we represent the interaction between user and product via large language model (LLM)-assisted knowledge graph construction. We capture users’ behavioral actions and static properties of the products from raw text data of “user review” and “product catalog”. Moreover, the information needed for updating the knowledge graph is captured by raw texts of “news related to the products”. The proposed methodology integrates them as a single knowledge graph to provide causal reasoning on user-product interaction. To alleviate the situation where a small quantity of annotated text exists in these data, we use LLM as a data annotator and augmentor.},
  keywords={Text mining;Reviews;Annotations;Semantics;Knowledge graphs;Data augmentation;Cognition;Knowledge graph;text mining;ontology;causality;LLM;user-product interaction},
  doi={10.1109/ICSC59802.2024.00043},
  ISSN={2472-9671},
  month={Feb},}@INPROCEEDINGS{10588309,
  author={Peng, Tao and Rao, Taiwen and Xu, Yansong and Yang, Chao and Xie, Xiaotian and Yang, Chunhua},
  booktitle={2024 36th Chinese Control and Decision Conference (CCDC)}, 
  title={Construction of Rail Transit Network Fault Knowledge Graph Based on Pseudo-Dynamic Relationship Ontology Architecture}, 
  year={2024},
  volume={},
  number={},
  pages={4582-4587},
  abstract={In this paper, an approach to construction of rail transit network fault knowledge graph based on pseudo-dynamic relationship ontology architecture is proposed. Firstly, an ontology architecture that includes pseudo-dynamic relationships is employed to completely separate fault patterns from fault entities without losing semantics. Secondly, a named entity recognition method based on the BERT-BiGRU-CRF model is adopted, which performs well in short entity extraction tasks. Thirdly, a pseudo-dynamic relationship extraction method based on the BERT-MEA(multi entities attention) model is adopted to extract static simple relationships, followed by treating triples as head and tail entities to determine pseudo-dynamic relationships. The implementation of rail transit network fault knowledge graph demonstrates the effectiveness of the proposed approach.},
  keywords={Rails;Databases;Semantics;Knowledge graphs;Tail;Named entity recognition;Ontologies;Ontology architecture;Pseudo-dynamic relationship;Short entity;Fault knowledge graph},
  doi={10.1109/CCDC62350.2024.10588309},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{10710789,
  author={Schoch, Nicolai and Hoernicke, Mario and Strem, Nika and Stark, Katharina},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Engineering Data Funnel (WIP) – An Ontology-Enhanced LLM-Based Agent and MoE System for Engineering Data Processing}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Automation Engineering of a process automation system is still a very manual effort due to limited support for the interpretation and processing of process design specification documents. Even though standards for digital data exchange between process and automation engineering do exist, those formats are rarely used and consequently the immense automation potential in automation engineering cannot be lifted. This contribution presents an AI -based approach and prototype - using an ontology-enhanced LLM -based agent and a mixture-of-experts system - to structure and formalize multimodal unstructured process design information as in PDF, Excel, and Word formats and make it available for state-of-the-art engineering tools for the long-known “Automation of Automation”.},
  keywords={Process design;Automation;Prototypes;Manuals;Portable document format;Data processing;Artificial intelligence;Standards;Manufacturing automation;engineering design specification;engineering data processing;LLM-based agent;mixture of experts;ontology-driven information processing;automation of automation},
  doi={10.1109/ETFA61755.2024.10710789},
  ISSN={1946-0759},
  month={Sep.},}@ARTICLE{10669737,
  author={Li, Yihao and Zhang, Ru and Liu, Jianyi and Lei, Qi},
  journal={IEEE Signal Processing Letters}, 
  title={A Semantic Controllable Long Text Steganography Framework Based on LLM Prompt Engineering and Knowledge Graph}, 
  year={2024},
  volume={31},
  number={},
  pages={2610-2614},
  abstract={With ongoing advancements in natural language technology, text steganography has achieved notable progress. However, existing methods primarily concentrate on the probability distribution between words, often overlooking comprehensive control over text semantics. Particularly in the case of longer texts, these methods struggle to preserve coherence and contextual consistency, thereby increasing the risk of detection in practical applications. To effectively improve steganography security, we propose a semantic controllable long-text steganography framework based on prompt engineering and knowledge graph (KG) integration, obviating supplementary training. This framework leverages triplets from the KG and task descriptions to construct prompts, directing the large language model (LLM) to generate text that aligns with the triplet content. Subsequently, the model effectively embeds secret information by encoding the candidate pools established around the sampled target words. The experimental results demonstrate that our framework ensures the concealment of steganographic text while maintaining the relevance and consistency of the content as expected. Moreover, it can be flexibly adapted to various application scenarios, showcasing its potential and advantages in practical implementations.},
  keywords={Steganography;Encoding;Semantics;Mathematical models;Training;Prompt engineering;Probability distribution;Text steganography;semantic controllable;LLM prompt engineering;knowledge graph},
  doi={10.1109/LSP.2024.3456636},
  ISSN={1558-2361},
  month={},}@INPROCEEDINGS{10544941,
  author={Dequan, Gao and Pengyu, Zhu and Sheng, Wang and Ziyan, Zhao},
  booktitle={2024 6th Asia Energy and Electrical Engineering Symposium (AEEES)}, 
  title={Deep Learning-Based Fault Knowledge Graph Construction for Power Communication Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1088-1093},
  abstract={Power communication network is a crucial infrastructure in the model power system, and its maintenance capability are crucial to ensuring the stable operation of power grid business. As an organized semantic knowledge base, the knowledge graph effectively organizes power communication network fault documentation and expert experience to enhance intelligent maintenance. This paper outlines a top-down approach to systematically construct a fault knowledge graph in the domain of power communication networks. The approach utilizes a seven-step method to establish a domain ontology model and integrates deep learning algorithms, including pre-trained language models, bidirectional long short time memory networks, convolutional neural networks and attention mechanisms. These algorithms process unstructured text to extract key entities and relationships. The effectiveness of the approach is verified through experiments using a product device document as a test case. Extracted knowledge is then visualized and stored using Neo4j database. Finally, this paper proposes a knowledge service model centered on fault knowledge graph and explores its application in fault diagnosis.},
  keywords={Deep learning;Patents;Semantics;Knowledge graphs;Ontologies;Real-time systems;Power grids;power communication networks;fault knowledge graph;deep learning;fault diagnosis},
  doi={10.1109/AEEES61147.2024.10544941},
  ISSN={},
  month={March},}@ARTICLE{9540703,
  author={Choi, Bonggeun and Jang, Daesik and Ko, Youngjoong},
  journal={IEEE Access}, 
  title={MEM-KGC: Masked Entity Model for Knowledge Graph Completion With Pre-Trained Language Model}, 
  year={2021},
  volume={9},
  number={},
  pages={132025-132032},
  abstract={The knowledge graph completion (KGC) task aims to predict missing links in knowledge graphs. Recently, several KGC models based on translational distance or semantic matching methods have been proposed and have achieved meaningful results. However, existing models have a significant shortcoming–they cannot train entity embedding when an entity does not appear in the training phase. As a result, such models use randomly initialized embeddings for entities that are unseen in the training phase and cause a critical decrease in performance during the test phase. To solve this problem, we propose a new approach that performs KGC task by utilizing the masked language model (MLM) that is used for a pre-trained language model. Given a triple (head entity, relation, tail entity), we mask the tail entity and consider the head entity and the relation as a context for the tail entity. The model then predicts the masked entity from among all entities. Then, the task is conducted by the same process as an MLM, which predicts a masked token with a given context of tokens. Our experimental results show that the proposed model achieves significantly improved performances when unseen entities appear during the test phase and achieves state-of-the-art performance on the WN18RR dataset.},
  keywords={Task analysis;Predictive models;Training;Bit error rate;Semantics;Micromechanical devices;Knowledge graph completion;link prediction;masked language model;pre-trained language model},
  doi={10.1109/ACCESS.2021.3113329},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10412761,
  author={Vijayakumar, Senthilkumar and Louis, Filious},
  booktitle={2023 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Revolutionizing Staffing and Recruiting with Contextual Knowledge Graphs and QNLP: An End-to-End Quantum Training Paradigm}, 
  year={2023},
  volume={},
  number={},
  pages={45-51},
  abstract={The staffing and recruiting industry is continuously evolving, and recent advancements in Knowledge Graphs (KG) and Quantum Natural Language Processing (QNLP) has garnered considerable attention. The integration of these state-of-the-art technologies is fueled by the necessity to improve language models' capacity to comprehend context and make precise decisions. This research paper presents a novel approach to revolutionize the staffing and recruiting industry by integrating Knowledge Graph (KG) and Quantum Natural Language Processing (QNLP) to formulate an end-to-end QNLP training pipeline. The proposed solution consists of three interdependent subsystems that work in unison to construct contextual KG and train language models. The Information Extraction subsystem extracts semantic relationships and connections between entities from large and complex recruitment data to construct domain specific contextual KG. The QNLP model training pipeline subsystem, which is fed with domain-rich KG data, runs on Quantum Circuits, accelerates the training process by effectively incorporating high-dimensional features to the deep layers of language models. Finally, the Information Retrieval subsystem is based on semantic data taxonomy, retrieving contextual data from the KG for the trained language models to be implemented on various distinctive use cases in the staffing and recruiting industry. This solution provides a faster and more contextual approach to analyze recruitment data, empowering recruiters to concentrate on strategic tasks such as candidate engagement and client relationship building, ultimately leading to better business decision-making capabilities.},
  keywords={Training;Industries;Knowledge graphs;Natural language processing;Data models;Integrated circuit modeling;Context modeling;Artificial Intelligence (AI);Knowledge Graph (KG);Quantum Natural Language Processing (QNLP);Large Language Models (LLM);Contextual Information Extraction & Retrieval Systems},
  doi={10.1109/ICKG59574.2023.00011},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10695412,
  author={Luo, Junjun and Zhu, Zhongyan and Zhu, Haijiang and Dong, Xiaohui},
  booktitle={2024 7th International Conference on Computer Information Science and Application Technology (CISAT)}, 
  title={Research on knowledge graph construction method for mine hoist fault field}, 
  year={2024},
  volume={},
  number={},
  pages={342-346},
  abstract={Mine hoists are integral to mining hoisting systems, with their safe and reliable operation being critical for ensuring the safety of mining operations. The consequences of hoist failure are severe, particularly when the root cause of the malfunction is not promptly identified and addressed, potentially compromising the overall safety of mining activities. The complexity of mine hoist systems stems from the interdependent and restrictive relationships among their components, each of which generates unique operational state information. This information, when aggregated and processed, can be distilled into various fault characteristic parameters. This paper introduces a novel approach to fault diagnosis within mine hoist systems by constructing a fault knowledge graph based on ontological principles. The proposed method harnesses the power of knowledge graphs to systematically represent and analyze the complex interplay of components within the hoist system. By doing so, it enhances the diagnostic capabilities and the preemptive identification of potential faults. The research focuses on the mine hoist as the subject of study and proposes the development of an ontologically-based fault knowledge graph. This approach is not only of significant importance to the coal mining industry but also offers innovative insights for knowledge graph construction across various domains. The implications of this study extend beyond the mining sector, providing a foundation for more robust and intelligent fault diagnosis systems in complex mechanical systems.},
  keywords={Fault diagnosis;Information science;Instruments;Knowledge graphs;Named entity recognition;Information retrieval;Safety;Complexity theory;Mechanical systems;Lifting equipment;artificial intelligence;entity recognition;fault knowledge graph;mine hoist;relation extraction},
  doi={10.1109/CISAT62382.2024.10695412},
  ISSN={},
  month={July},}@INPROCEEDINGS{10611970,
  author={Lee, Chang-Shing and Wang, Mei-Hui and Chiang, Jun-Kui and Kubota, Naoyuki and Sato-Shimokawara, Eri and Nojima, Yusuke and Acampora, Giovanni and Wu, Pei-Yu and Chiu, Szu-Chi and Yang, Sheng-Chi and Siow, Chyan-Zheng},
  booktitle={2024 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={Quantum Computational Intelligence with Generative AI Image for Human-Machine Interaction}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper introduces a Quantum Computational Intelligence (QCI) agent equipped with a content attention ontology model, specifically designed to enhance human-machine interaction based on a Generative Artificial Intelligence (GAI) image generation agent for Taiwanese/English learning and experience. Its diverse primary applications include social media analysis on Facebook groups and YouTube learning videos related to the 2023 IEEE CIS Education Portal (EP) Subcommittee, as well as in the areas of Taiwanese/English language learning and dialogue experience with GAI image generation. To establish the knowledge and inference models for the QCI agent, we initially developed a Taiwanese/English learning and experience ontology, including a content attention ontology, and an image attention ontology. The QCI agent utilizes metrics such as the number of views, posts, and comments to predict the fuzzy number of reactions. In addition, the GAI image agent generates Taiwanese speech-based/English text-based images and evaluates the fuzzy similarity score between Taiwanese/English and the attention ontology together with the Sentence BERT (SBERT) agent. This Taiwanese/English fuzzy similarity score is further validated through human assessments, with these evaluations subsequently serving as an additional metric for comparative analysis of Human-Machine Interaction (HMI). Furthermore, the GAI image agent is designed to create images and Chinese/English texts from text/speech translated by the Meta AI Universal Speech Translator (UST) Taiwanese/English agent. A Particle Swarm Optimization (PSO)-based machine learning mechanism is employed to train the QCI model for assessing learners' performance and predicting the performance of others. The National University of Tainan (NUTN) Taiwan-Large Language Model (NUTN.TW-LLM) agent has been further enhanced to support interactive learning experiences for HMI. An SBERT-based assessment agent is used to calculate fuzzy similarities between questions and answers in Taiwanese/English experiences and dialogues. Experimental results demonstrate the feasibility and efficacy of the proposed QCI model, equipped with QCI&AI-FML (Artificial Intelligence-Fuzzy Markup Language) and machine learning capabilities, for social media and language learning applications on HMI. In the future, we will extend the QCI model to various HMI applications for student learning around the world.},
  keywords={Human computer interaction;Measurement;Quantum computing;Social networking (online);Image synthesis;Generative AI;Computational modeling;Quantum CI Agent;Content Attention Ontology;ChatGPT;Generative AI Image Agent;IEEE CIS Education Portal;Fuzzy Markup Language;Sentence BERT;NUTN.TW-LLM},
  doi={10.1109/FUZZ-IEEE60900.2024.10611970},
  ISSN={1558-4739},
  month={June},}@INPROCEEDINGS{10863933,
  author={Meng, Qi and Wu, Zhenglong and Zhao, Zhongshi and Lian, Xi'nan},
  booktitle={2024 5th International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, 
  title={Research of Knowledge-Enhanced Large Language Model Based on Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={693-697},
  abstract={The rapid development and wide application of Knowledge Graph(KG) and Large Language Model(LLM) have demonstrated strong basic capabilities of artificial intelligence. In the last two years, related research has explored the combination of KG and LLM to improve the model's ability in text understanding and inference. In view of this kind of research, the basic knowledge of KG enhancing LLM is systematically introduced, and how KG enhances LLM from the perspective of functional realization is analyzed, and the existing research results of KG enhancing LLM is also summarized, which provides reference for further research on LLM enhanced by KG.},
  keywords={Training;Inference mechanisms;Large language models;Computational modeling;Semantics;Knowledge graphs;Natural language processing;Inference algorithms;Graph neural networks;Hardware acceleration;knowledge graph;large language model;knowledge enhancement;prospect of research},
  doi={10.1109/ICAICE63571.2024.10863933},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10822222,
  author={Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={An LLM supported approach to ontology and knowledge graph construction}, 
  year={2024},
  volume={},
  number={},
  pages={5240-5246},
  abstract={The continuous development in the medical field faces multiple challenges in managing a large amount of literature and research results using traditional ontology and knowledge graph construction methods. These challenges include high labor costs, limited coverage, and poor dynamism of traditional ontology and knowledge graph construction methods. Large language models (LLMs) can solve various natural language processing tasks and can understand and generate human-like natural language, which makes automated construction of ontology expansion and knowledge graphs (KGs) possible. This paper proposes an ontology expansion method based on LLMs, using LLMs to formulate competency questions (CQs) to extend the initial ontology, and then constructing the knowledge graph based on the extended ontology. We demonstrated the feasibility of the method by creating a knowledge graph for breast cancer treatment. The combination of LLMs-based medical ontology and knowledge graph can achieve more efficient medical knowledge management and application, promoting the informatization and intelligent development of the medical field.},
  keywords={Semantic Web;Large language models;Refining;Knowledge graphs;Medical services;Ontologies;Natural language processing;Iterative methods;Reliability;Usability;Ontology;Knowledge graph;LLM;Breast cancer treatment},
  doi={10.1109/BIBM62325.2024.10822222},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{9995517,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Adaptive Multi-view Graph Convolutional Network for Gene Ontology Annotations of Proteins}, 
  year={2022},
  volume={},
  number={},
  pages={90-93},
  abstract={Gene Ontology (GO) containing a set of standard concepts (or terms) is launched to unify the functional descriptions of proteins. Developing computational models based on GO to automatically annotate protein functions has been a longstanding active research area. In this paper, we propose a novel method to adaptively fuse functional and topological information between GO Terms. Our method is composed of a pre-trained language model for encoding protein sequences and an adaptive multi-view graph convolutional network (Multi-view GCN) for representing GO terms. Particularly, the Multi-view GCN considers multiple views from functional information, topological structures, and their combinations, and extracts multiple corresponding representations of GO terms. Then, an attention mechanism is applied to adaptively learn the importance weights of these representations. Finally, the predicted scores are calculated by using a dot product between protein sequence features and GO term representations. Experimental results on the datasets of two species (i.e., Human and Yeast) show that our method outperforms other state-of-the-art methods. The code of our proposed method is available at: https://github.com/Candyperfect/Master.},
  keywords={Convolutional codes;Adaptation models;Adaptive systems;Computational modeling;RNA;Ontologies;Feature extraction;gene ontology terms;protein function prediction;deep learning;adaptive multi-view graph convolutional network},
  doi={10.1109/BIBM55620.2022.9995517},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10827945,
  author={Chao, Gao and Xiaoyuan, Liu and Dongfang, Zhang},
  booktitle={2024 IEEE 6th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)}, 
  title={The integration of large language model and knowledge graph and its application in flight safety}, 
  year={2024},
  volume={},
  number={},
  pages={491-494},
  abstract={This article introduces the advantages and disadvantages of large language model and knowledge graph. Based on the concept of mutual integration, it summarizes the research on the application of large language model in knowledge graph construction and knowledge graph supporting the application of large model. Finally, it puts forward the application idea of the integration of large language model and knowledge graph in flight safety.},
  keywords={Deep learning;Data analysis;Large language models;Safety management;Knowledge graphs;Safety;Information technology;large language model;knowledge graph;integration;flight safety},
  doi={10.1109/ICCASIT62299.2024.10827945},
  ISSN={},
  month={Oct},}@ARTICLE{10129977,
  author={Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Yang, Yumeng and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Protein Function Prediction With Functional and Topological Knowledge of Gene Ontology}, 
  year={2023},
  volume={22},
  number={4},
  pages={755-762},
  abstract={Gene Ontology (GO) is a widely used bioinformatics resource for describing biological processes, molecular functions, and cellular components of proteins. It covers more than 5000 terms hierarchically organized into a directed acyclic graph and known functional annotations. Automatically annotating protein functions by using GO-based computational models has been an area of active research for a long time. However, due to the limited functional annotation information and complex topological structures of GO, existing models cannot effectively capture the knowledge representation of GO. To solve this issue, we present a method that fuses the functional and topological knowledge of GO to guide protein function prediction. This method employs a multi-view GCN model to extract a variety of GO representations from functional information, topological structure, and their combinations. To dynamically learn the significance weights of these representations, it adopts an attention mechanism to learn the final knowledge representation of GO. Furthermore, it uses a pre-trained language model (i.e., ESM-1b) to efficiently learn biological features for each protein sequence. Finally, it obtains all predicted scores by calculating the dot product of sequence features and GO representation. Our method outperforms other state-of-the-art methods, as demonstrated by the experimental results on datasets from three different species, namely Yeast, Human and Arabidopsis. Our proposed method’s code can be accessed at: https://github.com/Candyperfect/Master.},
  keywords={Proteins;Feature extraction;Amino acids;Annotations;Predictive models;Biological system modeling;Protein sequence;Convolutional neural networks;Graph neural networks;Protein function prediction;gene ontology;multi-view GCN;pre-trained language model},
  doi={10.1109/TNB.2023.3278033},
  ISSN={1558-2639},
  month={Oct},}@INPROCEEDINGS{10726129,
  author={Vidhate, Hutesh and Khobragade, Anish},
  booktitle={2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={GnBERT: Graph Neural Networks over BERT for Knowledge Graph Representation Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Many disciplines depend on knowledge graphs to represent and arrange organized data. Nevertheless, understanding knowledge graphs is still quite difficult because of their intricate relationships and great importance. The work develops a new framework named GnBERT by fusing Bidirectional Encoder Representations from Transformers (BERT) with Graph Neural Networks (GNNs). Knowledge graph representations are able to be obtained via this approach. Graph Neural Networks (GNNs) provide relational semantics; GnBERT improves the representation of entities and relationships in knowledge graphs by combining the hierarchical structure of BERT’s pre-trained language model with it. We have shown, using benchmark datasets, that GnBERT achieves the best degree of performance in knowledge graph completion, entity linking, and connection extraction over earlier techniques.},
  keywords={Representation learning;Semantics;Knowledge graphs;Bidirectional control;Benchmark testing;Transformers;Graph neural networks;Encoding;GnBERT;Graph Neural Network;BERT;Knowledge Graph;Representation Learning},
  doi={10.1109/ICCCNT61001.2024.10726129},
  ISSN={2473-7674},
  month={June},}@ARTICLE{10475356,
  author={Zhu, Huashi and Xu, Dexuan and Huang, Yu and Jin, Zhi and Ding, Weiping and Tong, Jiahui and Chong, Guoshuang},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, 
  title={Graph Structure Enhanced Pre-Training Language Model for Knowledge Graph Completion}, 
  year={2024},
  volume={8},
  number={4},
  pages={2697-2708},
  abstract={A vast amount of textual and structural information is required for knowledge graph construction and its downstream tasks. However, most of the current knowledge graphs are incomplete due to the difficulty of knowledge acquisition and integration. Knowledge Graph Completion (KGC) is used to predict missing connections. In previous studies, textual information and graph structural information are utilized independently, without an effective method for fusing these two types of information. In this paper, we propose a graph structure enhanced pre-training language model for knowledge graph completion. Firstly, we design a graph sampling algorithm and a Graph2Seq module for constructing sub-graphs and their corresponding contexts to support large-scale knowledge graph learning and parallel training. It is also the basis for fusing textual data and graph structure. Next, two pre-training tasks based on masked modeling are designed for capturing accurate entity-level and relation-level information. Furthermore, this paper proposes a novel asymmetric Encoder-Decoder architecture to restore masked components, where the encoder is a Pre-trained Language Model (PLM) and the decoder is a multi-relational Graph Neural Network (GNN). The purpose of the architecture is to integrate textual information effectively with graph structural information. Finally, the model is fine-tuned for KGC tasks on two widely used public datasets. The experiments show that the model achieves excellent performance and outperforms baselines in most metrics, which demonstrate the effectiveness of our approach by fusing the structure and semantic information to knowledge graph.},
  keywords={Knowledge graphs;Semantics;Predictive models;Micromechanical devices;Natural language processing;Graph neural networks;Knowledge graph completion;masked modeling;pre-trained language models;graph neural network},
  doi={10.1109/TETCI.2024.3372442},
  ISSN={2471-285X},
  month={Aug},}@INPROCEEDINGS{10512525,
  author={Zhao, Honda and Jiang, Wei and Deng, Jiewen and Ren, Qinghua and Zhang, Li},
  booktitle={2023 IEEE 7th Conference on Energy Internet and Energy System Integration (EI2)}, 
  title={Constructing Knowledge Graph for Electricity Keywords Based on Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={4844-4849},
  abstract={In the information age, the electric power industry, as a crucial pillar of modern society, has accumulated a wealth of valuable research literature. Knowledge graph technology offers the potential to tap into this knowledge repository, providing a better understanding of research outcomes in the electric power domain. However, due to the diversity and complexity of knowledge in the power industry, it is difficult to build a comprehensive and complete knowledge graph of power keywords. In recent years, large language models (LLMs) have made significant advancements. This paper harnesses LLM technology along with text similarity analysis and co-occurrence frequency analysis to establish a comprehensive framework for processing keyword knowledge in the field of electric power. Within this framework, various forms of information found in electric power research can be processed. This includes creating a thesaurus of electric power domain keywords; obtaining the individual attributes implied by the information keywords in this thesaurus and their interconnections; and generating a knowledge graph of electric power domain keywords. This knowledge graph includes attributes, interpretations, relationships, and associated literature for keywords. It serves as a valuable reference for the effective utilization of research outcomes in the electric power domain.},
  keywords={Knowledge engineering;Analytical models;Semantics;Knowledge based systems;Knowledge graphs;System integration;Power industry;LLM;knowledge graph;ChatGLM;similarity analysis;co-occurrence},
  doi={10.1109/EI259745.2023.10512525},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9863051,
  author={Ataei, Sima and Butler, Gregory},
  booktitle={2022 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)}, 
  title={Predicting the specific substrate for transmembrane transport proteins using BERT language model}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Transmembrane transport proteins play a vital role in cells' metabolism by the selective passage of substrates through the cell membrane. Metabolic network reconstruction requires transport reactions that describe the specific substrate transported as well as the metabolic reactions of enzyme catalysis. In this paper, we apply BERT (Bidirectional Encoder Representations from Transformers) language model for protein sequences to predict one of 12 specific substrates. Our UniProt-ICAT-100 dataset is automatically constructed from UniProt using the ChEBI and GO ontologies to identify 4,112 proteins transporting 12 inorganic anion or cation substrates. We classified this dataset using three different models including Logistic Regression with an MCC of 0.81 and accuracy of 97.5%; Feed-forward Neural Networks classifier with an MCC of 0.88 and accuracy of 98.5%. Our third model utilizes a Fine-tuned BERT language model to predict the specific substrate with an MCC of 0.95 and accuracy of 99.3% on an independent test set.},
  keywords={Proteins;Biological system modeling;Computational modeling;Bit error rate;Neural networks;Cells (biology);Predictive models;Classification;BERT model;Transport protein;Specific substrate Prediction;ChEBI ontology;Gene Ontology},
  doi={10.1109/CIBCB55180.2022.9863051},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10522716,
  author={Mateiu, Patricia and Groza, Adrian},
  booktitle={2023 25th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
  title={Ontology engineering with Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={226-229},
  abstract={We tackle the task of enriching ontologies by automatically translating natural language (NL) into Description Logic (DL). Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert NL into OWL Functional Syntax. For fine-tuning, we designed pairs of sentences in NL and the corresponding translations. This training pairs cover various aspects from ontology engineering: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, or cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protégé plugin.},
  keywords={Training;Scientific computing;Description logic;OWL;Natural languages;Syntactics;Task analysis;ontology engineering;large language models;Protege plugin;fine-tuning},
  doi={10.1109/SYNASC61333.2023.00038},
  ISSN={2470-881X},
  month={Sep.},}@INPROCEEDINGS{10446860,
  author={Luo, Zhizhao and Wang, Youchen and Ke, Wenjun and Qi, Rui and Guo, Yikai and Wang, Peng},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Boosting LLMS with Ontology-Aware Prompt for Ner Data Augmentation}, 
  year={2024},
  volume={},
  number={},
  pages={12361-12365},
  abstract={Named Entity Recognition (NER) data augmentation (DA) aims to improve the performance and generalization capabilities of NER models by generating scalable training data. The key challenge lies in ensuring the generated samples maintain contextual diversity while preserving label consistency. However, existing dominant methods fail to simultaneously satisfy both criteria. Inspired by the extensive generative capabilities of large language models (LLMs), we propose ANGEL, a frAmework integrating the oNtoloGy structure and instructivE prompting within LLMs. Specifically, the hierarchical ontology structure guides prompt ranking, while instructive prompting enhances LLMs’ mastery of domain knowledge, empowering synthetic sample generation and annotation. Experiments show ANGEL surpasses state-of-the-art (SOTA) baselines, conferring absolute F1 increases of 2.86% and 0.93% on two benchmark datasets, respectively.},
  keywords={Training data;Speech recognition;Ontologies;Syntactics;Signal processing;Data augmentation;Boosting;Named Entity Recognition;Data Augmentation;Large language Model;Knowledge Graph},
  doi={10.1109/ICASSP48485.2024.10446860},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{9231227,
  author={Kumar, Abhijeet and Pandey, Abhishek and Gadia, Rohit and Mishra, Mridul},
  booktitle={2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)}, 
  title={Building Knowledge Graph using Pre-trained Language Model for Learning Entity-aware Relationships}, 
  year={2020},
  volume={},
  number={},
  pages={310-315},
  abstract={Relations exhibited among entities from textual content can be a potential source of information for any business domain. This paper encompasses a wholesome approach to mine entity-relation and building knowledge graph from textual documents. The paper concentrates on two approaches to classify directional entity relations. We build on extending pretrained language model i.e. BERT for text classification along-side providing entity and directionality information as input making it entity-aware BERT classifier. We also did ablation studies of presented model in terms of various ways of providing entity information on the learning capabilities of model. We demonstrate the end to end pipeline for building an entity-relation extraction system in a business application. The techniques proposed in the paper are also evaluated against SemEval-2010 Task 8, a popular relation classification dataset. The experimental results demonstrate that learning entity-aware relations through language models outperforms almost all the previous state-of-the-art (SOTA) models.},
  keywords={Databases;Computational modeling;Conferences;Bit error rate;Text categorization;Pipelines;Buildings;knowledge graph;entity relations;relationship extraction;deep learning;BERT;language models;graph database},
  doi={10.1109/GUCON48875.2020.9231227},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10291525,
  author={Zhang, Junfeng and Zhang, Yang and Chu, Minnan and Yang, Shun and Zu, Taolei},
  booktitle={2023 IEEE 7th Information Technology and Mechatronics Engineering Conference (ITOEC)}, 
  title={A LLM-Based Simulation Scenario Aided Generation Method}, 
  year={2023},
  volume={7},
  number={},
  pages={1350-1354},
  abstract={In the simulation training system, the generation of simulation scenarios is a basic problem that needs to be studied. Firstly, expounds on the technical characteristics of LLM and knowledge graph; then structurally describe the simulation scenario related content, and build scenario knowledge graph; according to the characteristics of scenario aided generation, a simulation scenario generation method based on LLM is proposed, which uses prompt to fuse knowledge graph and LLM, next, the implementation steps of this method were elaborated; finally, the specific application proves that the method proposed in this paper is a good reference for the generation of simulation scenarios.},
  keywords={Training;Knowledge engineering;Bridges;Mechatronics;Fuses;Knowledge graphs;Information technology;knowledge graph;scenario;LLM;prompt},
  doi={10.1109/ITOEC57671.2023.10291525},
  ISSN={2693-289X},
  month={Sep.},}@INPROCEEDINGS{10490589,
  author={Rao, Qiang and Wang, Tiejun},
  booktitle={2023 7th International Conference on Electrical, Mechanical and Computer Engineering (ICEMCE)}, 
  title={Semantic Enhancement Based Knowledge Graph Completion for Graph Convolutional Neural Networks}, 
  year={2023},
  volume={},
  number={},
  pages={923-927},
  abstract={Knowledge Graph Completion (KGC) is a task that aims to predict missing links in a knowledge graph based on known triples. Recent studies have demonstrated outstanding performance in KGC employing models grounded on Graph Convolutional Networks (GCN). Nevertheless, prevailing GCN-based models solely utilize neighborhood information of entities to reason, disregarding the textual semantic information of entities and relationships in the knowledge graph. Existing GCN models suffer from poor prediction performance when dealing with tail entities due to limitations. Additionally, these models still have shortcomings in the semantic feature interaction between entities and relations. This paper proposes a Semantic-Enhanced Graph Convolutional Network (SEGCN) for knowledge graph completion. The SEGCN leverages textual descriptions of entities and relations to obtain better entity and relation embeddings using a language model. Additionally, a new Attention-Convolutions Network (ACN) has been developed to enhance the semantic interaction among entities and relations. Based on experimental findings, SEGCN outperforms the state-of-the-art GCN-based model, CompGCN, by showing 0.4%, 0.6%, 0.1 %, and 0.2% improvements in MRR, Hits@l, Hits@3, and Hits@10 on the FB15k-237 dataset, and 1.9%, 1.0%, 2.5%, and 2.9% improvements on the WN18RR dataset, respectively. These findings demonstrate that SEGCN displays improved generalization and accuracy.},
  keywords={Filtering;Computational modeling;Semantics;Noise;Neural networks;Knowledge graphs;Tail;component;knowledge graph completion;semantic information;neighborhood information;pre-trained language models;graph convolutional neural networks;attention mechanisms},
  doi={10.1109/ICEMCE60359.2023.10490589},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10193435,
  author={Belani, Hrvoje and Šolić, Petar and Perković, Toni and Pleština, Vladimir},
  booktitle={2023 8th International Conference on Smart and Sustainable Technologies (SpliTech)}, 
  title={IoT Ontology Development Process for Well-Being, Aging and Health: Challenges and Opportunities}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Ontology development processes are not trivial, given the inherently complex nature of knowledge capturing and management, as well as the need to provide structural and methodical approach on the process methodology itself in order for it to be adopted and usable. If aiming to develop an ontology for multidimensional concepts, such as well-being, aging and health, it is certain that knowledge from multiple domains have to be included, which only extends the time needed for ontology engineering. If such environments aim to be supported by the Internet of Things, than challenges rise even more. This paper provides a scoping analysis of existing well-known ontology development methodologies, with a note on the extent of their adoption and readiness to be used in a multi-domain circumstances. The approach to IoT ontology development process tailoring has been presented and elaborated, as well as the challenges specific to IoT ontology development for well-being, aging and health. Finally, research opportunities have been presented and future directions given on providing more comprehensive, more tailored and more usable ontology development methodologies.},
  keywords={Knowledge engineering;Semantics;Ontologies;Aging;Reliability;Internet of Things;Internet of Things;ontology;well-being;e-health;development process},
  doi={10.23919/SpliTech58164.2023.10193435},
  ISSN={},
  month={June},}@ARTICLE{9831788,
  author={Alam, Mirza Mohtashim and Rony, Md Rashad Al Hasan and Nayyeri, Mojtaba and Mohiuddin, Karishma and Akter, M. S. T. Mahfuja and Vahdati, Sahar and Lehmann, Jens},
  journal={IEEE Access}, 
  title={Language Model Guided Knowledge Graph Embeddings}, 
  year={2022},
  volume={10},
  number={},
  pages={76008-76020},
  abstract={Knowledge graph embedding models have become a popular approach for knowledge graph completion through predicting the plausibility of (potential) triples. This is performed by transforming the entities and relations of the knowledge graph into an embedding space. However, knowledge graphs often include further textual information stored in literal, which is ignored by such embedding models. As a consequence, the learning process stays limited to the structure and the connections between the entities, which has the potential to negatively influence the performance. We bridge this gap by leveraging the capabilities of pre-trained language models to include textual knowledge in the learning process of embedding models. This is achieved by introducing a new loss function that guides embedding models in measuring the likelihood of triples by taking such complementary knowledge into consideration. The proposed solution is a model-independent loss function that can be plugged into any knowledge graph embedding model. In this paper, Sentence-BERT and fastText are used as pre-trained language models from which the embeddings of the textual knowledge are obtained and injected into the loss function. The loss function contains a trainable slack variable that determines the degree to which the language models influence the plausibility of triples. Our experimental evaluation on six benchmarks, namely Nations, UMLS, WordNet, and three versions of CodEx confirms the advantage of using pre-trained language models for boosting the accuracy of knowledge graph embedding models. We showcase this by performing evaluations on top of the five well-known knowledge graph embedding models such as TransE, RotatE, ComplEx, DistMult, and QuatE. The results show an improvement in accuracy up to 9% on UMLS dataset for the Distmult model and 4.2% on the Nations dataset for the ComplEx model when they are guided by pre-trained language models. We additionally studied the effect of multiple factors such as the structure of the knowledge graphs and training steps and presented them as ablation studies.},
  keywords={Predictive models;Computational modeling;Task analysis;Adaptation models;Unified modeling language;Knowledge engineering;Knowledge graph;knowledge graph embeddings;language models;link prediction},
  doi={10.1109/ACCESS.2022.3191666},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9852902,
  author={Zhao, Qian and Wang, Rui and Xu, Peng and Yang, Wei},
  booktitle={2022 13th International Conference on Mechanical and Aerospace Engineering (ICMAE)}, 
  title={Construction and Application of NLP-based Knowledge Graph in CNC Equipment Fault Field}, 
  year={2022},
  volume={},
  number={},
  pages={514-519},
  abstract={Computer Numerical Control (CNC) equipment is a technology-intensive mechatronics complex system cov-ering multidisciplinary knowledge. How to effectively convert historical fault data into useful fault knowledge base is an urgent problem to be solved. A construction technology of CNC equipment fault knowledge graph based on Natural Language Processing (NLP) is proposed. BERT deep learning classification technology is used to build the sample classification model, the named entity recognition model is trained based on the Bi-LSTM technology, as well as the CNC equipment fault corpus sample data is trained, recognized, and modeled. Finally, Neo4j is used to form a knowledge graph model. It has been verified that the recognition rate of the model basically meets the requirements, and on this basis, a recommended solution for repairing equipment faults is proposed, which realizes the effective use of fault knowledge.},
  keywords={Training;Knowledge engineering;Deep learning;Mechatronics;Knowledge based systems;Bit error rate;Neural networks;knowledge graph;CNC equipment failure;NLP;BERT;Bi-LSTM},
  doi={10.1109/ICMAE56000.2022.9852902},
  ISSN={},
  month={July},}@INPROCEEDINGS{10343171,
  author={Reynolds, Sarah and Pate, William C. and Ochoa, Omar},
  booktitle={2023 IEEE Frontiers in Education Conference (FIE)}, 
  title={An Ontology and Management System for Learning Outcomes and Student Mastery}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Universities, faculty, and students use Learning Outcomes (LO) to create a shared understanding of the content provided in an individual course, known as Outcome-Based Education (OBE). One area of interest in OBE is evaluating whether the instructor and individual student performance have met the LO, which is integral to ensuring all invested parties are on the same page about class content and student performance. This work proposes a system for the management and evaluation of LO. Primarily, this work defines an ontology to support the management and evaluation of LO via Knowledge Graphs (KG). The KG links individual LO with individual assessment items. Two state-of-the-art Natural Language Processing models, BERT and ChatGPT, are evaluated in respect to their effectiveness in automating this linking. This data allows the educational professional to reflect on how well their assessments match the course's LO. The second part of this system harnesses student data to measure performance in relation to LO. In this Work-in-Progress paper, the system is prototyped and tested on the midterm results of a course in the Software Engineering curriculum. Student performance is documented in relation to each assessment question on the exams to measure student mastery of course material. Through this approach, courses can be evaluated and improved to deliver better quality education to all students. This includes improvements at the course level and possibilities for early intervention to ensure student success. This paper details the development of this system and through its implementation shows how it benefits engineering educators and their students.},
  keywords={Knowledge engineering;Taxonomy;Knowledge graphs;Ontologies;Market research;Chatbots;Software measurement;Learning outcomes;BERT;ontology;knowledge graph;assessment;Bloom's taxonomy},
  doi={10.1109/FIE58773.2023.10343171},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{10679814,
  author={Sun, Jianing and Zhang, Zhichao and He, Xueli},
  booktitle={2024 International Conference on Networking and Network Applications (NaNA)}, 
  title={LLM4EduKG: LLM for Automatic Construction of Educational Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={269-275},
  abstract={The field of education is undergoing a significant transformation towards digital and intelligent education, driven by advancements in artificial intelligence. Knowledge graphs (KGs), as a structured representation of knowledge and information, offering a powerful way to integrate diverse and multi-sourced heterogeneous data from across the Internet. The current methodologies for constructing educational knowledge graphs, however, are confronted with challenges including labor-intensive, time-consuming, and the necessity for substantial computational resources, which severely limit their practical application, especially in resource-constrained environments. In this paper, we proposed an LLM-based automatic construction method to alleviate the labor and time consumption in existing methods, and further explored LLM’s capabilities in Chinese-speaking context. Specifically, we designed a structured prompt framework to automatically extract and evaluate educational triples generated from original text. The prompt encompasses both task and model dimensions, allowing for flexible adjustments to different tasks and models, thus significantly improved the transferability of our method. Comparative experimental results from two real-world Chinese-datasets, across four advanced LLMs, demonstrate the effectiveness of the proposed method. We believe that our work represents a significant attempt by the LLM in the field of education.},
  keywords={Accuracy;Large language models;Education;Knowledge based systems;Knowledge graphs;Internet;Large Language Model;Knowledge graph;Prompt tuning;Intelligent Education},
  doi={10.1109/NaNA63151.2024.00051},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10386121,
  author={Lee, Hyun Jung and Sohn, Mye},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Context-based Fact-checking Using Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={6051-6056},
  abstract={There are many attentions for the fact-checking to prevent the hallucination, malfunction of circulating content on the web such as deception, counterfeit, fake news regardless of whether they are intended or not. For fact-checking, ConFcheKG (Context-based Fact-checking using Knowledge Graph) is proposed based on the Knowledge Graph (KG). In the content including multiple entities, the KG is adopted to check coherence between the entities to determine whether there are semantic conflicts among them. The coherence is based on the temporal, spatial, and logical arrangement of the contents based on the associated relationships among the entities using KG. To do this, it checks the existence of intersected KGs and conflicted KGs through mutual comparison of KGs step by step. According to the verifying process, if is the constructed KT with coherent entities of the content, then the reliability of the content’s coherence is high because it has a high probability of not being false, hallucination, fake-news, and so on. Otherwise, the probability of being false increases. To check the fact, ConFcheKG can be applied based on semantic coherence of the content, to reduce the hallucination, to determine the fake news, to detect deceptions, to prevent counterfeits, to generate the well-composed prompt for the generative AI, and so on.},
  keywords={Generative AI;Semantics;Coherence;Knowledge graphs;Big Data;Reliability;Fake news;fact-checking;knowledge graph;coherence;reliability;context},
  doi={10.1109/BigData59044.2023.10386121},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8241212,
  author={Abrahão, Elcio and Hirakawa, André Riyuiti},
  booktitle={2017 Second International Conference on Information Systems Engineering (ICISE)}, 
  title={Task Ontology Modeling for Technical Knowledge Representation in Agriculture Field Operations Domain}, 
  year={2017},
  volume={},
  number={},
  pages={12-16},
  abstract={Nowadays, ontologies have been used to describe knowledge in a wide quantity of domains. Knowledge discovery, reuse and share benefits are a common sense among information systems developers and there are many works related to ontology engineering, conceptual modeling, metadata and semantic representation in the computational systems area. Although there are many ontologies to describe the vocabulary related to generic domains such as medicine, law, engineering and biology, there are no specific ontologies to describe generic task vocabularies like agriculture field operations. This paper describes a model to represent the technical knowledge for the agriculture field operations as a task ontology that can be used with a domain ontology to build an application ontology for agriculture domain purposes. The field operations were analyzed to determine who are the task agents, agent roles, the input resources, task and sub-task decomposition, control-flow, task concepts, attributes and relations. As a result, a formal representation of the agriculture field operations is presented and a conceptual model was built using UML class and activity diagrams. The model consistency was verified in a case study of the sugar cane harvest operation. The model presented describes the domain operations knowledge and it could be the base for the development of computational application systems as field operations control, decision support and precision agriculture.},
  keywords={Unified modeling language;Ontologies;Agriculture;Computational modeling;Sugar industry;Mathematical model;task ontology;ontology engineering;agriculture;UML;modeling},
  doi={10.1109/ICISE.2017.18},
  ISSN={2160-1291},
  month={April},}@INPROCEEDINGS{9061292,
  author={Yang, Hao and Qin, Ying and Deng, Yao and Wang, Minghan},
  booktitle={2020 22nd International Conference on Advanced Communication Technology (ICACT)}, 
  title={NMT Enhancement based on Knowledge Graph Mining with Pre-trained Language Model}, 
  year={2020},
  volume={},
  number={},
  pages={185-189},
  abstract={Pre-trained language models like Bert, RoBERTa, GPT, etc. have achieved SOTA effects on multiple NLP tasks (e.g. sentiment classification, information extraction, event extraction, etc.). We propose a simple method based on knowledge graph to improve the quality of machine translation. First, we propose a multi-task learning model that learns subjects, objects, and predicates at the same time. Second, we treat different predicates as different fields, and improve the recognition ability of NMT models in different fields through classification labels. Finally, beam search combined with L2R, R2L rearranges results through entities. Based on the CWMT2018 experimental data, using the predicate's domain classification identifier, the BLUE score increased from 33.58% to 37.63%, and through L2R, R2L rearrangement, the BLEU score increased to 39.25%, overall improvement is more than 5 percentage.},
  keywords={Training;Optimization;Biological system modeling;Task analysis;Information retrieval;Mathematical model;Data models;NMT;Pre-trained Language Model;Knowledge Graph},
  doi={10.23919/ICACT48636.2020.9061292},
  ISSN={1738-9445},
  month={Feb},}@ARTICLE{10013735,
  author={Benarab, Achref and Sun, Jianguo and Rafique, Fahad and Refoufi, Allaoua},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Global Ontology Entities Embeddings}, 
  year={2023},
  volume={35},
  number={11},
  pages={11449-11460},
  abstract={Ontologies are among the most widely used types of knowledge representation formalisms. The application of deep learning techniques in the field of ontology engineering has reinforced the need to learn and generate representations of ontological data. This allows ontologies to be exploited by such models, and thus automate various ontology engineering tasks, where most of the existing tools and machine learning approaches require a numerical feature vectors associated with each concept. This paper outlines a novel approach for learning global ontology entities embeddings by exploiting the structure and the various taxonomic and semantic relationships present in ontologies, taking into account all the information present in the ontological graph and carried by the OWL/RDF triples. Thus, producing global ontology entities embeddings capturing the global ontological graph semantics and similarities enclosed in the source ontology. Three different neural network models have been proposed based on two architectures: multi-input and multi-output, trained using the contrastive estimation technique. The evaluation on OWL/RDF ontologies and word semantic similarity tasks using various graph and WordNet based similarity measures, show that our approach yields competitive results outperforming the state-of-the-art ontology and word embedding models.},
  keywords={Ontologies;Semantics;Task analysis;Adaptation models;Predictive models;Neural networks;Deep learning;Concept embeddings;feature representation;neural networks;ontology embeddings;ontology entities vector representations},
  doi={10.1109/TKDE.2023.3235779},
  ISSN={1558-2191},
  month={Nov},}@INPROCEEDINGS{9574558,
  author={Zhao, Tong and Wu, Ke-He},
  booktitle={2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)}, 
  title={Construction of power marketing user knowledge graph based on $\text{BERT}+\text{BILSTM}+\text{CRF}$ model}, 
  year={2021},
  volume={},
  number={},
  pages={396-399},
  abstract={In order to solve the fierce competition in the power market caused by the continuous improvement of the power consumption level of users, to reasonably deal with the contradiction between power supply and demand and to provide effective marketing strategies, the author proposes to construct the user knowledge graph based on the power marketing data. After preliminary data preprocessing, BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding) model is selected for entity recognition. Bi-lstm (Bidirectional Long short-term Memory) model and CRF (Conditional Random Field) model are combined to extract the relationship, build triples and establish knowledge graph. The graph can clearly show the relationship between users and power load, address, price package, etc., which is conducive to relevant enterprises to put forward corresponding marketing or emergency measures, and provide theoretical support for optimization schemes such as “power demand side management” and “peak load shifting and valley filling” under the new situation.},
  keywords={Analytical models;Power demand;Power measurement;Bit error rate;Tools;Transformers;Data models;Knowledge graph;Electric power marketing;Electrical behavior;BERT;Bi - LSTM;CRF},
  doi={10.1109/CEI52496.2021.9574558},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10585481,
  author={Gao, Yue and Luo, Xin and Tao, Ran and Feng, Xiangyang},
  booktitle={2024 4th International Conference on Computer, Control and Robotics (ICCCR)}, 
  title={Knowledge Graph Completion Based on Neighborhood-Aware Double-Layer Transformer}, 
  year={2024},
  volume={},
  number={},
  pages={390-394},
  abstract={In knowledge graph completion, most models embed entities and relations into low-dimensional vectors and use them as inputs to learn their latent interaction features. However, these models primarily focus on static embeddings for individual triplets, neglecting the rich contextual features related to entities. This paper introduces a knowledge graph embedding model based on Neighborhood-Aware Double-Layer Transformer (NADTKE). The model consists of two layers: the bottom layer is used to learn the interaction features between the source entity and its neighborhood with respect to relations, while the top layer is responsible for aggregating contextual information from the outputs of the bottom layer. This dual-layer design effectively balances feature information from both the source entity and its neighboring entities. Experimental evaluations on the FB15k-237 and WN18RR datasets demonstrate that the proposed model achieves an MRR of 0.37 and a Hit@1 score of 0.281 on the FB15k-237 dataset, providing evidence of its effectiveness.},
  keywords={Digital control;Computational modeling;Knowledge graphs;Transformers;Vectors;Robots;Context modeling;knowledge graph completion;contextual features;Neighborhood aware;Double Layer Transformer},
  doi={10.1109/ICCCR61138.2024.10585481},
  ISSN={},
  month={April},}@INPROCEEDINGS{10624590,
  author={Li, Haiping and Duan, Wenjing},
  booktitle={2024 International Conference on Informatics Education and Computer Technology Applications (IECA)}, 
  title={Construction and Application Research of Intelligent Education Knowledge Graph Based on Multi-modal Learning}, 
  year={2024},
  volume={},
  number={},
  pages={117-121},
  abstract={This study explores the methods and applications of constructing an intelligent education knowledge graph based on multi-modal learning materials (text, images, videos) Through deep learning algorithms for feature extraction and relationship modeling, a knowledge graph containing 120 entities and 100 relationships was built with an accuracy of $87 \%$. The graph demonstrates excellent performance in personalized learning path recommendations, capable of recommending based on students’ prior knowledge and learning styles, leading to a $27 \%$ improvement in learning efficiency. The study also considers the “uncanny valley” phenomenon, avoiding discomfort caused by AI mimicking human behavior excessively, ensuring a positive user experience.},
  keywords={Deep learning;Education;Knowledge graphs;Feature extraction;User experience;Artificial intelligence;Informatics;Knowledge graph;Image analysis;Intelligent education;Uncanny valley},
  doi={10.1109/IECA62822.2024.00029},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{9732087,
  author={Wang, Yan and Allouache, Yacine and Joubert, Christian},
  booktitle={2021 Eighth International Conference on Social Network Analysis, Management and Security (SNAMS)}, 
  title={A Staffing Recommender System based on Domain-Specific Knowledge Graph}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={In the economics environment, Job Matching is always a challenge involving the evolution of knowledge and skills. A good matching of skills and jobs can stimulate the growth of economics. Recommender System (RecSys), as one kind of Job Matching, can help the candidates predict the future job relevant to their preferences. However, RecSys still has the problem of cold start and data sparsity. The content-based filtering in RecSys needs the adaptive data for the specific staffing tasks of Bidirectional Encoder Representations from Transformers (BERT). In this paper, we propose a job RecSys based on skills and locations using a domain-specific Knowledge Graph (KG). This system has three parts: a pipeline of Named Entity Recognition (NER) and Relation Extraction (RE) using BERT; a standardization system for pre-processing, semantic enrichment and semantic similarity measurement; a domain-specific Knowledge Graph (KG). Two different relations in the KG are computed by cosine similarity and Term Frequency-Inverse Document Frequency (TF-IDF) respectively. The raw data used in the staffing RecSys include 3000 descriptions of job offers from Indeed, 126 Curriculum Vitae (CV) in English from Kaggle and 106 CV in French from Linx of Capgemini Engineering. The staffing RecSys is integrated under an architecture of Microservices. The autonomy and effectiveness of the staffing RecSys are verified through the experiment using Discounted Cumulative Gain (DCG). Finally, we propose several potential research directions for this research.},
  keywords={Economics;Social networking (online);Semantics;Bit error rate;Microservice architectures;Computer architecture;Standardization;Job Matching;Recommender System;Knowledge Graph;TF-IDF;BERT;NER;Cosine Similarity;K-hop;Discounted Cumulative Gain;Microservices},
  doi={10.1109/SNAMS53716.2021.9732087},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10800696,
  author={Sun, Jiawei and Zhang, Quan and Zhi, Yongqi and Ling, Weiqing},
  booktitle={2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)}, 
  title={Knowledge Graph Indexing Enhanced Q&A System Based on Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={548-553},
  abstract={With the advent of the digital age, the explosive growth of data has brought unprecedented opportunities and challenges to societal development. The rapid evolution of data science is driving innovation and progress across various industries. Both the business and academic communities recognize the immense potential hidden within vast amounts of data. Extracting truly valuable information from this data has become a critical issue that needs to be addressed. As technology advances swiftly and data accumulates rapidly, traditional processing and analysis methods can no longer keep up with the current volume and velocity of data. This has prompted data professionals to seek innovative solutions to handle increasingly complex and massive data relationships and quantities. Against this backdrop, two key technologies have emerged due to their exceptional performance-knowledge graphs and large language models},
  keywords={Knowledge engineering;Training;Technological innovation;Large language models;Atmospheric modeling;Computational modeling;Knowledge graphs;Aerospace electronics;Information age;Question answering (information retrieval);Knowledge graph;large language model;knowledge question answering;lapping process knowledge},
  doi={10.1109/EIECS63941.2024.10800696},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9534355,
  author={Wang, Bin and Wang, Guangtao and Huang, Jing and You, Jiaxuan and Leskovec, Jure and Kuo, C.-C. Jay},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Inductive Learning on Commonsense Knowledge Graph Completion}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Commonsense knowledge graph (CKG) is a special type of knowledge graph (KG), where entities are composed of free-form text. Existing CKG completion methods focus on transductive learning setting, where all the entities are present during training. Here, we propose the first inductive learning setting for CKG completion, where unseen entities may appear at test time. We emphasize that the inductive learning setting is crucial for CKGs, because unseen entities are frequently introduced due to the fact that CKGs are dynamic and highly sparse. We propose InductivE as the first framework targeted at the inductive CKG completion task. InductivE first ensures the inductive learning capability by directly computing entity embeddings from raw entity attributes. Second, a graph neural network with novel densification process is proposed to further enhance unseen entity representation with neighboring structural information. Experimental results show that InductivE performs especially well on inductive scenarios where it achieves above 48% improvement over previous methods while also outperforms state-of-the-art baselines in transductive settings.},
  keywords={Training;Transfer learning;Benchmark testing;Graph neural networks;Task analysis;Research and development;Commonsense Knowledge Graph;Inductive Learning;Graph Learning;Knowledge Graph Completion},
  doi={10.1109/IJCNN52387.2021.9534355},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{10743881,
  author={Zhang, Kaiwen and Su, Feiyu and Huang, Yixiang and Li, Yanming and Wu, Fengqi and Mao, Yuhan},
  booktitle={2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)}, 
  title={The Application of Fine-Tuning on Pretrained Language Model in Information Extraction for Fault Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={469-473},
  abstract={Constructing fault knowledge graphs holds significant importance for achieving intelligent maintenance and diagnosis in high-end equipment manufacturing. Effective information extraction and knowledge graph construction have proven challenging due to the lack of standardized representation of semantically complex unstructured text in the industrial domain. Therefore, in this study, we performed fine-tuning on the pre-trained language model (ChatGLM2-6B) with specific prompts to achieve information extraction from fault-related texts, ultimately leading to the construction of a fault knowledge graph. Experimental results demonstrate that the proposed method not only supports fine-tuning with limited data but also exhibits enhanced capability in understanding complex semantics related to fault symptoms and causes.},
  keywords={Computational modeling;Semantics;Knowledge graphs;Signal processing;Ontologies;Information retrieval;Stability analysis;Data models;Manufacturing;Maintenance;Pretrained language model;Parameter-efficient fine-tuning;Information extraction;Fault knowledge graph},
  doi={10.1109/ICSP62122.2024.10743881},
  ISSN={},
  month={April},}@INPROCEEDINGS{10674566,
  author={Li, Chen and Zheng, Haotian and Sun, Yiping and Wang, Cangqing and Yu, Liqiang and Chang, Che and Tian, Xinyu and Liu, Bo},
  booktitle={2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)}, 
  title={Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={In the realm of computational knowledge representation, Knowledge Graph Reasoning (KG-R) stands at the fore-front of facilitating sophisticated inferential capabilities across multifarious domains. The quintessence of this research elucidates the employment of reinforcement learning (RL) strategies, notably the REINFORCE algorithm, to navigate the intricacies inherent in multi-hop KG-R. This investigation critically addresses the prevalent challenges introduced by the inherent incompleteness of Knowledge Graphs (KGs), which frequently results in erroneous inferential outcomes, manifesting as both false negatives and misleading positives. By partitioning the Unified Medical Language System (UMLS) benchmark dataset into rich and sparse subsets, we investigate the efficacy of pretrained BERT embeddings and Prompt Learning methodologies to refine the reward shaping process. This approach not only enhances the precision of multi-hop KG-R but also sets a new precedent for future research in the field, aiming to improve the robustness and accuracy of knowledge inference within complex KG frameworks. Our work contributes a novel perspective to the discourse on KG reasoning, offering a methodological advancement that aligns with the academic rigor and scholarly aspirations of the Natural journal, promising to invigorate further advancements in the realm of computational knowledge representation.},
  keywords={Training;Navigation;Unified modeling language;Transfer learning;Knowledge graphs;Reinforcement learning;Cognition;Knowledge Graph Reasoning;Reinforcement Learning;Reward Shaping;Transfer Learning},
  doi={10.1109/MLISE62164.2024.10674566},
  ISSN={},
  month={June},}@INPROCEEDINGS{10719142,
  author={Yang, Zeyu and Duan, Yucong and Xue, Jun and Qi, Qi},
  booktitle={2024 IEEE 9th International Conference on Computational Intelligence and Applications (ICCIA)}, 
  title={An Intra-Network Multi-Teacher Distillation Method Towards Lightweight Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={109-114},
  abstract={Recently, Knowledge Graph Completion (KGC) based on Pre-trained Language Models (PLM) has made significant advancements. However, PLM typically have a large number of parameters, which makes lightweight research for low-resource challenging. For KGC, knowledge distillation can be an portable method. But traditional knowledge distillation is difficult to achieve efficient knowledge transfer. To solve this issue, this paper proposes an intra-network multi-teacher knowledge distillation, which can effectively reduce knowledge leakage through multi-level information transmission. Specifically, we divide the teacher model into multiple sub-teachers based on network depth, the sub-teachers deliver different knowledge representations. In addition, we use the loss variation of each sub-teacher as a confidence level, which can dynamically regulate the intensity of multi-teacher distillation and enable the student model to perceive distilled knowledge at a finer granularity. A series of experimental results show that our proposed method achieves state-of-the-art performance with the low number of parameters.},
  keywords={Computational modeling;Knowledge based systems;Knowledge graphs;Information processing;Knowledge transfer;Computational intelligence;multi-teacher;knowledge graph completion;knowledge distillation},
  doi={10.1109/ICCIA62557.2024.10719142},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10385745,
  author={Fu, Chengcheng and Yao, Yanan and Wu, Jieyu and Zhao, Weizhong and He, Tingting and Jiang, Xingpeng},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Multimodal reasoning for nutrition and human health via knowledge graph embedding}, 
  year={2023},
  volume={},
  number={},
  pages={1901-1904},
  abstract={The established links between nutrition and human health are widely acknowledged. Dietary nutrients play a crucial role in regulating gut microbial communities, influencing various human diseases. With a growing number of related studies, there’s a need to systematically organize these associations for coherent knowledge reasoning. However, due to the diverse and extensive nature of the knowledge landscape, significant challenges persist. To address this, we propose an approach using multimodal data and knowledge embeddings for effective knowledge reasoning in nutrition and human health. We create a comprehensive knowledge graph, KG4NH, covering dietary nutrition, gut microbiota, and human diseases. To ensure efficient knowledge representation, we employ knowledge embedding techniques to develop modality-specific encoders for structure, category, and description. Additionally, we introduce a mul-timodal fusion method to capture shared information across modalities. Our experimental results demonstrate the superiority of our approach over other state-of-the-art methods.},
  keywords={Knowledge graphs;Ontologies;Feature extraction;Cognition;Bioinformatics;Diseases;Knowledge graph;Multimodal embedding;Knowledge reasoning;Nutrition;Human health},
  doi={10.1109/BIBM58861.2023.10385745},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{9709043,
  author={Wang, Fang and Xie, Yongqiang and Zhang, Kai and Xia, Rui and Zhang, Yunchao},
  booktitle={2021 2nd International Conference on Big Data Economy and Information Management (BDEIM)}, 
  title={Bert-based Knowledge Graph Completion Algorithm for Few-Shot}, 
  year={2021},
  volume={},
  number={},
  pages={217-224},
  abstract={Knowledge graphs have shown increasing value in semantic search, intelligent Q&A, data analysis, natural language processing, visual understanding, IoT devices, etc. It is undeniable that knowledge graphs have become the mandatory path for the artificial intelligence fields development. However, the information contained in existing knowledge graphs is incomplete, which attracts a large number of researchers to enhance the completeness of knowledge graphs utilizing knowledge graph completion methods. Most traditional embedding-based knowledge graph completion models use structural information within data-rich triples from the knowledge graph is limited by the long-tail distribution of the relations in the triples. To address this problem, we propose a Bert-based knowledge graph completion algorithm for few-shot knowledge graphs, with the main goal of implementing the knowledge graph completion task with only a few sample triples of training instances. We improve the baseline model GMatching for handling few-shot knowledge graphs by introducing the Bert pre-trained linguistic representation model to enhance the semantic representation of entities and relations in the triples. Through experiments, we demonstrate that our improved model B-GMatching achieves good results.},
  keywords={Training;Visualization;Semantic search;Biological system modeling;Knowledge based systems;Training data;Linguistics;Knowledge Graph;Knowledge Graph Completion;Bert;few-shot},
  doi={10.1109/BDEIM55082.2021.00051},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10635576,
  author={Wei, Xiaoyang and Vagena, Zografoula and Kurtz, Camille and Cloppet, Florence},
  booktitle={2024 IEEE International Symposium on Biomedical Imaging (ISBI)}, 
  title={Integrating Expert Knowledge with Vision-Language Model for Medical Image Retrieval}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Content-Based Image Retrieval (CBIR) is an image search technique that can offer diagnostic guidance when facing difficult cases in radiology. State-of-the-art approaches propose to extract image features using vision-language models which learn image representations from supervision of text in medical literature. However, existing methods seldom take expert knowledge in medical domain into account. In this article, we propose a knowledge-and-language-guided contrastive visual representation learning framework for image retrieval. Our method consists of two steps: (1) modeling relationships between medical concepts and medical images using a knowledge graph, and translating each node in the graph into a knowledge embedding; (2) injecting knowledge embeddings into a vision-language model by aligning image representations using both encoded textual input and knowledge embeddings. Our experiments show that the proposed framework achieves comparable results to state-of-the-art methods on CBIR tasks using much less training data. Our code is publicly available at https://github.com/Wxy-24/KL-CVR.},
  keywords={Representation learning;Visualization;Biological system modeling;Image retrieval;Training data;Image representation;Radiology;Image retrieval;Representation learning;Vision-language model;Knowledge graph},
  doi={10.1109/ISBI56570.2024.10635576},
  ISSN={1945-8452},
  month={May},}@INPROCEEDINGS{10711054,
  author={Knollmeyer, Simon and Akmal, Muhammad Uzair and Koval, Leonid and Asif, Saara and Mathias, Selvine G. and Groβmann, Daniel},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Document Knowledge Graph to Enhance Question Answering with Retrieval Augmented Generation}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={Reusing and managing existing knowledge from available documents is crucial for success in the factory planning domain. By leveraging Artificial Intelligence (AI) and Question Answering (QA) systems, users can query a document corpus through a chat-based application and receive precise answers. The recent advancements in Large Language Models (LLMs) and their linguistic capabilities present new opportunities for such applications. Utilizing the methodology of Retrieval Augmented Generation (RAG), document sections are provided to the LLM based on user queries. However, existing RAG implementations that use vector databases as document repositories face limitations when answering questions that extend beyond the text content of the documents. To address this issue, this paper proposes a concept to enhance RAG systems by integrating a Knowledge Graph (KG) constructed from the document structures.},
  keywords={Databases;Large language models;Knowledge graphs;Linguistics;Question answering (information retrieval);Vectors;Production facilities;Planning;Manufacturing automation;Faces;Information management;Retrieval Augmented Generation;Knowledge Graph;Large Language Models},
  doi={10.1109/ETFA61755.2024.10711054},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10298399,
  author={Huang, Qing and Wan, Zhenyu and Xing, Zhenchang and Wang, Changjing and Chen, Jieshan and Xu, Xiwei and Lu, Qinghua},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={Let's Chat to Find the APIs: Connecting Human, LLM and Knowledge Graph through AI Chain}, 
  year={2023},
  volume={},
  number={},
  pages={471-483},
  abstract={API recommendation methods have evolved from literal and semantic keyword matching to query expansion and query clarification. The latest query clarification method is knowledge graph (KG)-based, but limitations include out-of-vocabulary (OOV) failures and rigid question templates. To address these limitations, we propose a novel knowledge-guided query clarification approach for API recommendation that leverages a large language model (LLM) guided by KG. We utilize the LLM as a neural knowledge base to overcome OOV failures, generating fluent and appropriate clarification questions and options. We also leverage the structured API knowledge and entity relationships stored in the KG to filter out noise, and transfer the optimal clarification path from KG to the LLM, increasing the efficiency of the clarification process. Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation. We verify the usefulness of each unit in our AI chain, which all received high scores close to a perfect 5. When compared to the baselines, our approach shows a significant improvement in MRR, with a maximum increase of 63.9% higher when the query statement is covered in KG and 37.2% when it is not. Ablation experiments reveal that the guidance of knowledge in the KG and the knowledge-guided pathfinding strategy are crucial for our approach's performance, resulting in a 19.0% and 22.2% increase in MAP, respectively. Our approach demonstrates a way to bridge the gap between KG and LLM, effectively compensating for the strengths and weaknesses of both.},
  keywords={Semantics;Knowledge based systems;Knowledge graphs;Oral communication;Data models;Software engineering;API recommendation;query clarification;knowledge graph;large language model;out-of-vocabulary},
  doi={10.1109/ASE56229.2023.00075},
  ISSN={2643-1572},
  month={Sep.},}@INPROCEEDINGS{10721152,
  author={Tian, Xiaoyun and Liu, Bin and Yang, Jianzhong and Ai, Bo and Zhao, Huailiang and Zhang, Shaoyang},
  booktitle={2024 4th Power System and Green Energy Conference (PSGEC)}, 
  title={Knowledge Graph Enhanced Interactive Fault Diagnosis O&M Approach Based on Large Models of Electrical Power}, 
  year={2024},
  volume={},
  number={},
  pages={137-142},
  abstract={Power equipment operation and maintenance has accumulated a large amount of knowledge recorded in the form of text, which provides the basis for power equipment diagnosis and operation and maintenance. However, the huge amount of redundant knowledge makes it difficult for the relevant personnel to quickly access the information, and it is also impossible to integrate it into the existing diagnostic algorithms. For this reason, this paper proposes an interactive equipment diagnosis and operation and maintenance method based on knowledge graph and big model. First, the unstructured knowledge is transformed and constitutes the O&M (Operation and Maintenance) knowledge graph using entity extraction method; second, the conditional probability of the equipment feature quantity to the failure mode is fitted using graph convolution to realize the equipment failure diagnosis; subsequently, an intention recognition algorithm based on self-attention network is proposed and retrieval is realized; finally, the results are generated into natural language using the big language model. It is verified that the proposed diagnosis and intent recognition F1 values are improved by 9.77% and 17.48%, respectively, confirming the effectiveness of the system.},
  keywords={Fault diagnosis;Power measurement;Intent recognition;Natural languages;Knowledge graphs;Feature extraction;Maintenance;Power systems;Power transformers;Personnel;knowledge graph;interactive system;large language model;fault diagnosis},
  doi={10.1109/PSGEC62376.2024.10721152},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10403258,
  author={Wu, Ling-I and Li, Guoqiang},
  booktitle={2023 IEEE International Conference on Medical Artificial Intelligence (MedAI)}, 
  title={Zero-Shot Construction of Chinese Medical Knowledge Graph with ChatGPT}, 
  year={2023},
  volume={},
  number={},
  pages={278-283},
  abstract={Knowledge graphs have revolutionized the organization and retrieval of real-world knowledge, prompting inter-est in automatic NLP-based approaches for extracting medical knowledge from texts. However, the availability of high-quality Chinese medical knowledge remains limited, posing challenges for constructing Chinese medical knowledge graphs. As LLMs like ChatGPT show promise in zero-shot learning for many NLP downstream tasks, their potential on constructing Chinese medical knowledge graphs is still uncertain. In this study, we create a Chinese medical knowledge graph by manually annotating textual data and using ChatGPT to automatically generate the graph. We refine the results using filtering and mapping rules to align with our schema. The manually generated graph serves as the ground truth for evaluation, and we explore different methods to enhance its accuracy through knowledge graph completion techniques. As a result, we emphasize the potential of employing ChatGPT for automated knowledge graph construction within the Chinese medical domain. While ChatGPT successfully identifies a larger number of entities, further en-hancements are required to improve its performance in extracting more qualified relations.},
  keywords={Zero-shot learning;Filtering;Knowledge graphs;Organizations;Chatbots;Task analysis;Artificial intelligence;knowledge graph;ChatGPT;nature language processing;named entity recognition;relation extraction},
  doi={10.1109/MedAI59581.2023.00043},
  ISSN={},
  month={Nov},}@ARTICLE{10463190,
  author={LIU, Peifeng and Qian, Lu and Zhao, Xingwei and Tao, Bo},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}, 
  year={2024},
  volume={20},
  number={6},
  pages={8160-8169},
  abstract={In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.},
  keywords={Task analysis;Fault diagnosis;Training;Hidden Markov models;Cognition;Knowledge engineering;Knowledge graphs;Data-driven;fault localization;intelligent fault diagnosis;knowledge graph (KG);large language model (LLM)},
  doi={10.1109/TII.2024.3366977},
  ISSN={1941-0050},
  month={June},}@INPROCEEDINGS{9845845,
  author={Yiming, Liu and Li, Duan},
  booktitle={2022 7th International Conference on Computer and Communication Systems (ICCCS)}, 
  title={Research on the Construction of Maritime Legal Knowledge Graph}, 
  year={2022},
  volume={},
  number={},
  pages={903-908},
  abstract={As the marine industry booms, the maritime legal documents are of great importance to the maneuver on the sea. However, the traditional way of consulting the text can not meet the demand of maritime operation nowadays. This paper aims to explore a way to extract and strengthen data from maritime legal texts to better support legal question answering. To mine knowledge from unstructured maritime laws and regulations, this paper proposes a method to build the maritime legal knowledge graph. To extract information from unstructured texts, BERT+BiLSTM+CRF is used for named entity recognition. DeepKE toolkit is used for relation extraction. And to strengthen the logics between entities, heterogeneous nodes are introduced to enhance the semantic associations in the maritime legal knowledge graph. The document-enhanced knowledge graph expanded in scale, so it can better support subsequent intelligent applications.},
  keywords={Law;Text recognition;Semantics;Pipelines;Ontologies;Information retrieval;Regulation;knowledge graph;maritime law;named entity recognition;heterogeneous entities},
  doi={10.1109/ICCCS55155.2022.9845845},
  ISSN={},
  month={April},}@ARTICLE{10489926,
  author={Liu, Xingyu and Wang, Zhenxing and Sun, Yue and Han, Junmei and Xiao, Gang and Jiang, Jianchun},
  journal={IEEE Access}, 
  title={ISA-KGC: Integrated Semantics-Structure Analysis in Knowledge Graph Completion}, 
  year={2024},
  volume={12},
  number={},
  pages={57250-57260},
  abstract={This paper presents Integrated Semantics-Structure Analysis in Knowledge Graph Completion (ISA-KGC), a new framework for Knowledge Graph Completion (KGC) aimed at addressing the incompleteness of knowledge graphs (KGs). ISA-KGC integrates Graph Neural Networks (GNN) with Transformer-based models, effectively blending structural and semantic information within Knowledge Graphs. This fusion enhances comprehension of KGs beyond what traditional methods offer. The framework utilizes Knowledge Graph Embedding (KGE) models, with GNN employed to augment these models, thus enhancing the overall analysis and interpretation of Knowledge Graphs. The effectiveness of ISA-KGC is validated through benchmark datasets FB15K-237 and WN18RR, showing notable improvements in performance metrics like hit@10 compared to existing methods.},
  keywords={Knowledge graphs;Semantics;Graph neural networks;Vectors;Adaptation models;Task analysis;Metalearning;Graph neural network;knowledge graph completion;knowledge graph embedding;pre-trained language model},
  doi={10.1109/ACCESS.2024.3384533},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10858688,
  author={Si, Jiaqi and Ouyang, Xiaoye and Zhu, Xiaoling and Zhang, Yi},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={A Relation Semantic Enhancement Method for Large Language Model Based Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={209-215},
  abstract={The task of Knowledge Graph Completion (KGC) is vital for leveraging the full potential of Knowledge Graphs (KGs), which encode rich semantic information about entities and their relationships. Recently, LLM-based KGC methods have achieved significant advancements, which usually leverage pre-trained Large Language Models (LLMs) to encode entity’s (or relation’s) descriptions. While promising, current LLM-based KGC methods face limitations due to the quality of available text data and the incomplete nature of KGs, especially the commonly missing relation’s descriptions. To address this kind of challenges, this paper proposes a general framework, which harnesses the power of LLMs, as well as high-quality pre-defined concept semantics from extra Knowledge Bases (KB), to enhance the input data for current LLM-based KGC models. Especially, our framework consists of two main components for better understanding relation’s semantics, from diverse levels (i.e., KG-level and triple-level), offering a more nuanced perspective on relation understanding. Actually, the proposed work could be used as an easy-to-accomplish plug-in unit, for improving many kinds of current native LLM-based KGC models. By leveraging LLMs and KBs from various levels, our framework could improve the understanding of relation meanings. These enhancements collectively contribute to the improved performance of LLM-based KGC models, particularly in scenarios where the KG is sparse or incomplete. We conduct extensive experiments on several LLM-based KGC models and real-world datasets to validate the efficacy of our approach. The empirical results indicate that our framework markedly enhances the efficacy of LLM-based KGC models across both entity prediction and triple classification tasks.},
  keywords={Large language models;Semantics;Knowledge based systems;Cyberspace;Knowledge graphs;Predictive models;Data science;Data models;Faces;Knowledge Graph;Knowledge Graph Completion;Representation Learning;Large Language Model},
  doi={10.1109/DSC63484.2024.00035},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10698632,
  author={Yadav, Divyanshi and Para, Hitesh and Sandhu, Komal and Selvakumar, Prakash},
  booktitle={2024 International Conference on Electrical, Computer and Energy Technologies (ICECET}, 
  title={Enhancing Response Generation Systems: Knowledge Graph & Generative AI Synergy for Business Communication and Strategic Decision Making}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={In the contemporary business environment, it is crucial to have an efficient and precise response generation system to build client trust, optimize operations, and provide customized solutions. Current response management systems often face challenges such as insufficient depth, scalability issues, and inconsistencies. There is an urgent requirement for a comprehensive system that can retrieve information for generating high-quality insights and convert it to a reply. To address these needs, an approach has been explored that integrates a domain-oriented Knowledge Graph (KG), vector embeddings, Large Language Model (LLM) and utilize the pathway parsing technique that allows for in-depth multi-hop analysis within the KG, resulting in detailed and contextually rich data retrieval. This combination enhances the performance and precision of handling inquiries, streamlines entity extraction and step identification, leverages KG for Standard Operating Procedure (SOP) guidance, and offers superior recommendations or strategies for informed decision making. The concept will be illustrated through a business study focusing on the collections department use case, which involves customer correspondence. This approach ensures a more efficient, and accurate responses, leading to reduced human intervention and latency, along with that customer satisfaction is improved and business processes are streamlined. By adopting this method, businesses can enhance their communication, make data-driven decisions, and ultimately achieve better results in the competitive market. The efficacy is evident in the practical instances, owing to its profound grasp of context.},
  keywords={Generative AI;Scalability;Large language models;Decision making;Focusing;Knowledge graphs;Vectors;Standards;Faces;Business;Information Retrieval;Email Handling;Response Generation;Decision Making;Pathways parsing;Retrieval Augmented Generation;Knowledge Graphs;Word Embeddings;Natural Language Processing;Large Language Model;Natural Language Understanding},
  doi={10.1109/ICECET61485.2024.10698632},
  ISSN={},
  month={July},}@INPROCEEDINGS{10603818,
  author={Lan, Richeng and Fan, Guangwei and Tian, Maochun and Yang, Yue and Wang, Gaodan and Wang, Qingzheng and Wang, Jingteng},
  booktitle={2024 5th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={Research on the framework of low-cost wide-domain Question-Answering system based on knowledge graph}, 
  year={2024},
  volume={},
  number={},
  pages={419-424},
  abstract={Knowledge graph-based question-answering (KBQA) systems suffer from problems such as low-quality datasets and limited categories of candidate entities, which lead to difficulties in system construction and limited applications. To address this problem, a low-cost wide-domain KBQA system construction framework called LCWD-QA is proposed by combining deep learning, knowledge graphs, and large language models. First, an entity span prediction model was designed to recognize potential entity mentions in sentences. Then, the concept of entity popularity is introduced, and an entity-linking algorithm is designed to link entity mentions to specific entities in the knowledge graph. Finally, the Bert style of text classification models was used for intent recognition, and then different methods were used to generate the answer to that question. In addition, large language models were used to enhance the experimental dataset and generate supplementary answers to the knowledge graph to improve the generalization of the system. The experimental results show that the LCWD-QA system presented in this paper exhibits good performance on both entity span prediction and intent recognition subtasks, with an accuracy of 99.5%, which is better than that of the prevalent benchmark models. The system avoids the high dependence of traditional named entity recognition schemes on manually labeled datasets and has high accuracy and interpretability, which has high application and reference value.},
  keywords={Deep learning;Accuracy;Intent recognition;Large language models;Computational modeling;Text categorization;Knowledge graphs;KBQA;Knowledge Graph;Entity Span Prediction;Large Language Model;Entity linking},
  doi={10.1109/ICCEA62105.2024.10603818},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{10313152,
  author={Ermakov, Ivan and Lanin, Viacheslav and Lyadova, Lyudmila and Proskuryakov, Kirill},
  booktitle={2023 IEEE 17th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Approach to the Development of Ontology-Driven Language Toolkits Based on Metamodeling}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={The information systems are to be conforming to the requirements defined by domain experts. These requirements are formalized as models created with modeling tools. Applying these tools is complicated for domain experts. Domain specific modeling (DSM) with domain specific languages (DSL) reduces the semantic gap. However, the system development complication shifts to the creation of languages and tools for transforming models and code generation. An approach to automating DSL creation and facilitating code generation based on using multifaceted ontology is proposed. The generalized description of the multifaceted ontology is given. Tools of automating generation of new DSL metamodels based on mapping the corresponding domain ontology onto the metamodels of the selected base languages are described. Metamodels of the visual languages, grammars of the target text languages and transformation rules are also included into the ontology. The proposed approach is implemented as a research prototype of the language toolkits. Examples of metamodels and rules described in the ontology, as well as the results of their application are shown. The results of experiments confirmed practical significance of the approach to the ontology-driven language toolkits development.},
  keywords={Visualization;Codes;Prototypes;Metamodeling;Ontologies;Grammar;DSL;domain-specific modeling;domain-specific languages;metamodeling;multifaceted ontology;metamodel generation;model transformation rules},
  doi={10.1109/AICT59525.2023.10313152},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{9994917,
  author={Choi, Kyudam and Lee, Yurim and Kim, Cheongwon},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={GCL-GO: A novel sequence-based hierarchy-aware method for protein function prediction}, 
  year={2022},
  volume={},
  number={},
  pages={51-56},
  abstract={Experimental protein functional annotation does not cover rapidly-expanding protein sequences. Sequence-based methods, one of the computational methods, have been developed for extending functional annotations to fast-growing sequence databases. We propose a novel sequence-based hierarchy-aware method, namely GCL-GO. GCL-GO applies a protein language model to represent sequences, applies graph contrastive learning to represent GO terms, and then predicts protein functions by combining these two features. By contrasting the GO graph and semantic features of GO terms, GCL-GO has generalizability and scalability by accurately embedding the features of GO terms while relying less on training data. We also suggest GCL-GO+, which combines a sequence similarity-based method with GCLGO, to improve performance. GCL-GO+ outperforms sequence-based competing methods on both the CAFA3 and the TALE datasets. Furthermore, GCL-GO and GCL-GO+ demonstrate functional generalization and scalability potential by having the best performance on new GO terms or on GO terms annotated infrequently in the training dataset. Our code is available in https://github.com/kch38896/GCL-GO},
  keywords={Proteins;Training;Protein engineering;Annotations;Databases;Scalability;Semantics;protein function prediction;gene ontology;graph constructive learning;protein language model},
  doi={10.1109/BIBM55620.2022.9994917},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10858988,
  author={Fu, Yibin and Ding, Zhaoyun and Xu, Xiaojie},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={LLM & Bagging for 1-shot Joint IE}, 
  year={2024},
  volume={},
  number={},
  pages={204-208},
  abstract={Domain-specific few-shot information extraction (IE) has always been the difficulty of domain knowledge graph construction, and there is a new solution in this direction after the emergence of large language models (LLM). In this paper, based on previous research, we propose LLM-based 1-shot relation-entity joint IE scheme, and the bagging enhance LLM IE method is proposed to take advantage of the randomness of the LLM output. Against the background of the concept of Internet of Things (IoT) which has received wide attention globally, we selects the IoT interconnective communication as a domain-specific example, crawls the text of the device pages of L3Harris, RockwellCollins as our corpus, selects large language models that differ in the number of parameters and invocation methods to test the proposed joint IE method in relations given by the IoT interconnective communication ontology. The bagging method is tested based on the IE results of GPT-4 Turbo, and there is an improvement of 1-3%, which shows the effectiveness of traditional machine learning methods in LLM. Finally, the results and shortcomings of this study are analyzed.},
  keywords={Training;Large language models;Knowledge graphs;Ontologies;Data science;Information retrieval;Internet of Things;Bagging;Standards;Random forests;Large Language Model (LLM);Information Extraction (IE);bagging;1-shot;Communication},
  doi={10.1109/DSC63484.2024.00034},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10263217,
  author={Wang, Xiao and Liu, Kai and Wang, Chunlei},
  booktitle={2023 IEEE 9th International Conference on Cloud Computing and Intelligent Systems (CCIS)}, 
  title={Knowledge-enhanced Pre-training large language model for depression diagnosis and treatment}, 
  year={2023},
  volume={},
  number={},
  pages={532-536},
  abstract={Depression, a pervasive psychiatric disorder characterized by concealment, dependence on expert judgment, and a notable rate of misdiagnosis, poses a substantial burden on society. To enhance the diagnosis and treatment of depression, this study puts forth a proposition of employing knowledge-enhanced pre-training technology leveraging large language models. By integrating domain knowledge and depression knowledge graph directives, the pre-trained model undergoes optimization. Expert involvement in depression diagnosis and treatment fosters a guided learning process facilitated by expert feedback. Through the application of dialogue therapy, the efficacy of treatment is augmented. This technical approach aims to ameliorate the societal burden by improving the diagnosis and treatment of depressed individuals.},
  keywords={Cloud computing;Computational modeling;Mental disorders;Medical treatment;Knowledge graphs;Depression;Intelligent systems;Depression;Large language model;Knowledge enhancement;Knowledge graph},
  doi={10.1109/CCIS59572.2023.10263217},
  ISSN={2376-595X},
  month={Aug},}@INPROCEEDINGS{10662704,
  author={Sun, Qi and Li, Yahui and Zhou, Chunjie and Tian, Yu-Chu},
  booktitle={2024 43rd Chinese Control Conference (CCC)}, 
  title={Root Cause Analysis for Industrial Process Anomalies through the Integration of Knowledge Graph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={6855-6860},
  abstract={Root cause analysis for industrial process anomalies is critical for manufacturing activities. Industrial process alarms can provide crucial information to enable root cause analysis. However, the complex system structure causes a large number of alarms to emerge at the same time. To address this issue, we proposed an approach that utilizes knowledge graphs and large language models to provide comprehensible root cause analysis. Firstly, we extract knowledge such as historical anomalies from catalytic cracking operation manuals to construct an industrial process safety knowledge graph. Then, named entities in each alarm are extracted as keywords to retrieve factual knowledge from the knowledge graph. Finally, factual knowledge will be provided to the large language model as prior knowledge to infer the root cause of anomalies. Experimental results show that the proposed approach can accurately identify the root cause, thereby ensuring the safety of industrial processes.},
  keywords={Root cause analysis;Large language models;Prevention and mitigation;Oils;Time series analysis;Process control;Knowledge graphs;Root cause analysis;Knowledge graph;Large language model;Named entity recognition},
  doi={10.23919/CCC63176.2024.10662704},
  ISSN={1934-1768},
  month={July},}@INPROCEEDINGS{9621874,
  author={Aigo, Kosuke and Tsunakawa, Takashi and Nishida, Masafumi and Nishimura, Masafumi},
  booktitle={2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)}, 
  title={Question Generation using Knowledge Graphs with the T5 Language Model and Masked Self-Attention}, 
  year={2021},
  volume={},
  number={},
  pages={85-87},
  abstract={Question generation is helpful for understanding reading comprehension, spontaneous questioning in chatting systems, and expanding datasets for answering questions. In previous studies, many models have been used to generate questions from contexts, but none was suitable in large-length contexts. To overcome this challenge, we generated questions from an intermediate representation of a context, such as knowledge graphs. In this study, we focused on developing questions using knowledge graphs with the T5 language model. We used the language model to create questions using the knowledge graph and mask the self-attention of the encoder to train the model by explicitly preserving the graph’s structure. As a result of the automatic evaluation, the T5 language model with and without mask was comparable with the bidirectional Graph2Seq model (G2S), known as the QG model, using knowledge graphs. More-over, the masked language model was slightly better than the non-masked model in t5-small on four benchmarks. The code and data are publicly available at https://github.com/Macho000/T5-for-KGQG.},
  keywords={Codes;Conferences;Benchmark testing;Task analysis;Consumer electronics;Context modeling;KG2QG;Question generation;T5 language model},
  doi={10.1109/GCCE53005.2021.9621874},
  ISSN={2378-8143},
  month={Oct},}@INPROCEEDINGS{10590653,
  author={Zhang, Jialei and Cai, Shubin and Jiang, Zhiwei and Xiao, Jian and Ming, Zhong},
  booktitle={2024 10th IEEE International Conference on Intelligent Data and Security (IDS)}, 
  title={FireRobBrain: Planning for a Firefighting Robot using Knowledge Graph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={37-41},
  abstract={Firefighting robots play a crucial role in improving fire safety. However, these robots have limitations in understanding their surroundings and adapting to changing situations. In this paper, we propose the “FireRobBrain”, a combination of a knowledge graph and a large language model, to act as the “brain” of firefighting robots. This approach aims to tackle challenges related to planning robots in dynamic environments. The FireRobBrain consists of a KG with a dynamic information base and a relatively static knowledge base. It also includes a prompting module that helps the language model generate suggestions for the robot's reactions. We evaluated the framework using a dataset of 864 samples and discovered that the combination of LLM and KG, facilitated by a well-designed prompt module, significantly improves the quality of answers, particularly for tasks involving specific contexts and structured information. Furthermore, we noted that providing a task scope in the input prefix contributes to a better understanding of the robot's task, resulting in enhanced performance.},
  keywords={Large language models;Prevention and mitigation;Knowledge based systems;Knowledge graphs;Planning;Security;Reliability;Knowledge Graph;Large Language Model;Robot;Planning},
  doi={10.1109/IDS62739.2024.00014},
  ISSN={},
  month={May},}@INPROCEEDINGS{10013619,
  author={Kulagin, Grigory and Ermakov, Ivan and Lyadova, Lyudmila},
  booktitle={2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Ontology-Based Development of Domain-Specific Languages via Customizing Base Language}, 
  year={2022},
  volume={},
  number={},
  pages={1-6},
  abstract={The quality of the systems depends on compliance to the domain requirements. High quality is achieved only with involving experts in the relevant fields to the system design as experts. Modern design methods are based on using professional tools and modeling languages. Using these tools are difficult for domain experts. Domain-Specific Languages (DSLs) can be considered as "user interfaces" for experts because they bridge the gap between the domain experts and the software development tools via customizing modeling languages. Usability of DSLs by domain experts is a key factor for their successful adoption. But DSL creation is challenging task. An approach to DSL customization based on using multifaceted ontology is proposed. General scheme of DSL metamodel generation based on multifaceted ontology is described. Examples of created DSLs and models illustrating the applicability of the proposed method are shown. The DSL metamodels were developed and tested in several domains. The results of experiments confirmed practical significance of the ontology-based approach to DSL creation.},
  keywords={Visualization;Prototypes;Ontologies;User interfaces;Software;DSL;Task analysis;domain-specific modeling;DSM;domain-specific language;DSL;metamodel generation;multifaceted ontology;language customization;GalileoSky;algorithm description language},
  doi={10.1109/AICT55583.2022.10013619},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{10286646,
  author={Sadirmekova, Zhanna and Sambetbayeva, Madina and Daiyrbayeva, Elmira and Yerimbetova, Aigerim and Altynbekova, Zhanar and Murzakhmetov, Aslanbek},
  booktitle={2023 8th International Conference on Computer Science and Engineering (UBMK)}, 
  title={Constructing the Terminological Core of NLP Ontology}, 
  year={2023},
  volume={},
  number={},
  pages={81-85},
  abstract={The basis of any intellectual resource is a knowledge base, which, based on the basic terms of the field under consideration, builds relationships between them. Therefore, the first task to building multilingual information system using Natural language processing (NLP)in scientific and educational activities will be the development of a multilingual dictionary on modern NLP methods, including terms in Kazakh, English and Russian. For its construction, linguistic models for semantic dictionaries and thesauruses will be used, as well as methods of automatic extraction of terms from the corpus of texts of a given subject area. In this paper, a system of concepts of the NLP domain will be formalized, which will form the terminological core of the NLP ontology. To systematize information and provide support for multilingualism and accessibility, we plan to apply ontological engineering methods to systematize information and build the upper levels of the NLP ontology (its terminological core) using the dictionary of terms obtained at the previous stage. The ontology developed by us can later become the conceptual basis for a multilingual information system used in scientific and educational activities using NLP. This system will provide systematization of all information, convenient navigation on it, integration into a single information space, as well as access to it.},
  keywords={Dictionaries;Navigation;Semantics;Knowledge based systems;Ontologies;Linguistics;Natural language processing;Natural language processing;ontology;conceptual model;scientific and educational information system;ontological design patterns},
  doi={10.1109/UBMK59864.2023.10286646},
  ISSN={2521-1641},
  month={Sep.},}@INPROCEEDINGS{10020417,
  author={Mijalcheva, Viktorija and Davcheva, Ana and Gramatikov, Sasho and Jovanovik, Milos and Trajanov, Dimitar and Stojanov, Riste},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Learning Robust Food Ontology Alignment}, 
  year={2022},
  volume={},
  number={},
  pages={4097-4104},
  abstract={In today’s knowledge society, large number of information systems use many different individual schemes to represent data. Ontologies are a promising approach for formal knowledge representation and their number is growing rapidly. The semantic linking of these ontologies is a necessary prerequisite for establishing interoperability between the large number of services that structure the data with these ontologies. Consequently, the alignment of ontologies becomes a central issue when building a worldwide Semantic Web. There is a need to develop automatic or at least semi-automatic techniques to reduce the burden of manually creating and maintaining alignments. Ontologies are seen as a solution to data heterogeneity on the Web. However, the available ontologies are themselves a source of heterogeneity. On the Web, there are multiple ontologies that refer to the same domain, and with that comes the challenge of a given graph-based system using multiple ontologies whose taxonomy is different, but the semantics are the same. This can be overcome by aligning the ontologies or by finding the correspondence between their components.In this paper, we propose a method for indexing ontologies as a support to a solution for ontology alignment based on a neural network. In this process, for each semantic resource we combine the graph based representations from the RDF2vec model, together with the text representation from the BERT model in order to capture the semantic and structural features. This methodology is evaluated using the FoodOn and OntoFood ontologies, based on the Food Onto Map alignment dataset, which contains 155 unique and validly aligned resources. Using these limited resources, we managed to obtain accuracy of 74% and F1 score of 75% on the test set, which is a promising result that can be further improved in future. Furthermore, the methodology presented in this paper is both robust and ontology-agnostic. It can be applied to any ontology, regardless of the domain.},
  keywords={Training;Semantic Web;Semantics;Neural networks;Taxonomy;Ontologies;Big Data;Ontology Alignment;Natural language processing;Text representation;Embeddings;Data normalization;Data linking},
  doi={10.1109/BigData55660.2022.10020417},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10809842,
  author={Yang, Weijie and Zhang, Chunyu and Zhang, Min and Jiang, Xunjie and Fan, Yanlin and Bi, Zhongbo and Xiong, Jiansheng and Wang, Danshi},
  booktitle={2024 Asia Communications and Photonics Conference (ACP) and International Conference on Information Photonics and Optical Communications (IPOC)}, 
  title={Multi-Modal Knowledge Graph with Large Language Model for Intelligent Fault Management in Optical Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={We propose a multi-modal knowledge graph with large language model for intelligent fault management in optical networks. Demonstrations of two real-world cases validate its capability to utilize multi-modal data and facilitate intelligent reasoning.},
  keywords={Knowledge engineering;Analytical models;Large language models;Knowledge based systems;Asia;Knowledge graphs;Optical fiber networks;Cognition;Photonics;Multi-modal knowledge graph;Large language models;Intelligent fault management;Optical networks},
  doi={10.1109/ACP/IPOC63121.2024.10809842},
  ISSN={2162-1098},
  month={Nov},}@INPROCEEDINGS{10411632,
  author={Huang, Fei and Deng, Yi and Zhang, Chen and Guo, Menghao and Zhan, Kai and Sun, Shanxin and Jiang, Jinling and Sun, Zeyi and Wu, Xindong},
  booktitle={2023 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={KOSA: KO Enhanced Salary Analytics based on Knowledge Graph and LLM Capabilities}, 
  year={2023},
  volume={},
  number={},
  pages={499-505},
  abstract={Knowledge base question answering (KBQA) is designed to respond to natural language inquiries by utilizing factual information, such as entities, relationships, and attributes, derived from a knowledge base (KB). The advent of large language models (LLMs) has significantly boosted the performance of KBQA, owing to their exceptional capabilities in content comprehension and generation. In this paper, we present a Knowledge Ocean enhanced Salary Analytics (KOSA) system based on knowledge graphs and LLMs tailored to employee salary data from a public university. This system encompasses an interactive conversational interface, visualization of knowledge graphs, and advanced data analysis. By employing the framework of knowledge engineering, we enable knowledge graph modeling, Cypher (the query engine of Neo4j) reasoning, and question answering functionalities. Furthermore, machine learning algorithms are integrated to facilitate advanced features, such as salary prediction and allocation.},
  keywords={Machine learning algorithms;Oceans;Knowledge based systems;Knowledge graphs;Question answering (information retrieval);Remuneration;Resource management;KBQA;LLM;Salary distribution;Knowledge Engineering},
  doi={10.1109/ICDMW60847.2023.00071},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{9620493,
  author={Lyadova, Lyudmila N. and Sukhov, Alexander O. and Nureev, Marsel R.},
  booktitle={2021 IEEE 15th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={An Ontology-Based Approach to the Domain Specific Languages Design}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={Developing software systems for various domains is a complex task. The quality of the system, corresponding to the domain requirements, can only be achieved via involving the model development of experts in the relevant fields. Traditional design methods based on the using professional tools and modeling languages are difficult for subject matter experts. Using Domain Specific Languages (DSL) have been increasingly gaining attention of developers because DSLs are created to cope with specific domain particularities. However, DSL development consists of several steps to be performed can be hard. Identifying the correct set of elements and constructions of DSL, defining their constraints can be very error-prone. Automation of the new DSLs development is relevant task. The designing of new DSLs should be based on the knowledge of experts, which can be represented using an ontology. An approach to DSM platform development based on using multifaceted ontology to DSL design is proposed. Examples of DSLs and models illustrating the applicability of the proposed methodology are described.},
  keywords={Natural languages;Prototypes;Metamodeling;Ontologies;Tools;Software systems;DSL;domain specific modeling;DSM;domain specific language;DSL;visual language;metamodeling;DSM platform;language toolkits;metamodel generation;multifaceted ontology},
  doi={10.1109/AICT52784.2021.9620493},
  ISSN={2472-8586},
  month={Oct},}@INPROCEEDINGS{10145722,
  author={Bandara, H. M. R. L. and Ranathunga, L.},
  booktitle={2023 3rd International Conference on Advanced Research in Computing (ICARC)}, 
  title={Ontology Based Restaurant Recommendation Approach}, 
  year={2023},
  volume={},
  number={},
  pages={78-83},
  abstract={As the world moves forward, the restaurant industry is rapidly expanding. Customers may never physically evaluate a restaurant based on its services until that customer has practical experience with it. A better recommendation mechanism can always direct the customer to the correct location, resulting in a positive outcome. The paper discusses an approach of a context rich chatbot that can identify the customer’s mode of thinking using a restaurant ontology that suggests relevant restaurants and foods. Most importantly the defined methodology will use a hybrid version of knowledge bases along with a two-way bind to the primary knowledge base. The chatbot will proceed to find relationships in the ontology by tracing concept definitions and properties while feeding information from the database. The main components related to the proposed system are Natural Language Understanding (NLU) pipeline, dialog management module, action server, knowledge query module, and data repository (MongoDB). This mechanism was evaluated through information retrieval measures.},
  keywords={Industries;Databases;Knowledge based systems;Pipelines;Ontologies;Chatbots;Information retrieval;Rasa Framework;Ontology;Database;Knowledge Query Module;Dialog Management Module;Action Server},
  doi={10.1109/ICARC57651.2023.10145722},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{10674576,
  author={Kuai, Ssu-Chi and Liu, Yao-Yu and Lin, Ching-Tzu and Liao, Wen-Hwa},
  booktitle={2024 International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)}, 
  title={Research of Image Generation Based on Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={47-48},
  abstract={In recent years, text-to-image generation technology has achieved significant breakthroughs, especially with the advancement of Diffusion models. Sometimes the model's lack of knowledge or insufficient understanding of the text prompts to generate some unreasonable images. This paper aims to explore how combining knowledge graphs with text prompts can enhance the image generation process by providing additional relevant knowledge about the text prompts, thereby improving the capability of image generation without retraining the diffusion model.},
  keywords={Deep learning;Accuracy;Image synthesis;Text to image;Knowledge graphs;Diffusion models;Vectors;Knowledge Graph;Diffusion Model;Generative AI;Multimodal Model},
  doi={10.1109/ICCE-Taiwan62264.2024.10674576},
  ISSN={2575-8284},
  month={July},}@INPROCEEDINGS{10825619,
  author={Zhang, Hongzhi and Shafiq, M. Omair},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={From Graph Paths to Natural Language: Enhancing LLM Reasoning for Multi-choice Question-Answering Tasks}, 
  year={2024},
  volume={},
  number={},
  pages={8882-8884},
  abstract={Language models have achieved good results in many tasks. However, there are still some challenges with reasoning due to insufficient knowledge, which leads to poor performance of the models on specific tasks. We proposed a framework based on the existing solution (i.e., GreaseLM), using a large language model to replace some complex modules. Our approach converts the knowledge graph paths into natural language sentences, providing contextual support to enhance the reasoning capability of the large language models. Using the framework based on a large language model is simpler and easier to manage, and it also achieves better accuracy on multi-choice question-answering tasks.},
  keywords={Accuracy;Large language models;Natural languages;Knowledge graphs;Big Data;Cognition;Data models;Language Model;Large Language Model;Knowledge Graph;Question-Answering Task},
  doi={10.1109/BigData62323.2024.10825619},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10466828,
  author={Ding, Ningpei and Mayer, Wolfgang and Geng, Yilin and Duan, Yucong and Feng, Zaiwen},
  booktitle={2023 IEEE International Conference on High Performance Computing & Communications, Data Science & Systems, Smart City & Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, 
  title={Generative Semantic Modeling for Structured Data Source with Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={1148-1152},
  abstract={The paper introduces a generative semantic model for representing human knowledge in a way that enables computer understanding and reasoning. The current approach to semantic modeling involves mapping between the space of plausible semantic models and the provided data source. However, this approach has limitations, as the score functions used to search for the best candidate semantic model are either trained on a specific integration knowledge graph or rely on manually designed features. To address these limitations, the authors propose a new approach that combines an encoder made with a pre-trained large language model (LLM) with a graph decoder customized to generate semantics. The encoder-decoder system is designed to be trained on knowledge graphs, and the authors introduce an algorithm to generate training samples from the big knowledge graph by decomposing training samples into construction actions using a method similar to the transition system of the Syntax Parser. The proposed method is novel, as it is the first time a generative method has been applied to the semantic modeling task, empowered with an LLM, and trained on knowledge graphs to achieve better performance on standard benchmarks than in past work. In conclusion, the proposed generative semantic model offers a promising new approach to representing and organizing human knowledge in a more generalizable way, using a combination of a pre-trained LLM and a customized graph decoder trained on knowledge graphs. The approach has shown improved performance on standard benchmarks and has the potential to advance the field of semantic modeling.},
  keywords={Training;Computational modeling;Soft sensors;Semantics;Knowledge graphs;Benchmark testing;Data models;Knowledge graph;Large Language Model;Graph Neural Network},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys60770.2023.00164},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10824535,
  author={Yu, Tong and Fu, Wenhui and Dai, Doneming and Zhang, Kunli and Song, Yu},
  booktitle={2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)}, 
  title={Research on Intelligent Diagnosis Integrating Disease Subgraph and Large Language Model}, 
  year={2024},
  volume={},
  number={},
  pages={557-562},
  abstract={AI-driven diagnostic tools proved to be an effective aid for healthcare professionals in predicting diseases, thereby streamlining healthcare services. In order to improve the accuracy and efficiency in the field of intelligent diagnosis, we used Chinese electronic medical records as the data base, incorporated knowledge graphs as the external knowledge source, and introduced a large language model to reason about electronic medical records. To solve the problems of difficulty in knowledge graph construction and insufficient semantic relationship extraction, we proposed an intelligent diagnosis method that combined disease subgraphs with a large language model (DeLL). The DeLL model constructed prompt templates using the text of the electronic medical record, guided the large language model to generate new descriptions, and combined these with the multi-granularity information in the EMR to obtain enhanced textual representations. Meanwhile, the DeLL model constructed disease subgraphs by utilizing the target diseases and the knowledge graph, and encoded them using graph convolutional neural networks to fuse the disease knowledge representations with the LLM-enhanced text representations for disease prediction. The model was experimentally evaluated on a dataset consisting of multi-label and single-label electronic medical records and achieved F1 micro scores of 85.01% and 93.19%, respectively, representing improvements of 3.20% and 1.80% over the optimal baseline results.},
  keywords={Large language models;Computational modeling;Semantics;Knowledge graphs;Medical services;Data models;Numerical models;Electronic medical records;Diseases;Software engineering;Intelligent Diagnosis;Knowledge Enhancement;Knowledge Graph;Large Language Model},
  doi={10.1109/CBASE64041.2024.10824535},
  ISSN={},
  month={Oct},}@ARTICLE{10313282,
  author={Razouk, Houssam and Liu, Xing Lan and Kern, Roman},
  journal={IEEE Access}, 
  title={Improving FMEA Comprehensibility via Common-Sense Knowledge Graph Completion Techniques}, 
  year={2023},
  volume={11},
  number={},
  pages={127974-127986},
  abstract={The Failure Mode Effect Analysis process (FMEA) is widely used in industry for risk assessment, as it effectively captures and documents domain-specific knowledge. This process is mainly concerned with causal domain knowledge. In practical applications, FMEAs encounter challenges in terms of comprehensibility, particularly related to inadequate coverage of listed failure modes and their corresponding effects and causes. This can be attributed to the limitations of traditional brainstorming approaches typically employed in the FMEA process. Depending on the size and diversity in terms of disciplines of the team conducting the analysis, these approaches may not adequately capture a comprehensive range of failure modes, leading to gaps in coverage. To this end, methods for improving FMEA knowledge comprehensibility are highly needed. A potential approach to address this gap is rooted in recent advances in common-sense knowledge graph completion, which have demonstrated the effectiveness of text-aware graph embedding techniques. However, the applicability of such methods in an industrial setting is limited. This paper addresses this issue on FMEA documents in an industrial environment. Here, the application of common-sense knowledge graph completion methods on FMEA documents from semiconductor manufacturing is studied. These methods achieve over 20% MRR on the test set and 70% of the top 10 predictions were manually assessed to be plausible by domain experts. Based on the evaluation, this paper confirms that text-aware knowledge graph embedding for common-sense knowledge graph completion are more effective than structure-only knowledge graph embedding for improving FMEA knowledge comprehensibility. Additionally we found that language model in domain fine-tuning is beneficial for extracting more meaningful embedding, thus improving the overall model performance.},
  keywords={Commonsense reasoning;Knowledge graphs;Knowledge based systems;Particle swarm optimization;Heuristic algorithms;Risk management;Semiconductor device manufacture;Natural language processing;Natural language processing;common-sense knowledge;failure mode effect analysis;FMEA;semiconductor manufacturing;knowledge graph completion},
  doi={10.1109/ACCESS.2023.3331585},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9070680,
  author={Han, Seung-Ho and Choi, Ho-Jin},
  booktitle={2020 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Domain-Specific Image Caption Generator with Semantic Ontology}, 
  year={2020},
  volume={},
  number={},
  pages={526-530},
  abstract={Image captioning is the task of generating textual descriptions of a given image, requiring techniques of computer vision and natural language processing. Recent models have utilized deep learning techniques for this task to gain performance improvement. However, these models can neither fully use information included in a given image such as object and attribute, nor generate a domain-specific caption because existing methods use open dataset such as MSCOCO which include general images. To overcome these limitations, this paper proposes a domain-specific image caption generator, which generates a caption based on attention mechanism with object and attribute information, and reconstruct a generate caption using a semantic ontology to provide natural language description for given specific-domain. To show the effectiveness of the proposed model, we evaluate the image caption generator with a dataset, MSCOCO, quantitatively and qualitatively.},
  keywords={Semantics;Visualization;Generators;Ontologies;Feature extraction;Image reconstruction;Machine learning;image captioning;attention model;attribute predictionm;domain-specific ontology},
  doi={10.1109/BigComp48618.2020.00-12},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{9031044,
  author={Vijayalakshmi, H C and Dixit, Bhavana S},
  booktitle={2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)}, 
  title={Information Retrieval in Kannada using Ontology}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  abstract={As Internet technology has become a part of the lifestyle of the common man, research efforts are extensively made in the fields of Natural Language Processing (NLP) and Information Retrieval. Studying regional languages for developing the system to store, retrieve, extract the information from the database has gained lots of prominence nowadays. Case studies show that Ontological Information Retrieval has many advantages over keyword-based approach. In this paper we have focused on the general architecture of ontology-based Information Retrieval used for Kannada.},
  keywords={Ontologies;Information retrieval;Databases;Semantics;Structured Query Language;Natural language processing;Ontology;Information Retrieval;Kannada;NLP},
  doi={10.1109/CSITSS47250.2019.9031044},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10849391,
  author={Saini, Anmol and Ethier, Jeffrey G. and Shimizu, Cogan},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={An Ontology for Conversations with Virtual Research Assistants}, 
  year={2024},
  volume={},
  number={},
  pages={181-186},
  abstract={Conversational artificial intelligence has expanded rapidly in recent years, especially with the growth of large language models (LLMs). Its incorporation in scientific research in the form of research assistants has also become more common-place but remains limited in some capacities, such as in the realm of polymer science. The limitations of LLMs, especially in terms of domain knowledge, warrant the need for other tools, such as knowledge graphs (KGs), to better guide conversations. While such conversational models have been developed in the past, they are generally restricted to particular domains and lack the ability to integrate semantics from various kinds of conversations. Thus, we make progress toward the construction of a universal conversational model that has a focus on the materials domain by combining aspects of existing models. We aim to implement it in such a way that renders it amenable to modifications and usable in a variety of situations. We posit that this model will be adopted and extended by others seeking to accomplish a similar goal in the future.},
  keywords={Adaptation models;Limiting;Conversational artificial intelligence;Large language models;Semantics;Natural languages;Oral communication;Knowledge graphs;Ontologies;Polymers;artificial intelligence;conversational model;knowledge graph;large language model;ontology;ontology design pattern;polymer science},
  doi={10.1109/ICTAI62512.2024.00034},
  ISSN={2375-0197},
  month={Oct},}@INPROCEEDINGS{8257822,
  author={Jayawardana, Vindula and Lakmal, Dimuthu and de Silva, Nisansa and Perera, Amal Shehan and Sugathadasa, Keet and Ayesha, Buddhi and Perera, Madhavi},
  booktitle={2017 Seventeenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
  title={Semi-supervised instance population of an ontology using word vector embedding}, 
  year={2017},
  volume={},
  number={},
  pages={1-7},
  abstract={In many modern-day systems such as information extraction and knowledge management agents, ontologies play a vital role in maintaining the concept hierarchies of the selected domain. However, ontology population has become a problematic process due to its nature of heavy coupling with manual human intervention. With the use of word embeddings in the field of natural language processing, it became a popular topic due to its ability to cope up with semantic sensitivity. Hence, in this study we propose a novel way of semi-supervised ontology population through word embeddings as the basis. We built several models including traditional benchmark models and new types of models which are based on word embeddings. Finally, we ensemble them together to come up with a synergistic model with better accuracy. We demonstrate that our ensemble model can outperform the individual models.},
  keywords={Ontologies;Sociology;Statistics;Law;Natural language processing;Semantics;Ontology;Ontology Population;Word Embeddings;word2vec},
  doi={10.1109/ICTER.2017.8257822},
  ISSN={2472-7598},
  month={Sep.},}@INPROCEEDINGS{10135342,
  author={Chen, Jiabao and Fan, Yongquan},
  booktitle={2023 4th International Conference on Computer Engineering and Application (ICCEA)}, 
  title={Improving temporal question answering using temporal knowledge graph embedding}, 
  year={2023},
  volume={},
  number={},
  pages={570-575},
  abstract={Knowledge graph question answering is an important research direction for question answering tasks. In recent years, there has been an increasing amount of research on knowledge graph question answering, but temporal knowledge graph question answering is still a relatively unexplored area. In question answering tasks, many natural language questions have explicit or implicit temporal constraints, and most existing research methods lack the temporal awareness to deal with complex temporal questions. To address these challenges, this paper proposes a question answering model based on temporal knowledge graph embedding (TKGETQA). The model uses TKG embeddings to root a question in the entities, relations and time horizons it references, and perceives temporal information from the question to improve the accuracy of answer prediction. Experiments on the dataset CronQuestions in this paper show that the TKGETQA model exhibits better results compared to existing temporal knowledge graph question answering approaches.},
  keywords={Knowledge engineering;Natural languages;Knowledge graphs;Predictive models;Question answering (information retrieval);Data mining;Task analysis;temporal knowledge graph;temporal knowledge graph embedding;question answering},
  doi={10.1109/ICCEA58433.2023.10135342},
  ISSN={2159-1288},
  month={April},}@INPROCEEDINGS{10568296,
  author={Tona, Claudia and Juárez-Ramírez, Reyes and Jiménez, Samantha and Murillo-Muñoz, Fernanda},
  booktitle={2023 11th International Conference in Software Engineering Research and Innovation (CONISOFT)}, 
  title={Q-Story: An Ontology-Based on Quality of User Stories in Scrum. A Quantitative Assessment}, 
  year={2023},
  volume={},
  number={},
  pages={55-64},
  abstract={Q-Story ontology was created utilizing the Methontology approach and further represented through the Meta Object Facility (MOF) and Unified Modeling Language (UML). Because aspects such as their structure, level of granularity, and comprehensibility hold considerable signifi-cance in ensuring a favorable project execution. That is, the quality of user stories significantly impacts the outcome of a software project, influencing its success or failure. Therefore, we performed a quantitative evaluation using the OntoQA method, resulting in a relationship richness value of 0.95, an attribute richness of 4.00, and an inheritance richness of 1.26. The outcome of this work will contribute to developing an ontology that can effectively create user stories with quality. Furthermore, it will serve as a valuable guide for development teams, aiding them in the creation, analysis, and development processes of user stories.},
  keywords={Technological innovation;Unified modeling language;Ontologies;Software;Software engineering;Ontology;Quantitative Assessment;Quality Met-rics;Software Engineering;User Story;Ontology Quality Evaluation},
  doi={10.1109/CONISOFT58849.2023.00017},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10677802,
  author={Gouidis, Filippos and Papantoniou, Katerina and Papoutsakis, Konstantinos and Patkos, Theodore and Argyros, Antonis and Plexousakis, Dimitris},
  booktitle={2024 14th International Conference on Pattern Recognition Systems (ICPRS)}, 
  title={LLM-aided Knowledge Graph construction for Zero-Shot Visual Object State Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={The problem of classifying the states of objects using visual information holds great importance in both applied and theoretical contexts. This work focuses on the special case of Zero-shot Object-Agnostic State Classification (ZS-OaSC). To tackle this problem, we introduce an innovative strategy that capitalizes on the capabilities of Graph Neural Networks to learn to project semantic embeddings into visual space and on the potential of Large Language Models (LLMs) to provide rich content for constructing Knowledge Graphs (KGs). Through a comprehensive ablation study, we explore the synergies between LLMs and KGs, uncovering critical insights about their integration in the context of the ZS-OSC problem. Our proposed methodology is rigorously evaluated against current state-of-the-art (SoA) methods, demonstrating superior performance in various image datasets.},
  keywords={Visualization;Large language models;Semantics;Knowledge graphs;Graph neural networks;Pattern recognition},
  doi={10.1109/ICPRS62101.2024.10677802},
  ISSN={},
  month={July},}@INPROCEEDINGS{9924844,
  author={Ramli, Inigo and Krisnadhi, Adila Alfa and Prasojo, Radityo Eko},
  booktitle={2022 7th International Workshop on Big Data and Information Security (IWBIS)}, 
  title={IndoKEPLER, IndoWiki, and IndoLAMA: A Knowledge-enhanced Language Model, Dataset, and Benchmark for the Indonesian Language}, 
  year={2022},
  volume={},
  number={},
  pages={19-26},
  abstract={Pretrained language models posses an ability to learn the structural representation of a natural language by processing unstructured textual data. However, the current language model design lacks the ability to learn factual knowledge from knowledge graphs. Several attempts have been made to address this issue, such as the development of KEPLER. KEPLER combines the BERT language model and TransE knowledge embedding method to achieve a language model that can incorporate knowledge graphs as training data. Unfortunately, such knowledge enhanced language model is not yet available for the Indonesian language. In this experiment, we propose IndoKEPLER: a language model trained usingWikipedia Bahasa Indonesia andWikidata. We also create a new knowledge probing benchmark named IndoLAMA to test the ability of a language model to recall factual knowledge. The benchmark is based on LAMA, which is designed to test the suitability of our language model to be used as a knowledge base. IndoLAMA tests a language model by giving cloze style question and compare the prediction of the model to the factually correct answer. This experiment shows that IndoKEPLER increases the ability of a normal DistilBERT model to recall factual knowledge by 0.8%. Moreover, the most significant increase happens when dealing with many-to-one relationships, where IndoKEPLER outperforms it’s original text encoder model by 3%.},
  keywords={Conferences;Knowledge based systems;Bit error rate;Training data;Information security;Benchmark testing;Predictive models;Language model;knowledge embedding;knowledge graph;natural language processing;Indonesian language},
  doi={10.1109/IWBIS56557.2022.9924844},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10391548,
  author={Taye, Mohammad Mustafa and Abulail, Rawan and Al-Oudat, Mohammad},
  booktitle={2023 7th International Symposium on Innovative Approaches in Smart Technologies (ISAS)}, 
  title={An Ontology Learning Framework for unstructured Arabic Text}, 
  year={2023},
  volume={},
  number={},
  pages={1-12},
  abstract={Ontologies are widely regarded as valuable sources of semantics and interoperability in all artificially intelligent systems. Due to the rapid growth of unstructured data on the web, studying how to automatically get ontology from unstructured text is important. Therefore, ontology learning (OL) is an important process in the business world. It involves finding and extracting concepts from the text so that these concepts can be used for things such as information retrieval. Unfortunately, learning ontology is not easy for some reasons, and there has not been much research on how to automatically learn a domain-specific ontology from data.Ontology Studying Arabic text is not as developed as learning Latin text. There is almost no automated support for using Arabic literary knowledge in semantically enabled systems. Machine learning (ML) has proven beneficial in numerous fields, including text mining. By employing neural language models such as AraBERT, it is possible to obtain word embeddings as distributed word representations from textual input using machine learning. However, the application of machine learning to aid the development of Arabic ontology is largely unexplored. This research examines the performance of AraBERT for ontology learning tasks in Arabic. Early performance results as an application of Arabic ontology learning are promising. In this research, we provide a method for populating an existing ontology with instance information extracted from the input natural language text. This prototype has achieved an information extraction accuracy of 91%.},
  keywords={Text mining;Semantics;Natural languages;Prototypes;Machine learning;Ontologies;Information retrieval;Arabic Ontology;Natural language Processing (NLP);Ontology;Ontology Learning (OL);Semantic Web;semantic representation},
  doi={10.1109/ISAS60782.2023.10391548},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10386124,
  author={Lee, Jeongbin and Kim, Kunyoung and Sohn, Mye and Kim, Jongmo},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={GCN-based Explainable Recommendation using a Knowledge Graph and a Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={2930-2936},
  abstract={In this paper, we propose a novel graph convolutional network (GCN)-based recommendation method using both knowledge graph (KG) and review texts to solve the cold start problem and provide explainability. In our model, GCN-based collaborative filtering (CF) is parallelly performed on the user-item interaction graph and the KG to utilize the items’ additional information in the recommendation process. Also, we use a pretrained language model to generate embeddings from the user reviews and utilize them in the GCN embedding propagation process to reflect the users’ subjective sentiment and opinion. After the recommendation is performed, we generate the paths from the target user to recommended items by using the KG and embeddings from review texts. To prove the superiority of the proposed method, we conduct the experiment by comparing recommendation performance with baseline models. In the experiment, the proposed method outperformed the other models.},
  keywords={Collaborative filtering;Knowledge graphs;Big Data;Data models;Convolutional neural networks;recommender system;collaborative filtering;graph convolutional network;knowledge graph;language model},
  doi={10.1109/BigData59044.2023.10386124},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10580566,
  author={Li, Wenyue and Phyu, Sapae and Liu, Qin and Du, Bowen and Zhang, Junyan and Zhu, Hongming},
  booktitle={2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
  title={RSTIE-KGC: A Relation Sensitive Textual Information Enhanced Knowledge Graph Completion Model}, 
  year={2024},
  volume={},
  number={},
  pages={2991-2998},
  abstract={Nowadays, many knowledge graph completion models are proposed to assist the automatic construction of large knowledge graphs. While knowledge graph embedding models and textual information enhanced models are two tendencies in this area, they both suffer from some shortcomings, for example, relying too much on one single modal information may cause the limitation of performance. However, only a few works attempt to integrate multiple modalities. Besides, we found that the semantic similarity between head and tail entities correlates to the enhancing effect of textual information, and the semantic similarity is also related to relation types. We call this phenomenon relational sensitivity. To address these issues, we propose a relation sensitive textual information enhanced knowledge graph completion model (RSTIE-KGC). In our work, by integrating pre-trained language models (PLM) with knowledge graph embedding models, we fuse structural information and textual information, taking advantage of both topological and semantic features. Instead of finetuning the whole PLMs as existing models do, we choose a more efficient way, using frozen PLMs followed by an adapter to fit the text embeddings to our tasks, so that we can reduce training costs and enlarge the scale of negative samples, which maintains accuracy and effectiveness of our model. Based on the relational sensitivity, we propose our RelRank block, which selectively enhances textual information by calculating and filtering mean rank proportion (MRP) score for each relation type, thereby making better use of beneficial semantic information and reducing the noise and redundancy caused by textual information. The prediction process makes more refined use of textual information, as a result improving the accuracy of the model in the link prediction task. We conducted link prediction experiments on two real-world datasets. On FB15k-237, our model outperformed the current state-of-art textual information enhanced models in both MRR and Hit@k metrics. On WN18RR, our model also showed a stable prediction performance, and the results of both MRR and Hits@1 metrics were better than the most textual information enhanced models.},
  keywords={Training;Adaptation models;Sensitivity;Accuracy;Computational modeling;Semantics;Noise;Knowledge graph;Link prediction;pre-trained language model;Multi modality},
  doi={10.1109/CSCWD61410.2024.10580566},
  ISSN={2768-1904},
  month={May},}@INPROCEEDINGS{10415793,
  author={Lu, Yi-Hong and Wang, Chang-Dong and Lai, Pei-Yuan and Lai, Jian-Huang},
  booktitle={2023 IEEE International Conference on Data Mining (ICDM)}, 
  title={PKAT: Pre-training in Collaborative Knowledge Graph Attention Network for Recommendation}, 
  year={2023},
  volume={},
  number={},
  pages={448-457},
  abstract={With the rapid growth of online platforms and the abundance of available information, personalized recommender systems have become essential for assisting users in discovering relevant and interesting content. Among the various methods, knowledge-aware recommendation model has achieved notable success by leveraging the rich semantic information encoded in knowledge graphs. However, it overlooks the fact that users’ historical click sequences can better reflect their preferences within a period of time, thus imposing certain limitations on the recommendation performance. On the other hand, the application of pre-trained language models in recommender systems has demonstrated increasingly significant potential, as they can capture sequential patterns and dependencies within users’ historical click sequences and effectively capture contextual information in user-item interactions. To this end, we propose a hybrid recommendation model that leverages Pre-training in the collaborative Knowledge graph Attention neTwork (PKAT), to extract both the high-order connectivity information in collaborative knowledge graphs and the contextual information in users’ historical click sequences captured by Bidirectional Encoder Representations from Transformers (BERT). The collaborative knowledge graph attention network enables the model to effectively capture the intricate relationships between users, items, and knowledge entities, thus enhancing the representation learning process. Furthermore, what sets PKAT apart from other state-of-the-art knowledge-aware recommendation methods is the incorporation of the BERT language model. This integration allows PKAT to capture the contextual sequence information of user behavior, enabling it to generate more accurate and personalized recommendations. Extensive experiments are conducted on multiple benchmark datasets. And the results demonstrate that our PKAT model outperforms several state-of-the-art baselines.},
  keywords={Knowledge engineering;Collaboration;Knowledge graphs;Encoding;Data mining;Recommender systems;Context modeling;Recommendation;Collaborative knowledge graph;Attention mechanism;Pre-trained language model},
  doi={10.1109/ICDM58522.2023.00054},
  ISSN={2374-8486},
  month={Dec},}@INPROCEEDINGS{10859004,
  author={Lai, Jiang and Conghui, Zheng and Xiaohan, Zhang and Fuhui, Sun and Xiaoyan, Wang and Li, Pan},
  booktitle={2024 IEEE 9th International Conference on Data Science in Cyberspace (DSC)}, 
  title={Leveraging LLM based Retrieval-Augmented Generation for Legal Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={196-203},
  abstract={Aiming at filling missing triple structures, Knowledge Graph Completion (KGC) is a crucial task in knowledge graph reasoning. Especially in the judicial domain, it plays a significant role not only in enhancing the completeness of legal knowledge graphs but also in improving the accuracy of legal supervisory inference. Due to extensive legal expertise, complex entity relationships, and strict requirements for interpretable reasoning, traditional methods, such as embedding-based methods and those that utilize pre-trained language models (e.g., BERT), often fail to delve into deep semantic information or overlook the interconnections among triples, which leads to significant shortcomings in generalizability and capabilities in practical applications. Motivated by the great generation power of Large Language Models (LLMs), we propose a legal knowledge graph completion model based on Retrieval Augmented Generation (RAG), which we have named RA-KG-LLM. This model effectively combines the generative capabilities of LLMs with retrieval-augmented technology to enhance the semantic information and interrelations mining within knowledge graphs. The retrieval framework within our model utilizes text embeddings and vector similarity matching to provide the LLM with relevant triples, while fine-tuning techniques are employed to infuse the semantic information from the knowledge graph into the LLM. Extensive experiments on five real datasets demonstrate the effectiveness and potentiality in the judicial field of the proposed model. Specifically, on the legal domain dataset Cail2022, it achieves better Hits@1 score in the relation prediction task than the state-of-the-art related works.},
  keywords={Law;Large language models;Retrieval augmented generation;Semantics;Knowledge graphs;Predictive models;Data models;Cognition;Vectors;Filling;Large Language Models;Judicial Knowledge Graph;Knowledge Graph Completion;Retrieval-Augmented Generation},
  doi={10.1109/DSC63484.2024.00033},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10826002,
  author={Bharti, Suman and Lo, Dan Chia-Tien and Shi, Yong},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Enhancing Contextual Understanding in Knowledge Graphs: Integration of Quantum Natural Language Processing with Neo4j LLM Knowledge Graph}, 
  year={2024},
  volume={},
  number={},
  pages={8628-8630},
  abstract={Traditional Knowledge Graphs (KGs), such as Neo4j, face challenges in managing high-dimensional relationships and capturing semantic nuances due to their deterministic nature. Quantum Natural Language Processing (QNLP) introduces probabilistic reasoning into the KG context. This integration leverages quantum principles, such as superposition, which allows relationships to exist in multiple states simultaneously, and entanglement, where the state of one entity dynamically influences the state of another. This quantum-based probabilistic reasoning provides a richer, more flexible representation of connections, moving beyond binary relationships to model the nuances and variability of real-world interactions. Our research demonstrates that QNLP enhances Neo4j’s ability to analyze context-rich data, improving tasks like entity extraction and knowledge inference. By modeling relationship states probabilistically, QNLP addresses limitations in traditional methods, providing nuanced insights and enabling more advanced, context-aware NLP applications.},
  keywords={Quantum computing;Quantum entanglement;Semantics;Knowledge graphs;Predictive models;Probabilistic logic;Natural language processing;Cognition;Planning;Context modeling;Neo4j;QNLP;LLM;KGs;NLP},
  doi={10.1109/BigData62323.2024.10826002},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10295846,
  author={Wang, Huan and Guo, Huifeng and Mao, Zehui and Li, Haibin and Qu, Qiang and Yang, Yulei},
  booktitle={2023 CAA Symposium on Fault Detection, Supervision and Safety for Technical Processes (SAFEPROCESS)}, 
  title={Bidirectional Mapping RTE for Fault Knowledge Graph Construction}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In this paper, Bidirectional Mapping Relation Triple Extraction (BMRTE) is proposed to address the challenges of numerous overlapping triples and centralized knowledge in fault maintenance texts, enabling the construction of a fault knowledge graph. The pre-trained language model BERT is employed in BMRTE for the generation of an initial vector representation for each token in the texts. A binary tagging framework is used to identify base entities that may be related in the text on the basis of encoding tokens. To extract entity pairs from the text effectively, the bidirectional mapping framework is designed to map the base entities to their associated entities. Finally, multiple relation classification matrices are used to identify entity pairs and determine the relation triple. Our proposed model outperforms the compared baselines, as evidenced by the experimental results obtained from both a widely used public dataset and our labeled fault maintenance dataset.},
  keywords={Fault diagnosis;Fault detection;Knowledge graphs;Maintenance engineering;Tagging;Encoding;Safety;knowledge graph construction;relation triple extraction;overlapping relations;intelligent fault diagnosis},
  doi={10.1109/SAFEPROCESS58597.2023.10295846},
  ISSN={},
  month={Sep.},}@ARTICLE{10064113,
  author={Cui, Zhaojian and Yuan, Zhenming and Wu, Yingfei and Sun, Xiaoyan and Yu, Kai},
  journal={IEEE Access}, 
  title={Intelligent Recommendation for Departments Based on Medical Knowledge Graph}, 
  year={2023},
  volume={11},
  number={},
  pages={25372-25385},
  abstract={The clinical sub-speciality departments are increasing. It is usually difficult for patients without medical education to choose a suitable department when they need to make an online registration with doctor. A novel approach is presented for departments recommendation in this paper. The ICD codes and symptoms of medical health data are used to describe the patients’ characteristics, diseases, and departments in the quantitative relation. The knowledge graph is built with the relations to automatically recommend departments. The MacBERT-BiLSTM-CRF medical entity recognition model (MacNER) is proposed to identify the patient’s symptoms, parts, treatment methods and drug entities to structure a knowledge graph. The knowledge graph is used to provide knowledge for the intelligent department recommendation. The identified entities are mapped to nodes in the graph with the entity mapping method. Finally, an algorithm named Feature Rate Multiply Converse Disease Rate (FRMCDR) is proposed. The most appropriate department can be recommended by fusing the patient’s chief complaint and past medical history. The experiment shows that our method obtained an 88.77% precision rate in the recommendation.},
  keywords={Knowledge graphs;Hidden Markov models;Diseases;Task analysis;Hospitals;Medical diagnostic imaging;Drugs;Intelligent recommendation;departments recommendation;knowledge graph;medical entity recognition},
  doi={10.1109/ACCESS.2023.3254303},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10711065,
  author={Reif, Jonathan and Jeleniewski, Tom and Gill, Milapji Singh and Gehlhoff, Felix and Fay, Alexander},
  booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards}, 
  year={2024},
  volume={},
  number={},
  pages={1-4},
  abstract={The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research.},
  keywords={Fabrication;Accuracy;Large language models;Semantics;Natural languages;Ontologies;Chatbots;Fake news;Standards;Manufacturing automation;Semantic Web;Ontologies;Large Language Models;Cyber-Physical Systems;Industry 4.0},
  doi={10.1109/ETFA61755.2024.10711065},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{10705235,
  author={Baddour, Moussa and Paquelet, Stéphane and Rollier, Paul and De Tayrac, Marie and Dameron, Olivier and Labbe, Thomas},
  booktitle={2024 IEEE 12th International Conference on Intelligent Systems (IS)}, 
  title={Phenotypes Extraction from Text: Analysis and Perspective in the LLM Era}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Collecting the relevant list of patient phenotypes, known as deep phenotyping, can significantly improve the final diagnosis. As textual clinical reports are the richest source of phenotypes information, their automatic extraction is a critical task. The main challenges of this Information Extraction (IE) task are to identify precisely the text spans related to a phenotype and to link them unequivocally to referenced entities from a source such as the Human Phenotype Ontology (HPO). Recently, Language Models (LMs) have been the most suc-cessful approach for extracting phenotypes from clinical reports. Solutions such as PhenoBERT, relying on BERT or GPT, have shown promising results when applied to datasets built on the hypothesis that most phenotypes are explicitly mentioned in the text. However, this assumption is not always true in medical genetics. Hence, although the LMs carry powerful semantic abilities, their contributions are not clear compared to syntactic string-matching steps that are used within the current pipelines. The goal of this study is to improve phenotype extraction from clinical notes related to genetic diseases. Our contributions are threefold: First, we provide a clear definition of the phenotype extraction task from free text, along with a high-level overview of the involved functions. Second, we conduct an in-depth analysis of PhenoBERT, one of the best existing solutions, to evaluate the proportion of phenotypes predicted with simple string-matching. Third, we demonstrate how utilizing and incorporating large language models (LLMs) for span detection step can improve performance especially with implicit phenotypes. In addition, this experiment revealed that the annotations of existing dataset are not exhaustive, and that LLM can identify relevant spans missed by human labelers.},
  keywords={Phenotypes;Annotations;Large language models;Semantics;Detectors;Syntactics;Ontologies;Data mining;Intelligent systems;Medical diagnostic imaging;phenotype;genetic;entity linking;phenoBERT;LLM;embed dings},
  doi={10.1109/IS61756.2024.10705235},
  ISSN={2767-9802},
  month={Aug},}@INPROCEEDINGS{10825937,
  author={Russo, Diego and Orlando, Gian Marco and Romano, Antonio and Riccio, Giuseppe and Gatta, Valerio La and Postiglione, Marco and Moscato, Vincenzo},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Scaling LLM-Based Knowledge Graph Generation: A Case Study of Italian Geopolitical News}, 
  year={2024},
  volume={},
  number={},
  pages={3494-3497},
  abstract={Geopolitical news provides vast amounts of information essential for understanding international relations and political events. However, organizing this information into a coherent, structured format poses challenges due to the complexity and dynamic nature of the domain. This paper introduces a scalable system leveraging Large Language Models to build continuously updated Knowledge Graphs from Italian geopolitical news. The system features a modular architecture, including a Collector Node for scalable article extraction, a Redis-based reliable queue to manage large-scale data ingestion, and a Named Entity Recognition/Relation Extraction Engine to standardize entity-relation triples. The framework addresses key challenges, such as continuous updating and hallucination mitigation, ensuring the reliability of the graph. Our evaluations demonstrate significant improvements in scalability, uniformity of extracted triples, and graph accuracy, making this architecture particularly suitable for real-time geopolitical analysis.},
  keywords={Scalability;Prevention and mitigation;Large language models;Knowledge graphs;International relations;Feature extraction;Real-time systems;Data mining;Reliability;Engines;Knowledge Graph (KG);Retrieval-Augmented Generation (RAG);Named Entity Recognition (NER);Relation Extraction (RE);Large Language Models (LLMs)},
  doi={10.1109/BigData62323.2024.10825937},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10068451,
  author={Bhana, Nimesh and van Zyl, Terence L.},
  booktitle={2022 9th International Conference on Soft Computing & Machine Intelligence (ISCMI)}, 
  title={Knowledge Graph Fusion for Language Model Fine-Tuning}, 
  year={2022},
  volume={},
  number={},
  pages={167-172},
  abstract={Language Models such as BERT (Bidirectional Encoder Representations from Transformers) have grown in popularity due to their ability to be pre-trained and perform robustly on a wide range of Natural Language Processing tasks. Often seen as an evolution over traditional word embedding techniques, they can produce semantic representations of text, useful for tasks such as semantic similarity. However, state-of-the-art models often have high computational requirements and lack global context or domain knowledge which is required for complete language understanding. To address these limitations, we investigate the benefits of knowledge incorporation into the fine-tuning stages of BERT. An existing K-BERT model, which enriches sentences with triplets from a Knowledge Graph, is adapted for the English language and extended to inject contextually relevant information into sentences. As a side-effect, changes made to K-BERT for accommodating the English language also extend to other word-based languages. Experiments conducted indicate that injected knowledge introduces noise. We see statistically significant improvements for knowledge-driven tasks when this noise is minimised. We show evidence that, given the appropriate task, modest injection with relevant, high-quality knowledge is most performant.},
  keywords={Adaptation models;Computational modeling;Semantics;Bit error rate;Knowledge graphs;Transformers;Natural language processing;Language Model;BERT;Knowledge Graph},
  doi={10.1109/ISCMI56532.2022.10068451},
  ISSN={2640-0146},
  month={Nov},}@INPROCEEDINGS{9727616,
  author={Cheng, Zheng and Wu, Jiaju and Ji, Bin and Liu, Huijun},
  booktitle={2021 China Automation Congress (CAC)}, 
  title={Pre-trained Language Model based Medical Named Entity Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={4337-4341},
  abstract={Medical named entity recognition is an important part of structuring Chinese electronic medical records and construction of medical knowledge graph. The CCKS2019 conference organized a medical named entity recognition evaluation task to extract six types of medical entities from unstructured Chinese electronic medical records. Based on the data set of this evaluation task, pre-trained language model based entity recognition approaches are studied. First, select the BiLSTM-CRF model based on random initialized word embedding as the baseline system; secondly, apply word2vec to the baseline system; thirdly, apply ELMo to the baseline system. Experimental results show that the pre-trained language model is comparable to the best approach of this evaluation task, and the context-related pre-trained language model performs better.},
  keywords={Automation;Data models;Task analysis;Electronic medical records;Context modeling;BiLSTM-CRF;ELMo;pre-trained language model;Chinese electronic medical record;named entity recognition},
  doi={10.1109/CAC53003.2021.9727616},
  ISSN={2688-0938},
  month={Oct},}@INPROCEEDINGS{10404356,
  author={Li, Fan and Tian, Yingjie and Su, Yun and Guo, Naiwang and Gao, Jun and Pan, Haiyan},
  booktitle={2023 3rd International Conference on Intelligent Power and Systems (ICIPS)}, 
  title={Construction and Application of Power Grid Fault Handling Knowledge Graph}, 
  year={2023},
  volume={},
  number={},
  pages={431-436},
  abstract={The fault handling emergency plan of power grid has great instructive significance for the quick and proper emergency disposal when the failures or accidents occur. To solve the problem of poor application effectiveness of the current fault handling emergency plan, a new method to construct the knowledge graph for the fault handling based on the BERT-BiLSTM-CRF model is proposed. In response to the problem of insufficient data available for deep learning model training and high data labeling costs in the power grid, the knowledge extraction module utilizes Bert pre-training method to construct the entity recognition model to improve the performance. The experimental results show that F1-score reaches 86.75% and the accuracy rate reaches 95.81%. Finally, the Neo4j graph database is adopted for highly visual management of the knowledge graph. When power grid faults occur, the fault handling knowledge graph can assist dispatchers to improve the capabilities of power grid's emergency disposal and the level of dispatch intelligence.},
  keywords={Training;Visualization;Costs;Knowledge graphs;Data models;Power grids;Labeling;deep learning;fault handling;knowledge graph;entity recognition;BERT-BiLSTM-CRF},
  doi={10.1109/ICIPS59254.2023.10404356},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10291739,
  author={Tian, Erlin and Liang, Weide and Li, Pu},
  booktitle={2023 IEEE 7th Information Technology and Mechatronics Engineering Conference (ITOEC)}, 
  title={Building a knowledge graph for dietary services targeting specific groups of people}, 
  year={2023},
  volume={7},
  number={},
  pages={1633-1637},
  abstract={Currently, most research focuses on users' behavioral preferences while ignoring the impact of food properties on human health. This article extracts a large amount of knowledge about the relationship between food properties and human health from multiple heterogeneous data sources. Based on this, a knowledge graph in the field of food is constructed for special populations to help them plan their diet more reasonably and reduce the risk of common diseases. Using background data sources such as Baidu Baike and Wikipedia, the BERT-BiLSTM-MHA-CRF method is proposed to extract food-related attributes from more than 14,731 descriptions of food properties. Combined with the differentiated features of special populations, a knowledge graph in the field of food is constructed. The knowledge graph mainly includes six entity types: food nutrition, efficacy and function, food name, population, dish name, and seasoning, with a total of 11,218 entities and 96,186 relationships. The experiment shows that compared with traditional static word vector models, BERT can generate dynamic word vectors based on context in large-scale corpus, making semantic encoding more accurate. The multi-head self-attention mechanism weights various entities in the food domain to reduce the interference of invalid information, making the model more accurate in capturing entity features. The BERT-BiLSTM-MHA-CRF method proposed in this article achieves P, R, and F1 greater than 90%.},
  keywords={Knowledge engineering;Soft sensors;Sociology;Web and internet services;Semantics;Knowledge graphs;Encyclopedias;food nutrition;special population;knowledge graph;knowledge extraction},
  doi={10.1109/ITOEC57671.2023.10291739},
  ISSN={2693-289X},
  month={Sep.},}@INPROCEEDINGS{10471711,
  author={Qin, Xiaodong and He, Yuxuan and Ma, Jie and Peng, Weiyuan and Zio, Enrico and Su, Huai},
  booktitle={2023 International Conference on Computer Science and Automation Technology (CSAT)}, 
  title={An Effective Knowledge Mining Method for Compressor Fault Text Data Based on Large Language Model}, 
  year={2023},
  volume={},
  number={},
  pages={44-48},
  abstract={The fault diagnosis method of compressors determines the reliability of the gas transmission pipeline station. Existing compressor fault diagnosis methods mostly relies on data-driven, which leads to a high application threshold from the mechanism. To address this issue, this paper introduces the knowledge graph into the compressor fault diagnosis for the first time and proposes a compressor fault text data knowledge mining method based on large language model. Firstly, the characteristics and principles of compressor faults are analyzed. Then, a text data knowledge mining model called CFRTE for compressors is constructed. Experimental results show that the Fl score of the CFRTE model can reach 0.98, meeting the requirements of compressor fault knowledge mining. Finally, combined with the results of knowledge mining and the graph database, a new system for the storage and indexing of the compressor fault knowledge graph is proposed. To further verify the role of the large language model in compressor fault knowledge mining, this paper conducts a comparative experiment of CFRTE models based on RNN encoder and BERT encoder. Experimental results show that compared with GRU, BiGRU, LSTM, and BiLSTM as the encoder layer, the Fl score of the CFRTE model with BERT as the encoder layer has increased by 26.78%, 6.18%, 21.89%, and 5.49% respectively. This work provides a systematic feasible scheme for introducing knowledge graphs into compressor fault diagnosis, which can be used for reference in the fault diagnosis of related equipment.},
  keywords={Fault diagnosis;Systematics;Computational modeling;Pipelines;Knowledge graphs;Ontologies;Compressors;compressor station;compressor;large language model;knowledge mining;knowledge graph},
  doi={10.1109/CSAT61646.2023.00024},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9194539,
  author={Lin, Zhimin and Lei, Dajiang and Han, Yuting and Wang, Guoyin and Deng, Wei and Huang, Yuan},
  booktitle={2020 IEEE International Conference on Knowledge Graph (ICKG)}, 
  title={Siamese BERT Model with Adversarial Training for Relation Classification}, 
  year={2020},
  volume={},
  number={},
  pages={291-296},
  abstract={Relation classification is a very important Natural Language Processing (NLP) task to classify the relations from the plain text. It is one of the basic tasks of constructing a knowledge graph. Most existing state-of-the-art methods are primarily based on Convolutional Neural Networks(CNN) or Long Short-Term Memory Networks(LSTM). Recently, many pre-trained Bidirectional Encoder Representation from Transformers (BERT) models have been successfully used in the sequence labeling and many NLP classification tasks. Relation classification is different in that it needs to pay attention to not only the sentence information but also the entity pairs. In this paper, a Siamese BERT model with Adversarial Training (SBERT-AT) is proposed for relation classification. Firstly, the features of the entities and the sentence can be extracted separately to improve the performance of relation classification. Secondly, the adversarial training is applied to the SBERT architecture to improve the robustness. Lastly, the experimental results demonstrate that we achieve significant improvement compared with the other methods on real-world datasets.},
  keywords={Bit error rate;Task analysis;Training;Robustness;Labeling;Predictive models;Telecommunications;NLP;Relation Classification;Siamese BERT;Adversarial Training;Knowledge Graph},
  doi={10.1109/ICBK50248.2020.00049},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10650003,
  author={Zhou, Xin and Shan, Yongxue and Dong, Zixuan and Liu, Haijiao and Wang, Xiaodong},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Temporal Closing Path for PLM-based Temporal Knowledge Graph Completion}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Temporal Knowledge Graph Completion (TKGC) aims to predict missing parts of quadruples, which is crucial for real-life knowledge graphs. Compared with methods that only use graph neural networks, the emergence of pre-trained model has introduced a trend of simultaneously leveraging text and graph structure information. However, most current methods based on pre-trained models struggle to effectively utilize both text and multi-hop graph structure information concurrently, resulting in insufficient association mining of relations. To address the challenge, we propose a novel model: Temporal Closing Path for Pre-trained Language Model-based TKGC (TCP-PLM). We obtain the temporal closing relation path of the target relation through sampling, and use the relation path as a bridge to simultaneously utilize text and multi-hop graph structure information. Moreover, the relation path serves as a tool for mining associations between relations. At the same time, due to the design of entity-independent relation paths, our model can also handle the inductive setting. Our experiments on three benchmarks, along with extensive analysis, demonstrate that our model not only achieves substantial performance enhancements across four metrics compared to other models but also adeptly handles inductive settings.},
  keywords={Training;Measurement;Knowledge engineering;Bridges;Analytical models;Knowledge graphs;Transforms;Benchmark testing;Market research;Graph neural networks;Temporal Knowledge Graph;Knowledge Graph Completion;Pre-train Language Model},
  doi={10.1109/IJCNN60899.2024.10650003},
  ISSN={2161-4407},
  month={June},}

@INPROCEEDINGS{10476299,
  author={Zhai, Guohao and Pei, Songwen},
  booktitle={2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={Knowledge Enhancement and Feature Purification for Single-stage Joint Entity and Relation Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={1733-1740},
  abstract={Joint entity and relation extraction aim to achieve named entity recognition and relation extraction in unstructured text. We use the form of triples (subject, relation, object) to describe entity and relation. Joint entity and relation extraction play an important role in knowledge graph construction, question and answering, data analysis, and other natural language processing domains. Most of the existing works suffered from insufficient interaction between entity features and relation features due to the extraction order. Moreover, there is a prevalence of heterogeneous representations of entities and relations. They not only increase the amount of model design and the complexity of training, but also lead to the problem of exposure bias due to the extraction order. Therefore, in this paper, we propose KFRel, where we enhance the entity and relation representations by encoding text and relation. Then, the features are fused to enhance the entity and relation representations, and adopt a feature purification module, where the features are enhanced by a feature purification module that removes feature information irrelevant to the joint entity and relation extraction while retaining feature information of relevance. In addition, we adopt a single-stage joint entity and relation extraction module to address the issue of overlapping triples. This module aims to help improve the effectiveness of joint entity and relation extraction. Comprehensive experiments are conducted on two widely-used datasets, and the experimental results demonstrate that the proposed method is effective and outperforms the state-of-the-art baselines.},
  keywords={Training;Data analysis;Purification;Semantics;Focusing;Knowledge graphs;Feature extraction;Joint Entity and Relation Extraction;Knowledge Enhancement;Feature Purification},
  doi={10.1109/ICPADS60453.2023.00241},
  ISSN={2690-5965},
  month={Dec},}@INPROCEEDINGS{10381546,
  author={Yao, Yutong and Potikas, Petros and Potika, Katerina},
  booktitle={2023 IEEE Sixth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)}, 
  title={Knowledge Graphs for Textbooks: Extraction and Completion Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={38-45},
  abstract={This paper aims to apply knowledge graph construction techniques to textbooks, explicitly focusing on the challenge of the absence of domain-specific schema for each textbook. Various entity and relation extraction models are utilized to capture logical and semantic information related to the textbook’s topic. These models include a Text-Encoding-Initiative (TEI) model to extract hierarchical concepts, spaCy Natural Language Processing (NLP), and Google Cloud Natural Language to extract semantic information from the main textual content. The study includes a case study on a cloud computing textbook, where each approach is evaluated and analyzed. Ultimately, the goal is to create knowledge graphs of textbooks, enabling the completion task of predicting missing entities or relations in a low-dimensional space.},
  keywords={Knowledge engineering;Cloud computing;Face recognition;Computational modeling;Semantics;Pipelines;Knowledge graphs;Knowledge Graphs;Natural Language Processing;Textbook analysis;Entity recognition;Relation extraction;Domain-enrichment},
  doi={10.1109/AIKE59827.2023.00014},
  ISSN={2831-7203},
  month={Sep.},}@INPROCEEDINGS{10447566,
  author={Yang, Caihua and Bao, Jianzhu and Liang, Bin and Xu, RuiFeng},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Enhancing Argumentative Relation Classification by Multi-Granularity Retrieval and Heterogeneous Graph Reasoning}, 
  year={2024},
  volume={},
  number={},
  pages={12772-12776},
  abstract={Argumentative relation classification (ARC) aims to identify the relation between arguments. Previous methods that employ structured knowledge graphs to tackle the ARC task have achieved promising results. However, the prerequisite for structured knowledge to function is that the knowledge includes the topics of arguments. In practice, the topics of arguments are constantly emerging, making it impractical to construct a structured knowledge graph that contains all potential topics in advance. To address this issue, we investigate ARC from a novel perspective by utilizing unstructured knowledge to enhance the learning of ARC, where useful information for topics and arguments could be flexibly obtained from unstructured knowledge. Specifically, to retrieve diverse and comprehensive knowledge for topics and arguments, we first propose a multi-granularity retrieval method tailored for ARC, which acquires unstructured knowledge by dense retrieval at three levels of granularity: the concept level, the concept relation level, and the argument level. Further, we introduce a Knowledge-aware Heterogeneous Graph Reasoner (KHGR), which enables better utilization of retrieved knowledge to facilitate ARC. Extensive experiments on three publicly available datasets verify the superiority of our model compared with several state-of-the-art baselines. Further analysis shows that our method yields more significant benefits in low-resource scenarios.},
  keywords={Knowledge graphs;Signal processing;Benchmark testing;Cognition;Acoustics;Task analysis;Speech processing;argumentative relation classification;unstructured knowledge retrieval;heterogeneous graph reasoning},
  doi={10.1109/ICASSP48485.2024.10447566},
  ISSN={2379-190X},
  month={April},}@INPROCEEDINGS{10761764,
  author={Gan, Lu and Wang, Peng and Zhang, Di and Cao, Yiqun and Zhou, Liwan and Zhang, Yongjun},
  booktitle={2024 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia)}, 
  title={Application Analysis and Prospect of GPT-enabled Digital Transformation of Power Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The rapid development of Generative Pre-trained Transformer (GPT) models brings both opportunities and challenges to power system digital and intelligence transformation. Starting with the summarization of the concept and technical architecture of GPT models, four kinds of potential application scenarios in new power systems related to GPT models are presented. Then, the integration of GPT models with several typical digital technologies in new power systems are reviewed, such as computer vision, human-computer interaction, knowledge graph, Internet of Things, and Blockchain. Finally, the challenge and development direction of GPT models for new power systems are prospected. The comprehensive analysis could provide theoretical and application guidance for the development of GPT-enabled models in new power systems.},
  keywords={Human computer interaction;Analytical models;Computer vision;Computational modeling;Knowledge graphs;Predictive models;Data models;Power systems;Blockchains;Internet of Things;new power system;digital transformation;large language model;GPT model;application scenarios},
  doi={10.1109/ICPSAsia61913.2024.10761764},
  ISSN={},
  month={July},}@ARTICLE{9237126,
  author={Eckhart, Matthias and Ekelhart, Andreas and Weippl, Edgar},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Automated Security Risk Identification Using AutomationML-Based Engineering Data}, 
  year={2022},
  volume={19},
  number={3},
  pages={1655-1672},
  abstract={Systems integrators and vendors of industrial components need to establish a security-by-design approach, which includes the assessment and subsequent treatment of security risks. However, conducting security risk assessments along the engineering process is a costly and labor-intensive endeavor due to the complexity of the system(s) under consideration and the lack of automated methods. This, in turn, hampers the ability of security analysts to assess risks pertaining to cyber-physical systems (CPSs) in an efficient manner. In this work, we propose a method that automatically identifies security risks based on the CPS's data representation, which exists within engineering artifacts. To lay the foundation for our method, we present security-focused semantics for the engineering data exchange format AutomationML (AML). These semantics enable the reuse of security-relevant know-how in AML artifacts by means of a formal knowledge representation, modeled with a security-enriched ontology. Our method is capable of automating the identification of security risk sources and potential consequences in order to construct cyber-physical attack graphs that capture the paths adversaries may take. We demonstrate the benefits of the proposed method through a case study and an open-source prototypical implementation. Finally, we prove that our solution is scalable by conducting a rigorous performance evaluation.},
  keywords={Security;IEC Standards;Risk management;Topology;Semantics;Data models;Knowledge engineering;Cyber-physical systems;information security;AutomationML;security modeling;security risk assessment;industrial control systems;IEC 62443},
  doi={10.1109/TDSC.2020.3033150},
  ISSN={1941-0018},
  month={May},}@INPROCEEDINGS{8088273,
  author={Mordecai, Yaniv and Dori, Dov},
  booktitle={2017 IEEE International Systems Engineering Symposium (ISSE)}, 
  title={Model-based requirements engineering: Architecting for system requirements with stakeholders in mind}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={Specifying system requirements (SysReqs) is a critical activity in complex systems development. The SysReqs and emerging architecture are constructed through gradual and iterative transition from the problem domain and operational stakeholder requirements to the conceptual solution domain. They later constitute the basis for functional requirements elaborating, concept formation, technology selection, function-to-form allocation, and asset utilization. Only rarely can stakeholder requirements (SHRs) readily translate to SysReqs. Systems engineers must therefore elicit, analyze, and evolve the SysReqs, as these will radically affect the system's performance, robustness, endurance, and appeal. Model-Based Systems Engineering (MBSE) provides a framework for effective and consistent systems engineering and architecting. MBSE relies on modeling languages, such as Object-Process Methodology (OPM). OPM is a holistic MBSE paradigm and language for complex systems and processes, standardized as ISO 19450, which relies on the principle of minimal universal ontology. In this paper, we propose a model-based requirement engineering (MBRE) approach to facilitate the transition from SHRs to SysReqs, and from SysReqs to system architecture specification. We demonstrate the applicability of this framework in architecting a robotic baggage loading system for a leading international airport.},
  keywords={Modeling;Stakeholders;Unified modeling language;Requirements engineering;Standards;Complex systems;Model-Based Systems Engineering;Object-Process Methodology;Requirements Engineering;Systems Architecting},
  doi={10.1109/SysEng.2017.8088273},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8241499,
  author={Wally, Bernhard and Huemer, Christian and Mazak, Alexandra},
  booktitle={2017 IEEE 10th Conference on Service-Oriented Computing and Applications (SOCA)}, 
  title={Aligning Business Services with Production Services: The Case of REA and ISA-95}, 
  year={2017},
  volume={},
  number={},
  pages={9-17},
  abstract={Industrie 4.0 aims at flexible production networks that require horizontal integration across companies. Evidently, any production related information exchanged in the network must be vertically forwarded to the corresponding service endpoints of the local production system. Accordingly, there is a need to align information that flows between companies and within each company. The Resource-Event-Agent (REA) business ontology describes a metamodel for internal business activities (e.g., production) and for inter-organizational exchange constellations on the enterprise resource planning (ERP) level. ISA-95 is a series of standards targeting the integration of enterprise control systems on the interface between ERP systems and manufacturing execution systems. Consequently, we align elements of REA and ISA-95 and define conversion rules for the transformation of elements from one system to the other. By interleaving the semantics of both standards, we formally strengthen the links between the services of the business level and the production level, and support multi-system adaptation in flexible production environments.},
  keywords={Production;Manufacturing;Unified modeling language;IEC Standards;Companies;REA;ISA-95;Metamodel Alignment},
  doi={10.1109/SOCA.2017.10},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9534250,
  author={Choudhary, Rishabh and Doboli, Simona and Minai, Ali A.},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Comparative Study of Methods for Visualizable Semantic Embedding of Small Text Corpora}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Text embedding has recently emerged as a very useful and successful method for semantic representation. Following initial word-level embedding methods such as Latent Semantic Analysis (LSA) and topic-based bag-of-words approaches like Latent Dirichlet Allocation (LDA), the focus has turned to language models and text encoders implemented as neural networks - ranging from word-level models to those embedding whole documents. The distinctive feature of these models is their ability to infer semantic spaces at all levels based purely on data, with no need for complexities such as syntactic analysis or ontology building. Many of these models are available pre-trained on enormous amounts of data, providing downstream applications with general-purpose semantic spaces. In particular, embedding models at the sentence level or higher are most useful in applications because the meaning of text only becomes clear at that level. Most text embedding methods produce text embeddings in high-dimensional spaces, with a dimensionality ranging from a few hundred to thousands. However, it is often useful to visualize semantic spaces in very low dimension, which requires the use of dimensionality reduction methods. It is not clear what language models and what method of dimensionality reduction would work well in these cases. In this paper, we compare four text embedding methods in combination with three methods of dimensionality reduction to map three related real-world datasets comprising textual descriptions of items in a particular domain (sports) to a 2-dimensional semantic visualization space. The results provide several insights into the utility of these methods for data of this type.},
  keywords={Dimensionality reduction;Training;Analytical models;Visualization;Semantics;Neural networks;Bit error rate;semantic spaces;text embedding;language models;semantic visualization},
  doi={10.1109/IJCNN52387.2021.9534250},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{8501495,
  author={Rabinia, Amin and Ghanavati, Sepideh},
  booktitle={2018 IEEE 8th International Model-Driven Requirements Engineering Workshop (MoDRE)}, 
  title={The FOL-Based Legal-GRL (FLG) Framework: Towards an Automated Goal Modeling Approach for Regulations}, 
  year={2018},
  volume={},
  number={},
  pages={58-67},
  abstract={In recent years, several goal modeling approaches have been used and extended to capture the complexity of legal requirements and help modeling them in notations familiar to the requirements engineers and analysts. Legal-GRL, which is an extension of the Goal-oriented Requirements Language (GRL), is used for modeling and analyzing legal requirements. However, creating Legal-GRL models is still a manual process, which limits its effectiveness and scalability. In this paper, we propose a new goal modeling framework based on GRL to facilitate the automation of the legal requirements modeling process. Our FOL-based Legal-GRL (FLG) framework uses a legal ontology, which entails a modal theory and First-order Logic (FOL) approach, for the purpose of extraction, refinement, and representation of legal requirements. Our FLG framework consists of a database design and a set of methods for automating the modeling process. We evaluate our work by modeling several statements from HIPAA, PHIPA, the EU GDPR and EU-US Privacy Shield.},
  keywords={Law;Ontologies;Analytical models;Complexity theory;Computational modeling;Databases;Goal Modeling, Legal Requirements, First Order Logic, GRL},
  doi={10.1109/MoDRE.2018.00014},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10309534,
  author={Nanwani, Laksh and Agarwal, Anmol and Jain, Kanishk and Prabhakar, Raghav and Monis, Aaron and Mathur, Aditya and Jatavallabhula, Krishna Murthy and Abdul Hafez, A. H. and Gandhi, Vineet and Krishna, K. Madhava},
  booktitle={2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={Instance-Level Semantic Maps for Vision Language Navigation}, 
  year={2023},
  volume={},
  number={},
  pages={507-512},
  abstract={Humans have a natural ability to perform semantic associations with the surrounding objects in the environment. This allows them to create a mental map of the environment, allowing them to navigate on-demand when given linguistic instructions. A natural goal in Vision Language Navigation (VLN) research is to impart autonomous agents with similar capabilities. Recent works take a step towards this goal by creating a semantic spatial map representation of the environment without any labeled data. However, their representations are limited for practical applicability as they do not distinguish between different instances of the same object. In this work, we address this limitation by integrating instance-level information into spatial map representation using a community detection algorithm and utilizing word ontology learned by large language models (LLMs) to perform open-set semantic associations in the mapping representation. The resulting map representation improves the navigation performance by two-fold (233%) on realistic language commands with instance-specific descriptions compared to the baseline. We validate the practicality and effectiveness of our approach through extensive qualitative and quantitative experiments.},
  keywords={Measurement;Visualization;Three-dimensional displays;Navigation;Semantics;Linguistics;Ontologies},
  doi={10.1109/RO-MAN57019.2023.10309534},
  ISSN={1944-9437},
  month={Aug},}@INPROCEEDINGS{8945629,
  author={Daneth, Horn and Ali, Nazakat and Hong, Jang-Eui},
  booktitle={2019 26th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Automatic Identifying Interaction Components in Collaborative Cyber-Physical Systems}, 
  year={2019},
  volume={},
  number={},
  pages={197-203},
  abstract={Due to diverse set of heterogeneous computing devices communicating with one another and fusing with physical components in Cyber-Physical Systems, software engineers may use different tools and/or modeling languages to formally describe or verify the system properties. As a result, the integration of these diverse constituents poses key challenges such as task for identifying interactions of components to be synthesized for a function in the systems. Although existing studies such as ontology and integration semantic languages have been used for specifying interactions of components in a Cyber-Physical System, these are still not applicable to discover the component interactions in collaborative Cyber-Physical Systems. It is due to the fact that functionalities of Cyber-Physical Systems are generally realized through interactions among multiple systems in a collaborative environment. This paper proposes a model interaction language, CyPhyML+ which can identify component interactions of realized functions in collaborative Cyber-Physical Systems. We show the proposed approach validity and applicability via an Automatic Incident Detection System.},
  keywords={Safety;Finite element analysis;Collaboration;Cyber-physical systems;Computational modeling;Software;Logic gates;Model Interaction Language, Component Interactions , Cyber-Physical Systems},
  doi={10.1109/APSEC48747.2019.00035},
  ISSN={2640-0715},
  month={Dec},}@INPROCEEDINGS{9679873,
  author={Sakhrani, Harsh and Parekh, Saloni and Ratadiya, Pratik},
  booktitle={2021 International Conference on Data Mining Workshops (ICDMW)}, 
  title={Transformer-based Hierarchical Encoder for Document Classification}, 
  year={2021},
  volume={},
  number={},
  pages={852-858},
  abstract={Document Classification has a wide range of applications in various domains like Ontology Mapping, Sentiment Analysis, Topic Categorization and Document Clustering, to mention a few. Unlike Text Classification, Document Classification works with longer sequences that typically contain multiple paragraphs. Previous approaches for this task have achieved promising results, but have often relied on complex recurrence mechanisms that are expensive and time-consuming in nature. Recently, self-attention based models like Transformers and BERT have achieved state-of-the-art performance on several Natural Language Understanding (NLU) tasks, but owing to the quadratic computational complexity of the self-attention mechanism with respect to the input sequence length, these approaches are generally applied to shorter text sequences. In this paper, we address this issue, by proposing a new Transformer-based Hierarchical Encoder approach for the Document Classification task. The hierarchical framework we adopt helps us extend the self-attention mechanism to long-form text modelling thereby reducing the complexity considerably. We use the Bidirectional Transformer Encoder (BTE) at the sentence-level to generate a fixed-size sentence embedding for each sentence in the document. A document-level Transformer Encoder is then used to model the global document context and learn the inter-sentence dependencies. We also carry out experiments with the BTE in a feature-extraction and a fine-tuning setup, allowing us to evaluate the trade-off between computation power and accuracy. Furthermore, we also conduct ablation experiments, and evaluate the impact of different pre-training strategies on the overall performance. Experimental results demonstrate that our proposed model achieves state-of-the-art performance on two standard benchmark datasets.},
  keywords={Training;Sentiment analysis;Computational modeling;Transfer learning;Text categorization;Natural languages;Ontologies;Transformer;Self-attention;Document Classification},
  doi={10.1109/ICDMW53433.2021.00109},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{9582510,
  author={Odukoya, Kofoworola Adebowale and Whitfield, Robert Ian and Hay, Laura and Harrison, Neil and Robb, Malcolm},
  booktitle={2021 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={An Architectural Description For The Application Of Mbse In Complex Systems}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The design of a complex warship is a multidisciplinary effort which often encounters major challenges, particularly with respect to integration across interfaces in the System of Systems (SoS). In principle, the goal of Model Based Systems Engineering (MBSE) with respect to system design is to provide a means of capturing and communicating the system design in a structured, consistent, and coherent fashion; that can be easily assessed by engineering teams and quickly analysed using queries and toolsets. The focus of this paper is to investigate the potential to achieve a consistent description, identify a viable methodology that minimises mismatch in requirements and to avoid an extended design lifecycle. This study highlights the need to develop a generic Architectural Description (AD) that is based on a common ontology which would clearly define the fundamental tenets of applying state-of-the-art Architectural Frameworks (AFs) in naval ship design. An investigation on the effectiveness and accuracy of a graph-based approach is needed to assess whether it is possible to create a ‘Rosetta stone’ for AFs, which links any two or more different model viewpoints in different AF’s using the approach.},
  keywords={Analytical models;Ontologies;Stakeholders;Marine vehicles;Complex systems;System analysis and design;System of systems;Systems architecture;System of systems;Complex systems1},
  doi={10.1109/ISSE51541.2021.9582510},
  ISSN={2687-8828},
  month={Sep.},}@INPROCEEDINGS{10020339,
  author={Wullschleger, Pascal and Lionetti, Simone and Daly, Donnacha and Volpe, Francesca and Caro, Grégoire},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Auto-Regressive Self-Attention Models for Diagnosis Prediction on Electronic Health Records}, 
  year={2022},
  volume={},
  number={},
  pages={1950-1956},
  abstract={Insufficient data and data lacking the diversity to represent the general public is a common challenge when modelling diagnosis prediction. We consider a much larger and more diverse database of commercial Electronic Health Records than what is prevalent in the literature. We formulate a simplified version of diagnosis prediction that focuses on major developments in medical histories of patients. To this end, we leverage Auto-Regressive Self-Attention models that have seen promising applications in language modelling and extend them to incorporate ontological representations of medical codes. Additionally, we include time-intervals between diagnoses into the attention calculation. We evaluate models and baselines at different levels of diagnostic granularity and our results suggest that using very detailed clinical classifications does not significantly degrade performance, possibly allowing their use in practice. Our model outperforms all baselines and we suggest that leveraging the ontology for generating diagnosis representations is mostly helpful for rare diagnoses.},
  keywords={Codes;Databases;Predictive models;Big Data;Ontologies;Data models;History;Electronic Health Record;Transformer;Ontological Representation;Diagnosis Prediction},
  doi={10.1109/BigData55660.2022.10020339},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10651359,
  author={Liu, Xu and Chen, Xinming and Zhu, Yangfu and Wu, Bin},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Prompt-Enhanced Prototype Framework for Few-shot Event Detection}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={Few-shot event detection (ED) aims at identifying and typing event mentions from text with limited annotations. Most existing methods for few-shot ED use event ontology and related knowledge to construct prototypes and fail to fully leverage the rich knowledge of pre-trained language models (PLMs) which could help improve the representation of prototypes. Motivated by this, we propose an prompt-enhanced prototype framework which combines prototype and prompt for few-shot ED. Considering the scarcity of labeled data, we also introduce contrastive learning to enrich prototypes. Specifically, we use heuristic rules to align FrameNet with annotated data to get corresponding prompts for each event and convert them into prompt prototype. We then leverage contrastive learning to aggregate event mentions into prototypes and maintain these prototypes for few-shot ED. Furthermore, We explore diverse prompt formats for representing prompt prototypes and introduce a more comprehensive lexical prompt which improves the performance of few-shot ED. We conduct extensive experiments on the MAVEN corpus to reveal the effectiveness of the proposed framework compared to state-of-the-art methods.},
  keywords={Event detection;Annotations;Aggregates;Semantics;Neural networks;Prototypes;Contrastive learning},
  doi={10.1109/IJCNN60899.2024.10651359},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10655304,
  author={Ma, Jiawei and Huang, Po-Yao and Xie, Saining and Li, Shang-Wen and Zettlemoyer, Luke and Chang, Shih-Fu and Yih, Wen-Tau and Xu, Hu},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={MoDE: CLIP Data Experts via Clustering}, 
  year={2024},
  volume={},
  number={},
  pages={26344-26353},
  abstract={The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web- crawled data. We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering. Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters. At inference time, we ensemble their outputs by applying weights determined through the correlation between task metadata and cluster conditions. To estimate the correlation pre-cisely, the samples in one cluster should be semantically similar, but the number of data experts should still be rea-sonable for training and inference. As such, we consider the ontology in human language and propose to use fine- grained cluster centers to represent each data expert at a coarse-grained level. Experimental studies show that four CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and OpenCLIP on zero-shot image classification but with less (<35%) training cost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly include new data experts. The code is available here.},
  keywords={Training;Adaptation models;Correlation;Costs;Computational modeling;Noise;Semantics;Data Expert;Multi-Modal;Data Clustering},
  doi={10.1109/CVPR52733.2024.02489},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10357717,
  author={Sevastjanova, Rita and Vogelbacher, Simon and Spitz, Andreas and Keim, Daniel and El-Assady, Mennatallah},
  booktitle={2023 IEEE Visualization in Data Science (VDS)}, 
  title={Visual Comparison of Text Sequences Generated by Large Language Models}, 
  year={2023},
  volume={},
  number={},
  pages={11-20},
  abstract={Causal language models have emerged as the leading technology for automating text generation tasks. Although these models tend to produce outputs that resemble human writing, they still suffer from quality issues (e.g., social biases). Researchers typically use automatic analysis methods to evaluate the model limitations, such as statistics on stereotypical words. Since different types of issues are embedded in the model parameters, the development of automated methods that capture all relevant aspects remains a challenge. To tackle this challenge, we propose a visual analytics approach that supports the exploratory analysis of text sequences generated by causal language models. Our approach enables users to specify starting prompts and effectively groups the resulting text sequences. To this end, we leverage a unified, ontology-driven embedding space, serving as a shared foundation for the thematic concepts present in the generated text sequences. Visual summaries provide insights into various levels of granularity within the generated data. Among others, we propose a novel comparison visualization that slices the embedding space and represents the differences between two prompt outputs in a radial layout. We demonstrate the effectiveness of our approach through case studies, showcasing its potential to reveal model biases and other quality issues.},
  keywords={Analytical models;Visual analytics;Semantics;Layout;Data visualization;Writing;Linguistics;Causal Language Models;Text Generation;Prompt Output Comparison},
  doi={10.1109/VDS60365.2023.00007},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10020509,
  author={Parolin, Erick Skorupa and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D’Orazio, Vito},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Confli-T5: An AutoPrompt Pipeline for Conflict Related Text Augmentation}, 
  year={2022},
  volume={},
  number={},
  pages={1906-1913},
  abstract={Recent advances in natural language processing (NLP) and Big Data technologies have been crucial for scientists to analyze political unrest and violence, prevent harm, and promote global conflict management. Government agencies and public security organizations have invested heavily in deep learning-based applications to study global conflicts and political violence. However, such applications involving text classification, information extraction, and other NLP-related tasks require extensive human efforts in annotating/labeling texts. While limited labeled data may drastically hurt the models’ performance (over-fitting), large demands on annotation tasks may turn real-world applications impracticable. To address this problem, we propose Confli-T5, a prompt-based method that leverages the domain knowledge from existing political science ontology to generate synthetic but realistic labeled text samples in the conflict and mediation domain. Our model allows generating textual data from the ground up and employs our novel Double Random Sampling mechanism to improve the quality (coherency and consistency) of the generated samples. We conduct experiments over six standard datasets relevant to political science studies to show the superiority of Confli-T5. Our codes are publicly available 1.},
  keywords={Text categorization;Standards organizations;Pipelines;Big Data;Natural language processing;Data models;Safety;text augmentation;generation;classification;natural language processing;conflict;coding event data;CAMEO},
  doi={10.1109/BigData55660.2022.10020509},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8808078,
  author={Kreines, Mikhail G. and Kreines, Elena M.},
  booktitle={2019 IEEE 21st Conference on Business Informatics (CBI)}, 
  title={Artificial Intelligence Tools for Business Applications: Objective Map of Science and Analysis of Texts}, 
  year={2019},
  volume={01},
  number={},
  pages={445-451},
  abstract={Business is looking for technological and investment possibilities in research and development (R&D). Here the basic problems are to find R&D's results and/or teams for solving the professional tasks and for making investment. But business has no personal view on scientific problems. So business is seeking the objective tools for forecasting and evaluation of R&D prospects and results. Experts have own interests and require a lot of funding. R&D reflections are the texts. The modern methods of computer analysis of texts can do a lot of the experts' work for making it more objective and cheaper. The tools for search, systematization and ranking R&D's results and teams are computer analysis of texts and the map of science. The map of science is the distribution of the collection of texts of a scientific nature by the topics. The map of science is a way to navigate through the world of scientific publications and R&D's teams, a tool for identifying trends and assessing R&D directions. The usual ways for the map of science formation use bibliometric/scientometric data, general probability models of the texts, expert's opinion or artificial intelligence (AI) models and methods based on the thesaurus or on the ontology of the subject domains. The interests of business are not in line with the orientation on a priori established ideas about possible topics or there number for rapidly changing scientific fields. Precisely these fields are of the greatest interest to business. On the basis of mathematical modeling of texts and large-scale text collections, an approach is proposed for the computational formation of the adaptive dynamic map of science that does not use a priori classification schemes and data of the scientific publications' citation. Topics (thematic groups), their number and the distribution of texts over the topics are determined computationally without experts' involvement. Examples of the maps of science for various collections of scientific publications are given. The original method is proposed for checking the adequacy of the text models and the map of science. The method uses the categorization of articles and their abstracts as the separate objects on the basis of computationally generated map (its topics). The results of the large-scale experiment confirmed the high efficiency of the proposed mathematical modeling of texts and text collections. The possibilities of practical use of the map of science for business applications are considered.},
  keywords={Research and development;Computational modeling;Analytical models;Business;Semantics;Tools;Adaptation models;text;collection;semantics;semiotics;modeling;interpretation;research and development;information retrieval;analytical support},
  doi={10.1109/CBI.2019.00058},
  ISSN={2378-1971},
  month={July},}@ARTICLE{10713368,
  author={Lee, Jinkyu and Kim, Jihie},
  journal={IEEE Access}, 
  title={Improving Commonsense Bias Classification by Mitigating the Influence of Demographic Terms}, 
  year={2024},
  volume={12},
  number={},
  pages={161480-161489},
  abstract={Understanding commonsense knowledge is crucial in the field of Natural Language Processing (NLP). However, the presence of demographic terms in commonsense knowledge poses a potential risk of compromising the performance of NLP models. This study aims to investigate and propose methods for enhancing the performance and effectiveness of a commonsense polarization classifier by mitigating the influence of demographic terms. Three methods are introduced in this paper: (1) hierarchical generalization of demographic terms (2) threshold-based augmentation and (3) integration of hierarchical generalization and threshold-based augmentation methods(IHTA). The first method involves replacing demographic terms with more general ones based on a term hierarchy ontology, aiming to mitigate the influence of specific terms. To address the limited bias-related information, the second method measures the polarization of demographic terms by comparing the changes in the model’s predictions when these terms are masked versus unmasked. This method augments commonsense sentences containing terms with high polarization values by replacing their predicates with synonyms generated by ChatGPT. The third method combines the two approaches, starting with threshold-based augmentation followed by hierarchical generalization. The experiments show that the first method increases the accuracy over the baseline by 2.33%, and the second one by 0.96% over standard augmentation methods. The IHTA techniques yielded an 8.82% and 9.96% higher accuracy than threshold-based and standard augmentation methods, respectively.},
  keywords={Accuracy;Predictive models;Commonsense reasoning;Standards;Ontologies;Chatbots;Training data;Systematics;Semantics;Prevention and mitigation;Demography;Natural language processing;Classification algorithms;Commonsense bias;demographic term;bias mitigation;hierarchical generalization;threshold-based augmentation},
  doi={10.1109/ACCESS.2024.3477599},
  ISSN={2169-3536},
  month={},}@ARTICLE{9032420,
  author={},
  journal={IEEE Std 2413-2019}, 
  title={IEEE Standard for an Architectural Framework for the Internet of Things (IoT)}, 
  year={2020},
  volume={},
  number={},
  pages={1-269},
  abstract={An architecture framework description for the Internet of Things (IoT) which conforms to the international standard ISO/IEC/IEEE 42010:2011 is defined. The architecture framework description is motivated by concerns commonly shared by IoT system stakeholders across multiple domains (transportation, healthcare, Smart Grid, etc.). A conceptual basis for the notion of things in the IoT is provided and the shared concerns as a collection of architecture viewpoints is elaborated to form the body of the framework description.},
  keywords={IEEE Standards;Computer architecture;Internet of Things;architectural framework;IEEE 2413;Internet of Things (IoT)},
  doi={10.1109/IEEESTD.2020.9032420},
  ISSN={},
  month={March},}@INPROCEEDINGS{10805070,
  author={Derin, Mehmet Oguz and Uçar, Erdem and Yergesh, Banu and Shimada, Yuki and Hong, Youjin and Lin, Xin-Yu},
  booktitle={2024 IEEE 3rd International Conference on Problems of Informatics, Electronics and Radio Engineering (PIERE)}, 
  title={Evaluating the Impact of Model Size on Multilingual JSON Structuring for Knowledge Graphs with Recent LLMs}, 
  year={2024},
  volume={},
  number={},
  pages={1890-1895},
  abstract={This study investigates the impact of model size on the multilingual JSON structuring capabilities of commercial Large Language Models (LLMs) for Knowledge Graph creation, emphasizing the integration of expert feedback and in-context learning. Focusing on Old Uyghur and Old Turkic as the subject language and various study or work languages, including Japanese and Kazakh, we evaluated the performance of the latest generation LLMs of two sizes within the same family in structuring complex philological plain text. Our methodology involved a comparative analysis of LLM performance across different model sizes and languages, incorporating expert feedback and in-context learning techniques through a custom-built annotation tool to anonymize the specific LLM size for fair evaluation and integrate structurization and structure translation. Our findings indicate that smaller models can perform comparably to larger, more costly models in JSON structuring tasks when leveraging expert feedback and in-context learning despite struggling with the quality of initial structurization. This trend was consistent across the evaluated work languages, albeit with some performance variations. The study underscores the significant potential of in-context learning and expert feedback in enhancing LLMs' structuring capabilities, particularly for under-resourced languages with unstructured yet comprehensive publications. These results have important implications for efficient and cost-effective Knowledge Graph creation in multilingual contexts, offering new avenues for processing and integrating complex philological data into structured, machine-readable formats.},
  keywords={Knowledge engineering;Analytical models;Translation;Annotations;Large language models;Focusing;Knowledge graphs;Market research;Multilingual;Informatics;Knowledge Graphs;Multilingual Structuring;Large Language Models (LLMs);In-Context Learning;GPT-4o},
  doi={10.1109/PIERE62470.2024.10805070},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10830995,
  author={Li, Jinghong and Phan, Huy and Gu, Wen and Ota, Koichi and Hasegawa, Shinobu},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Fish-Bone Diagram of Research Issue: Gain a Bird's-Eye View on a Specific Research Topic}, 
  year={2024},
  volume={},
  number={},
  pages={4936-4941},
  abstract={Novice researchers often face difficulties in understanding a multitude of academic papers and grasping the fundamentals of a new research field. To solve such problems, the knowledge graph supporting research survey is gradually being developed. Existing keyword-based knowledge graphs make it difficult for researchers to deeply understand abstract concepts. Meanwhile, novice researchers may find it difficult to use ChatGPT effectively for research surveys due to their limited understanding of the research field. Without the ability to ask proficient questions that align with key concepts, obtaining desired and accurate answers from this large language model (LLM) could be inefficient. This study aims to help novice researchers by providing a fish-bone diagram that includes causal relationships, offering an overview of the research topic. The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors. Furthermore, we evaluate the strengths and improvable points of the fish-bone diagram derived from this study's development pattern, emphasizing its potential as a viable tool for supporting research survey.},
  keywords={Surveys;Training;Reviews;Knowledge graphs;Machine learning;Ontologies;User interfaces;Information retrieval;Prompt engineering;Sustainable development},
  doi={10.1109/SMC54092.2024.10830995},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10810521,
  author={Sophaken, Chotanansub and Vongpanich, Kantapong and Intaphan, Wachirawit and Utasri, Tharathon and Deepho, Chutamas and Takhom, Akkharawoot},
  booktitle={2024 8th International Conference on Information Technology (InCIT)}, 
  title={Leveraging Graph-RAG for Enhanced Diagnostic and Treatment Strategies in Dentistry}, 
  year={2024},
  volume={},
  number={},
  pages={606-611},
  abstract={This paper presents a method for extracting and interpreting information from diverse, unstructured dental literature using advanced AI techniques. By integrating information extraction, ontologies, and knowledge graphs, the approach enhances the efficiency and accuracy of dental data analysis. Named Entity Recognition (NER) and a Large Language Model (LLM) are employed to extract relevant entities and relationships, which are then structured into triples and integrated with a dental ontology to ensure contextual relevance. This enriched ontology supports Retrieval-Augmented Generation (RAG) applications, enabling advanced querying and analysis. The methodology improves the identification and categorization of dental conditions, treatments, and anatomical terms, providing a structured representation of dental knowledge. Knowledge graphs facilitate the representation and analysis of relationships between entities, fostering insightful interpretations and supporting hypothesis generation, thereby enhancing the accessibility and usability of dental knowledge. Experimental results demonstrate the effectiveness of this approach in managing complex dental information, showcasing the benefits of combining Knowledge Representation (KR) with Machine Learning (ML). This research contributes to dental studies by offering a robust framework for extracting and utilizing knowledge from diverse and extensive datasets.},
  keywords={Large language models;Retrieval augmented generation;Knowledge graphs;Named entity recognition;Machine learning;Ontologies;Dentistry;Data mining;Usability;Information technology;Information Extraction;Dental Literature;Large Language Models;Ontologies;Knowledge Graphs;Oral Health;Dentistry},
  doi={10.1109/InCIT63192.2024.10810521},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10727135,
  author={Maninger, Daniel and Narasimhan, Krishna and Mezini, Mira},
  booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)}, 
  title={Towards Trustworthy AI Software Development Assistance}, 
  year={2024},
  volume={},
  number={},
  pages={112-116},
  abstract={It is expected that in the near future, AI software development assistants will play an important role in the software industry. However, current software development assistants tend to be unreliable, often producing incorrect, unsafe, or low-quality code. We seek to resolve these issues by introducing a holistic architecture for constructing, training, and using trustworthy AI software development assistants. In the center of the architecture, there is a foundational LLM trained on datasets representative of real-world coding scenarios and complex software architectures, and fine-tuned on code quality criteria beyond correctness. The LLM will make use of graph-based code representations for advanced semantic comprehension. We envision a knowledge graph integrated into the system to provide up-to-date background knowledge and to enable the assistant to provide appropriate explanations. Finally, a modular framework for constrained decoding will ensure that certain guarantees (e.g., for correctness and security) hold for the generated code.},
  keywords={Training;Codes;Software architecture;Semantics;Computer architecture;Knowledge graphs;Software;Security;Artificial intelligence;Software development management},
  doi={10.1145/3639476.3639770},
  ISSN={2832-7632},
  month={April},}@INPROCEEDINGS{10822514,
  author={Yang, Wenjun and Shen, Yiqing and Wang, Zehong and Zhao, Rui and Lu, Qitong and Liu, Xinsheng and Liu, Yungeng and Debbah, Merouane and Wang, Yu Guang and Li Wang, Shir},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={FalconProtein: Finetuning Falcon Foundation Model for Protein Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={5409-5417},
  abstract={Large Language Models (LLMs) have demonstrated zero-shot generalization capabilities in analyzing and predicting protein properties through natural language interactions. However, existing protein-focused datasets for LLM fine-tuning, such as ProteinLMDataset with its 17.46 billion tokens for pre-training and 893,000 instructions for fine-tuning, face limitations. Specifically, they include insufficient coverage of protein functional properties, inadequate protein-protein interaction data, and limited integration of contextual information from biomedical literature. To overcome these challenges, we present ProteinPFAIDataset, which integrates data from UniProt and PubMed, comprising 72.8 million tokens for Supervised Fine-Tuning (SFT). ProteinPFAIDataset encompasses important protein characteristics including enzyme activities, molecular functions, pH dependence, tissue specificity, temperature sensitivity, subunit structure, and disease associations. Additionally, we propose a novel knowledge graph-based approach that incorporates over 300,000 biomedical literature entries, providing rich contextual information about protein functions and interactions. To validate the effectiveness of our dataset, we fine-tuned Falcon2-11B LLM, resulting in a model we call Falcon2-11B-PFAI. The fine-tuned model achieved state-of-the-art performance on ProteinLMBench, improving accuracy from 47.10% to 58.37%. The dataset is available at https://huggingface.co/datasets/xiaorui1/PFAI. The fine-tuned model is available at https://huggingface.co/xiaorui1/PFAI/tree/main.},
  keywords={Proteins;Temperature sensors;Protein engineering;Temperature dependence;Accuracy;Temperature;Sensitivity;Large language models;Biological system modeling;Protein sequence},
  doi={10.1109/BIBM62325.2024.10822514},
  ISSN={2156-1133},
  month={Dec},}@INPROCEEDINGS{10722791,
  author={Haque, Mohd Ariful and Kamal, Marufa and George, Roy and Gupta, Kishor Datta},
  booktitle={2024 IEEE 11th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Utilizing Structural Metrics from Knowledge Graphs to Enhance the Robustness Quantification of Large Language Models (Extended Abstract)}, 
  year={2024},
  volume={},
  number={},
  pages={1-2},
  abstract={The goal of this study is to determine whether large language models (LLMs) like CodeLlama, Mistral, and Vicuna can be used to build knowledge graphs (KGs) from textual data. We create class descriptions for well-known KGs such as DBpedia, YAGO, and Google Knowledge Graph, from which we extract RDF triples and enhance these graphs using different preprocessing methods. Six structural quality measures are used in the study to compare the constructed and existing KGs. Our results demonstrate how important LLMs are to improving KG construction and provide insightful information for KG construction researchers. Moreover, an in-depth analysis of popular open-source LLM models enables researchers to identify the most efficient model for various tasks, ensuring optimal performance in specific applications.},
  keywords={Measurement;Analytical models;Large language models;Knowledge graphs;Ontologies;Data science;Resource description framework;Robustness;Internet},
  doi={10.1109/DSAA61799.2024.10722791},
  ISSN={2766-4112},
  month={Oct},}@INPROCEEDINGS{10411620,
  author={Shi, Yucheng and Ma, Hehuan and Zhong, Wenliang and Tan, Qiaoyu and Mai, Gengchen and Li, Xiang and Liu, Tianming and Huang, Junzhou},
  booktitle={2023 IEEE International Conference on Data Mining Workshops (ICDMW)}, 
  title={ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs}, 
  year={2023},
  volume={},
  number={},
  pages={515-520},
  abstract={ChatGPT, as a recently launched large language model (LLM), has shown superior performance in various natural language processing (NLP) tasks. However, two major limitations hinder its potential applications: 1) the inflexibility of finetuning on downstream tasks, and 2) the lack of interpretability in the decision-making process. To tackle these limitations, we propose a novel framework that leverages the power of ChatGPT for specific tasks, such as text classification, while improving its interpretability. The proposed framework conducts a knowledge graph extraction task to extract refined and structural knowledge from the raw data using ChatGPT. The rich knowledge is then converted into a graph, which is further used to train an interpretable linear classifier to make predictions. To evaluate the effectiveness of our proposed method, we conduct experiments on four benchmark datasets. The results demonstrate that our method can significantly improve the prediction performance compared to directly utilizing ChatGPT for text classification tasks. Furthermore, our method provides a more transparent decision-making process compared with previous text classification methods. The code is available at https://github.com/sycny/ChatGraph.},
  keywords={Text categorization;Decision making;Knowledge graphs;Chatbots;Data models;Data mining;Task analysis;Text Classification;Large Language Models;Interpretability},
  doi={10.1109/ICDMW60847.2023.00073},
  ISSN={2375-9259},
  month={Dec},}@INPROCEEDINGS{10801853,
  author={Cornelio, Cristina and Diab, Mohammed},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery}, 
  year={2024},
  volume={},
  number={},
  pages={12435-12442},
  abstract={Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor’s logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io.},
  keywords={Costs;Large language models;Ontologies;Intelligent robots},
  doi={10.1109/IROS58592.2024.10801853},
  ISSN={2153-0866},
  month={Oct},}@INPROCEEDINGS{9216770,
  author={Jiang, Chen and Jagersand, Martin},
  booktitle={2020 IEEE 16th International Conference on Automation Science and Engineering (CASE)}, 
  title={Bridging Visual Perception with Contextual Semantics for Understanding Robot Manipulation Tasks}, 
  year={2020},
  volume={},
  number={},
  pages={1447-1452},
  abstract={Understanding manipulation scenarios allows intelligent robots to plan for appropriate actions to complete a manipulation task successfully. It is essential for intelligent robots to semantically interpret manipulation knowledge by describing entities, relations and attributes in a structural manner. In this paper, we propose an implementing framework to generate high-level conceptual dynamic knowledge graphs from video clips. A combination of a Vision-Language model and an ontology system, in correspondence with visual perception and contextual semantics, is used to represent robot manipulation knowledge with Entity-Relation-Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and well-versed. Using the framework, we present a case study where robot performs manipulation actions in a kitchen environment, bridging visual perception with contextual semantics using the generated dynamic knowledge graphs.},
  keywords={Semantics;Ontologies;Manipulator dynamics;Visual perception;Robot kinematics;Task analysis},
  doi={10.1109/CASE48305.2020.9216770},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10782119,
  author={Munzir, Syed I. and Hier, Daniel B. and Carrithers, Michael D.},
  booktitle={2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models}, 
  year={2024},
  volume={},
  number={},
  pages={1-5},
  abstract={Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past 30 years, progress toward making high-throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping physician notes.Clinical relevance: Large language models will likely emerge as the dominant method for the high throughput phenotyping of signs and symptoms in physician notes},
  keywords={Accuracy;Large language models;Biological system modeling;Medical services;Machine learning;Ontologies;Assistive technologies;Throughput;Vectors;Electronic medical records},
  doi={10.1109/EMBC53108.2024.10782119},
  ISSN={2694-0604},
  month={July},}@INPROCEEDINGS{10679445,
  author={von der Assen, Jan and Sharif, Jamo and Feng, Chao and Killer, Christian and Bovet, Gérôme and Stiller, Burkhard},
  booktitle={2024 IEEE International Conference on Cyber Security and Resilience (CSR)}, 
  title={Asset-Centric Threat Modeling for AI-Based Systems}, 
  year={2024},
  volume={},
  number={},
  pages={437-444},
  abstract={Threat modeling for systems relying on Artificial In-telligence is not well explored. While conventional threat modeling methods and tools do not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice. Consequently, this paper presents ThreatFinderAI, an approach and tool providing guidance and automation to model AI-related assets, threats, countermeasures, and quantify residual risks. To do so, ThreatFinderAI presents a novel AI-based stencil library for automated asset extraction, a threat knowledge graph spanning several community initiatives, and a novel method to identify business impacts of AI threats and an approach to quantify them. To evaluate the practicality of the approach, participants were tasked to recreate a threat model developed by cybersecurity experts of an AI-based healthcare platform. Secondly, the approach was used to identify and discuss strategic risks in an LLM-based application through a case study. Overall, the solution's usability was well-perceived and effectively supports threat identification and risk discussion.},
  keywords={Threat modeling;Automation;Medical services;Knowledge graphs;Libraries;Artificial intelligence;Usability;AI Security;Threat Modeling;Risk Analysis},
  doi={10.1109/CSR61664.2024.10679445},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10825705,
  author={Purohit, Sumit and Chin, George and Mackey, Patrick S and Cottam, Joseph A},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={GraphAide: Advanced Graph-Assisted Query and Reasoning System}, 
  year={2024},
  volume={},
  number={},
  pages={3485-3493},
  abstract={Curating knowledge from multiple siloed sources that contain both structured and unstructured data is a major challenge in many real-world applications. Pattern matching and querying represent fundamental tasks in modern data analytics that leverage this curated knowledge. The development of such applications necessitates overcoming several research challenges, including data extraction, named entity recognition, data modeling, and designing query interfaces. Moreover, the explainability of these functionalities is critical for their broader adoption.The emergence of Large Language Models (LLMs) has accelerated the development lifecycle of new capabilities. Nonetheless, there is an ongoing need for domain-specific tools tailored to user activities. The creation of such digital assistants has gained considerable traction in recent years, with LLMs offering a promising avenue to develop such assistants utilizing domain-specific knowledge and assumptions.In this context, we introduce an advanced query and reasoning system, GraphAide, which constructs a knowledge graph (KG) from diverse sources and allows to query and reason over the resulting KG. GraphAide harnesses both the KG and LLMs to rapidly develop domain-specific digital assistants. It integrates design patterns from retrieval augmented generation (RAG) and the semantic web to create an agentic LLM application. GraphAide underscores the potential for streamlined and efficient development of specialized digital assistants, thereby enhancing their applicability across various domains.},
  keywords={Semantic Web;Accuracy;Large language models;Scalability;Retrieval augmented generation;Semantics;Knowledge graphs;Cognition;Usability;Pattern matching},
  doi={10.1109/BigData62323.2024.10825705},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{9949792,
  author={Lu, Yao and Feng, Ding and Sun, Xiaojun and Lin, Sheng},
  booktitle={2022 IEEE/IAS Industrial and Commercial Power System Asia (I&CPS Asia)}, 
  title={Research on Efficient Information Mining Method for Defect Record of Traction Power Supply Equipment}, 
  year={2022},
  volume={},
  number={},
  pages={109-114},
  abstract={The defect record of traction power supply equipment not only describes the defect situation of the equipment, but also provides historical experience for the defect treatment of similar equipment. The recognition of named entity information of the defect record is the premise of constructing the knowledge graph. In order to mine the entity information of defect records more efficiently and accurately, this paper proposes a named entity recognition algorithm to solve the current situation of lack of utilization and analysis of defect records of traction power supply equipment and low efficiency of manual information processing modes. The bidirectional encoder representation from transformers (BERT) pre-training language model is used as the encoding layer of the word vector, the bidirectional long short term memory (BiLSTM) is used as the character label prediction layer, and the conditional random field (CRF) is used to output the global optimal label. The defect records of the traction power supply equipment of a maintenance department from 2016 to 2019 are used as the data set for case study, and, the comprehensive evaluation index of entity recognition of the model has reached 94.66%, saving 98.50% time compared with manual recognition. The results show that the entity information recognition algorithm proposed in this paper can effectively and accurately mine the defect record information of traction power supply equipment.},
  keywords={Decision making;Manuals;Information processing;Predictive models;Maintenance engineering;Transformers;Traction power supplies;named entity recognition;traction power supply equipment;defect record;deep learning;natural language processing},
  doi={10.1109/ICPSAsia55496.2022.9949792},
  ISSN={},
  month={July},}@INPROCEEDINGS{10822688,
  author={Chen, Dehua and Shen, Zijian and Wang, Mei and Dong, Na and Pan, Qiao and Su, Jianwen},
  booktitle={2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Extracting Structure Information from Narrative Medical Reports based on LLMs}, 
  year={2024},
  volume={},
  number={},
  pages={5616-5623},
  abstract={Extracting structured information and key details from medical report narratives is crucial to support healthcare data management, analysis and decision-making. However, the specialized nature of the reports, the complexity of the contents, and the high accuracy requirements of the results pose significant challenges to the structuring task. In this paper, we develop an LLM-based method to extract structure information from medical report narratives. Defining the structuring problem as mapping the narrative reports to the domain ontology, we design a framework to develop specialized LLMs that automatically learn and establish the mappings. At the core of this framework are report partitioning and interactive training data generation modules are. By separating complete reports into logically independent segments and training the LLMs on these segments independently, the trained LLMs can accurately capture the semantic relationships within each segment. Additionally, we explore different LLMs and formulate a simplistic scoring method to compare their accuracy, enabling us to select the best-performing model. Experimental evaluation on a real-world breast ultrasound report dataset demonstrates that our method achieves high accuracy with a small training dataset (400 samples). Specifically, the accuracy of structural information extraction and the attribute-value matching accuracy both exceed 96%.},
  keywords={Training;Accuracy;Ultrasonic imaging;Semantics;Training data;Medical services;Ontologies;Information retrieval;Data mining;Standards;Medical examination reports;Large language models;Report structuring},
  doi={10.1109/BIBM62325.2024.10822688},
  ISSN={2156-1133},
  month={Dec},}

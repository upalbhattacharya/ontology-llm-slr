@INPROCEEDINGS{9970920,
  author={Cahyaningsih, Elin and Silalahi, Natascha Lestari Eunike and Rohajawati, Siti and Avianti, Yuliza Maulina},
  booktitle={2022 International Conference on Information Technology Systems and Innovation (ICITSI)}, 
  title={COMMONKADS for Knowledge Based System Development: A Literature Study}, 
  year={2022},
  volume={},
  number={},
  pages={213-218},
  abstract={COMMONKADS is a method for developing knowledge-based system. This method describes foundation, technique, modeling language and document structure for develop the knowledge-based system. COMMONKADS is people-oriented system development methodology, and this methodology is often used for developing organizational knowledge management system. COMMONKADS approach is divided based on context (organizational model, task model, agent model), concept (knowledge model) and artifact (design model). COMMONKADS have been used widely for knowledge-based system in several fields, such as COMMONKADS that integrated in tourism knowledge-based system, COMMONKADS for irrigation expert system, expertise model using COMMONKADS in manufactured company, COMMONKADS in energy management system and many more. Generally, there are eight strengths of COMMONKADS methodology for develop knowledge-based system. Its strength is flexible to use in any scope, represent knowledge (organizational, domain, task and inference knowledge), complete (representation, model, and form), powerful, accurate, comprehensive, represent KM process, systematic and effective. While the weakness of COMMONKADS methodology only three, there are don't have validation process and difficult to acquisition knowledge and use semi formal language, large data storage. Nevertheless, COMMONKADS is recommended methodology for develop knowledge-based system.},
  keywords={Technological innovation;Systematics;Knowledge based systems;Government;Memory;Software;Regulation;COMMONKADS;strength of COMMONKADS;knowledge-based system;weakness of COMMONKADS;methodology;knowledge-based engineering},
  doi={10.1109/ICITSI56531.2022.9970920},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{8453933,
  author={Gand, Kai},
  booktitle={2018 IEEE 20th Conference on Business Informatics (CBI)}, 
  title={Towards Conceptual Enhancements of the Business Model Canvas: The Case of Health Information Technology}, 
  year={2018},
  volume={02},
  number={},
  pages={62-71},
  abstract={Business models (BM) describe mechanisms of value creation, delivery and capture. Business Model Representation (BMRs) in terms of conceptual models systematically and formally visualize BMs. The field of BMRs is characterized by conceptual inconsistencies such as heterogeneous notations and insufficiently described semantics. Describing and constructing structured and comparable BMs capturing concrete business cases is hampered. Even the success of the most prominent BMR approach - the business model canvas (BMC) - could only lead to slight advances. So, we aim to mature research on BMRs by enhancing the BMC with further conceptual modeling's concepts. We will focus on Health Information Technology (HIT) initiatives that are prone to fail. Analyses showed that the domain's networked nature of value creation requires to link and interrelate the BM's building blocks. Current BMR approaches, including the BMC, can only hardly provide such. For addressing BMC's conceptual weaknesses, we propose to substantiate the BMC by means of a layered concept making use of conceptual modeling's principles and considering the domain's specifics to a greater extent. Systematic and HITspecific BM(R) approaches are worthwhile as these identify and map all relevant stakeholders as well as their interrelations for value creation. That is a primary prerequisite for sustainable HIT solutions. So, we aim at bringing necessary complex information into a structured concept that allows to make profound managerial decisions. In turn, this provides more profound guidance for the implementation of HIT solutions that may lead to a higher success rates of such.},
  keywords={Organizations;Visualization;Stakeholders;Semantics;Information systems;Information technology;Business Models;Business Model Representations;Conceptual Modeling;Health Information Technology},
  doi={10.1109/CBI.2018.10047},
  ISSN={2378-1971},
  month={July},}@INPROCEEDINGS{10131078,
  author={Chen, Rui and Wang, Guoxin and Wu, Shouxuan and Lu, Jinzhi and Yan, Yan},
  booktitle={2023 IEEE International Systems Conference (SysCon)}, 
  title={A Service-oriented Approach Supporting Model Integration in Model-based Systems Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={1-7},
  abstract={When using Model-Based Systems Engineering (MBSE) to develop complex systems, models using different syntax and semantics are typically implemented in a heterogeneous environment which leads to difficulties to realize data integrations across the entire lifecycle. Specifically, seamless exchanges between models of different modeling tools are needed to support system lifecycle activities such as requirement analysis, function analysis, verification and validation. This article illustrates a service-oriented approach to support model integration for model-based systems engineering, especially between architecture design and system verification. First, a set of semantic mapping rules between architecture models and simulation models based on Open Service of Lifecycle Collaboration (OSLC) are proposed to support the formalization of technical resources (models, data, APIs). Then OSLC adapters are developed to transform models, data and APIs into web-based services. The services are deployed by a service discovering plug-in within a specific modeling tool for model information exchange. The approach is illustrated by a case study on KARMA architecture model and Modelica simulation model for a six-degree-of-freedom robot (RobotR3) system. We evaluate the availability and efficiency of this method from both qualitative and quantitative perspectives. The results show that our approach is effective in model and data integration.},
  keywords={Adaptation models;Analytical models;System verification;Semantics;Data integration;Transforms;Syntactics;Model integration;Model-Based Systems Engineering;Open Service for Lifecycle Collaboration;Modelica},
  doi={10.1109/SysCon53073.2023.10131078},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10708354,
  author={Kawther, Dridi and Wahiba, Ben Abdessalem Karaa},
  booktitle={2024 10th International Conference on Control, Decision and Information Technologies (CoDIT)}, 
  title={Comparative Analysis of Multilingual Text Classification Techniques: A Review of Current Approaches and Emerging}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The widespread availability of electronic documents and the exponential growth of the World Wide Web have made the automatic categorization of documents a critical method for organizing information and facilitating knowledge discovery, and information retrieval. This paper aims to examine the key techniques and methodologies utilized in multilingual document classification, while also bringing attention to some of the complex challenges that still need to be addressed. In particular, the paper presents a thorough review of the literature concerning the theory and methods of multilingual document representation and classification.},
  keywords={Dimensionality reduction;Reviews;Text categorization;Process control;Ontologies;Knowledge discovery;Vectors;Web sites;Information technology;Indexing},
  doi={10.1109/CoDIT62066.2024.10708354},
  ISSN={2576-3555},
  month={July},}@INPROCEEDINGS{8595070,
  author={Tueno Fotso, Steve Jeffrey and Frappier, Marc and Laleau, Régine and Mammar, Amel},
  booktitle={2018 23rd International Conference on Engineering of Complex Computer Systems (ICECCS)}, 
  title={Back Propagating B System Updates on SysML/KAOS Domain Models}, 
  year={2018},
  volume={},
  number={},
  pages={160-169},
  abstract={Nowadays, the usefulness of the formal verification and validation of system specifications is well established, at least for critical systems. However, one of the main obstacles to their adoption lies in obtaining the formal specification of the system, and, in the case of refinement-based formal methods such as B System or Event-B, in obtaining the most abstract specification that heads the development of the system. The SysML/KAOS requirements engineering method is proposed to overcome this difficulty. It includes a goal modeling language to model requirements from stakeholders needs. Translation rules from a goal model to a B System specification have already been defined. They allow to obtain a skeleton of the system specification. To complete it, a language has been defined to express the domain model associated to the goal model. Its translation gives the structural part of the B System specification. However, it very often appears that new elements must be added in the B System specification obtained from SysML/KAOS models, discovered for instance when specifying the body of events and/or by using formal validation and/or verification tools. We have therefore defined a set of rules allowing the back propagation, within domain models, of every newly added element. This paper describes these rules and how they are specified in Event-B. Their consistency is proved using the Rodin tool. We show that they are structure preserving: two related elements within the B System specification remain related within the domain model. This is done by proving various isomorphisms between the B System specification and the domain models.},
  keywords={Optimized production technology;Backpropagation;Actuators;Boilers;Sensor phenomena and characterization;Requirements engineering;Requirements Engineering;Domain Modeling;SysML/KAOS;Event-B;B System},
  doi={10.1109/ICECCS2018.2018.00025},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10621570,
  author={Motevallian, Mahsa and Esfar-E-Alam, AM and Taherkordi, Amir and Abbasi, Golnoush},
  booktitle={2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT)}, 
  title={Semantic Modeling of Waste Dataflow for Automating Circular Economy Systems}, 
  year={2024},
  volume={},
  number={},
  pages={677-684},
  abstract={Circular Economy (CE) is a model with a concrete action plan covering the whole life cycle of a product, from production and consumption to waste management (WM). Information technologies considerably contribute to the transition towards CE, e.g., waste tracking using Internet of Things (IoT). This will cause the businesses and organizations to confront a large diversity of data (i.e. waste amount, types, locations, etc.). The generated data is often stored and processed through manual or semi-manual methods by each business or organization. However, an automated method which can also interpret and integrate the diverse data in WM fields across different organizations is still in its infancy. Often, such data is not organized and falls short of reaching its full potential in facilitating coordinated management and enabling Circular Economy initiatives. In this paper, we aim to address this need through automated interpretation and integration of municipal waste data by applying semantic data modeling. Our approach proposes to capture the semantical description of entities in the WM process and their relations, which can appear between waste producers, authorities and consumers. Then, the obtained semantic model will facilitate and automate the required interpretation and integration of waste data, both for intra- and inter-organization scenarios. We realize intelligent semantic-based searching using natural language processing and large language models.},
  keywords={Waste management;Computational modeling;Large language models;Semantics;Organizations;Production;Manuals;Circular Economy;Waste Data Modeling;Semantic Data;Neural Search;NLP;Large Language Models},
  doi={10.1109/DCOSS-IoT61029.2024.00105},
  ISSN={2325-2944},
  month={April},}@INPROCEEDINGS{10298592,
  author={Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai, Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={LiSum: Open Source Software License Summarization with Multi-Task Learning}, 
  year={2023},
  volume={},
  number={},
  pages={787-799},
  abstract={Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.},
  keywords={Training;Measurement;Learning systems;Surveys;Formal languages;Licenses;Multitasking;Open Source Software Licenses;Multi-Task Learning;License comprehension},
  doi={10.1109/ASE56229.2023.00150},
  ISSN={2643-1572},
  month={Sep.},}@INPROCEEDINGS{10427395,
  author={Hendrik, Hendrik and Permanasari, Adhistya Erna and Fauziati, Silmi and Kusumawardani, Sri Suning},
  booktitle={2023 8th International Conference on Information Technology and Digital Applications (ICITDA)}, 
  title={Judging Knowledge by its Cover: Leveraging Large Language Models in Establishing Criteria for Knowledge Graph Sources Selection}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={The Knowledge Graph (KG) is a widely used paradigm for knowledge representation that leverages graph structures. It facilitates the discovery of relationships and provides context across various domains. Despite its popularity, there's a notable lack of emphasis on quality control during the KG construction process, particularly concerning the selection of knowledge sources. Our paper addresses this research gap by introducing criteria for selecting high-quality websites during the data acquisition phase of KG construction. We propose a set of criteria—credibility, relevance, content quality, coverage, comprehensiveness, and accessibility—that serves as a benchmark for evaluating potential online knowledge sources for KGs. We determine the weight of each criterion through expert judgment and validation using Large Language Models (LLMs) representing AI technologies. Our panel, consisting of academicians and practitioners, ranks the criteria by importance, with the Rank Ordered Centroid and Rank Reciprocal weighting methods generating weights from these rankings. We employ four LLMs: Google Bard, OpenAI ChatGPT 4, Anthropic Claude-2, and Meta Llama-2. Our findings suggest that LLMs can effectively validate human weighting results, as evidenced by a high correlation (Pearson's $r > 0.9$) between human and LLM criterion weights. Furthermore, hypothesis testing reveals no significant differences ($p > 0.8$), with the models demonstrating high internal consistency. Our study offers valuable insights for academics and industry professionals seeking to improve KG construction processes and establish standards for data quality and verification.},
  keywords={Correlation;Process control;Knowledge graphs;Quality control;Chatbots;Reliability;Standards;knowledge graphs construction;knowledge source selection;large language models;website evaluation criteria;criteria development approach},
  doi={10.1109/ICITDA60835.2023.10427395},
  ISSN={},
  month={Nov},}@ARTICLE{10227516,
  author={Xie, Qianqian and Tiwari, Prayag and Ananiadou, Sophia},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Knowledge-Enhanced Graph Topic Transformer for Explainable Biomedical Text Summarization}, 
  year={2024},
  volume={28},
  number={4},
  pages={1836-1847},
  abstract={Given the overwhelming and rapidly increasing volumes of the published biomedical literature, automatic biomedical text summarization has long been a highly important task. Recently, great advances in the performance of biomedical text summarization have been facilitated by pre-trained language models (PLMs) based on fine-tuning. However, existing summarization methods based on PLMs do not capture domain-specific knowledge. This can result in generated summaries with low coherence, including redundant sentences, or excluding important domain knowledge conveyed in the full-text document. Furthermore, the black-box nature of the transformers means that they lack explainability, i.e. it is not clear to users how and why the summary was generated. The domain-specific knowledge and explainability are crucial for the accuracy and transparency of biomedical text summarization methods. In this article, we aim to address these issues by proposing a novel domain knowledge-enhanced graph topic transformer (DORIS) for explainable biomedical text summarization. The model integrates the graph neural topic model and the domain-specific knowledge from the Unified Medical Language System (UMLS) into the transformer-based PLM, to improve the explainability and accuracy. Experimental results on four biomedical literature datasets show that our model outperforms existing state-of-the-art (SOTA) PLM-based summarization methods on biomedical extractive summarization. Furthermore, our use of graph neural topic modeling means that our model possesses the desirable property of being explainable, i.e. it is straightforward for users to understand how and why the model selects particular sentences for inclusion in the summary. The domain-specific knowledge helps our model to learn more coherent topics, to better explain the performance.},
  keywords={Biological system modeling;Semantics;Unified modeling language;Correlation;Task analysis;Transformers;Knowledge based systems;Biomedical text summarization;domain knowledge;graph neural topic model;explainability;pre-trained language models},
  doi={10.1109/JBHI.2023.3308064},
  ISSN={2168-2208},
  month={April},}@ARTICLE{10375098,
  author={Yang, Xu and Deng, Cheng and Wei, Kun and Tao, Dacheng},
  journal={IEEE Transactions on Cybernetics}, 
  title={Robust Commonsense Reasoning Against Noisy Labels Using Adaptive Correction}, 
  year={2024},
  volume={54},
  number={7},
  pages={4138-4149},
  abstract={Commonsense reasoning based on knowledge graphs (KGs) is a challenging task that requires predicting complex questions over the described textual contexts and relevant knowledge about the world. However, current methods typically assume clean training scenarios with accurately labeled samples, which are often unrealistic. The training set can include mislabeled samples, and the robustness to label noises is essential for commonsense reasoning methods to be practical, but this problem remains largely unexplored. This work focuses on commonsense reasoning with mislabeled training samples and makes several technical contributions: 1) we first construct diverse augmentations from knowledge and model, and offer a simple yet effective multiple-choice alignment method to divide the training samples into clean, semi-clean, and unclean parts; 2) we design adaptive label correction methods for the semi-clean and unclean samples to exploit the supervised potential of noisy information; and 3) finally, we extensively test these methods on noisy versions of commonsense reasoning benchmarks (CommonsenseQA and OpenbookQA). Experimental results show that the proposed method can significantly enhance robustness and improve overall performance. Furthermore, the proposed method is generally applicable to multiple existing commonsense reasoning frameworks to boost their robustness. The code is available at https://github.com/xdxuyang/CR_Noisy_Labels.},
  keywords={Noise measurement;Commonsense reasoning;Training;Robustness;Adaptation models;Context modeling;Knowledge graphs;Commonsense reasoning;knowledge graph (KG);noisy labels},
  doi={10.1109/TCYB.2023.3339629},
  ISSN={2168-2275},
  month={July},}@ARTICLE{10179264,
  author={Wang, Guan and Li, Weihua and Bai, Quan and Lai, Edmund M-K},
  journal={IEEE Transactions on Emerging Topics in Computing}, 
  title={Maximizing Social Influence With Minimum Information Alteration}, 
  year={2024},
  volume={12},
  number={2},
  pages={419-431},
  abstract={With the rapid advancement of the Internet and social platforms, how to maximize the influence across popular online social networks has attracted great attention from both researchers and practitioners. Almost all the existing influence diffusion models assume that influence remains constant in the process of information spreading. However, in the real world, people tend to alternate information by attaching opinions or modifying the contents before spreading it. Namely, the meaning and idea of a message normally mutate in the process of influence diffusion. In this article, we investigate how to maximize the influence in online social platforms with a key consideration of suppressing the information alteration in the diffusion cascading process. We leverage deep learning models and knowledge graphs to present users’ personalised behaviours, i.e., actions after receiving a message. Furthermore, we investigate the information alteration in the process of influence diffusion. A novel seed selection algorithm is proposed to maximize the social influence without causing significant information alteration. Experimental results explicitly show the rationale of the proposed user behaviours deep learning model architecture and demonstrate the novel seeding algorithm's outstanding performance in both maximizing influence and retaining the influence originality.},
  keywords={Integrated circuit modeling;Social networking (online);Deep learning;Biological system modeling;Mathematical models;Diffusion processes;Knowledge graphs;Information alteration;information diffusion;influence maximization;knowledge graph;natural language processing},
  doi={10.1109/TETC.2023.3292384},
  ISSN={2168-6750},
  month={April},}@ARTICLE{9822405,
  author={Harrando, Ismail and Reboud, Alison and Schleider, Thomas and Ehrhart, Thibault and Troncy, Raphael},
  journal={IEEE Internet Computing}, 
  title={ProZe: Explainable and Prompt-Guided Zero-Shot Text Classification}, 
  year={2022},
  volume={26},
  number={6},
  pages={69-77},
  abstract={As technology accelerates the generation and communication of textual data, the need to automatically understand this content becomes a necessity. In order to classify text, being it for tagging, indexing, or curating documents, one often relies on large, opaque models that are trained on preannotated datasets, making the process unexplainable, difficult to scale, and ill-adapted for niche domains with scarce data. To tackle these challenges, we propose ProZe, a text classification approach that leverages knowledge from two sources: prompting pretrained language models, as well as querying ConceptNet, a common-sense knowledge base which can be used to add a layer of explainability to the results. We evaluate our approach empirically and we show how this combination not only performs on par with state-of-the-art zero shot classification on several domains, but also offers explainable predictions that can be visualized.},
  keywords={Task analysis;Predictive models;Internet;Computational modeling;Semantics;Adaptation models;Transformers;Text classification;Knowledge graphs;Text classification;zero-shot;explainability;common sense knowledge graph;prompting language models},
  doi={10.1109/MIC.2022.3187080},
  ISSN={1941-0131},
  month={Nov},}@INPROCEEDINGS{10650108,
  author={Wang, Chaoguo and Zhang, Liang and Yan, Wei},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={A Combination of LERT and CNN-BILSTM Models for Chinese Music Named Entity Recognition}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Named Entity Recognition (NER) is an important task in the field of natural language processing(NLP), serving as the foundation for knowledge graphs, recommendation systems, machine translation, and other applications. However, progress in NER for the music domain has been relatively slow. This is mainly due to the diverse and irregular nature of music entities, as well as the labor-intensive process of entity annotation, resulting in a lack of publicly available datasets, therefore, this paper organizes music texts on the internet through techniques such as data mining and NLP, constructing a NER dataset tailored to the field of music. Pre-trained models have been widely adopted in various fields, including NLP and computer vision, as they can learn rich language knowledge and semantic representations. To address the limitations of existing NER methods in capturing contextual information and handling word sense disambiguation, this paper proposes a deep learning model called LERT-CNN-BILSTM-CRF for entity recognition in the Chinese music domain. Experimental results demonstrate that our proposed method achieves good performance, with precision reaching 93.87%, recall reaching 95.06%, and an F1 score of 94.46%. Compared to previous machine learning-based methods for music NER, our proposed approach better captures the semantic information of complex music named entities, thereby improving the performance and generalization ability of the model.},
  keywords={Deep learning;Training;Recurrent neural networks;Computational modeling;Semantics;Named entity recognition;Transformers;Named entity recognition;Deep learning;Linguistically-motivated bidirectional Encoder Representation from Transformer;Long Short-Term Memory;Knowledge Graph},
  doi={10.1109/IJCNN60899.2024.10650108},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10488152,
  author={Ren, Yuan and Li, Xutong and Liu, Xudong and Zhang, Richong},
  booktitle={2024 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Few-Shot KBQA Method Based on Multi-Task Learning}, 
  year={2024},
  volume={},
  number={},
  pages={226-233},
  abstract={Question-answering systems have become a prominent topic in the field of artificial intelligence. A crucial aspect is knowledge-based question answering (KBQA), used in search engines and intelligent customer service to enhance user experiences. However, existing methods often struggle to model complex relationships and operations in few-shot learning environments. To solve this problem, a multi-task KBQA method has been proposed. This method includes various auxiliary tasks such as relational sequence prediction, knowledge completion prediction, and query program reconstruction. A multi-task fusion training approach was adopted for model generation. Experimental results show that accuracy can be significantly improved by more than 6% in few-shot learning environments, achieving better performance with an accuracy rate of 92.45%.},
  keywords={Training;Customer services;Knowledge based systems;Search engines;Big Data;Multitasking;Question answering (information retrieval);knowledge graph;question-answering system;semantic parsing;few-shot learning;multi-task learning},
  doi={10.1109/BigComp60711.2024.00043},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10650892,
  author={Xiao, Mengxi and Liu, Ben and Peng, Miao and Xu, Wenjie and Peng, Min},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Clarified Aggregation and Predictive Modeling (CAPM): High-Interpretability Framework for Inductive Link Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={In inductive link prediction for evolving knowledge graphs (KGs), interpretability is crucial yet often overlooked in relational message aggregation methods. Previous approaches typically neglect deeper relational insights, limiting their explanatory power. In this paper, we propose CAPM (Constrained Aggregation and Predictive Modeling) to address this critical gap by uniquely incorporating semantic descriptions of entities. This integration allows for a clearer understanding of how and why certain links are predicted and strengthens its ability to contextualize and clarify the relationships within the KG. In particular, CAPM combines entity type and an attention mechanism during aggregation, ensuring a sophisticated blend of structured and semantic information. This method also improves relevance assessment in relational paths, leveraging prior knowledge in adjacent relations. Tested on sparse KGs, CAPM demonstrates exceptional performance in inductive link prediction scenarios. Ablation studies confirm its superiority, particularly when combined with embedding-based methods for entity-type representation, highlighting its effectiveness in evolving KGs. Through this innovative approach, CAPM offers a comprehensive solution that balances the dynamic nature of KGs with the essential need for interpretability in link prediction.},
  keywords={Attention mechanisms;Limiting;Semantics;Neural networks;Knowledge graphs;Predictive models;knowledge graph completion;inductive link prediction;message aggregation;model interpretability},
  doi={10.1109/IJCNN60899.2024.10650892},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{9750529,
  author={Zhou, Ling and Luan, Tianjiao and Yao, Na and Li, Jianming and Ding, Jie},
  booktitle={2021 IEEE Sixth International Conference on Data Science in Cyberspace (DSC)}, 
  title={BDCP: An Improved Nested Named Entity Recognition Model Based on LSTM}, 
  year={2021},
  volume={},
  number={},
  pages={346-352},
  abstract={In construction of knowledge graphs (KGs), named entity recognition (NER) is a sub-task to identify the boundaries of entities with special meaning and predict their categories in texts. The instance that an entity contains one or more entities is called nested entity. The current work on NER usually ignores the nested NER. We propose an improved model named Boundary Detection and Category Prediction (BDCP) for Nested NER. Our model uses LSTM to extract the contextual features of the sequence, and uses boundary detection unit to mark the boundaries of entities in the text. By introducing boundary detection unit, our model extracts the boundaries of entities and restrict the number of candidate entities. We also design a layer-by-layer decoding module by boundary detection unit for Nested NER. Experiments on Nested NER datasets named GENIA [1] demonstrate the effectiveness of our model on nested NER.},
  keywords={Knowledge engineering;Text recognition;Conferences;Cyberspace;Predictive models;Data science;Feature extraction;knowledge graph;neural network;nested entity;nested named entity recognition},
  doi={10.1109/DSC53577.2021.00055},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9657857,
  author={He, Bingjun and Chen, Jianfeng},
  booktitle={2021 IEEE 21st International Conference on Communication Technology (ICCT)}, 
  title={Named Entity Recognition Method in Network Security Domain Based on BERT-BiLSTM-CRF}, 
  year={2021},
  volume={},
  number={},
  pages={508-512},
  abstract={With the increase of the number of network threats, the knowledge graph is an effective method to quickly analyze the network threats from the mass of network security texts. Named entity recognition in network security domain is an important task to construct knowledge graph. Aiming at the problem that key Chinese entity information in network security related text is difficult to identify, a named entity recognition model in network security domain based on BERT-BiLSTM-CRF is proposed to identify key named entities in network security related text. This model adopts the BERT pre-training model to obtain the word vectors of the preceding and subsequent text information, and the obtained word vectors will be input to the subsequent BiLSTM module and CRF module for encoding and sorting. The test results show that this model has a good effect on the data set of network security domain. The recognition effect of this model is better than that of LSTM-CRF, BERT-LSTM-CRF, BERT-CRF and other models, and the F1=93.81%.},
  keywords={Knowledge engineering;Text recognition;Conferences;Network security;Encoding;Data models;Communications technology;cybersecurity;named entity recognition;BERT;BiLSTM;CRF},
  doi={10.1109/ICCT52962.2021.9657857},
  ISSN={2576-7828},
  month={Oct},}@INPROCEEDINGS{10763589,
  author={Zhu, Zhui and Qi, Guangpeng and Shang, Guangyong and He, Qingfeng and Zhang, Weichen and Li, Ningbo and Chen, Yunzhi and Hu, Lijun and Zhang, Wenqiang and Dang, Fan},
  booktitle={2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={Enhancing Large Language Models with Knowledge Graphs for Robust Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={262-269},
  abstract={In recent years, large language models (LLMs) have shown rapid development, becoming one of the most popular topics in the field of artificial intelligence. LLMs have demonstrated powerful generalization and learning capabilities, and their performance on various language tasks has been remarkable. Despite their successes, LLMs face significant challenges, particularly in domain-specific tasks that require structured knowledge, often leading to issues such as hallucinations. To mitigate these challenges, we propose a novel system, SynaptiQA, which integrates LLMs with Knowledge Graphs (KGs) to answer more questions about knowledge. Our approach leverages the generative capabilities of LLMs to create and optimize KG queries, thereby improving the accuracy and contextual relevance of responses. Experimental results in an industrial data set demonstrate that SynaptiQA outperforms baseline models and naive retrieval-augmented generation (RAG) systems, demonstrating improved accuracy and reduced hallucinations. This integration of KGs with LLMs paves the way for more reliable and interpretable domain-specific question answering systems.},
  keywords={Accuracy;Large language models;Distributed databases;Knowledge graphs;Question answering (information retrieval);Vectors;Cognition;Data models;Reliability;Faces;Artificial Intelligence;Knowledge Graph;Large Language Model},
  doi={10.1109/ICPADS63350.2024.00042},
  ISSN={2690-5965},
  month={Oct},}@INPROCEEDINGS{10575550,
  author={Li, Yiyuan},
  booktitle={2024 IEEE 6th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)}, 
  title={Knowledge Base Question Answering based on Machine Reading Comprehension}, 
  year={2024},
  volume={6},
  number={},
  pages={1616-1620},
  abstract={Currently, society has entered the era of "Internet +," where technology brings more convenience to people. Knowledge Graph (KG) is a structured information system of world knowledge and serves as the cornerstone for artificial intelligence reasoning and memory. In recent years, with the improvement of deep models and computing power, Knowledge Graph technology has rapidly developed, providing foundational support for applications such as question answering, recommendation systems, and search engines. Knowledge Based Question Answering (KBQA) utilizes knowledge bases (KB) to answer questions and finds wide application in fields like search, finance, and healthcare. Due to the gap in representation between unstructured natural language questions and structured Knowledge Graphs (KGs), answering questions on knowledge graphs presents certain challenges. Existing methods based on semantic parsing struggle to construct structured queries that can be executed on knowledge bases, thereby failing to cover various complex questions. Conversely, methods based on information retrieval exhibit poor interpretability. This paper proposes a novel approach driven by machine reading comprehension. To transform KG subgraphs centered around subject entities into text, this paper employs schema trees to depict the subgraphs, which facilitates retrieving multiple semantically equivalent answer entities. The KG2Text method is utilized to generate corresponding text from the constructed schema trees. In contrast to seeking answers from all automatically generated paragraphs, this paper employs a contrastive learning approach to filter out paragraphs that may contain answers. Finally, answers are obtained based on answer fragments detected by the machine reading comprehension module. Experimental results demonstrate that this approach significantly outperforms existing methods.},
  keywords={Knowledge based systems;Semantics;Knowledge graphs;Transforms;Medical services;Search engines;Cognition;KG2Text model;Knowledge Based Question Answering;Machine Reading Comprehension;contrastive learning},
  doi={10.1109/IMCEC59810.2024.10575550},
  ISSN={2693-2776},
  month={May},}@INPROCEEDINGS{10711734,
  author={Wen, Sijie and Chen, Yongming and Pan, Xinyu and Zhuang, Weibin and Li, Xinyu},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Enhancing Fault Troubleshooting through Human-Machine Collaboration: A Multi-Stage Reasoning Approach}, 
  year={2024},
  volume={},
  number={},
  pages={460-467},
  abstract={Ensuring the stable operation of critical industrial equipment is pivotal for maintaining production efficiency and economic gains. The complexity of modern industrial machinery, however, places a substantial cognitive load on maintenance personnel. To alleviate this, a Diagnostic Semantic-Enhanced Fault Causality Knowledge Graph (DSFCKG) is proposed to formalize fault information for computational analysis. Additionally, a Large Language Model (LLM)-based Knowledge Graph Construction (KGC) method is introduced for the automated assembly of DSFCKG. Building upon this, a multi-stage reasoning approach is designed for human-machine collaborative fault Troubleshooting. Experiments on real-world fault tickets demonstrate that our proposed method significantly enhances fault diagnosis and troubleshooting accuracy, especially in complex scenarios with long fault causal chains, which bring insights into futuristic smart maintenance.},
  keywords={Fault diagnosis;Accuracy;Human-machine systems;Semantics;Collaboration;Cause effect analysis;Knowledge graphs;Production facilities;Maintenance;Standards},
  doi={10.1109/CASE59546.2024.10711734},
  ISSN={2161-8089},
  month={Aug},}@ARTICLE{10848141,
  author={Zhu, Lilu and Su, Xiaolu and Tang, Jiaxuan and Hu, Yanfeng and Wang, Yang},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={View-Based Knowledge-Augmented Multimodal Semantic Understanding for Optical Remote Sensing Images}, 
  year={2025},
  volume={63},
  number={},
  pages={1-33},
  abstract={Optical remote sensing (RS) images serve as a pivotal source of geographic information. Due to the continuous development of deep learning technology, the evolving demands for multisource optical RS of the public shifted from recognition and acquisition of explicit features to comprehension and application of the fine-grained semantics and relationships implied in images. To address this challenge, we propose a semantic-augmented approach integrated multiview knowledge graph for a comprehensive understanding of optical RS images (RSMVKF). The RSMVKF delves into the structured representations of external knowledge from different human-like cognitive views and further explores the discovery ability of high-level features on the basis of multiple modalities and granularities. Specifically, the RSMVKF consists of two stages. First, we guide a large language model (LLM) to condense relevant knowledge from lengthy external knowledge passages and generate a view-level knowledge graph (RS-VKG). Then, an asymmetric multimodal contrastive network model (RS-M2CL) is designed to investigate efficient semantic augmentation. In this way, two types of contrastive loss functions, cross-modal and cross-granularity, are adopted to improve the understanding of implicit knowledge. The experimental results demonstrate that the RSMVKF greatly improves several perception tasks and reasoning tasks with rich features in optical RS imagery. In particular, in perception tasks such as fine-grained object detection and k-nearest neighbor (KNN) retrieval, the RSMVKF yields enhancements of 6.7% and 8.1%, respectively. In addition, in knowledge-driven reasoning tasks such as RS image captioning (RSCP), RS visual grounding (RSVG), and RS visual question answering (RSVQA), the RSMVKF demonstrates superior performance with margins of 8.9%, 5.3%, and 11.4%, respectively.},
  keywords={Urban areas;Optical sensors;Optical imaging;Lakes;Semantics;Bridges;Roads;Rivers;Satellites;Buildings;Fine-grained semantics and relationships;multimodal contrastive network;multiview knowledge graph;optical remote sensing (RS) images;semantic-augmented approach},
  doi={10.1109/TGRS.2025.3532349},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{8003819,
  author={Suri, Kunal and Gaaloul, Walid and Cuccuru, Arnaud and Gerard, Sebastien},
  booktitle={2017 IEEE 26th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)}, 
  title={Semantic Framework for Internet of Things-Aware Business Process Development}, 
  year={2017},
  volume={},
  number={},
  pages={214-219},
  abstract={The proliferation of connected devices, wherein Sensors, Actuators and Tags (such as Radio-Frequency Identification (RFID)) are able to seamlessly communicate to their environment and to each other for sharing information or to perform some actions has created the Internet of Things (IoT) ecosystem. These devices expose their functionality via standard services and application programming interfaces (APIs). They are considered to be one of the key technology enablers to foster the vision of a smart world, comprising of smart objects, smart supply chain management, smart manufacturing (Industry 4.0), smart buildings, to name a few. In fact, today these IoT devices continuously take part in various business processes that are being executing within the boundaries of the same enterprise or in different enterprises. Thus, there is an evident need to model these processes that are associated with IoT resources in a formal and unambiguous manner. However, in context of business processes, these is a lack of formalized and explicit description for IoT resources, thus hampering their efficient modeling and management. To bridge this gap, we propose a semantic framework for developing IoT-aware business processes as follows, (i) formalizing IoT resource description w.r.t Internet of Things Architecture (IoT-A) reference model in context of business processes, (ii) formalizing IoT properties and allocation rules for optimal resource management and (iii) resolving resource conflicts based on strategies. To illustrate the feasibility of our framework, we evaluated our semantic model for coverage of concepts in IoT-A reference model along with development of a proof of concept tool for integrating the IoT resources and our semantic model during the process modeling phase.},
  keywords={Semantics;Temperature measurement;Temperature sensors;Radiofrequency identification;Resource management;Internet of Things (IoT);Business Process Management;Resource Management;Semantics;Ontology;Process Modeling},
  doi={10.1109/WETICE.2017.54},
  ISSN={},
  month={June},}@INPROCEEDINGS{8374390,
  author={Bendib, Issam and Laouar, Mohammed Ridda},
  booktitle={2018 2nd International Conference on Natural Language and Speech Processing (ICNLSP)}, 
  title={A semantic indexing approach of multimedia documents content based partial transcription}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  abstract={The complexity of searching and indexing spoken document retrieval depends closely on content structure and access strategies. However, Spoken Document Retrieval (SDR) approaches based on the transcription of content by LVCSR systems must treat the impact of recognition errors on the performance of the information retrieval system. Currently, several multimedia resources (spoken document) are unexploited, because the errors generated in the transcription process, decrease the performance of the systems of searches for spoken documents. Our contribution in this paper is the proposition a novel approach semantic based indexing content of multimedia documents by using the results of a partial automatic transcription to ride out research and indexing problems on these resources. In this context, the main hypothesis is developed around the following question: is it possible to identify and retrieve spoken document by using a semantic content indexing system based on his partial transcription? Also, does the partial content transcription of a spoken document is sufficient for his indexing? First, we are interested to look for a linguistic syntactic representation of the whole of the content spoken document. Then, it will be used to define the most discriminating indexing terms for his content. Nevertheless, in our approach, we have an assumption that the use of segments of a spoken document with an efficient semantic enrichment process instead of the whole spoken document content is sufficient. Also, with this strategy, we can resolve OOV problems, recognition errors and technical terms. Finally, in validation and experimentation phase, we tested the different modules proposed on the TED-LIUM corpora. The results obtained are interesting and encourage us to lead off our future perspectives.},
  keywords={Semantics;Indexing;Multimedia communication;Speech recognition;Syntactics;Lattices;Semantic content Indexing;Spoken Term detection;L VCSR;Keyword Spotting;Spoken Document;WordNet Ontology;similarity measures},
  doi={10.1109/ICNLSP.2018.8374390},
  ISSN={},
  month={April},}@INPROCEEDINGS{10020513,
  author={Chen, Xianlai and Lin, Jiamiao and An, Ying},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={DL-BERT: a time-aware double-level BERT-style model with pre-training for disease prediction}, 
  year={2022},
  volume={},
  number={},
  pages={1801-1808},
  abstract={Disease prediction based on the Electronic Health Record (EHR) is an important task in healthcare. EHR records patients’ every visit by time, and there are many kinds of medical codes within a visit, therefore, EHR has characteristics of temporal irregularity and hierarchical structure. Some recent works employ BERT-style models to process EHR data for disease prediction. However, few of these models can give consideration to capture both the interaction between medical codes and the impact of temporal irregularity. To solve this problem, we propose the Double-Level BERT-style model (DL-BERT). Considering EHR’s hierarchical structure, the model contains a code-level and a visit-level representation layer which can learn the relationship between medical codes and temporal influence respectively. In the code-level representation layer, the model achieves the representation power by employing external medical ontologies to provide multi-resolution information of medical codes and the Transformer to embed medical codes. Besides, the model adopts two pre-training tasks to enhance the ability to capture the link between different kinds of codes. In the visit-level representation layer, DL-BERT utilizes a special time-aware Transformer to model temporal information. And the model adopts a visit-level pre-training task for better learning context information. Experiments are conducted on two real-world healthcare datasets and show that our model outperforms all baselines demonstrating the effectiveness of our model.},
  keywords={Codes;Medical services;Ontologies;Predictive models;Big Data;Transformers;Data models;Electronic Health Record;BERT;Transformer;medical ontology;pre-train},
  doi={10.1109/BigData55660.2022.10020513},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8306008,
  author={Wouters, Laurent and Creff, Stephen and Bella, Emma Effa and Koudri, Ali},
  booktitle={2017 24th Asia-Pacific Software Engineering Conference (APSEC)}, 
  title={Towards Semantic-Aware Collaborations in Systems Engineering}, 
  year={2017},
  volume={},
  number={},
  pages={719-724},
  abstract={The design of nowadays complex systems requires a collaboration between a diversity of stakeholders: from domain experts to customers. For a collaboration to be efficient, the relevant information have to reach the right stakeholder at the right time in a format that is understandable to him/her. We propose a formal framework to integrate the meaning projected by stakeholders onto their data (the denotation), so that it can be unambiguously used by others. An implementation of this framework, relying on the existing language xOWL (extension of OWL2 with behavioral constructs), is then provided to perform the semantic integration of the captured denotations in an MBSE approach.},
  keywords={Semantics;Syntactics;Stakeholders;Collaboration;Unified modeling language;Assistive technology;Silicon;Collaborative Engineering;Semantic Integration;Modeling;Domain Intention;Ontology Mapping;Behavioral Semantics},
  doi={10.1109/APSEC.2017.92},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10350799,
  author={Elaasar, Maged and Rouquette, Nicolas and Wagner, David and Oakes, Bentley James and Hamou-Lhadj, Abdelwahab and Hamdaqa, Mohammad},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={openCAESAR: Balancing Agility and Rigor in Model-Based Systems Engineering}, 
  year={2023},
  volume={},
  number={},
  pages={221-230},
  abstract={Model-Based System Engineering (MBSE) employs models and formal languages to support development of complex (systems-of-) systems. NASA Jet Propulsion Laboratory (JPL) sees MBSE as a key approach to managing the complexity of system development. However, balancing agility and rigor in MBSE has been reported as a challenging task not yet addressed by modeling tools and frameworks. This is because existing MBSE approaches may enable agility but compromise rigor, or enhance rigor but impede agility. We discuss the challenges of balancing agility and rigor in MBSE across seven systems engineering architectural functions defined by the JPL Integrated Model-Centric Engineering (IMCE) initiative. We demonstrate how openCAESAR, an open-source MBSE methodology and framework created at JPL, can strike a balance between agility and rigor through a case study of the Kepler16b project and discussion of lessons learned from past projects.},
  keywords={NASA;Formal languages;Propulsion;Model driven engineering;Complexity theory;Modeling;Task analysis;Systems Engineering;Model-Based Systems Engineering;Ontology-based Modeling;OML;openCAESAR},
  doi={10.1109/MODELS-C59198.2023.00051},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9525789,
  author={Weerakoon, Charmy and Ranathunga, Surangika},
  booktitle={2021 Moratuwa Engineering Research Conference (MERCon)}, 
  title={Question Classification for the Travel Domain using Deep Contextualized Word Embedding Models}, 
  year={2021},
  volume={},
  number={},
  pages={573-578},
  abstract={Question answering can be considered as a key area in Natural Language Processing and Information Retrieval, where users construct queries in natural language and receive suitable answers in return. In the travel domain, most questions are “content questions”, where the expected answer is not the equivalent of “yes” or “no”, but rather factual information. Replying to a free-form factual question based on a large collection of text is challenging. Previous research has shown that the accuracy of question answering systems can be improved by adding a classification phase based on the expected answer type. This paper focuses on implementing a multi-level, multi-class question classification system focusing on the travel domain. Existing research for the travel domain is conducted using language-specific features and traditional Machine Learning models. In contrast, this research employs transformer-based state-of-the-art deep contextualized word embedding models for question classification. The proposed method improves the coarse class Micro F1-Score by 5.43% compared to the baseline. Fine-grain Micro F1-Score has also improved by 3.8%. We also present an empirical analysis of the effectiveness of different transformer-based deep contextualized word embedding models for multi-level multi-class classification.},
  keywords={Analytical models;Focusing;Machine learning;Transformers;Knowledge discovery;Information retrieval;Natural language processing;question classification;expected answer type;ontology learning;transformers;RoBERTa},
  doi={10.1109/MERCon52712.2021.9525789},
  ISSN={2691-364X},
  month={July},}@INPROCEEDINGS{9610664,
  author={Kiv, Soreangsey and Heng, Samedi and Wautelet, Yves and Kolp, Manuel},
  booktitle={2021 IEEE 23rd Conference on Business Informatics (CBI)}, 
  title={Towards a Systematic Socio-Intentional Framework for Agile Methods Tailoring}, 
  year={2021},
  volume={02},
  number={},
  pages={143-152},
  abstract={Agile has become one of the most popular software development approaches thanks to its flexible and evolutive features. To find further suitable practices, teams start to follow the tailoring approach by choosing only the fragments of different methods that fit their needs and context. Many tailoring approaches have been proposed by orienting different aspects such as process, resource and goal. While the interaction between team members is very important in agile methods, none of these approaches focuses on the socio-intentional aspect. In the literature, we can find many case studies that link socio-intentional aspects to the tailoring of agile practices. Even though it is helpful to know it, locating relevant information can be effort and time-consuming. This research proposes a socio-intentional framework that can analyze agile practices and indicate how to tailor them with the help of an evidence-based tool and a modeling language. This framework will allow practitioners to identify the right practices to achieve their goals and analyze their suitability and vulnerability. It will also indicate how to successfully implement them in the software development process.},
  keywords={Analytical models;Visualization;Systematics;Focusing;Tools;Software;Planning;Evidence-based System;Ontology;Tailoring Agile;Socio-intentional Modeling},
  doi={10.1109/CBI52690.2021.10065},
  ISSN={2378-1971},
  month={Sep.},}@INPROCEEDINGS{10450023,
  author={Pal, Suman and Gaur, Monica and Chaudhuri, Rupanjali and Benny Anto, Oshin and R, Kalaivanan and KV, Chetan and Pradhan, Pragnya},
  booktitle={2023 International Conference on Computational Intelligence, Networks and Security (ICCINS)}, 
  title={Recommendation System for Clinical Concept Mapping}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In the past decade, the healthcare industry has shifted from paper-based document storage to Electronic Health Records (EHR), enabling quick, safe access to patient data. A key role is played by Semantic Interoperability (SI) which enables seamless data exchange between diverse care settings and clinical software. SI necessitates the linking of bio-medical data (aka. clinical events) with shared, standardized, and controlled vocabularies like SNOMED, LOINC, etc. However, the healthcare data across various client domains are filled with ambiguous textual representations of clinical events that may be present in the form of synonyms, acronyms, and abbreviations. To make interoperability work, Healthcare IT service providers must map related clinical events with the appropriate standard concepts, which requires additional time and resources. Natural Language Processing (NLP) plays a vital role in addressing the challenges of SI by learning effective representations of text words in the bio-medical domain thereby capturing their semantic meaning. Our method utilizes various pre-trained word embeddings trained on the bio-medical corpus like BioWordVec fastText and SapBERT that captures fine-grained semantic relationships. In this study, we have developed a recommendation system that provides recommendations of Top ‘N’ clinical events for mapping to a standard concept. The recommendation system showed good performance with a sensitivity of above 99 % using both the pre-trained word embedding. Further, this product can be integrated into the mapping workflow to help make accurate automated suggestions that minimize manual effort.},
  keywords={Sensitivity;Semantics;Medical services;Manuals;Natural language processing;Recommender systems;Interoperability;semantic interoperability;ontology;mapping;natural language processing;word embeddings;SapBERT},
  doi={10.1109/ICCINS58907.2023.10450023},
  ISSN={},
  month={Dec},}@ARTICLE{10804145,
  author={Villegas-Ch, William and Govea, Jaime and Gutierrez, Rommel},
  journal={IEEE Access}, 
  title={Optimizing Language Model-Based Educational Assistants Using Knowledge Graphs: Integration With Moodle LMS}, 
  year={2024},
  volume={12},
  number={},
  pages={191994-192012},
  abstract={Chatbots in educational settings have grown significantly, facilitating interaction between students and learning platforms. However, current systems, such as Rasa, Moodle Integrated Chatbots, and ChatterBot, present significant limitations in precision, adaptability, and response time, affecting their effectiveness in resolving academic queries and personalizing learning. To address these shortcomings, this work proposes the development of an advanced educational chatbot that combines large language models (LLMs) with knowledge graphs, allowing for more accurate and contextualized responses and offering valuable suggestions to enrich the learning process. The system is evaluated based on its ability to adjust to different student profiles and offer fast and accurate responses. The results show that the proposed chatbot achieves a precision of 85%, outperforming Rasa and ChatterBot, which achieved accuracies of 83% and 81%, respectively. Furthermore, the chatbot reduces response times to 0.41 seconds, improving efficiency compared to other solutions. The system also demonstrates adaptability, effectively adjusting to students’ learning styles and academic levels. This work indicates that knowledge graph integration and hyperparameter optimization are crucial to improving educational chatbots’ precision, speed, and adaptability, presenting an innovative solution that overcomes the limitations of current systems.},
  keywords={Chatbots;Knowledge graphs;Time factors;Accuracy;Servers;Real-time systems;Data models;Adaptation models;Context modeling;Scalability;Educational chatbots;large language models;knowledge graphs;learning personalization},
  doi={10.1109/ACCESS.2024.3518952},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9006589,
  author={Lieder, Itay and Segal, Meirav and Avidan, Eran and Cohen, Asaf and Hope, Tom},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)}, 
  title={Learning a Faceted Customer Segmentation for Discovering new Business Opportunities at Intel}, 
  year={2019},
  volume={},
  number={},
  pages={6136-6138},
  abstract={For sales and marketing organizations within large enterprises, identifying and understanding new markets, customers and partners is a key challenge. Intel's Sales and Marketing Group (SMG) faces similar challenges while growing in new markets and domains and evolving its existing business. In today's complex technological and commercial landscape, there is need for intelligent automation supporting a fine-grained understanding of businesses in order to help SMG sift through millions of companies across many geographies and languages and identify relevant directions. We present a system developed in our company that mines millions of public business web pages, and extracts a faceted customer representation. We focus on two key customer aspects that are essential for finding relevant opportunities: industry segments (ranging from broad verticals such as healthcare, to more specific fields such as “video analytics”) and functional roles (e.g., “manufacturer” or “retail”). To address the challenge of labeled data collection, we enrich our data with external information gleaned from Wikipedia, and develop a semi-supervised multi-label, multi-lingual deep learning model that parses customer website texts and classifies them into their respective facets. Our system scans and indexes companies as part of a large-scale knowledge graph that currently holds tens of millions of connected entities with thousands being fetched, enriched and connected to the graph by the hour in real time, and also supports knowledge and insight discovery. In experiments conducted in our company, we are able to significantly boost the performance of sales personnel in the task of discovering new customers and commercial partnership opportunities.},
  keywords={Companies;Industries;Encyclopedias;Electronic publishing;Internet;Bit error rate;AI for Enterprise;NLP;Web Mining},
  doi={10.1109/BigData47090.2019.9006589},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9325805,
  author={Wang, Yu and Sun, Yining and Ma, Zuchang and Gao, Lisheng and Xu, Yang and Wu, Yichen},
  booktitle={2020 13th International Symposium on Computational Intelligence and Design (ISCID)}, 
  title={A Method of Relation Extraction Using Pre-training Models}, 
  year={2020},
  volume={},
  number={},
  pages={176-179},
  abstract={Relation Extraction (RE), as an essential task of Natural Language Processing (NLP), aims to extract potential relations between two entities in a sentence. It is a crucial step in information extraction from unstructured data and building a Knowledge Graph (KG). The performance of deep learning methods for RE, like Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN), heavily depends on the quality and scale of the training set. Recently, pre-training models like BERT and ERNIE, have achieved State-Of-The-Art (SOTA) results in many NLP tasks, because they can obtain the prior semantic knowledge during the procedure of pre-training. Therefore, it is interesting to know whether the performance of RE can be improved utilizing the pre-training models. In this paper, we propose a method of RE using two kinds of pre-training models: BERT and ERNIE. First, in the input sequence, unique symbols are appended around the entities. RE is then regarded as a text classification task, and the prior semantic knowledge obtained by pre-training models is used to improve the performance. Experiments are carried on the SemEval 2010 Task 8 dataset. Results demonstrate that the method we proposed improves the performance of RE compared with previous approaches.},
  keywords={Task analysis;Semantics;Bit error rate;Training;Natural language processing;Feature extraction;Predictive models;relation extraction;pre-training models;BERT;ERNIE},
  doi={10.1109/ISCID51228.2020.00046},
  ISSN={2473-3547},
  month={Dec},}@INPROCEEDINGS{9245454,
  author={Sirirattanajakarin, Sorratat and Jitkongchuen, Duangjai and Intarapaiboon, Peerasak},
  booktitle={2020 1st International Conference on Big Data Analytics and Practices (IBDAP)}, 
  title={BoydCut: Bidirectional LSTM-CNN Model for Thai Sentence Segmenter}, 
  year={2020},
  volume={},
  number={},
  pages={1-4},
  abstract={Sentence is imperative in order to build the top level of NLP (Natural Language Processing) applications such as Information retrieval, News Summarization, Knowledge graph. In Thai language, neither each word token is not separated by using just only space like English language, nor the sentence is verified its boundary by using full stop symbols. This paper proposes BoydCut, an NLP framework for identifying sentence boundaries based on Bidirectional LSTM-CNN Model. We develop this framework by utilizing the combination of character, word, and part of speech features. With Bidirectional LSTM, it can learn sequent of word-level in sentences and learn extracted features from character level. With the benefit of the combination Bidirectional LSTM-CNN Model, we do not need feature engineering for feature extraction that can be saved a lot of cost and time in order to build a sentence segmentation model. We also simply design the experiments in a different bucket of features extracted from a deep sequential model. The result empirically shows well perform in internal and external data, as well as help a lot in order to build several useful on the top level of NLP applications.},
  keywords={Analytical models;Big Data;Feature extraction;Information retrieval;Natural language processing;Data models;natural language processing;sequent labeling;sentence segmentation;Thai language},
  doi={10.1109/IBDAP50342.2020.9245454},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10393214,
  author={Jha, Aashish and Purohit, Madhavi and Samanta, Gaurav and Jadhav, Dipti},
  booktitle={2023 International Conference on Advanced Computing Technologies and Applications (ICACTA)}, 
  title={Questify: Your Intelligent Web Page Assistant for Quick Answers}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  abstract={Retaining and recalling information from web pages can often feel like a daunting task, especially when faced with the vast amount of information contained within an article. However, researchers have come up with an innovative solution to tackle this. In this study, we propose a hybrid question-answering system that uses Natural Language Processing (NLP) to logically describe the data on a web page and then react to user inquiries. The system consists of a web scraper, named entity recognition module, and classifiers to learn more about the information.The process begins with a web scraper, which gets the text from a certain online page and makes sure that the required data is gathered. The system learns more about the information by identifying and classifying items like persons, locations, organizations, or other pertinent phrases. The question-answering module uses a knowledge graph to provide accurate and comprehensive answers to user queries. Using benchmark datasets, we evaluated the system’s performance to confirm the efficacy of their strategy. Across several datasets, the F1 score, which is a metric for precision and recall, varied from 0.7 to 0.95. The system can serve as a useful resource for information retrieval from web sites, according to these results, which show that it is able to provide users trustworthy responses to their questions.},
  keywords={Training;Surveys;Measurement;Text recognition;Web pages;Organizations;Natural language processing;natural language understating;web scraping;information;neural networks;text preprocessing;transformer models},
  doi={10.1109/ICACTA58201.2023.10393214},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10431540,
  author={Sadiq, Mohamed Abdul Karim and Shanmugam, Thirumurugan and Al-Fannah, Nasser Mohammed and Kausar, Firdous},
  booktitle={2023 IEEE 5th International Conference on Cognitive Machine Intelligence (CogMI)}, 
  title={Skills Extraction from Entities in Arabic CVs}, 
  year={2023},
  volume={},
  number={},
  pages={204-208},
  abstract={Named Entity Recognition in low resource languages such as Arabic is relatively less accurate. Despite this, many applications attempt query-document matching, which necessitates entity resolution. We explore certain possible solutions in this regard to enhance the matching process. A system is proposed to build a sub graph of terms in the Knowledge Graph. These terms specifically deal with skills in the Arabic documents related to the Human Resources domain. The accuracy measures of skills extraction using two different Language Models viz., SparkNLP and Hatmi are presented. The results are promising and have scope for improvement in entity resolution.},
  keywords={Knowledge graphs;Machine intelligence;automated recruitment;human resources;language models;named entity recognition;natural language processing},
  doi={10.1109/CogMI58952.2023.00037},
  ISSN={},
  month={Nov},}@ARTICLE{9745473,
  author={},
  journal={International Journal of Crowd Science}, 
  title={Front cover}, 
  year={2022},
  volume={6},
  number={1},
  pages={c1-c1},
  abstract={In recent years, neural networks have been widely used in natural language processing, especially in sentence similarity modeling. Most of the previous studies focused on the current sentence, ignoring the commonsense knowledge related to the current sentence in the task of sentence similarity modeling. Commonsense knowledge can be remarkably useful for understanding the semantics of sentences. CK-Encoder, which can effectively acquire commonsense knowledge to improve the performance of sentence similarity modeling, is proposed in this paper. Specifically, the model first generates a commonsense knowledge graph of the input sentence and calculates this graph by using the graph convolution network. In addition, CKER, a framework combining CK-Encoder and sentence encoder, is introduced. Experiments on two sentence similarity tasks have demonstrated that CK-Encoder can effectively acquire commonsense knowledge to improve the capability of a model to understand sentences.},
  keywords={Convolution;Semantics;Neural networks;Natural language processing;Task analysis;Commonsense reasoning;CK-Encoder;sentence similarity;commonsense knowledge},
  doi={},
  ISSN={2398-7294},
  month={April},}@INPROCEEDINGS{10331969,
  author={Wan, Lu and Wan, Xue},
  booktitle={2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)}, 
  title={Entity Relationship Extraction for Chinese Electronic Medical Records Based on Tightly Cascaded Binary Pointers}, 
  year={2023},
  volume={},
  number={},
  pages={1212-1217},
  abstract={Relationship extraction, as a subtask of information extraction, converts unstructured medical data in electronic medical records into structured data, which lays the foundation for subsequent knowledge graph construction. The existing relationship extraction ignores the potential relationships of similar entities in a single sentence and fails to make full use of the relationships among the closer entities in the text. Thus, we propose a novel tightness mechanism that increases the correlation between head and tail entities to better determine the relationship between similar entities. The experiments of the proposed model on the public dataset demonstrate that the model can extract the triples more accurately and the F1 value is improved by 2.84% over the baseline model.},
  keywords={Electric potential;Head;Costs;Knowledge graphs;Transforms;Tail;Tagging;Chinese electronic medical records;Entity relationship extraction;cascade binary pointer tagging},
  doi={10.1109/PRAI59366.2023.10331969},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10066681,
  author={Lee, JinKyu and Kim, Jihie},
  booktitle={2023 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Improving Generation of Sentiment Commonsense by Bias Mitigation}, 
  year={2023},
  volume={},
  number={},
  pages={308-311},
  abstract={Commonsense knowledge graphs (CSKG) are crucial for artificial intelligence systems to understand natural language. Recently, with the construction of COMET (Commonsense Transformer) and ATOMIC2020, a comprehensive coverage commonsense reasoning knowledge graph, CSKG research is increasingly vital in natural language understanding and reasoning. Since sentiment commonsense knowledge is understudied yet, our work focuses on improving the generation of sentiment commonsense in ATOMIC2020. We first show a problem in natural language generation that degrades the accuracy due to the unbalanced sentiment distribution in the dataset. Next, we propose the EDA (Easy Data Augmentation) and UDA(Unsupervised Data Augmentation) based methods that improve the accuracy through biased mitigation of the unbalanced dataset. Our experimental results show that EDA method has little effect on the accuracy, while UDA-based method has some accuracy improvements in ROUGE-I, ROUGE-2, and ROUGE-L.},
  keywords={Comets;Tail;Knowledge graphs;Big Data;Transformers;Natural language processing;Task analysis;Commonsense;Sentiment;Bias;EDA;UDA},
  doi={10.1109/BigComp57234.2023.00061},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10597884,
  author={Le, Duy and Zhao, Kris and Wang, Mengying and Wu, Yinghui},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={GraphLingo: Domain Knowledge Exploration by Synchronizing Knowledge Graphs and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={5477-5480},
  abstract={Knowledge graphs (KGs) are routinely curated to provide factual data for various domain-specific analyses. Nevertheless, it remains nontrivial to explore domain knowledge with standard query languages. We demonstrate GraphLingo, a natural language (NL)-based knowledge exploration system designed for exploring domain-specific knowledge graphs. It differs from conventional knowledge graph search tools in that it enables an interactive exploratory NL query over domain-specific knowledge graphs. GraphLingo seamlessly integrates graph query processing and large language models with a graph pattern-based prompt generation approach to guide users in exploring relevant factual knowledge. It streamlines NL-based question & answer, graph query optimization & refining, and automatic prompt generation. A unique feature of GraphLingo is its capability to enable users to explore by seamlessly switching between a more ‘open’ approach and a more relevant yet ‘conservative’ one, facilitated by diversified query suggestions. We show cases of GraphLingo in curriculum suggestion, and materials scientific data search.},
  keywords={Knowledge engineering;Query processing;Large language models;Refining;Natural languages;Knowledge graphs;Switches;Knowledge graphs;Large Language Models;Graph query;Exploratory search},
  doi={10.1109/ICDE60146.2024.00432},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10350261,
  author={Liu, Xinyu},
  booktitle={2023 International Conference on Computers, Information Processing and Advanced Education (CIPAE)}, 
  title={EU-BERT: Efficient Unified Named Entity Recognition via Multi-exit BERT}, 
  year={2023},
  volume={},
  number={},
  pages={643-649},
  abstract={Named entity recognition (NER) is an important Natural Language Processing (NLP) task with wide application, such as document analysis, knowledge graph and query understanding. Although pretrain-based language models such as BERT has achieved great results on NER task, the large amount of parameters of BERT makes it slow during inference, which limits its usage in industry. Experiments show that the metric of traditional early exiting has great defects. In this work, we propose EU-BERT, an Efficient Unified named entity recognition framework via multi-exit BERT, to accelerate BERT inference on NER task. To handle nested and discontinuous NER tasks, we adopt W2NER [1], a unified NER framework using table filling technique. EU-BERT proposes a better metric for early exiting and uses contrastive learning to enhance its ability. Experiments on 9 benchmark NER datasets demonstrate that our method can improve the performance of multi-exit BERT on NER task while maintaining its inference speed.},
  keywords={Measurement;Industries;Text analysis;Knowledge graphs;Information processing;Benchmark testing;Natural language processing;BERT;Named Entity Recognition;Early Exit;Contrastive Learning},
  doi={10.1109/CIPAE60493.2023.00125},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10824634,
  author={Liu, Xiangfeng and Sun, Jianting and Lei, Aidi and Zhu, Junzhi},
  booktitle={2024 3rd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)}, 
  title={Research and Applications of Large Language Models for Converting Unstructured Data into Structured Data}, 
  year={2024},
  volume={},
  number={},
  pages={305-308},
  abstract={The rapid growth of data has led to a significant increase in unstructured data, such as text, audio, and images, which dominate modern information processing. However, the complexity of unstructured data presents challenges for automated analysis and processing. Converting unstructured data into structured formats is crucial for tasks like data mining, information extraction, and knowledge graph construction. Traditional methods that rely on manual rules or statistical models struggle with complex, context-dependent data. Recently, large language models (LLMs), such as GPT-4 and BERT, have demonstrated great potential in unstructured data processing due to their powerful natural language understanding and generation capabilities. This study explores the application of large language models in transforming unstructured data into structured formats. It begins by reviewing the limitations of traditional approaches and then presents a framework for unstructured data processing using LLMs, covering data preprocessing, information extraction, and structured representation. The study's experiments demonstrate the superior performance of LLMs in handling various types of unstructured data, particularly in tasks like named entity recognition, relation extraction, and contextual understanding. The results show that LLMs achieve higher accuracy and generalization while reducing reliance on manual rules. The paper discusses the strengths and limitations of this method, proposing future improvements, such as combining domain-specific knowledge for model optimization and expanding applications to multimodal data processing. The research highlights the promising role of large language models in converting unstructured data into structured formats.},
  keywords={Adaptation models;Accuracy;Data conversion;Large language models;Computational modeling;Manuals;Information retrieval;Data models;Data mining;Software engineering;Large language model;unstructured data;information extraction;data management system;database query processing},
  doi={10.1109/CBASE64041.2024.10824634},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10825160,
  author={Kim, Edward and Shrestha, Manil and Foty, Richard and DeLay, Tom and Seyfert-Margolis, Vicki},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search}, 
  year={2024},
  volume={},
  number={},
  pages={3421-3430},
  abstract={Creation and curation of knowledge graphs at scale can be used to exponentially accelerate the discovery, matching, and analysis of diseases in real-world data. While disease ontologies are useful for annotation, integration, and analysis of biological data, codified disease and procedure categories e.g. SNOMED-CT, ICD10, CPT, etc. rarely capture all of the nuances in a patient condition or, in the case of rare disease, may not even exist. Furthermore, there are multiple disease definitions used in data sources and publications, each having its own structure and hierarchy. Mapping between ontologies, finding disease clusters, and building a representation of the chosen disease area are resource-intensive, often requiring significant human capital. We propose the creation and curation of a patient knowledge graph utilizing large language model extraction techniques. In order to expand in volume and scale, knowledge graphs with generalized language capability allow for data to be extracted using natural language rather than being constrained by the exact terminology or hierarchy of existing ontologies. We develop a method of mapping back to existing ontologies such as MeSH, SNOMED-CT, RxNORM, HPO, etc. to ground the extracted entities to known entities in the medical community.We have access to one of the largest ambulatory care EHR databases in the country. To demonstrate the effectiveness of our method, we benchmark our extraction in a test set with over 33.6M unique patients, in the area of patient search. In this case study, we perform a patient search for a rare disease: Dravet syndrome. Dravet syndrome was codified as an ICD10 recognizable disease in October 2020. In the following research, we describe our method of the construction of patient-specific knowledge graphs and subsequent searches for patients who exhibit symptoms of a particular disease. Using patients with confirmed ICD10 codes for Dravet syndrome as our ground truth, we utilize our LLM-based entity extraction techniques and formalize an algorithmic way of characterizing patients in a grounded ontology to assist in mapping patients to specific diseases. Finally, we present the results of a real-world discovery method on Beta-propeller protein-associated neurodegeneration (BPAN), identifying patients with a rare disease, where no ground truth currently exists.},
  keywords={Proteins;Translation;Annotations;Terminology;Large language models;Knowledge graphs;Ontologies;Benchmark testing;Data mining;Diseases;Large Language Models;Knowledge Graphs;Ontology Mapping;Structured Extraction;Dravet Syndrome;Beta-propeller protein-associated neurodegeneration (BPAN)},
  doi={10.1109/BigData62323.2024.10825160},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{9576612,
  author={Zhu, Biru and Zhang, Xingyao and Gu, Ming and Deng, Yangdong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Knowledge Enhanced Fact Checking and Verification}, 
  year={2021},
  volume={29},
  number={},
  pages={3132-3143},
  abstract={As the Internet and social media offer increasing opportunities for organizations and individuals to publicize online contents, it has become essential to develop effective means to identify misinformation like fake news. Recently, fact checking systems have been regarded as a promising tool to automatically deal with large amounts of information. How to effectively take advantage of existing unstructured document knowledge bases and structured knowledge graphs to build robust fact checking systems, however, remains to be a challenge. In this paper, we propose a knowledge enhanced fact checking system, which leverages the Wikidata5M knowledge graph and Wikipedia documents to incorporate external knowledge into the claim to be checked for more robust and accurate fact checking. First, we devise a contextualized knowledge graph selection method to identify the most relevant sub-graph with the checked claim from the large knowledge graph. We then construct a novel claim-evidence-knowledge graph and use a graph attention network to integrate natural language evidence with structured knowledge triplets by allowing them to propagate information among each other. By integrating the claim, retrieved evidence and selected knowledge triplets in a unified claim-evidence-knowledge graph, our method improves the label accuracy of predicted claims by more than 4% on the FEVER dataset over state-of-the-art fact checking models.},
  keywords={Internet;Knowledge based systems;Encyclopedias;Online services;Semantics;Feature extraction;Tools;Automated fact checking;knowledge selection;knowledge enhanced fact checking},
  doi={10.1109/TASLP.2021.3120636},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{10388202,
  author={Bringsjord, Selmer and Slowik, John and Govindarajulu, Naveen Sundar and Giancola, Michael and Oswald, James and Ghosh, Rikhiya},
  booktitle={2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)}, 
  title={Affect-based Planning for a Meta-Cognitive Robot Sculptor: First Steps}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Today’s so-called “generative AI” is in the minds of some capable of specifically generating genuine visual art; DALL-E is an example. While in fact intensely skeptical, we grant for the sake of argument that the likes of DALL-E can in fact generate genuine visual art. However, we observe that fine visual artistry is by no means monolithic; in particular, some forms of fine visual artistry are seemingly harder than others for artificial agents to achieve. In the traditional ontology, perhaps the very hardest type of fine visual artistry for an AI to achieve is literary sculpture, an activity carried out in the human sphere at the very highest level by Rodin. Artificial agents that operate in this sphere must for obvious reasons be cognitive robots. We explain such literary sculpture in broad terms, making clear that it’s undertaken by the sculptor in question with the aim of bringing about certain affective states in the mind of the viewer of the sculpture. In short, literary sculpting is affect-driven. We provide a case study of robot sculpting, with help from pre-existing logic-based formalisms and automated-reasoning technology, and the cognitive robot PERI.2, operating as a sculptor. To our knowledge, ours is the very first foray into literary sculpture in AI and cognitive robotics.},
  keywords={Visualization;Affective computing;Art;Generative AI;Conferences;Ontologies;Planning;creativity;AI;cognitive robotics;sculpture;logic-based AI},
  doi={10.1109/ACIIW59127.2023.10388202},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{7970506,
  author={Pisarev, I. A.},
  booktitle={2017 XX IEEE International Conference on Soft Computing and Measurements (SCM)}, 
  title={Probabilistic methods of automated dynamic thesauri creation from heterogeneous knowledge sources}, 
  year={2017},
  volume={},
  number={},
  pages={96-98},
  abstract={Probabilistic methods of the automated formation of dynamic thesauri are considered on the basis of algorithms for processing, analyzing and classifying linguistic resources of a very large volume from heterogeneous data sources in application to the support systems of learning processes. Probabilistic methods are implemented in the automated software tool “Ontomaster-Ontology”. An essential feature of the implementation is the provision of automated processing of very large volumes of linguistic resources from heterogeneous sources. Probabilistic methods have been applied to the formation of thematic text corpora in Russian and English languages, extraction of terms of subject domains, classification of documents. Reprezented the results of the study of improving the efficiency of document classification based on the Bayesian approach are presented using the n-gram model and verbose terms. Probabilistic methods of computer linguistics are applied in the development of a system for supporting the process of teaching students in the direction “Information Systems and Technologies”.},
  keywords={Thesauri;Classification algorithms;Pragmatics;Probabilistic logic;Databases;Information systems;Bayes methods;algorithms of computational linguistics;classification;learning process Introduction},
  doi={10.1109/SCM.2017.7970506},
  ISSN={},
  month={May},}@INPROCEEDINGS{9887476,
  author={Dudija, Nidya and Natalia, Lezia and Alamsyah, Andry and Romadhony, Ade},
  booktitle={2022 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Identification of Extraversion and Neuroticism Personality Dimensions Using IndoBERT’s Deep Learning Model}, 
  year={2022},
  volume={},
  number={},
  pages={155-159},
  abstract={Human resources are essential for the business organization to adapt to change. Identifying the personality dimensions of new talent could help recruiters conduct the selection process of matching skilled talent to the organization’s needs. The objective of this study is to identify the personality dimensions corresponding to the job need, which correlates with extraversion and neuroticism. The legacy methodology to determine personality dimensions is through interviews or questionnaire surveys, but this process is costly and takes longer time to complete. This paper proposes a work on a person personality identification based on social media text as a complementary methodology. We utilize the textual data to support identifying new talent personality dimensions. In this study, we use IndoBERT model to capture person personality dimension based on their post on Twitter social media. As a result, our model achieves 96% accuracy in identifying extraversion and neuroticism personality dimensions. We also compare our result with the previous work based on the ontology model.},
  keywords={Deep learning;Costs;Social networking (online);Blogs;Ontologies;Communications technology;Fourth Industrial Revolution;Human Resource;Talent Selection;Personality Identification Dimension;Deep Learning;IndoBERT},
  doi={10.1109/IAICT55358.2022.9887476},
  ISSN={},
  month={July},}@INPROCEEDINGS{9896311,
  author={Rogachev, Aleksey and Melikhova, Elena},
  booktitle={2022 International Russian Automation Conference (RusAutoCon)}, 
  title={Automation of Architecture Justification and Parameters Selection of Artificial Neural Networks for Intelligent Detection of Cyber-Physical Threats}, 
  year={2022},
  volume={},
  number={},
  pages={908-912},
  abstract={The problems of improving the quality of training of deep artificial neural networks (ANN) for various applied tasks require automatization of the selection of hyperparameters of neural networks. The KerasTuner software toolkit can be used to automate the search for optimal values of ANN hyperparameters. It includes random search methods, Bayesian optimization, etc. The formation of training text samples for neural network identification of cyber-physical threats is a separate scientific and methodological task. The complexity of the problem is due to the diversity of the ontology of the key terms of the cyberphysical thesaurus, the variety of styles of lexicological content, as well as the partial intersection of the content of previously identified ontological categories. In the process of experimental study of hyperparameters of deep ANNs being developed, models of “embedding”, “bag of words” and dense vector representation in Python were compared. On the basis of a systematic approach, an information-morphological matrix of thematic blocks is constructed. In the conducted experiments, the values of parameters such as the number of convolutional blocks, the number of their filters, the type of activation functions, the parameters of the “dropout” layers, etc. were changed. The studied tools provided optimization of hyperparameters of the convolutional network, while the calculation time on the Colaboratory platform for the studied ANN architectures using GPU graphics accelerators was 5…9 o’clock. The developed modified algorithm for computer detection of cyberphysical threats in electronic resources allowed to substantiate alternative architectures and optimize the main hyperparameters of ANN.},
  keywords={Training;Automation;Graphics processing units;Artificial neural networks;Computer architecture;Ontologies;Cyber-physical systems;cyber-physical threat;artificial neural network;hyperparameter;intelligent detection;Automation},
  doi={10.1109/RusAutoCon54946.2022.9896311},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10507138,
  author={Anass, Bayaga},
  booktitle={2024 Conference on Information Communications Technology and Society (ICTAS)}, 
  title={Advancing STEM cognition with current AI landscape and systems}, 
  year={2024},
  volume={},
  number={},
  pages={20-25},
  abstract={Application of AI explores the potential of algorithms to ensure fairness, accuracy, and efficiency in grading students' performance, offering valuable insights into their strengths and areas for improvement. While the current AI landscape showcases remarkable progress, there are several areas ripe for exploration. One such avenue is AI steps to consider in STEM, wherein researchers aim to develop specialised steps/models to understand and generate domain-specific STEM content. The systematic literature review highlighted the importance of domain adaptation techniques for enhancing STEM comprehension by fine-tuning transformer-based language models like BERT. Integrating domain knowledge through ontology-based and context of STEM disciplines. Future research should focus on building domain-specific annotated datasets to improve the performance models in STEM comprehension. Additionally, exploring unsupervised domain adaptation techniques and leveraging domain-specific knowledge graphs can further enhance the NLP models’ adaptability to diverse STEM domains.},
  keywords={Adaptation models;Vocabulary;Systematics;Bibliographies;Laboratories;Knowledge graphs;Transformers;Data models;Artificial intelligence;STEM;Artificial Intelligence;STEM Education;Explainable AI;Natural Language Processing;AI-Augmented Laboratories},
  doi={10.1109/ICTAS59620.2024.10507138},
  ISSN={},
  month={March},}@INPROCEEDINGS{9206790,
  author={Shen, Tianhao and Wang, Xiaojie},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Multi-Domain Dialogue State Tracking with Hierarchical Task Graph}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  abstract={Multi-domain dialogue state tracking (DST), which tracks user goals and intentions across multiple domains, is a core task for multi-domain task-oriented dialogue system. Previous works in multi-domain DST focus on the open-vocabulary setting to alleviate the over-dependence on pre-defined ontology. However, they come up short of modeling the relationships among domains and slots in an explicit and efficient way. In this paper, we propose a multi-domain dialogue state tracker with hierarchical task graph (DST-HTG) to address the above issues. DST-HTG uses a copy mechanism to perform DST under the open-vocabulary setting, which makes our model eliminate the dependence on pre-defined full ontology. Moreover, we extend our DST model with a hierarchical task graph which has simple structure and rich semantic information to incorporate the relationships among domains and slots into DST process explicitly and efficiently. Empirical results show that DST-HTG achieves the state-of-the-art joint goal accuracy and slot accuracy in MultiWOZ 2.0, a recently proposed multi-domain task-oriented dialogue dataset, which indicates the effectiveness of our proposed model.},
  keywords={Task analysis;History;Ontologies;Context modeling;Generators;Fuses;Computational modeling;natural language processing;task-oriented dialogue systems;multi-domain dialogue state tracking;task graph},
  doi={10.1109/IJCNN48605.2020.9206790},
  ISSN={2161-4407},
  month={July},}@ARTICLE{9861204,
  author={Huang, Weiming and Xu, Shiting and Yuhan, Wang and Fan, Jin and Chang, Qingling},
  journal={IEEE Access}, 
  title={AttractionDetailsQA: An Attraction Details Focused on Chinese Question Answering Dataset}, 
  year={2022},
  volume={10},
  number={},
  pages={86215-86221},
  abstract={With the increase in the number of domestic tourists and the popularity of digital upgrades in attractions, it is crucial to develop a question-answering(QA) system about the details of the attractions. However, there is little work on attractions QA, and the main bottleneck is the lack of available datasets. While previous QA datasets usually focus on news domain like CNN/DAILYMAIL and NewsQA, we present the first large-scale dataset for QA over attraction details. To ensure that the data we collected are useful, we only gather the data from public travel information website. Unlike other QA datasets like SQuAD, which is labeled manually, we formed the dataset by manual and question-answer pair generation(QAG) annotated model. Finally, we obtained a dataset covering 2,808 attractions with a total of 18,245 QA pairs, including seven types of attraction details: location, time, component, area, layout, rating, and character. The dataset is available at https://github.com/wyman130/AttractionDetailsQA. Considering that QAG has not been much studied in attraction details, we experimented some QAG models on this dataset and obtained the benchmark. This provides a basis for subsequent improvements to the dataset and research on QAG in attraction details.},
  keywords={Annotations;Data models;Question answering (information retrieval);Manuals;Layout;Benchmark testing;Tourism industry;Attraction detail dataset;question-answering pair generation},
  doi={10.1109/ACCESS.2022.3181188},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9172685,
  author={Mordecai, Yaniv and James, Nicholas K. and Crawley, Edward F.},
  booktitle={2020 IEEE Aerospace Conference}, 
  title={Object-Process Model-Based Operational Viewpoint Specification for Aerospace Architectures}, 
  year={2020},
  volume={},
  number={},
  pages={1-15},
  abstract={Remote-controlled or autonomous multi-rotor air vehicles, or drones, have become common and commercially available even to individual consumers, mostly for imaging purposes. Drones appeal to mission architects looking to extend the toolbox provided to operators performing challenging missions such as public safety operations. However, careful analysis of the operational context and concept of operations must take place before major acquisitions. The purpose of this paper is to propose a model-based operational architecture definition framework, which is based on the Department of Defense Architecture Framework (DoDAF) ontology and uses Object Process Methodology (OPM) as its underlying modeling language. Through careful mapping of DoDAF Operational Viewpoint (OV) ontology to OPM ontology, we were able to show that the entire OV ontology can be covered by a small set of objects, processes, relations among them, and constructs comprising them. We then show how to instantiate the ontology to create a model of an actual architecture of interest (AoI) while maintaining strong typing of the model elements to ensure validity, integrity, consistency, and continuous compliance with the OV. We demonstrate our approach on the case of using drones in public safety enterprises for the purpose of crowd management in massively attended events and locations. The proposed framework allows for capturing ConOps and OpsCon in a lightweight, yet robust and consistent manner, and improve communication and concept validation between operational stakeholders and enterprise architects.},
  keywords={Conferences;Imaging;Ontologies;Safety;Stakeholders;Drones},
  doi={10.1109/AERO47225.2020.9172685},
  ISSN={1095-323X},
  month={March},}@ARTICLE{10348568,
  author={Johnston, Penny and Nogueira, Keiller and Swingler, Kevin},
  journal={IEEE Access}, 
  title={NS-IL: Neuro-Symbolic Visual Question Answering Using Incrementally Learnt, Independent Probabilistic Models for Small Sample Sizes}, 
  year={2023},
  volume={11},
  number={},
  pages={141406-141420},
  abstract={This paper is motivated by the challenge of providing accurate and contextually relevant answers to natural language questions about visual scenes, particularly in support of individuals with visual impairments. We present a system that is capable of incrementally learning both visual concepts and symbolic facts to answer natural language questions about visual scenes via rich concepts. Deep neural networks are used to learn a feature space from which visual classes are learned as independent probability distributions, allowing new classes to be added arbitrarily with small sample sizes and without the risk of catastrophic forgetting associated with traditional neural networks. Visual classes are not limited to object labels, but also include visual attributes. A knowledge graph is used to represent facts about objects, such as their actions, locations and the relationships between different objects. This allows facts to be stored explicitly and added incrementally. A large language model is used to translate between natural language questions and knowledge graph traversal queries, providing a natural visual question answering process.},
  keywords={Visualization;Natural languages;Knowledge graphs;Cognition;Question answering (information retrieval);Adaptation models;Training;Neuro-symbolic system;visual question answering;classification system;Gaussian mixture model;incremental learning},
  doi={10.1109/ACCESS.2023.3341007},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10729340,
  author={Li, Daifeng and Xu, Fan},
  booktitle={2024 IEEE 2nd International Conference on Sensors, Electronics and Computer Engineering (ICSECE)}, 
  title={The Deep Integration of Knowledge Graphs and Large Language Models: Advancements, Challenges, and Future Directions}, 
  year={2024},
  volume={},
  number={},
  pages={157-162},
  abstract={In recent years, large language models have emerged with amazing capabilities, but they also have limitations such as hallucinations and black boxes, while knowledge graphs have accurate knowledge and symbolic reasoning capabilities. Therefore, the integration of knowledge graph and large language model becomes necessary. The paper systematically conducts the recent research pertaining to the integration of knowledge graph with large language mode. By analyzing the methods and locations of their confluence, we propose a unifying framework that aims to facilitate comprehension and inspiration for fellow researchers in related fields. This framework not only consolidates existing knowledge but also enhances the translational potential of research by delineating innovative pathways for practical implementation.},
  keywords={Knowledge engineering;Accuracy;Large language models;Government;Knowledge graphs;Reliability engineering;Robustness;Telecommunications;Sensors;Optimization;knowledge graphs;large language models;deep integration;system review},
  doi={10.1109/ICSECE61636.2024.10729340},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10692530,
  author={Rong, Shiqiang and Li, Rili and Sun, Xiaocui and Yi, Faling},
  booktitle={2024 6th International Conference on Internet of Things, Automation and Artificial Intelligence (IoTAAI)}, 
  title={Research on Named Entity Recognition Algorithm for Diabetes Intelligent Question and Answer System}, 
  year={2024},
  volume={},
  number={},
  pages={631-637},
  abstract={Named Entity Recognition (NER) is an important basis for constructing knowledge graph. Combining knowledge graph with pre-trained large language model is an important research direction of current intelligent question and answer system. Based on the knowledge system of daily prevention and management of diabetes and the corresponding answers, the corresponding named entity recognition method is studied, and the recognition method of dictionary rules and Bert + BiLSTM + CRF model fusion is proposed. Firstly, the dictionary rules using AC automaton character matching and context analysis method to effectively identify the disease, symptoms, indicators and other fixed types of entities and classification are built; then, the Bert + BiLSTM + CRF model is constructed to achieve good performances in the identification of extensible entities such as inspections, drugs, treatment methods, and food. In the process of training Bert + BiLSTM + CRF model, various strategies such as entity replacement, entity masking and entity splicing are adopted, which effectively improves the accuracy of recognition. The experimental results show that the recognition accuracy of the fusion algorithm reaches 93 %.},
  keywords={Training;Dictionaries;Accuracy;Splicing;Prevention and mitigation;Large language models;Knowledge based systems;Named entity recognition;Knowledge graphs;Diabetes;Knowledge Graph;Entity Recognition;Dictionary Rules;Bert + BiLSTM + CRF;Diabetes},
  doi={10.1109/IoTAAI62601.2024.10692530},
  ISSN={},
  month={July},}@ARTICLE{10879360,
  author={Zhao, Wenqing and Ding, Yingxue and Zhang, Le and Liu, Bin and Yang, Cen and Zhao, Zhenhuan and Zhao, Zhenbing and Zhai, Yongjie and Xu, Minfu},
  journal={IEEE Transactions on Power Delivery}, 
  title={CMKR-PBDM: A Transmission Line Pin-missing Bolts Detection Method Based on Cross-media and Knowledge Reasoning}, 
  year={2025},
  volume={},
  number={},
  pages={1-10},
  abstract={To address the limited single-image information-processing method, insufficient global knowledge-processing capability of the traditional model, and the lack of explainability in detection results for the task of transmission line pin-missing bolts detection, the researchers propose a pin-missing bolts detection method based on cross-media and knowledge reasoning (CMKR-PBDM). First, we construct a fitting-bolt image-text pair dataset and a bolt knowledge graph (BoltKG). Subsequently, a cross-media bolt knowledge fusion model (CBKFM) is proposed, thus generating the image's overall text description by fusing the global knowledge extracted by the FB-GPT with the local knowledge captured by YOLOv8. Finally, the study proposes a large language reasoning model based on the knowledge graph (LLRM-KG), which utilizes BoltKG to guide the big language model in performing knowledge reasoning on the CBKFM output information; thus, explainable pin-missing bolts detection results are obtained. In the experimental stage, the researchers select bolts on four types of background fittings as experimental objects. The experimental results indicate that the method not only improves the accuracy of pin-missing bolts detection, but also makes the pin-missing bolts detection results explainable.},
  keywords={Fasteners;Cognition;Fitting;Pins;Feature extraction;Power transmission lines;Shock absorbers;Vibrations;Knowledge graphs;Electronic mail;Bolt;Knowledge graph;Knowledge reasoning;Cross-media;Pin-missing detection;Transmission line},
  doi={10.1109/TPWRD.2025.3540475},
  ISSN={1937-4208},
  month={},}@INPROCEEDINGS{10849427,
  author={Armary, Pauline and El-Vaigh, Cheikh-Brahim and Spicher, Antoine and Narsis, Ouassila Labbani and Nicolle, Christophe},
  booktitle={2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Identifying Logical Patterns in Text for Reasoning}, 
  year={2024},
  volume={},
  number={},
  pages={837-844},
  abstract={Translating unstructured text into logical format is a key challenge for building ontologies automatically and addressing deductive inference. Most of the approaches have tackled the identification of concepts and relations in text, but few of them have addressed the most complex axioms like class expression subsumption. This work proposes DeLIR, a neuro-symbolic approach to identify complex logical patterns in text by combining a grammatical translation of dependency parsing trees and a fine-tuned Large language Model (LLM). DeLIR combines the strength of the parsing accuracy provided by a grammatical approach and pattern flexibility provided by a finetuned LLM. We evaluated our approach on FOLIO dataset for both translation capacity and inference capability. Our grammatical approach has a perfect parsing accuracy and combining the grammatical approach with LLMs improves the LLMS translation capacity: tinyLlama, T5-small-text2logic, Llama-7B and Mistral-7B. We also evaluate the inference capacity of the different LLMs. Mistral-7B, while being smaller than the state-of-the-art approach using GPT-4, presents similar results to predict the correct inference labels.},
  keywords={Hands;Translation;Accuracy;Large language models;Zero shot learning;Buildings;Ontologies;Syntactics;Cognition;Ontology Learning;Translation to Logic;Natural Language Inference},
  doi={10.1109/ICTAI62512.2024.00122},
  ISSN={2375-0197},
  month={Oct},}@ARTICLE{10592819,
  author={Sun, Zhigang and Wang, Zixu and Halilaj, Lavdim and Luettin, Juergen},
  journal={IEEE Robotics and Automation Letters}, 
  title={SemanticFormer: Holistic and Semantic Traffic Scene Representation for Trajectory Prediction Using Knowledge Graphs}, 
  year={2024},
  volume={9},
  number={9},
  pages={7381-7388},
  abstract={Trajectory prediction in autonomous driving relies on accurate representation of all relevant contexts of the driving scene, including traffic participants, road topology, traffic signs, as well as their semantic relations to each other. Despite increased attention to this issue, most approaches in trajectory prediction do not consider all of these factors sufficiently. We present SemanticFormer, an approach for predicting multimodal trajectories by reasoning over a semantic traffic scene graph using a hybrid approach. It utilizes high-level information in the form of meta-paths, i.e. trajectories on which an agent is allowed to drive from a knowledge graph which is then processed by a novel pipeline based on multiple attention mechanisms to predict accurate trajectories. SemanticFormer comprises a hierarchical heterogeneous graph encoder to capture spatio-temporal and relational information across agents as well as between agents and road elements. Further, it includes a predictor to fuse different encodings and decode trajectories with probabilities. Finally, a refinement module assesses permitted meta-paths of trajectories and speed profiles to obtain final predicted trajectories. Evaluation of the nuScenes benchmark demonstrates improved performance compared to several SOTA methods. In addition, we demonstrate that our knowledge graph can be easily added to two graph-based existing SOTA methods, namely VectorNet and LaFormer, replacing their original homogeneous graphs. The evaluation results suggest that by adding our knowledge graph the performance of the original methods is enhanced by 5% and 4%, respectively.},
  keywords={Trajectory;Knowledge graphs;Semantics;Ontologies;Encoding;Predictive models;Transformers;Semantic scene understanding;autonomous agents;intelligent transportation systems},
  doi={10.1109/LRA.2024.3426386},
  ISSN={2377-3766},
  month={Sep.},}@INPROCEEDINGS{9534360,
  author={Li, Jipeng and Sun, Yujing and Li, Chenhui and Hu, Yanpeng and Wang, Changbo},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Industry Chain Graph Building Based on Text Semantic Association Mining}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={The current volume of data in the field of securities investment is increasing dramatically. Simultaneously, the linkage of data from multiple parties makes investment reasoning decisions more challenging than ever. In response to this problem, the financial field's knowledge graph can improve the efficiency, depth, and breadth of financial practitioners' information analysis. Some existing financial knowledge graphs analyze the shareholding relationship between companies. Still, because they are limited to observing data from the company's perspective, users without professional industry background cannot quickly find the industry factors of stock market changes. This paper proposes a financial knowledge graph from the industry chain's perspective. This paper builds upstream and downstream relationships between industries through Transformer-based bidirectional encoder to mine potential industry chain associations from text data and completes the long industry chain of the stock market. This paper also builds a visualization system to display and explore the connection between listed companies and industries. Users can inspect the industry chain's composition and each company's revenue status and stock market conditions in the industry chain. The experiment shows that when the market price fluctuation is detected, the stock price fluctuation can be traced back to its origin in the knowledge graph.},
  keywords={Industries;Visualization;Fluctuations;Semantics;Neural networks;Companies;Transformers;Industry Chain;Relation Extraction},
  doi={10.1109/IJCNN52387.2021.9534360},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9879230,
  author={Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={TableFormer: Table Structure Understanding with Transformers}, 
  year={2022},
  volume={},
  number={},
  pages={4604-4613},
  abstract={Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc, since they enhance their predictive capabilities. Unfortu-nately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct iden-tification of the table-structure from an image is a nontrivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from program-matic PDF's directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.},
  keywords={Deep learning;Training;Shape;Optical character recognition;Object detection;Search engines;Transformers;retrieval; Vision applications and systems;categorization;Document analysis and understanding; Deep learning architectures and techniques; Recognition: detection},
  doi={10.1109/CVPR52688.2022.00457},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10192008,
  author={Liu, Shu and Ye, Zhiqiang and Liao, Jian and Wu, Jinxin and Li, Zhao},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Unsupervised Product Title Optimization Based on Search Behavior Knowledge in E-commerce}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Nowadays, E-commercial platforms such as Taobao and Amazon, provide massive products which greatly improve the convenience of online shopping. However, there have been many redundant and lengthy titles, which are misleading and hurt the experience of consumers. Under this background, the product title compression task has been proposed to generate readable short titles. The existing supervised models rely on paired corpora, which is often expensive to annotate. Whereas unsupervised methods are not adopted in the E-commercial scenario due to lack of domain knowledge. To overcome the above limitations, we propose an unsupervised title compression method based on search behavior graph. Specifically, we build the knowledge graph extracted from search queries to detect redundancy. Moreover, click behaviors are utilized to give a clue to recognize inaccurate information. Then we employ a novel graph-to-text model consisting of an edge-importance encoder and a length-countdown decoder. The encoder combines the title with both query knowledge and click behaviors, and the decoder generates the compressed title with the desired length. Experiments on datasets show that our model outperforms the state-of-the-art models on both automatic and human evaluations. Moreover, further analysis demonstrates that our method has the ability to detect redundancy and keep the appropriate information.},
  keywords={Image edge detection;Neural networks;Termination of employment;Knowledge graphs;Search problems;Behavioral sciences;Decoding;Unsupervised Title Compression Framework;Query Knowledge;Click Behavior},
  doi={10.1109/IJCNN54540.2023.10192008},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10711793,
  author={Pan, Xinyu and Gong, Jie and Wen, Sijie and Zhuang, Weibin and Li, Xinyu},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Mining User Requirement Scenarios and Generating Design Solutions for Rehabilitation Aids Based on Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={3155-3161},
  abstract={The development of the rehabilitation aids industry for the disabled has been pivotal in recent years, particularly in the personalized design of lower limb rehabilitation aids. Facing challenges in meeting individualized demands in design practice and the information gap between medical professionals and users, we propose a design Knowledge Graph (KG) method based on the Function-Behavior-Structure (FBS) model. This approach utilizes open-source large language models (LLMs) and fine-tunes them with instruction data generated by self-instructions to improve the accuracy of user requirements mining. The method aims to enhance the personalization and innovation of rehabilitation aids design through the integration of KG and LLM, effectively narrowing the cognitive gap between service providers and users. The anticipated results of the study are expected to promote efficient innovation in rehabilitation aids design, better meeting the needs of the disabled community.},
  keywords={Industries;Technological innovation;Computer aided software engineering;Automation;Accuracy;Large language models;Conferences;Knowledge graphs;Data mining},
  doi={10.1109/CASE59546.2024.10711793},
  ISSN={2161-8089},
  month={Aug},}@ARTICLE{9525274,
  author={Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice C. and Mofrad, Mohammad R.K.},
  journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics}, 
  title={TripletProt: Deep Representation Learning of Proteins Based On Siamese Networks}, 
  year={2022},
  volume={19},
  number={6},
  pages={3744-3753},
  abstract={Pretrained representations have recently gained attention in various machine learning applications. Nonetheless, the high computational costs associated with training these models have motivated alternative approaches for representation learning. Herein we introduce TripletProt, a new approach for protein representation learning based on the Siamese neural networks. Representation learning of biological entities which capture essential features can alleviate many of the challenges associated with supervised learning in bioinformatics. The most important distinction of our proposed method is relying on the protein-protein interaction (PPI) network. The computational cost of the generated representations for any potential application is significantly lower than comparable methods since the length of the representations is significantly smaller than that in other approaches. TripletProt offers great potentials for the protein informatics tasks and can be widely applied to similar tasks. We evaluate TripletProt comprehensively in protein functional annotation tasks including sub-cellular localization (14 categories) and gene ontology prediction (more than 2000 classes), which are both challenging multi-class, multi-label classification machine learning problems. We compare the performance of TripletProt with the state-of-the-art approaches including a recurrent language model-based approach (i.e., UniRep), as well as a protein-protein interaction (PPI) network and sequence-based method (i.e., DeepGO). Our TripletProt showed an overall improvement of F1 score in the above mentioned comprehensive functional annotation tasks, solely relying on the PPI network. Availability: The source code and datasets are available at https://github.com/EsmaeilNourani/TripletProt.},
  keywords={Proteins;Task analysis;Computational modeling;Training;Protein engineering;Feature extraction;Computational efficiency;Protein representation learning;triplet loss;siamese networks},
  doi={10.1109/TCBB.2021.3108718},
  ISSN={1557-9964},
  month={Nov},}@ARTICLE{10504595,
  author={Wu, Xiang and Wang, Huanhuan and Zhang, Yongting and Zou, Baowen and Hong, Huaqing},
  journal={IEEE Transactions on Learning Technologies}, 
  title={A Tutorial-Generating Method for Autonomous Online Learning}, 
  year={2024},
  volume={17},
  number={},
  pages={1532-1541},
  abstract={Generative artificial intelligence has become the focus of the intelligent education field, especially in the generation of personalized learning resources. Current learning resource generation methods recommend customized courses based on learning styles and interests, improving learning efficiency. However, these methods cannot generate personalized tutorials according to learners’ preferences, nor can they adjust tutorial content as moods or levels of knowledge change. Therefore, this study develops an intelligent tutorial-generating system (Self-GT) for self-aid learning, integrating cognitive computing and generative learning to capture learners’ dynamic preferences. The critical components of Self-GT are the tutorial-generating model based on cyclic deep reinforcement learning (RL) and the multimodal knowledge graph containing complex relationships. Specifically, the proposed RL model dynamically explores learners’ preferences from the temporal dimension, enabling RL agents to express learning behavior characteristics accurately and generate personalized tutorials. Then, relying on the internal self-developed education base and external Internet sources, a multimodal knowledge graph with multiple self-defined relationships is designed to enhance the precision of tutorial generation. Finally, the experimental results indicate that the Self-GT performs well in generating tutorials and has been successfully applied in the generating tutorial for “Hospital Network Architecture Planning and Design.”},
  keywords={Tutorials;Knowledge graphs;Hospitals;Visualization;Task analysis;Sociology;Scalability;Cognitive computing;course tutorial generating;deep reinforcement learning (RL);generative learning;multimodal knowledge graph},
  doi={10.1109/TLT.2024.3390593},
  ISSN={1939-1382},
  month={},}@INPROCEEDINGS{9577534,
  author={Marino, Kenneth and Chen, Xinlei and Parikh, Devi and Gupta, Abhinav and Rohrbach, Marcus},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA}, 
  year={2021},
  volume={},
  number={},
  pages={14106-14116},
  abstract={One of the most challenging question types in VQA is when answering the question requires outside knowledge not present in the image. In this work we study open-domain knowledge, the setting when the knowledge required to answer a question is not given/annotated, neither at training nor test time. We tap into two types of knowledge representations and reasoning. First, implicit knowledge which can be learned effectively from unsupervised language pretraining and supervised training data with transformer-based models. Second, explicit, symbolic knowledge encoded in knowledge bases. Our approach combines both—exploiting the powerful implicit reasoning of transformer models for answer prediction, and integrating symbolic representations from a knowledge graph, while never losing their explicit semantics to an implicit embedding. We combine diverse sources of knowledge to cover the wide variety of knowledge needed to solve knowledge-based questions. We show our approach, KRISP (Knowledge Reasoning with Implicit and Symbolic rePresentations), significantly out-performs state-of-the-art on OK-VQA, the largest available dataset for open-domain knowledge-based VQA. We show with extensive ablations that while our model successfully exploits implicit knowledge reasoning, the symbolic answer module which explicitly connects the knowledge graph to the answer vocabulary is critical to the performance of our method and generalizes to rare answers.1},
  keywords={Training;Vocabulary;Knowledge based systems;Semantics;Training data;Knowledge representation;Predictive models},
  doi={10.1109/CVPR46437.2021.01389},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{10753166,
  author={Lotfy, Abdelrahman and Ashraf, Ahmed and Barakat, Mariam and Saleh, Kirollos and Ahmed, Eslam and Sherif, Bassel and Badawy, Ali Nasser and Khoriba, Ghada},
  booktitle={2024 6th Novel Intelligent and Leading Emerging Sciences Conference (NILES)}, 
  title={A Comparative Analysis of Large Language Models for Automated Course Content Generation from Books}, 
  year={2024},
  volume={},
  number={},
  pages={437-442},
  abstract={Large Language Models (LLMs) have emerged as powerful tools for extracting course topics from textbooks in today's fast-paced educational landscape. Additionally, harnessing the potential of Knowledge Graphs to visualize the mutuality among topics enhances the informativeness of the extracted content. This paper presents a comprehensive comparative study that explores and assesses the effectiveness of different LLMs in extracting, identifying, and summarizing course topics within textbooks and generating knowledge graphs to visualize topic interdependencies. Moreover, we present a comprehensive methodology for knowledge graph development, incorporating specialized models, GPT2, Falcon 7B, and Llama-2-7b-chat-hf, fine-tuned with eight book tables of contents. Also, we have used Llama3, Llama3.1, Gemma, and Mistral-Nemo as a zero-shot model. Our findings show that llama3 has achieved the best performance among the zero-shot models in the following constraints: quality of content, correctness, clarity, and overall rating. Also, GPT2- Large excels in generating meaningful content, while GPT2-Base performs efficiently. In addition, challenges in knowledge graph integration were addressed by representing table of content data as knowledge graphs, providing more meaningful insights. This research enhances knowledge representation, demonstrating LLMs' value in knowledge graphs and data balance optimization.},
  keywords={Analytical models;Large language models;Education;Knowledge graphs;Tutorials;Organizations;Data mining;Tuning;Optimization;Context modeling;Large Language Models (LLMs);Knowledge Graphs & Finetuning},
  doi={10.1109/NILES63360.2024.10753166},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10698122,
  author={Prudhvith, Tavva and Swattik, Chakrabarty and Prakash, Selvakumar},
  booktitle={2024 International Conference on Electrical, Computer and Energy Technologies (ICECET}, 
  title={Enhancing Retrieval Augmented Generation Systems with Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Knowledge graphs play a pivotal role in information retrieval, yet their effectiveness can be further optimized. This paper introduces a comprehensive approach to enriching knowledge graphs, addressing contemporary challenges in answering user queries. Our methodology integrates key phrase extraction indulged with a custom prompt to create a Knowledge Graph (KG) with every node connected with each other, node embedding generation w.r.t properties, and an autonomous updating agent during the creation of KG and inference. Furthermore, we explore the incorporation of traditional vector search to enhance contextual understanding. Our experiments demonstrate a substantial improvement in accuracy, reaching approximately 96% compared to traditional KG approaches and our creation of KG process. Notably, the hybrid model sometimes outperforms the Retrieval-Augmented Generation (RAG) system, showcasing the efficacy of the integrated approach. This enhancement is attributed to the methodology proposed coupled with the additional context provided by traditional vector search. The results underscore the significance of our approach in delivering more accurate and contextually relevant information, showcasing the potential of this integrated method in advancing knowledge graph systems.},
  keywords={Electric potential;Accuracy;Knowledge graphs;Information retrieval;Vectors;Hybrid power systems;Knowledge Graphs;Key Phrase Extraction;Embeddings;Vector Search;Information Retrieval;GPT4},
  doi={10.1109/ICECET61485.2024.10698122},
  ISSN={},
  month={July},}@INPROCEEDINGS{9534192,
  author={Ranade, Priyanka and Piplai, Aritran and Mittal, Sudip and Joshi, Anupam and Finin, Tim},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Generating Fake Cyber Threat Intelligence Using Transformer-Based Models}, 
  year={2021},
  volume={},
  number={},
  pages={1-9},
  abstract={Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as training input to subvert cyber defense systems, forcing their models to learn incorrect inputs to serve the attackers' malicious needs. In this paper, we show how to automatically generate fake CTI text descriptions using transformers. Given an initial prompt sentence, a public language model like GPT-2 with fine-tuning can generate plausible CTI text that can mislead cyber-defense systems. We use the generated fake CTI text to perform a data poisoning attack on a Cybersecurity Knowledge Graph (CKG) and a cybersecurity corpus. The attack introduced adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. We evaluate with traditional approaches and conduct a human evaluation study with cyber-security professionals and threat hunters. Based on the study, professional threat hunters were equally likely to consider our fake generated CTI and authentic CTI as true.},
  keywords={Training;Neural networks;Transformers;Cognition;Computer crime;Open source software;Cybersecurity;Cyber Threat Intelligence;Artificial Intelligence;Data Poisoning Attack},
  doi={10.1109/IJCNN52387.2021.9534192},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9835701,
  author={Zhong, Ziyue and Zhang, Meihui and Fan, Ju and Dou, Chenxiao},
  booktitle={2022 IEEE 38th International Conference on Data Engineering (ICDE)}, 
  title={Semantics Driven Embedding Learning for Effective Entity Alignment}, 
  year={2022},
  volume={},
  number={},
  pages={2127-2140},
  abstract={Knowledge-based data service has become an emerging form of service in the world wide web (WWW). To ensure the service quality, a comprehensive knowledge base has to be constructed. Knowledge base integration is often a primary way to improve the completeness. In this paper, we focus on the fundamental problem in knowledge base integration, i.e., entity alignment (EA). EA has been studied for years. Traditional approaches focus on the symbolic features of entities and propose various similarity measures to identify equivalent entities. With recent development in knowledge graph representation learning, embedding-based entity alignment has emerged, which encodes the entities into vectors according to the semantic or structural information and computes the relatedness of entities based on the vector representation. While embedding-based approaches achieve promising results, we identify some important information that are not well exploited in existing works: 1) The neighboring entities contribute differently in the EA process, and should be carefully assigned the importance in learning the relatedness of entities; 2) The attribute values (especially the long texts) contain rich semantics that can build supplementary associations between entities. To this end, we propose SDEA - a Semantics Driven entity embedding method for Entity Alignment. SDEA consists of two modules, namely attribute embedding and relation embedding. The attribute embedding captures the semantic information from attribute values with a pre-trained transformer-based language model. The relation embedding selectively aggregates the semantic information from neighbors using a GRU model equipped with an attention mechanism. Both attribute embedding and relation embedding are driven by semantics, building bridges between entities. Experimental results show that our method significantly outperforms the state-of-the-art approaches on three benchmarks.},
  keywords={Representation learning;Conferences;Semantics;Knowledge based systems;Buildings;Benchmark testing;World Wide Web;Entity Alignment;Semantics Driven;Transformer;Knowledge Base Integration},
  doi={10.1109/ICDE53745.2022.00205},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10203925,
  author={Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Wang, Zirui and Chang, Kai-Wei and Sun, Yizhou and Schmid, Cordelia and Ross, David A. and Fathi, Alireza},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory}, 
  year={2023},
  volume={},
  number={},
  pages={23369-23379},
  abstract={In this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REVEAL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries. Reveal consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowledge (e.g. image-text pairs, question answering pairs, knowledge graph triplets, etc.) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Furthermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that Reveal achieves state-of-the-art results on visual question answering and image captioning. The project page of this work is reveal. github. io.},
  keywords={Visualization;Computer vision;Fuses;Memory management;Knowledge graphs;Generators;Question answering (information retrieval);Vision;language;and reasoning},
  doi={10.1109/CVPR52729.2023.02238},
  ISSN={2575-7075},
  month={June},}@INPROCEEDINGS{9623313,
  author={Latif, Shahid and Agarwal, Shivam and Gottschalk, Simon and Chrosch, Carina and Feit, Felix and Jahn, Johannes and Braun, Tobias and Tchenko, Yanick Christian and Demidova, Elena and Beck, Fabian},
  booktitle={2021 IEEE Visualization Conference (VIS)}, 
  title={Visually Connecting Historical Figures Through Event Knowledge Graphs}, 
  year={2021},
  volume={},
  number={},
  pages={156-160},
  abstract={Knowledge graphs store information about historical figures and their relationships indirectly through shared events. We developed a visualization system, VisKonnect, for analyzing the intertwined lives of historical figures based on the events they participated in. A user’s query is parsed for identifying named entities, and related data is retrieved from an event knowledge graph. While a short textual answer to the query is generated using the GPT-3 language model, various linked visualizations provide context, display additional information related to the query, and allow exploration.},
  keywords={Conferences;Natural languages;Data visualization;Context modeling;Knowledge graphs;general public;question answering;visualization;natural language generation},
  doi={10.1109/VIS49827.2021.9623313},
  ISSN={},
  month={Oct},}@ARTICLE{9770789,
  author={Rony, Md Rashad Al Hasan and Chaudhuri, Debanjan and Usbeck, Ricardo and Lehmann, Jens},
  journal={IEEE Access}, 
  title={Tree-KGQA: An Unsupervised Approach for Question Answering Over Knowledge Graphs}, 
  year={2022},
  volume={10},
  number={},
  pages={50467-50478},
  abstract={Most Knowledge Graph-based Question Answering (KGQA) systems rely on training data to reach their optimal performance. However, acquiring training data for supervised systems is both time-consuming and resource-intensive. To address this, in this paper, we propose Tree-KGQA, an unsupervised KGQA system leveraging pre-trained language models and tree-based algorithms. Entity and relation linking are essential components of any KGQA system. We employ several pre-trained language models in the entity linking task to recognize the entities mentioned in the question and obtain the contextual representation for indexing. Furthermore, for relation linking we incorporate a pre-trained language model previously trained for language inference task. Finally, we introduce a novel algorithm for extracting the answer entities from a KG, where we construct a forest of interpretations and introduce tree-walking and tree disambiguation techniques. Our algorithm uses the linked relation and predicts the tree branches that eventually lead to the potential answer entities. The proposed method achieves 4.5% and 7.1% gains in F1 score in entity linking tasks on LC-QuAD 2.0 and LC-QuAD 2.0 (KBpearl) datasets, respectively, and a 5.4% increase in the relation linking task on LC-QuAD 2.0 (KBpearl). The comprehensive evaluations demonstrate that our unsupervised KGQA approach outperforms other supervised state-of-the-art methods on the WebQSP-WD test set (1.4% increase in F1 score) - without training on the target dataset.},
  keywords={Prediction algorithms;Context modeling;Question answering (information retrieval);Training data;Information retrieval;Knowledge engineering;Knowledge based systems;information retrieval;question answering;entity linking;relation linking;indexing;pre-trained language models},
  doi={10.1109/ACCESS.2022.3173355},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10449869,
  author={Omar, Mussa A.},
  booktitle={2023 IEEE 11th International Conference on Systems and Control (ICSC)}, 
  title={Measurement of ChatGPT Performance in Mapping Natural Language Speficaction into an Entity Relationship Diagram}, 
  year={2023},
  volume={},
  number={},
  pages={530-535},
  abstract={This paper explores the entity relationship diagram, a popular conceptual model used to depict entities, attributes, and relationships graphically. To help with this, we use ChatGPT, a sophisticated language model based on the GPT architecture, which can translate natural language text into an entity relationship diagram. The paper details the process of evaluating how well ChatGPT can perform compared to other state-of-the-art approaches for entity and relationship extraction. Our experimental findings demonstrate the strong ability of ChatGPT to translate natural language text into entity relationship diagrams, which has potential applications for knowledge graph building, data integration, and database schema design. Moreover, it can aid in automating the extraction and organization of information from unstructured text data, thereby simplifying the study of complex systems.},
  keywords={Adaptation models;Natural languages;Machine learning;Companies;Chatbots;Task analysis;Software engineering;entity relationship diagram;ChatGPT;natural language processing},
  doi={10.1109/ICSC58660.2023.10449869},
  ISSN={2379-0067},
  month={Dec},}@INPROCEEDINGS{10650292,
  author={Liu, Ching-Hsuan and Chen, Chih-Ming and Lou, Jing-Kai and Tsai, Ming-Feng and Huang, Jiun-Lang and Wang, Chuan-Ju},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={SARA: Semantic-assisted Reinforced Active Learning for Entity Alignment}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  abstract={This paper introduces SARA, a semantic-assisted reinforced active learning framework for enhancing entity alignment (EA) under limited supervision scenarios. SARA addresses the challenges of EA in real-world scenarios, including knowledge graph heterogeneity and limited training ground truth. SARA effectively selects valuable entity pairs with limited labeled data by combining reinforced active learning and semantic information. It utilizes a pair-wise language model based on Sentence-BERT to learn informative name embeddings that capture entity name semantics. These embeddings are combined with structural embeddings and trained using a novel semantic-assisted alignment loss. Extensive experiments on benchmark datasets and a real-world dataset demonstrate the superiority of SARA over existing approaches, particularly in limited labeled data scenarios. The paper also provides insights into fine-tuning strategies, presents ablation studies, and conducts sensitivity analyses to validate the effectiveness of SARA.},
  keywords={Training;Sensitivity analysis;Semantics;Neural networks;Knowledge graphs;Benchmark testing;Data models},
  doi={10.1109/IJCNN60899.2024.10650292},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{9582475,
  author={Mandel, Constantin and Böning, Jannis and Behrendt, Matthias and Albers, Albert},
  booktitle={2021 IEEE International Symposium on Systems Engineering (ISSE)}, 
  title={A Model-Based Systems Engineering Approach to Support Continuous Validation in PGE - Product Generation Engineering}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Increasing customer demands, especially regarding functionality, safety and environmental sustainability, are major drivers of nowadays product development processes. Those increasing demands as well as today’s product development context of distributed interdisciplinary development teams lead to an increasing complexity of systems as well as their respective development processes. MBSE - Model-Based Systems Engineering is regarded as a promising approach to cope with this complexity. MBSE aims at supporting during the whole product lifecycle in system analysis, requirements management, design as well as verification and validation. Especially validation plays a central role in product development as it is the only activity that can ensure customer satisfaction and thus a successful product on the market. However, comprehensive MBSE-approaches to support validation in product development seem to be missing. This paper describes such a MBSE approach to support validation in product development. The approach includes an ontology of terms and their interrelations in the context of validation. The ontology is used to construct viewpoints, views and a modeling framework to structure a system model in the understanding of MBSE. In addition, a modeling method interacting with the constructed views is developed and presented. The presented approach aims at enabling a continuous validation concept, starting in the early phase of PGE - Product Generation Engineering and continuing throughout the entire lifecycle. Furthermore, the approach should support in integrating the development of products and appropriate validation systems, creating a consistent traceability of information throughout the created models. Finally, a specific focus of the approach lies on usability in order to guarantee individual and organizational acceptance. This acceptance is of particular importance to realize a human centered development as it is envisioned in approaches such as ASE - Advanced Systems Engineering.},
  keywords={Requirements management;Green products;Customer satisfaction;Ontologies;Product development;Complexity theory;Safety;MBSE;Validation;Continuous Validation Concept;Modeling Framework;PGE;Product Generation Engineering},
  doi={10.1109/ISSE51541.2021.9582475},
  ISSN={2687-8828},
  month={Sep.},}@ARTICLE{10711243,
  author={Malusare, Aditya and Aggarwal, Vaneet},
  journal={IEEE Transactions on Computational Biology and Bioinformatics}, 
  title={Improving Molecule Generation and Drug Discovery With a Knowledge-Enhanced Generative Model}, 
  year={2025},
  volume={22},
  number={1},
  pages={375-381},
  abstract={Recent advancements in generative models have established state-of-the-art benchmarks in the generation of molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called KARL. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity, and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while ensuring validity and synthesizability. KARL outperforms state-of-the-art generative models on both unconditional and targeted generation tasks.},
  keywords={Biological system modeling;Drugs;Knowledge graphs;Computational modeling;Biological information theory;Drug discovery;Diseases;Context modeling;Training;Data models;Diffusion models;drug discovery;generative AI;knowledge representation},
  doi={10.1109/TCBB.2024.3477313},
  ISSN={2998-4165},
  month={Jan},}@INPROCEEDINGS{10749735,
  author={Casalicchio, Emiliano and Cotumaccio, Alberto},
  booktitle={2024 IEEE International Conference on Cloud Engineering (IC2E)}, 
  title={AI-CRAS: AI-driven Cloud Service Requirement Analysis and Specification}, 
  year={2024},
  volume={},
  number={},
  pages={11-21},
  abstract={Automated analysis and specification of software requirements expressed in natural language is a challenge addressed by the research community and is becoming a reality thanks to the advances in Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques. While the research community focuses mainly on generic software requirements or specialized solutions for security requirements, we find a gap in the automation of analysis and specification for requirements in the cloud computing domain and the automatic mapping of requirements on actual products offered in the cloud service market. In this research work, we propose AI-CRAS an AI-driven cloud service requirement analysis and specification methodology. The proposed method, which leverages state-of-the-art transformer-based large language model, has been implemented and validated in a real case. Experimental results demonstrate that the model performed well in binary and multi-label classification of requirements (achieving recall/F1-score of $0.96 / 0.92$ and $0.86 / 0.76$, respectively) and mapping requirements into actual cloud services.},
  keywords={Training;Cloud computing;Accuracy;Feature extraction;Transformers;Natural language processing;Software;Vectors;Stakeholders;Testing;cloud computing;cloud migration;cloud services;requirement engineering;artificial intelligence;natural language processing;cloud service broker},
  doi={10.1109/IC2E61754.2024.00009},
  ISSN={2694-0825},
  month={Sep.},}@ARTICLE{9357868,
  author={Gaur, Manas and Faldu, Keyur and Sheth, Amit},
  journal={IEEE Internet Computing}, 
  title={Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?}, 
  year={2021},
  volume={25},
  number={1},
  pages={51-59},
  abstract={The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, and human-computer interactions. However, DL's black-box nature and over-reliance on massive amounts of data condensed into labels and dense representations pose challenges for interpretability and explainability. Furthermore, DLs have not proven their ability to effectively utilize relevant domain knowledge critical to human understanding. This aspect was missing in early data-focused approaches and necessitated knowledge-infused learning (K-iL) to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL using K-iL. Through examples from natural language processing applications in healthcare and education, we discuss the utility of K-iL towards interpretability and explainability.},
  keywords={Deep learning;Computational modeling;Semantics;Medical services;Natural language processing;Artificial intelligence;Computer vision;Human computer interaction},
  doi={10.1109/MIC.2020.3031769},
  ISSN={1941-0131},
  month={Jan},}@ARTICLE{10234662,
  author={Hu, Linmei and Liu, Zeyi and Zhao, Ziwang and Hou, Lei and Nie, Liqiang and Li, Juanzi},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey of Knowledge Enhanced Pre-Trained Language Models}, 
  year={2024},
  volume={36},
  number={4},
  pages={1413-1430},
  abstract={Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.},
  keywords={Task analysis;Natural language processing;Training;Taxonomy;Linguistics;Computational modeling;Surveys;Knowledge enhanced pre-trained language models;natural language generation;natural language processing;natural language understanding;pre-trained language models},
  doi={10.1109/TKDE.2023.3310002},
  ISSN={1558-2191},
  month={April},}@ARTICLE{9153759,
  author={Jing, Yun and Zhiwei, Xu and Guanglai, Gao},
  journal={IEEE Access}, 
  title={Context-Driven Image Caption With Global Semantic Relations of the Named Entities}, 
  year={2020},
  volume={8},
  number={},
  pages={143584-143594},
  abstract={Automatic image captioning has achieved a great progress. However, the existing captioning frameworks basically enumerate the objects in the image. The generated captions lack the real-world knowledge about named entities and their relations, such as the relations among famous persons, organizations and buildings. On the contrary, humans interpret images in a specific way by providing real-world knowledge with relations of the aforementioned named entities. To generate human-like captions, we focus on captioning the images of news, which could provide real-world knowledge of the whole story behind the images. Then we propose a novel model that makes captions closer to the human-like description of the image, by leveraging the semantic relevance of the named entities. The named entities are not only extracted from news under the guidance of the image content, but also extended with external knowledge based on the semantic relations. In detail, we propose a sentence correlation analysis algorithm to selectively draw the contextual information from news, and use entity-linking algorithm based on knowledge graph to discover the relations of entities with a global sight. The results of extensive experiments based on real-world dataset which is collected from the news show that our model generates image captions closer to the corresponding real-world captions.},
  keywords={Semantics;Visualization;Task analysis;Computational modeling;Knowledge based systems;Correlation;Organizations;Image caption;named entity;semantic relation},
  doi={10.1109/ACCESS.2020.3013321},
  ISSN={2169-3536},
  month={},}@ARTICLE{9352490,
  author={Zhang, Zhuosheng and Li, Junlong and Zhao, Hai},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Multi-Turn Dialogue Reading Comprehension With Pivot Turns and Knowledge}, 
  year={2021},
  volume={29},
  number={},
  pages={1161-1173},
  abstract={Multi-turn dialogue reading comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection and answering questions. The major challenges involve noisy history contexts and especial prerequisites of commonsense knowledge that is unseen in the given material. Existing works mainly focus on context and response matching approaches. This work thus makes the first attempt to tackle the above two challenges by extracting substantially important turns as pivot utterances and utilizing external knowledge to enhance the representation of context. We propose a pivot-oriented deep selection model (PoDS) on top of the Transformer-based language models for dialogue comprehension. In detail, our model first picks out the pivot utterances from the conversation history according to the semantic matching with the candidate response or question, if any. Besides, knowledge items related to the dialogue context are extracted from a knowledge graph as external knowledge. Then, the pivot utterances and the external knowledge are combined together with a well-designed mechanism for refining predictions. Experimental results on four dialogue comprehension benchmark tasks show that our proposed model achieves great improvements on baselines. A series of empirical comparisons are conducted to show how our selection strategies and the extra knowledge injection influence the results.},
  keywords={Context modeling;Task analysis;Speech processing;History;Bit error rate;Computational modeling;Benchmark testing;Multi-turn dialogue comprehension;response selection;utterance selection;commonsense modeling},
  doi={10.1109/TASLP.2021.3058616},
  ISSN={2329-9304},
  month={},}@ARTICLE{10328469,
  author={Zhu, Yi and Wang, Ye and Qiang, Jipeng and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Prompt-Learning for Short Text Classification}, 
  year={2024},
  volume={36},
  number={10},
  pages={5328-5339},
  abstract={In the short text, the extremely short length, feature sparsity, and high ambiguity pose huge challenges to classification tasks. Recently, as an effective method for tuning Pre-trained Language Models for specific downstream tasks, prompt-learning has attracted a vast amount of attention and research. The main intuition behind the prompt-learning is to insert the template into the input and convert the tasks into equivalent cloze-style tasks. However, most prompt-learning methods only consider the class name and monotonous strategy for knowledge incorporating in cloze-style prediction, which will inevitably incur omissions and bias in short text classification tasks. In this paper, we propose a short text classification method with prompt-learning. Specifically, the top $M$M concepts related to the entity in the short text are retrieved from the open Knowledge Graph like Probase, these concepts are first selected by the distance with class labels, which takes both the short text itself and the class name into consideration during expanding label word space. Then, we conducted four additional strategies for the integration of the expanded concepts, and the union of these concepts are adopted finally in the verbalizer of prompt-learning. Experimental results show that the obvious improvement is obtained compared with other state-of-the-art methods on five well-known datasets.},
  keywords={Task analysis;Business;Sports;Semantics;Blogs;Companies;Tuning;Prompt-Learning;short text classification},
  doi={10.1109/TKDE.2023.3332787},
  ISSN={1558-2191},
  month={Oct},}@INPROCEEDINGS{9882574,
  author={Sovrano, Francesco and Vitali, Fabio},
  booktitle={2022 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)}, 
  title={How to Quantify the Degree of Explainability: Experiments and Practical Implications}, 
  year={2022},
  volume={},
  number={},
  pages={1-9},
  abstract={Explainable AI was born as a pathway to allow humans to explore and understand the inner working of complex systems. Though, establishing what is an explanation and objectively evaluating explainability, are not trivial tasks. With this paper, we present a new model-agnostic metric to measure the Degree of Explainability of (correct) information in an objective way, exploiting a specific theoretical model from Ordinary Language Philosophy called the Achinstein’s Theory of Explanations, implemented with an algorithm relying on deep language models for knowledge graph extraction and information retrieval. In order to understand whether this metric is actually behaving as explainability is expected to, we have devised an experiment on two realistic Explainable AI-based systems for healthcare and finance, using famous AI technology including Artificial Neural Networks and TreeSHAP. The results we obtained suggest that our proposed metric for measuring the Degree of Explainability is robust on several scenarios.},
  keywords={Measurement;Philosophical considerations;Finance;Medical services;Information retrieval;Data mining;Artificial intelligence;Objective Explainability Metric;XAI;Degree of Explainability},
  doi={10.1109/FUZZ-IEEE55066.2022.9882574},
  ISSN={1558-4739},
  month={July},}@ARTICLE{9852225,
  author={Chatterjee, Joyjit and Dethlefs, Nina},
  journal={IEEE Access}, 
  title={Automated Question-Answering for Interactive Decision Support in Operations & Maintenance of Wind Turbines}, 
  year={2022},
  volume={10},
  number={},
  pages={84710-84737},
  abstract={Intelligent question-answering (QA) systems have witnessed increased interest in recent years, particularly in their ability to facilitate information access, data interpretation or decision support. The wind energy sector is one of the most promising sources of renewable energy, yet turbines regularly suffer from failures and operational inconsistencies, leading to downtimes and significant maintenance costs. Addressing these issues requires rapid interpretation of complex and dynamic data patterns under time-critical conditions. In this article, we present a novel approach that leverages interactive, natural language-based decision support for operations & maintenance (O&M) of wind turbines. The proposed interactive QA system allows engineers to pose domain-specific questions in natural language, and provides answers (in natural language) based on the automated retrieval of information on turbine sub-components, their properties and interactions, from a bespoke domain-specific knowledge graph. As data for specific faults is often sparse, we propose the use of paraphrase generation as a way to augment the existing dataset. Our QA system leverages encoder-decoder models to generate Cypher queries to obtain domain-specific facts from the KG database in response to user-posed natural language questions. Experiments with an attention-based sequence-to-sequence (Seq2Seq) model and a transformer show that the transformer accurately predicts up to 89.75% of responses to input questions, outperforming the Seq2Seq model marginally by 0.76%, though being 9.46 times more computationally efficient. The proposed QA system can help support engineers and technicians during O&M to reduce turbine downtime and operational costs, thus improving the reliability of wind energy as a source of renewable energy.},
  keywords={Wind turbines;Natural languages;Question answering (information retrieval);Predictive models;Maintenance engineering;Data models;Transformers;Decision support systems;Knowledge engineering;Wind energy;Deep learning;Decision support;artificial intelligence;interactive systems;wind energy;questionanswering;knowledge graphs;formal language generation;deep learning},
  doi={10.1109/ACCESS.2022.3197167},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9658757,
  author={Upadhyay, Chirayu and Abu-Rasheed, Hasan and Weber, Christian and Fathi, Madjid},
  booktitle={2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Explainable Job-Posting Recommendations Using Knowledge Graphs and Named Entity Recognition}, 
  year={2021},
  volume={},
  number={},
  pages={3291-3296},
  abstract={The growth of online job-posting repositories provided job-seekers with access to a large number of potential jobs. User assessment of recommended jobs becomes especially a tedious and time-consuming task with the overwhelming number of job recommendations. To enhance the job-seeker’s ability to evaluate the suitability of a recommended job, we propose an explainable job recommendation system, which matches the user to the most relevant jobs based on their profile. Then, the system explains to the user why each job-posting has been recommended to them. The proposed system uses a knowledge graph (KG) structure to model job-postings and user profiles in one homogeneous structure. Graph relations between the job-seekers and job-postings are mined through natural language processing (NLP) of the textual content from job-postings and user-profiles. Based on the graph structure itself and a customized named entity classifier, a human-readable explanation is generated for each recommendation and provided to the job-seeker. The explanation includes information about the matching factors that led the system to recommend a certain job-posting to the user. The proposed system is implemented and tested on a sample data-set of user profiles and job-postings from open online repositories. We use BELU and Rouge-L scores to show that the proposed systems generated relevant explanations for recommended jobs.},
  keywords={Conferences;Natural language processing;Task analysis;Cybernetics},
  doi={10.1109/SMC52423.2021.9658757},
  ISSN={2577-1655},
  month={Oct},}@ARTICLE{10177927,
  author={Naseem, Usman and Khushi, Matloob and Dunn, Adam G. and Kim, Jinman},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={K-PathVQA: Knowledge-Aware Multimodal Representation for Pathology Visual Question Answering}, 
  year={2024},
  volume={28},
  number={4},
  pages={1886-1895},
  abstract={Pathology imaging is routinely used to detect the underlying effects and causes of diseases or injuries. Pathology visual question answering (PathVQA) aims to enable computers to answer questions about clinical visual findings from pathology images. Prior work on PathVQA has focused on directly analyzing the image content using conventional pretrained encoders without utilizing relevant external information when the image content is inadequate. In this article, we present a knowledge-driven PathVQA (K-PathVQA), which uses a medical knowledge graph (KG) from a complementary external structured knowledge base to infer answers for the PathVQA task. K-PathVQA improves the question representation with external medical knowledge and then aggregates vision, language, and knowledge embeddings to learn a joint knowledge-image-question representation. Our experiments using a publicly available PathVQA dataset showed that our K-PathVQA outperformed the best baseline method with an increase of 4.15% in accuracy for the overall task, an increase of 4.40% in open-ended question type and an absolute increase of 1.03% in closed-ended question types. Ablation testing shows the impact of each of the contributions. Generalizability of the method is demonstrated with a separate medical VQA dataset.},
  keywords={Visualization;Medical diagnostic imaging;Task analysis;Pathology;Transformers;Bioinformatics;Training;Pathology images;medical visual question answering (MedVQA);multimodal representation},
  doi={10.1109/JBHI.2023.3294249},
  ISSN={2168-2208},
  month={April},}@INPROCEEDINGS{9035170,
  author={Albared, Mohammed and Gallofré Ocaña, Marc and Ghareb, Abdullah and Al-Moslmi, Tareq},
  booktitle={2019 First International Conference of Intelligent Computing and Engineering (ICOICE)}, 
  title={Recent Progress of Named Entity Recognition over the Most Popular Datasets}, 
  year={2019},
  volume={},
  number={},
  pages={1-9},
  abstract={Named entity recognition (NER) has been considered as an initial step for many applications and tasks such as information retrieval and extraction, question answering, topic modelling, open information extraction, knowledge graph construction, and so forth. Therefore, NER has been receiving increasing attention in the research community. Despite the abundant availability of previous studies on NER, few of them have been applied for more than one dataset. Hence, one system might outperform other systems in one dataset and fail to do in another one. The previous NER surveys have mostly focused on reporting the NER systems without providing a clear comparison for all systems proposed for each dataset. In this paper, we will track the NER performance progress for the most commonly used datasets in NER and report the most recent best systems that have been proposed for each dataset during the last few years.},
  keywords={Task analysis;Labeling;Neural networks;Hidden Markov models;Bit error rate;Context modeling;Training;Named entity recognition;Natural language processing;Machine learning;Deep learning},
  doi={10.1109/ICOICE48418.2019.9035170},
  ISSN={},
  month={Dec},}@ARTICLE{9115025,
  author={Quiroz-Mercado, Job Isaias and Barrón-Fernández, Ricardo and Ramírez-Salinas, Marco Antonio},
  journal={IEEE Access}, 
  title={Semantic Similarity Estimation Using Vector Symbolic Architectures}, 
  year={2020},
  volume={8},
  number={},
  pages={109120-109132},
  abstract={For many natural language processing applications, estimating similarity and relatedness between words are key tasks that serve as the basis for classification and generalization. Currently, vector semantic models (VSM) have become a fundamental language modeling tool. VSMs represent words as points in a high-dimensional space and follow the distributional hypothesis of meaning, which assumes that semantic similarity is related to the context. In this paper, we propose a model whose representations are based on the semantic features associated with a concept within the ConceptNet knowledge graph. The proposed model is based on a vector symbolic architecture framework, which defines a set of arithmetic operations to encode the semantic features within a single high-dimensional vector. In addition to word distribution, these vector representations consider several types of information. Moreover, owing to the properties of high-dimensional spaces, they have the additional advantage of being interpretable. We analyze the model's performance on the SimLex-999 dataset, a dataset where commonly used distributional models (e.g., word2vec or GloVe) perform poorly. Our results are similar to those of other hybrid models, and they surpass several state-of-the-art distributional and knowledge-based models.},
  keywords={Semantics;Benchmark testing;Computational modeling;Correlation;Computer architecture;Natural language processing;Estimation;Concept representation;semantic similarity;vector symbolic architectures;word embeddings},
  doi={10.1109/ACCESS.2020.3001765},
  ISSN={2169-3536},
  month={},}@ARTICLE{9758661,
  author={Jiang, Tao and Kang, Fengjian and Guo, Wei and He, Wei and Liu, Lei and Lu, Xudong and Xu, Yonghui and Cui, Lizhen},
  journal={International Journal of Crowd Science}, 
  title={CK-Encoder: Enhanced Language Representation for Sentence Similarity}, 
  year={2022},
  volume={6},
  number={1},
  pages={17-22},
  abstract={In recent years, neural networks have been widely used in natural language processing, especially in sentence similarity modeling. Most of the previous studies focused on the current sentence, ignoring the commonsense knowledge related to the current sentence in the task of sentence similarity modeling. Commonsense knowledge can be remarkably useful for understanding the semantics of sentences. CK-Encoder, which can effectively acquire commonsense knowledge to improve the performance of sentence similarity modeling, is proposed in this paper. Specifically, the model first generates a commonsense knowledge graph of the input sentence and calculates this graph by using the graph convolution network. In addition, CKER, a framework combining CK-Encoder and sentence encoder, is introduced. Experiments on two sentence similarity tasks have demonstrated that CK-Encoder can effectively acquire commonsense knowledge to improve the capability of a model to understand sentences.},
  keywords={Convolution;Semantics;Neural networks;Natural language processing;Task analysis;Commonsense reasoning;CK-Encoder;sentence similarity;commonsense knowledge},
  doi={10.26599/IJCS.2022.9100001},
  ISSN={2398-7294},
  month={April},}@INPROCEEDINGS{10020207,
  author={Ostapuk, Natalia and Difallah, Djellel and Cudré-Mauroux, Philippe},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={ParaGraph: Mapping Wikidata Tail Entities to Wikipedia Paragraphs}, 
  year={2022},
  volume={},
  number={},
  pages={6008-6017},
  abstract={Bridging unstructured data with knowledge bases is an essential task in many problems related to natural language understanding. Traditionally, this task is considered in one direction only: linking entity mentions in a text to their counterpart in a knowledge base (also known as entity linking). In this paper, we propose to tackle this problem from a different angle: linking entities from a knowledge base to paragraphs describing those entities. We argue that such a new perspective can be beneficial to several applications, including information retrieval, knowledge base population, and joint entity and word embedding. We present a transformer-based model, ParaGraph, which, given a Wikidata entity as input, retrieves its corresponding Wikipedia section. To perform this task, ParaGraph first generates an entity summary and compares it to sections to select an initial set of candidates. The candidates are then ranked using additional information from the entity’s textual description and contextual information. Our experimental results show that ParaGraph achieves 87% Hits@10 when ranking Wikipedia sections given a Wikidata entity as input. The obtained results show that ParaGraph can reduce the information gap between Wikipedia-based entities and tail entities and demonstrate the effectiveness of our proposed approach towards linking knowledge graph entities to their text counterparts.},
  keywords={Knowledge based systems;Encyclopedias;Tail;Big Data;Transformers;Internet;Online services;Linked Data;Knowledge Graphs;Entity Linking},
  doi={10.1109/BigData55660.2022.10020207},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9995551,
  author={He, Kai and Mao, Bing and Zhou, Xiangyu and Li, Yufei and Gong, Tieliang and Li, Chen and Wu, Jialun},
  booktitle={2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Knowledge Enhanced Coreference Resolution via Gated Attention}, 
  year={2022},
  volume={},
  number={},
  pages={2287-2293},
  abstract={Coreference resolution aims at linking all mentions that refer to the same entity, which are widely adopted in many biomedical and bioinformatics tasks, such as biomedical knowledge graph construction and metabolic pathway integration. Many recent studies focus on improving neural model structures. However, we argue that a practical method that integrates commonsense knowledge can further improve coreference resolution performance, because commonsense delivers extra prior knowledge for reasoning and can enhance related representations, rather than naive mention-context occurrence modeling. In this work, we propose an effective method to integrate external commonsense knowledge into a neural coreference resolution model. Specially, a gated attention mechanism is employed in our method to leverage commonsense according to different contexts. By using ConceptNet as the knowledge base in three span-ranking backbone models, the models can yield significant performance gains on used datasets. We also achieve improvements in tasks of long-term mention detection and cross-sentence coreferences after incorporating knowledge.},
  keywords={Biological system modeling;Knowledge based systems;Logic gates;Performance gain;Task analysis;Bioinformatics;Commonsense reasoning;Coreference Resolution;Attention;Knowledge Enhanced},
  doi={10.1109/BIBM55620.2022.9995551},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9669713,
  author={Yang, Songchun and Zheng, Xiangwen and Xiao, Yu and Yang, Yu and Zhao, Dongsheng},
  booktitle={2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Combining Query Reformulation and Re-ranking to Improve Query Expansion in Chinese EMR Retrieval}, 
  year={2021},
  volume={},
  number={},
  pages={2912-2919},
  abstract={The methods of query expansion (QE) have achieved significant performance in information retrieval of electronic medical records (EMR). It is a pity that the direct addition of expansion terms may cause query drift, which decreases the precision of EMR retrieval. To solve the issue, we combine the methods of query reformulation and re-ranking to improve the performance of QE in Chinese EMR retrieval. First, the synonyms and hyponyms are extracted from a Chinese medical knowledge graph as expansion terms, and the weights of expansion terms are calculated with the combination of semantic similarities, category weights, and co-occurrence frequencies. Then the query is reformulated by these expansion terms, and the EMR documents are retrieved by the expanded query. Second, four categories of medical terms in top retrieval results are selected, and the performances of all combinations in re-ranking are tested to select the most proper terms for re-ranking. Then the original retrieval results are re-ranked by these terms. Experiments show that the algorithm can promote the effectiveness of EMR retrieval compared with baselines, which shows that the combination of query reformulation and re-ranking can differentiate expansion terms of different categories and maximize their effects.},
  keywords={Conferences;Semantics;Information retrieval;Electronic medical records;Bioinformatics;information retrieval;query expansion;query reformulation;re-ranking;electronic medical record},
  doi={10.1109/BIBM52615.2021.9669713},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10020399,
  author={Xu, Zihang and Fu, Luoyi and Wang, Xinbing},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={PromptRGD: Prompt Learning with Relation-aware Gradient Denoising for Low-resource Relation Extraction}, 
  year={2022},
  volume={},
  number={},
  pages={1162-1171},
  abstract={Relation extraction is a fundamental task to construct a knowledge graph, which aims to find the relation categories between two entities from a string of text input sequences. Considering the fact that high-quality labeled data is rare, a recent trend is low-resource setting. Existing works either utilize prompt-based learning method to achieve encouraging results for few-shot tasks by determining an appropriate prompt template, or leverage unlabeled data by generating pseudo labels. However, the manual design of the prompt template requires not only labor-intensive but also expert knowledge. Roughly utilizing a semi-supervised method to assign pseudo-label to unlabeled data will suffer from noise accumulation. To tackle these problems, we propose a novel semi-supervised prompt learning framework with relation-aware gradient denoising for low-resource relation extraction(PromptRGD). Firstly, using learnable template words and virtual labels for prompt learning, we introduce entity type and relation prior knowledge into prompt template construction. Secondly, given the relation-aware gradient similarity between labeled and unlabeled data, PromptRGD generates a pseudo label and then improves the quality of pseudo labels on unlabeled samples in a self-training fashion. The main experimental results and a series of analyses prove the effectiveness of PromptRGD.},
  keywords={Learning systems;Computer vision;Noise reduction;Training data;Manuals;Big Data;Market research;low-resource relation extraction;semi-supervised prompt learning;self-training;gradient denoising},
  doi={10.1109/BigData55660.2022.10020399},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10593437,
  author={Samuel, Prithi and Berna, Eugene and Kumar, Arun and Reshmy, A K and Sriraj, Swaroop Kadaba and Saketh, Yanda},
  booktitle={2024 International Conference on Communication, Computer Sciences and Engineering (IC3SE)}, 
  title={A Hybrid Solution for Summarizing Diverse Medical Texts in the Health Domain}, 
  year={2024},
  volume={},
  number={},
  pages={131-136},
  abstract={Since medical text summarization has become essential in facilitating rapid access to vital information for healthcare professionals, this research paper introduces a novel hybrid medical text summarizer that combines both extractive and abstractive summarization techniques while incorporating domain knowledge for the extractive process. Our approach first employs a domain-specific knowledge graph to guide the identification of salient and clinically relevant content from medical literature. This is followed by the application of advanced natural language processing techniques for abstractive summarization, ensuring the generated summaries maintain coherence and readability. We detail the use of the domain-specific knowledge base containing embedder, which captures medical concepts and relationships, enabling the system to discern important information from the input text. The cosine similarity-based ranking algorithm is adapted to prioritize sentences based on their relevance to the domain and their connectivity. The hybrid medical text summarizer is evaluated on a diverse set of medical articles at the same time to value summarizing multiple documents at the same time and compare its performance with existing approaches. Results demonstrate significant improvements in summary quality, as measured by ROUGE scores and human evaluations. Furthermore, we observe that incorporating domain knowledge in the extractive process enhances the overall effectiveness of the summarization system. We achieve a range-topping ROUGE score of a bit more than 0.6 for most of the texts summarized.},
  keywords={Accuracy;Knowledge based systems;Text summarization;Medical services;Knowledge graphs;Information retrieval;Natural language processing;abstractive summarization;extractive summarization;embedder;Text rank},
  doi={10.1109/IC3SE62002.2024.10593437},
  ISSN={},
  month={May},}@INPROCEEDINGS{10455613,
  author={Xu, Zhitong and Duan, Jianyong and He, Li and Wang, Hao and Zhang, Qing},
  booktitle={2023 3rd International Conference on Digital Society and Intelligent Systems (DSInS)}, 
  title={Evidence-driven Entity Relation Matching Enhances Document-level Relation Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={464-472},
  abstract={To address the issues of limited evidence and a single extraction method in Document-level Relation Extraction (DocRE), we propose an evidence self-training approach and a novel DocRE processing method. We introduce two methods to enhance the performance of existing models: (1) utilizing an evidence self-training approach to generate evidence sentences on distantly supervised data and assisting the model’s performance in DocRE through data augmentation. (2) Inspired by Triplet completion in the knowledge graph, we present an entity and predefined relation-matching method, guiding the model to focus on evidence sentences in the document via evidence distribution to enhance the model’s inference and prediction of entity relations. The EdEM model proposed in this paper achieves 63.43 in F1 and 60.31 in Ign F1 on the Dev set of the DocRED dataset, and 62.27 in F1 and 60.26 in Ign F1 on the Test set. Experimental results demonstrate that our improved model outperforms current baseline models. Evidence self-training effectively addresses the issue of limited evidence and guides the model’s focus on the location of evidence sentences through evidence distribution, enhancing the performance of DocRE tasks.},
  keywords={Training;Knowledge graphs;Predictive models;Data augmentation;Data models;Task analysis;Intelligent systems;Relation Extraction;Evidence Sentences;Evidence Self-Training},
  doi={10.1109/DSInS60115.2023.10455613},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10651447,
  author={Gao, Shang and Li, Yanling and Ge, Fengpei and Lin, Min and Yu, Haiqing and Wang, Sukun and Miao, Zhongyi},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={LeGalFormer: A Graph Representation Learning and Transformer-based Approach for Legal Similar Case Retrieval}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  abstract={Legal Similar Case Retrieval (LSCR) is a critical application in legal Artificial Intelligence (AI). It involves retrieving the most relevant cases from legal case databases through query cases. Legal cases are semi-structured documents characterized by long text sequences and high specialization. Existing approaches rely on pre-trained language models for retrieval. However, these methods are constrained by the length of text input, preventing them from fully comprehending cases, which results in poor retrieval performance. Knowledge Graph (KG) are a type of graph structure data with dense knowledge and clear logic, which can represent the criminal processes and relationships among characters within legal cases. Currently, the mainstream approach for handling KG remains Graph Neural Networks (GNNs). However, these methods are limited by message passing and are prone to over-smoothing problems in the process of aggregating node features. To address these issues, we propose a Legal similar case retrieval model that combines Graph representation learning with the Transformer, called LeGalFormer. Three encoding methods are introduced to incorporate the structural information of the graph into the Transformer architecture. We evaluate the model on a real legal dataset, and the experimental results show that LeGalFormer significantly enhances the model's understanding capacity and achieves state-of-the-art performance.},
  keywords={Representation learning;Training;Law;Message passing;Transformers;Encoding;Graph neural networks;information retrieval;legal similar case retrieval;graph transformer;deep learning},
  doi={10.1109/IJCNN60899.2024.10651447},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10105766,
  author={Peng, Da and Pei, Zhongmin and Mo, Delin},
  booktitle={2023 3rd International Conference on Neural Networks, Information and Communication Engineering (NNICE)}, 
  title={MTL-JER: Meta-Transfer Learning for Low-Resource Joint Entity and Relation Extraction}, 
  year={2023},
  volume={},
  number={},
  pages={78-83},
  abstract={Joint entity and relation extraction has achieved impressive advances in NLP, such as document understanding and knowledge graph construction. The typical methods for entity and relation extraction typically break down the joint task into several smaller components or stages for ease of implementation, but this leads to a loss of the interconnected knowledge in the triple. Hence, we propose to model the triple in one module jointly. Furthermore, the labeling of a joint entity and relation extraction tasks is costly and domain-specific; therefore, it is important to improve its performance on low-resource data and domain adaption. To address this issue, we suggest using two sources that are rich in information, namely pre-trained models on large data and multi-domain text corpora. Pretraining allows us to provide the model with the fundamental ability to perform joint entity and relationship extraction. Second, through meta-learning on multi-domain text, we can improve the model's generalization capabilities, enabling it to perform well even with limited data. We present MTL-JER, a Meta-Transfer Learning method for Joint Entity and Relation Extraction in low-resource settings in this paper. Using exhaustive experiments on five datasets, we prove that our model obtains optimal results.},
  keywords={Adaptation models;Training data;Knowledge graphs;Artificial neural networks;Data models;Data mining;Labeling;Joint entity and relation extraction;Meta-Learning;Low-resource},
  doi={10.1109/NNICE58320.2023.10105766},
  ISSN={},
  month={Feb},}@INPROCEEDINGS{8992818,
  author={Lindén, Johanne and Wang, Xutao and Forsström, Stefan and Zhang, Tingting},
  booktitle={2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI)}, 
  title={Bilingual Auto-Categorization Comparison of Two LSTM Text Classifiers}, 
  year={2019},
  volume={},
  number={},
  pages={599-604},
  abstract={Multi linguistic problems such as auto-categorization is not an easy task. It is possible to train different models for each language, another way to do auto-categorization is to build the model in one base language and use automatic translation from other languages to that base language. Different languages have a bias to a language specific grammar and syntax and will therefore pose problems to be expressed in other languages. Translating from one language into a non-verbal language could potentially have a positive impact of the categorization results. A non-verbal language could for example be pure information in form of a knowledge graph relation extraction from the text. In this article a comparison is conducted between Chinese and Swedish languages. Two categorization models are developed and validated on each dataset. The purpose is to make an auto-categorization model that works for n'importe quel langage. One model is built upon LSTM and optimized for Swedish and the other is an improved Bidirectional-LSTM Convolution model optimized for Chinese. The improved algorithm is trained on both languages and compared with the LSTM algorithm. The Bidirectional-LSTM algorithm performs approximately 20% units better than the LSTM algorithm, which is significant.},
  keywords={neural network;chinese dataset;memory cell;convolutional neural network;feature map;chinese language;swedish dataset;auto categorization;information system;natural language processing;news article},
  doi={10.1109/IIAI-AAI.2019.00127},
  ISSN={},
  month={July},}@INPROCEEDINGS{10484052,
  author={Sahu, Pragya Paramita and Raut, Abhishek and Samant, Jagdish Singh and Gorijala, Mahesh and Lakshminarayanan, Vignesh and Bhaskar, Pinaki},
  booktitle={2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={POP-VQA – Privacy preserving, On-device, Personalized Visual Question Answering}, 
  year={2024},
  volume={},
  number={},
  pages={8455-8464},
  abstract={The next generation of device smartness needs to go beyond being able to understand basic user commands. As our systems become more efficient, they need to be taught to understand user interactions and intents from all possible input modalities. This is where the recent advent of large scale multi-modal models can form the foundation for next-gen technologies. However, the true power of such interactive systems can only be realized with privacy conserving personalization. In this paper, we propose an on-device visual question answering system that generates personalized answers using on-device user knowledge graph. These systems have the potential to serve as a fundamental ground-work for the development of genuinely intelligent and tailored assistants, targeted specifically to the needs and preferences of each individual. We validate our model performance on both in-realm, public datasets and personal user data. Our results show consistent performance increase across both tasks, with an absolute improvement of ≈36% with KVQA data-set on 1-hop inferences and ≈6% improvement on user personal data. We also conduct and showcase user-study results to validate our hypothesis of the need and relevance of proposed system.},
  keywords={Training;Visualization;Privacy;Biological system modeling;Computational modeling;System performance;Optical character recognition;Applications;Smartphones / end user devices;Algorithms;Generative models for image;video;3D;etc.;Algorithms;Vision + language and/or other modalities},
  doi={10.1109/WACV57701.2024.00828},
  ISSN={2642-9381},
  month={Jan},}@INPROCEEDINGS{10488237,
  author={Lim, Chae-Gyun and Choi, Ho-Jin and Mswahili, Medard Edmund and Ndomba, Goodwill Erasmo and Jeong, Young-Seob},
  booktitle={2024 IEEE International Conference on Big Data and Smart Computing (BigComp)}, 
  title={Machine Feeling by Knowledge Acquisition with Emotion Map}, 
  year={2024},
  volume={},
  number={},
  pages={90-96},
  abstract={Artificial intelligent systems have been developed by many industries and academic institutes, and there are also many recent studies about emotion or sentiment. The previous studies about the emotion commonly treated the emotion as knowledge trainable from emotion-annotated data, and focused on emotion perception or expression but not emotion generation. In this paper, we design a graph attention network (GAT) for emotion generation based on the assumption that emotion is intrinsic and propagates as more knowledge is acquired. We represent the knowledge as a graph, and formulate that the knowledge acquisition as an expansion of the knowledge graph. The graph gets larger as time-step goes by, and the GAT learns from the time-series graph data. We simulate this knowledge acquisition based on the assumption that emotion propagates to the newly acquired knowledge. Interestingly, the simulation results exhibited behaviors that are consistent with previous findings of psychiatry. We believe our study will contribute to development of human-like emotional agents that have its own unique emotion about what it experienced or learned.},
  keywords={Industries;Knowledge acquisition;Simulation;Scalability;Psychology;Medical services;Knowledge graphs;graph attention networks;machine feeling;time-series graph data;emotion map},
  doi={10.1109/BigComp60711.2024.00023},
  ISSN={2375-9356},
  month={Feb},}@INPROCEEDINGS{10479333,
  author={Piat, Guilhem and Semmar, Nasredine and Tourille, Julien and Allauzen, Alexandre and Essafi, Hassane},
  booktitle={2023 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA)}, 
  title={What does KnowBert-UMLS forget?}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Integrating a source of structured prior knowledge, such as a knowledge graph, into transformer-based language models is an increasingly popular method for increasing data efficiency and adapting them to a target domain. However, most methods for integrating structured knowledge into language models require additional training in order to adapt the model to the non-textual modality. This process typically leads to some amount of catastrophic forgetting on the general domain. KnowBert is one such knowledge integration method which can incorporate information from a variety of knowledge graphs to enhance the capabilities of transformer-based language models such as BERT. We conduct a qualitative analysis of the results of KnowBert-UMLS, a biomedically specialized KnowBert model, on a variety of linguistic tasks. Our results reveal that its increased understanding of biomedical concepts comes at the cost, specifically, of general common-sense knowledge and understanding of casual speech.},
  keywords={Training;Adaptation models;Costs;Biological system modeling;Knowledge graphs;Linguistics;Transformers;Domain Adaptation;Knowledge based systems;Catastrophic Forgetting;Machine learning;Biomedical informatics},
  doi={10.1109/AICCSA59173.2023.10479333},
  ISSN={2161-5330},
  month={Dec},}

@INPROCEEDINGS{8731457,
  author={Wang, Shuo},
  booktitle={2019 IEEE 35th International Conference on Data Engineering (ICDE)}, 
  title={Knowledge Representation for Emotion Intelligence}, 
  year={2019},
  volume={},
  number={},
  pages={2096-2100},
  abstract={Emotion intelligence (EI) is a traditional topic for psychology, sociology, biology and medical science. Because emotion is related with the personality, interpersonal effect, social function, disease treatment, etc. Analyzing the emotion from the Web data by computer technology becomes more and more popular, and the scientists from the non-computer domains need more helpful computing models to deal with professional problems that are not traditional for computer science. Knowledge representation is a basic and possible solution as a bridge between emotion intelligence and artificial intelligence. For the sentiment words, word embedding can map the words to vectors that represent the semantic context of the words. Sentiment embedding based on the word embedding can capture both semantics and the emotion information. We have introduced two kinds of improving embedding methods (MEC and Emo2Vec) for the sentiment words embedding. For emotion structure based on the psychology of emotion, knowledge graph can represent the cognitive relations between different emotion types. The same emotional expressions can affect the reaction and behaviors of the recipient in different ways due to factors such as social relations, information processing, time pressure, etc. Knowledge graph can represent these complicated situations as the relations between the entities and attributes. Based on this graph, we make the inference or prediction of the emotion influence on decision making.},
  keywords={Appraisal;Semantics;Psychology;Sentiment analysis;Neural networks;Context modeling;Data mining;emotion intelligence, sentiment embedding, cognitive structure of emotion, knowledge graph},
  doi={10.1109/ICDE.2019.00247},
  ISSN={2375-026X},
  month={April},}@INPROCEEDINGS{10456739,
  author={Zhang, Heyan and Xing, Chenlin and Luo, Tao},
  booktitle={2023 3rd International Conference on Electronic Information Engineering and Computer Communication (EIECC)}, 
  title={Semantic Propagation Method Enhanced by Structure for Link Prediction}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={In link prediction, text is widely used due to its rich semantic information. However, existing researches have the problems of high information redundancy, large complexity of algorithms, and lack of integration with structure knowledge of knowledge graph. This paper proposes a component-focused Siamese textual encoder CFSTE, which reduces the negative impact of redundant text description by adding a component focusing module and introducing a Siamese structure to reuse the same entities and relationships to reduce the complexity of the encoder. In addition, the structure knowledge of knowledge graph is also jointed through the integration strategy of interpolation prediction. Our proposed model is tested on multiple datasets and achieves state-of-the-art performance on the overall indicators. On the WN18rr, Hits@1, Hits@3 and MRR are increased by 0.22%, 1.63% and 5.28%, respectively. For FB15k-237, Hits@1, Hits@3, Hits@10 and MRR are increased by 8.76%, 4.99%, 3.04% and 4.68%, respectively.},
  keywords={Interpolation;Computational modeling;Semantics;Redundancy;Focusing;Knowledge graphs;Predictive models;Link prediction;Siamese textual encoder;knowledge graph;text extraction},
  doi={10.1109/EIECC60864.2023.10456739},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10702264,
  author={Li, Qiang and Ouyang, Hong and Zheng, Jianning and Xiang, Hui and Zhao, Feng and Zhao, Linlin and Wang, Guaiqiang and Li, Wenpu},
  booktitle={2024 20th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
  title={An Auxiliary Decision Model for Distribution Network Dispatching Based on Scientific Computing}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={As distributed generation resources are interconnected on a large scale, the management and dispatching of distribution networks have become increasingly complex, and there is an urgent need for the auxiliary decision-making of distribution network dispatching to transition towards intelligence. Currently, operations such as load transfer still rely on manual handling, encountering issues like low digitalization levels and poor plan optimization. Decisions on load transfer strategies fail to utilize human-machine intelligent collaboration for adaptive decision-making in actual operational contexts. Therefore, an auxiliary decision model for distribution network dispatching based on scientific computing is proposed. Firstly, an optimized model for load transfer in distribution networks based on the collaborative optimization of primary and secondary distribution was developed. The objective is to minimize the operation costs of the transmission network and the costs of load shedding in the distribution network, thereby refining load supply transfer strategies and improving the computational efficiency and the optimality of these strategies. Secondly, an intent understanding and information recognition model is built upon LLaMA -CRF and knowledge graph methodologies. This model utilizes the LLaMA large model in conjunction with a parallel application architecture of the knowledge graph to facilitate the comprehension of intentions and the extraction of load supply transfer information. Finally, by establishing evaluation indicators and a test dataset for the load transfer operation auxiliary decision-making model, the effectiveness of intent understanding, information extraction, and optimization strategies for load transfer are assessed, verifying the validity of the proposed scientific computing-based auxiliary decision-making model for distribution network dispatching and operation.},
  keywords={Adaptation models;Scientific computing;Computational modeling;Decision making;Distribution networks;Knowledge graphs;Dispatching;Computational efficiency;Optimization;Load modeling;component;formatting;style;styling;intelligent decision support;load transfer;scientific computing;LLaMA;knowledge graph},
  doi={10.1109/ICNC-FSKD64080.2024.10702264},
  ISSN={},
  month={July},}@INPROCEEDINGS{10035573,
  author={Min, Chanwook and Ahn, Jinhyun and Lee, Taewhi and Im, Dong-Hyuk},
  booktitle={2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={TK-BERT: Effective Model of Language Representation using Topic-based Knowledge Graphs}, 
  year={2023},
  volume={},
  number={},
  pages={1-4},
  abstract={Recently, the K-BERT model was proposed to add knowledge for language representation in specialized fields. The K-BERT model uses a knowledge graph to perform transfer learning on the pre-trained BERT model. However, the K-BERT model adds the knowledge that exists in the knowledge graph rather than the data relevant to the topic of the input data when using the knowledge graph of the corresponding field. Hence, the K-BERT model can cause confusion in the training. To solve this problem, this study proposes a topic-based knowledge graph BERT (TK-BERT) model, which uses the topic modeling technique. The TK-BERT model divides the knowledge graph by topic using the knowledge graph&#x0027;s topic model and infers the topic for the input sentence, adding only knowledge relevant to the topic. Therefore, the TK-BERT model does not add unnecessary knowledge to the knowledge graph. Moreover, the proposed TK-BERT model outperforms the K-BERT model.},
  keywords={Training;Bit error rate;Transfer learning;Data models;Information management;Knowledge Graphs;Language Representation;BERT;K-BERT;Topic Modeling},
  doi={10.1109/IMCOM56909.2023.10035573},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10826046,
  author={Ojima, Yuta and Sakaji, Hiroki and Nakamura, Tadashi and Sakata, Hiroaki and Seki, Kazuya and Teshigawara, Yuu and Yamashita, Masami and Aoyama, Kazuhiro},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Knowledge Management for Automobile Failure Analysis Using Graph RAG}, 
  year={2024},
  volume={},
  number={},
  pages={6624-6631},
  abstract={This paper presents a knowledge management system for automobile failure analysis using retrieval-augmented generation (RAG) with large language models (LLMs) and knowledge graphs (KGs). In the automotive industry, there is a growing demand for knowledge transfer of failure analysis from experienced engineers to young engineers. However, failure events are phenomena that occur in a chain reaction, making them difficult for beginners to analyze them. While knowledge graphs, which can describe semantic relationships and structure information is effective in representing failure events, due to their capability of representing the relationships between components, there is much information in KGs, so it is challenging for young engineers to extract and understand sub-graphs from the KG. On the other hand, there is increasing interest in the use of Graph RAG, a type of RAG that combines LLMs and KGs for knowledge management. However, when using the current Graph RAG framework with an existing knowledge graph for automobile failures, several issues arise because it is difficult to generate executable queries for a knowledge graph database which is not constructed by LLMs. To address this, we focused on optimizing the Graph RAG pipeline for existing knowledge graphs. Using an original Q&A dataset, the ROUGE F1 score of the sentences generated by the proposed method showed an average improvement of 157.6% compared to the current method. This highlights the effectiveness of the proposed method for automobile failure analysis.},
  keywords={Large language models;Semantics;Retrieval augmented generation;Pipelines;Failure analysis;Knowledge graphs;Automobiles;Prompt engineering;Knowledge transfer;Automotive engineering;Graph RAG;Large Language Model;Knowledge Graph;Knowledge Management;Automobile Failure},
  doi={10.1109/BigData62323.2024.10826046},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{10530439,
  author={Qiu, Zhangchi and Tao, Ye and Pan, Shirui and Liew, Alan Wee-Chung},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Knowledge Graphs and Pretrained Language Models Enhanced Representation Learning for Conversational Recommender Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  abstract={Conversational recommender systems (CRSs) utilize natural language interactions and dialog history to infer user preferences and provide accurate recommendations. Due to the limited conversation context and background knowledge, existing CRSs rely on external sources such as knowledge graphs (KGs) to enrich the context and model entities based on their interrelations. However, these methods ignore the rich intrinsic information within entities. To address this, we introduce the knowledge-enhanced entity representation learning (KERL) framework, which leverages both the KG and a pretrained language model (PLM) to improve the semantic understanding of entities for CRS. In our KERL framework, entity textual descriptions are encoded via a PLM, while a KG helps reinforce the representation of these entities. We also employ positional encoding to effectively capture the temporal information of entities in a conversation. The enhanced entity representation is then used to develop a recommender component that fuses both entity and contextual representations for more informed recommendations, as well as a dialog component that generates informative entity-related information in the response text. A high-quality KG with aligned entity descriptions is constructed to facilitate this study, namely, the Wiki Movie Knowledge Graph (WikiMKG). The experimental results show that KERL achieves state-of-the-art results in both recommendation and response generation tasks. Our code is publicly available at the link: https://github.com/icedpanda/KERL.},
  keywords={Oral communication;Motion pictures;Encoding;Task analysis;Knowledge graphs;History;Semantics;Conversational recommender system (CRS);knowledge graph (KG);pretrained language model (PLM);representation learning},
  doi={10.1109/TNNLS.2024.3395334},
  ISSN={2162-2388},
  month={},}@INPROCEEDINGS{10184605,
  author={Chen, Zhuo and Zhang, Wen and Huang, Yufeng and Chen, Mingyang and Geng, Yuxia and Yu, Hongtao and Bi, Zhen and Zhang, Yichi and Yao, Zhen and Song, Wenting and Wu, Xinliang and Yang, Yi and Chen, Mingyi and Lian, Zhaoyang and Li, Yingying and Cheng, Lei and Chen, Huajun},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Tele-Knowledge Pre-training for Fault Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={3453-3466},
  abstract={In this work, we share our experience on tele-knowledge pre-training for fault analysis, a crucial task in telecommunication applications that requires a wide range of knowledge normally found in both machine log data and product documents. To organize this knowledge from experts uniformly, we propose to create a Tele-KG (tele-knowledge graph). Using this valuable data, we further propose a tele-domain language pre-training model TeleBERT and its knowledge-enhanced version, a tele-knowledge re-training model KTeleBERT. which includes effective prompt hints, adaptive numerical data encoding, and two knowledge injection paradigms. Concretely, our proposal includes two stages: first, pre-training TeleBERT on 20 million tele-related corpora, and then re-training it on 1 million causal and machine-related corpora to obtain KTeleBERT. Our evaluation on multiple tasks related to fault analysis in tele-applications, including root-cause analysis, event association prediction, and fault chain tracing, shows that pretraining a language model with tele-domain data is beneficial for downstream tasks. Moreover, the KTeleBERT re-training further improves the performance of task models, highlighting the effectiveness of incorporating diverse tele-knowledge into the model.},
  keywords={Adaptation models;Analytical models;Semantics;Predictive models;Data models;Encoding;Communications technology;telecommunication;model pre-training;knowledge graph;numeric encoding;fault analysis},
  doi={10.1109/ICDE55515.2023.00265},
  ISSN={2375-026X},
  month={April},}@ARTICLE{9961919,
  author={Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Minimising Biasing Word Errors for Contextual ASR With the Tree-Constrained Pointer Generator}, 
  year={2023},
  volume={31},
  number={},
  pages={345-354},
  abstract={Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
  keywords={Training;Generators;Decoding;Hidden Markov models;Context modeling;Speech recognition;Error analysis;Contextual speech recognition;end-to-end;language model discounting;minimum Bayes' risk;Pointer generator},
  doi={10.1109/TASLP.2022.3224286},
  ISSN={2329-9304},
  month={},}@INPROCEEDINGS{9529829,
  author={Neji, Sameh and Chenaina, Tarek and Shoeb, Abdullah M. and Ben Ayed, Leila},
  booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={HIR: A Hybrid IR Ranking Model}, 
  year={2021},
  volume={},
  number={},
  pages={1717-1722},
  abstract={The aim of information retrieval (IR) process is to respond effectively to user queries by retrieving information that are better meets their expectations. A gap could exist between user information needs and his/her defined context as the level of the user's expertise in the search domain is directly influencing the query richness. The information retrieval system (IRS) must be intelligent enough to identify information needs and respond effectively to meet the expected needs, regardless the level of user's expertise level. This process is difficult and it remains a major and open challenge in the domain of IR. IR models integrate many sources to achieve an effective retrieval system. Semantic IR is an environment in which semantic techniques were applied to sort the documents according to their degree of relevance to the query. The present work proposes a hybrid model to rank documents. The proposed model is based on a query likelihood language model and the semantic similarity between concepts to assess the relevance between query-document pairs. Concepts were extracted by projection on WordNet ontology then word sense disambiguation was conducted. A semantic index was built to validate the proposed model. The conducted empirical experiments show that the proposed model is outperformed the compared benchmarks in the measured IR metrics.},
  keywords={Measurement;Semantic search;Computational modeling;Semantics;Ontologies;Benchmark testing;Information retrieval;semantic information retrieval;query-document relevance;language model;conceptual model;semantic similarity},
  doi={10.1109/COMPSAC51774.2021.00256},
  ISSN={0730-3157},
  month={July},}@INPROCEEDINGS{9999076,
  author={Japa, Sai Sharath and Green, Sarah},
  booktitle={2022 IEEE Eighth International Conference on Multimedia Big Data (BigMM)}, 
  title={Question Answering over Knowledge Base with Variational Auto-Encoder}, 
  year={2022},
  volume={},
  number={},
  pages={29-36},
  abstract={Knowledge Base is a semantic data repos-itory made available over the web. Knowledge bases are multi-relational graphs consisting of entities and relations representing facts about the world. These facts follow a controlled vocabulary that drives the knowledge base, often called ontology. Entities are the graph nodes, while relations are the edges connecting the nodes. While knowledge base attention prolifer-ates, extracting information from these resources is challenging. One of the promising approaches is Question Answering systems. The Question Answering over Knowledge Base(KB-QA) system aims to provide a natural language interface to pose a query to the KB. QA systems use embeddings to map the posed Question to the facts stored in the knowledge bases. QA systems can be effectively improved using large transformer models. These transformer models are computationally expensive, which prevents their use in real-world applications. To solve this, we started our study with the use-case of Knowledge Base Question Answering systems. In KB-QA systems, representing the entities and relationships play a pivotal role in extracting the answer to the posed query. Traditional KB embedding techniques represent entities/relations as vectors, mapping them in different semantic spaces and ignoring the subtle cor-relation. A pre-training language model for encoding the KB facts will preserve the semantic and contextual correlation. While learning linguistic knowledge, these models also store relational knowledge in the training data. These Language Models are often trained on a massive corpus addressing the Out-Of- Vocabulary problem. In this paper, we incorporate a co-embedding model for knowledge base embedding, which learns low-dimensional representations of entities and relations in the same semantic space. We propose a Variational Auto-Encoder, an efficient transformer architecture that represents knowledge base representations as Gaussian distributions to address the issue of neglecting uncertainty. In addition, our method has high quality and interpretability advantages compared with previous methods. Our experimental results show the effectiveness and the superiority of the VAE approach for question-answering systems on knowledge bases over other well-known pre-trained embedding methods.},
  keywords={Vocabulary;Uncertainty;Computational modeling;Knowledge based systems;Semantics;Training data;Transformers;knowledge base question answering;Bert;Language Model;KBQA;Multi-Head Attention;VAE;Encoder;Transformers},
  doi={10.1109/BigMM55396.2022.00012},
  ISSN={},
  month={Dec},}@ARTICLE{10091124,
  author={Tu, Yamei and Wang, Xiaoqi and Qiu, Rui and Shen, Han-Wei and Miller, Michelle and Rao, Jinmeng and Gao, Song and Huber, Patrick R. and Hollander, Allan D. and Lange, Matthew and Garcia, Christian R. and Stubbs, Joe},
  journal={IEEE Computer Graphics and Applications}, 
  title={An Interactive Knowledge and Learning Environment in Smart Foodsheds}, 
  year={2023},
  volume={43},
  number={3},
  pages={36-47},
  abstract={The Internet of Food (IoF) is an emerging field in smart foodsheds, involving the creation of a knowledge graph (KG) about the environment, agriculture, food, diet, and health. However, the heterogeneity and size of the KG present challenges for downstream tasks, such as information retrieval and interactive exploration. To address those challenges, we propose an interactive knowledge and learning environment (IKLE) that integrates three programming and modeling languages to support multiple downstream tasks in the analysis pipeline. To make IKLE easier to use, we have developed algorithms to automate the generation of each language. In addition, we collaborated with domain experts to design and develop a dataflow visualization system, which embeds the automatic language generations into components and allows users to build their analysis pipeline by dragging and connecting components of interest. We have demonstrated the effectiveness of IKLE through three real-world case studies in smart foodsheds.},
  keywords={Knowledge graphs;Ontologies;Data visualization;Food products;Smart agriculture;Internet of Things;Data models},
  doi={10.1109/MCG.2023.3263960},
  ISSN={1558-1756},
  month={May},}@INPROCEEDINGS{9533494,
  author={Ji, Xin and Zhao, Wen},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={SKGSUM: Abstractive Document Summarization with Semantic Knowledge Graphs}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={In abstractive single-document summarization task, generated summaries always suffer from fabricated and less informative content. An intuitive way to alleviate this problem is to merge external semantic knowledge into the model framework. In this paper, we incorporate explicit graphs based on semantic knowledge, including term frequency, discourse information, and entities with their relations, into neural abstractive summarization for the problem. We propose a novel model for abstractive single-document Summarization based on Semantic Knowledge Graphs (SKGSUM), which regards sentences and entities as nodes, captures the relations between units in different textual levels, and focuses on salient content in the source documents to guide the summary generation process. To the best of our knowledge, we are the first to exploit different textual-unit levels explicit graph representations in a unified framework for the neural abstractive summarization task. Results show that our model achieves significant improvements on both XSum and CNN/Daily Mail datasets over some strong baselines. Human evaluations further indicate that our model can generate informative and coherent summaries.},
  keywords={Semantics;Neural networks;Decoding;Task analysis;Postal services},
  doi={10.1109/IJCNN52387.2021.9533494},
  ISSN={2161-4407},
  month={July},}@INPROCEEDINGS{9874185,
  author={Huang, Yen-Hao and Harryyanto, Kevin and Tsai, Che-Wei and Pornvattanavichai, Ratana and Chen, Yi-Shin},
  booktitle={2022 IEEE 23rd International Conference on Information Reuse and Integration for Data Science (IRI)}, 
  title={Graph Knowledge Transfer for Offensive Language Identification with Graph Neural Networks}, 
  year={2022},
  volume={},
  number={},
  pages={216-221},
  abstract={Identifying offensive language (OL) has become ever more important with the rise of online social media (OSM). Most works on OL identification have applied sequential models to learn offensive semantics. In a different light, recent popular graph neural networks (GNNs) model text in a word graph and learn local word-level usages for each document. This work aims to explore text GNNs on learning OL usages in a non-sequential view yet there are two barriers: (1) Similar to sequential models, GNNs also suffer from the out-of-vocabulary (OOV) issue due to the informal usages of OSM texts. (2) Lacking of appropriate edge weights derived from short content to the word graph. Motivated by the rich resources of OSM, this work leverages existing OL corpus to build a global reference graph and proposes a graph knowledge transfer mechanism (GKtran) to tackle the limited extent of learning from short texts. To embed OL knowledge, GKtran infers edge weights from a reference graph and transfers weights to each document's word graph. Edges that connect OOV words are assigned weights through weight propagation. Thus, each document graph is fully weighted for graph classification. Experimental results show that the additional information from training or external data transferred by GKtran effectively improves OL identification with graphs.},
  keywords={Training;Social networking (online);Semantics;Data science;Graph neural networks;Rough surfaces;Knowledge transfer;Natural language processing;Graph neural network;Knowledge transfer;offensive identification;graph},
  doi={10.1109/IRI54793.2022.00056},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10385584,
  author={He, Yichen and Liu, Xiaofeng and Hu, Jinlong and Dong, Shoubin},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Entity Relation Aware Graph Neural Ranking for Biomedical Information Retrieval}, 
  year={2023},
  volume={},
  number={},
  pages={1118-1124},
  abstract={The performance of biomedical information retrieval greatly depends on biomedical knowledge; however the knowledge of available medical knowledge base is often incomplete and out-of-dated. To solve the problem that incomplete knowledge bases cannot provide the medical knowledge required for biomedical information retrieval, the paper proposes an Entity Relation Aware Graph Neural Ranking model (ERAGNR), aiming to fully leverage the internal knowledge of the document to alleviate the problem caused by incomplete external knowledge bases. ERAGNR mines the relationships between biomedical entities in the document through entity relation extraction and combines them with external knowledge. It increases the semantic association and reduces the semantic gap between the query and the document. The method first constructs a knowledge-query graph and a document-entity graph, and then fuses the two graphs to obtain a knowledge-query-document-entity graph. In a multi-task learning framework that combines text retrieval and relation extraction tasks, ERAGNR employs a shared text encoder and a graph neural network. This enables ERAGNR to learn semantic matching patterns between queries and documents and recognize relationships between entities in the documents. As a result, the model can capture semantic matching signals between entity relationships in the context and queries. The experimental results show that ERAGNR outperforms the state-of-the-art models. Through biomedical relation extraction task, the model can learn the ability to capture the context of the entity relations in the document, so that the model can more accurately match the semantics between the query and the document.},
  keywords={Fuses;Biological system modeling;Semantics;Knowledge based systems;Predictive models;Information retrieval;Multitasking;Biomedical Text Retrieval;Multi-task Learning;Graph Neural Ranking;Document-level Biomedical Relation Extraction},
  doi={10.1109/BIBM58861.2023.10385584},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10864471,
  author={Wickramarachchi, Ruwan and Henson, Cory and Sheth, Amit},
  journal={IEEE Internet Computing}, 
  title={Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities of Neurosymbolic AI}, 
  year={2024},
  volume={28},
  number={6},
  pages={62-67},
  abstract={In the era of generative artificial intelligence (AI), neurosymbolic AI is emerging as a powerful approach for tasks spanning from perception to cognition. The use of neurosymbolic AI has been shown to achieve enhanced capabilities, including improved grounding, alignment, explainability, and reliability. However, due to its nascent stage, there is a lack of widely available real-world benchmark datasets that are tailored to neurosymbolic AI tasks. To address this gap and support the evaluation of current and future methods, we introduce DSceneKG (Driving Scenes Knowledge Graph), a suite of KGs of driving scenes that is built from real-world, high-quality scenes from multiple open autonomous driving datasets. In this article, we detail the construction process of DSceneKG and highlight its application in seven different tasks.},
  keywords={Generative AI;Knowledge graphs;Benchmark testing;Cognition;Internet;Reliability;Artificial intelligence;Autonomous vehicles;Neural engineering;Autonomous driving;Task analysis;Autonomous vehicles;Advanced driver assistance systems},
  doi={10.1109/MIC.2024.3494972},
  ISSN={1941-0131},
  month={Nov},}@INPROCEEDINGS{10831494,
  author={Geng, Haibin and Shi, Chenglong and Jiang, Xuesong and Kong, Zan and Liu, Song},
  booktitle={2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={An Entity Relation Extraction Framework Based on Large Language Model and Multi-Tasks Iterative Prompt Engineering}, 
  year={2024},
  volume={},
  number={},
  pages={4763-4769},
  abstract={Document-level entity relation extraction is an important task in the field of natural language processing, which plays an important role in semantic understanding and knowledge graph construction. However, existing deep neural networks and graph neural networks models are limited by their performance and parameters number, which can not capture global semantics and have poor generalization ability. Furthermore, existing methods employing large language model for entity relation extraction do not establish good relationships among multi-tasks of entity relation extraction task, resulting in more information can not be effectively shared and transmitted between tasks. In addition, previous approaches can not effectively eliminate false entities and relationships. To solve these problems, we propose an entity relation extraction framework based on large language model and multi-tasks iterative prompt engineering. In our model, we design an iterative prompt engineering, which can better establish the relationship among multi-tasks, and ensure every task to obtain the optimal results. Moreover, we design semantic merging, group disambiguation and self-verification modules to eliminate the false entity relations and noise nodes. Additionally, we design summary prompts to provide sufficient global semantics for better text segmentation. Finally, we evaluated our model on wikiann, wikineural, ACE2005, CoNLL2003, CoNLL2004, and SciERC datasets and compared it with other baseline models.},
  keywords={Large language models;Semantics;Noise;Merging;Knowledge graphs;Multitasking;Natural language processing;Iterative methods;Prompt engineering;Data mining},
  doi={10.1109/SMC54092.2024.10831494},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10796004,
  author={Bouquet, Paolo and Molinari, Andrea and Sandri, Simone},
  booktitle={2024 4th International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)}, 
  title={Challenges in Working with Unstructured Data in the LLM Era: NER Processes Using Graph Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={The application of neural networks and large language models (LLMs) in the financial technology (fintech) sector has significantly enhanced capabilities of providing better services based on unstructured documents, mainly thanks to the astonishing interactive capabilities of large language models (LLMs). However, in these contexts, precisely recognizing named entities (for example, the beneficiary of an insurance policy) and creating reliable, domain-dependent, private knowledge graphs is more crucial than conversational capabilities. Moreover, optimizing these models for dynamic data environments and ensuring Explainability presents notable challenges. This paper aims to present our approach for the creation of a knowledge graph that uses graph neural nets (GNN) to provide a better named-entity recognition (NER) process, to identify entities in the text precisely and to solve other issues in knowledge graph management, like the co-ference resolution problem or focusing on dynamic data environments and the critical need for Explainability of modern AI-based solutions. The paper presents our preliminary results on one of the various pipeline steps that creates the navigable knowledge graph.},
  keywords={Mechatronics;Large language models;Pipelines;Insurance;Focusing;Knowledge graphs;Graph neural networks;Data models;Reliability;Artificial Intelligence;Graph Neural Nets;Knolwedge Graph;Named entity resolution},
  doi={10.1109/ICECCME62383.2024.10796004},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{9164163,
  author={Li, FangGuo and Zhang, BeiKe and Gao, Dong},
  booktitle={2020 Chinese Control And Decision Conference (CCDC)}, 
  title={Chinese Named Entity Recognition for Hazard And Operability Analysis Text}, 
  year={2020},
  volume={},
  number={},
  pages={374-378},
  abstract={To solve the problem that it is difficult to identify the key Chinese entity information in the hazard and operability analysis text, a deep neural network model based on bidirectional long short-term memory and conditional random field (BiLSTM-CRF) is proposed to identify key named entities in the text. In the word vector pre-training process, bidirectional encoder representation from transformers (BERT) model is used to pre-train word vectors instead of the static word vectors in the traditional word2vec model, and then obtain context-related dynamic word vectors, to improve the representational ability of word vectors, and solve the problem of word boundary division when word vectors are used in Chinese corpus training. This model has the ability of complete the task of Chinese named entity recognition. The F1 value on the test corpus reaches 93.31%, which is 21.82% higher than conditional random field (CRF) Baseline, and 4.49% higher than the traditional BiLSTM-CRF model. The experimental results show that the BERT-BiLSTM-CRF model is effective for the named entity recognition (NER) task of the hazard and operability analysis text, and it is helpful to automatically extract the relationship between the entities in the hazard and operability analysis text and build safety analysis knowledge graph.},
  keywords={Hazards;Context modeling;Bit error rate;Task analysis;Analytical models;Computer architecture;Logic gates;Chinese named entity recognition;Hazard and operability analysis;BERT;BiLSTM-CRF model},
  doi={10.1109/CCDC49329.2020.9164163},
  ISSN={1948-9447},
  month={Aug},}@INPROCEEDINGS{10020770,
  author={Zhang, Yu and Zhang, Yunyi and Jiang, Yucheng and Michalski, Martin and Deng, Yu and Popa, Lucian and Zhai, ChengXiang and Han, Jiawei},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Entity Set Co-Expansion in StackOverflow}, 
  year={2022},
  volume={},
  number={},
  pages={4792-4795},
  abstract={Given a few seed entities of a certain type (e.g., Software or Programming Language), entity set expansion aims to discover an extensive set of entities that share the same type as the seeds. Entity set expansion in software-related domains such as StackOverflow can benefit many downstream tasks (e.g., software knowledge graph construction) and facilitate better IT operations and service management. Meanwhile, existing approaches are less concerned with two problems: (1) How to deal with multiple types of seed entities simultaneously? (2) How to leverage the power of pre-trained language models (PLMs)? Being aware of these two problems, in this paper, we study the entity set co-expansion task in StackOverflow, which extracts Library, OS, Application, and Language entities from StackOverflow question-answer threads. During the co-expansion process, we use PLMs to derive embeddings of candidate entities for calculating similarities between entities. Experimental results show that our proposed SECoExpan framework outperforms previous approaches significantly.},
  keywords={Computer languages;Big Data;Software;Libraries;Data mining;Task analysis;set expansion;entity extraction;StackOverflow},
  doi={10.1109/BigData55660.2022.10020770},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10021030,
  author={Khabiri, Elham and Agrawal, Bhavna and Lindquist, Joseph and Li, Yingjie and Bhamidipaty, Anuradha},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)}, 
  title={Big data techniques for industrial problems with little data}, 
  year={2022},
  volume={},
  number={},
  pages={2285-2289},
  abstract={Technicians and maintenance managers in industrial environments would benefit from automatically extracting entities and relationships from different text data sources such as logs, event reports, and manuals. Extracting components from pieces of text and classifying them to the right failure type is not trivial in the domain specific setting where the vocabulary has specific meaning to the industry or domain, and labeled data set is very small. In this paper we address how to overcome these challenges in named entity recognition and classification of text, and present a way to improve the model iteratively and quickly. This interaction between components and related failures in the system can be represented in a knowledge graph, which enables further investigations such as Root Cause Analysis and Problem Diagnosis.},
  keywords={Root cause analysis;Vocabulary;Text recognition;Soft sensors;Pipelines;Training data;Manuals},
  doi={10.1109/BigData55660.2022.10021030},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{9735670,
  author={Yiyi, Zha and Chong, Wang and Mingming, Zhang and Kai, Liu},
  booktitle={2021 IEEE Sustainable Power and Energy Conference (iSPEC)}, 
  title={Research on Power Information Knowledge Extraction Model Based on BERT}, 
  year={2021},
  volume={},
  number={},
  pages={3900-3904},
  abstract={Knowledge extraction is the basic step of constructing power information knowledge graph. Aiming at the problems of polysemy, poor context awareness and limited sentence types in the previous text processing methods in the power field, we propose a knowledge extraction method based on BERT from the characteristics of power information text. In the first part, BERT-BiGRU-CRF model is used to complete the named entity recognition of power information text. In the second part, BERT-BiGRU-attention model is used to extract the relation between entities. The model can improve the effect of named entity recognition and relation extraction in complex context, and can process diversified power information texts.},
  keywords={Text recognition;Conferences;Bit error rate;Context awareness;Data mining;Text processing;Context modeling;power information;natural language processing;named entity recognition;relation extraction},
  doi={10.1109/iSPEC53008.2021.9735670},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10628558,
  author={Fieblinger, Romy and Alam, Md Tanvirul and Rastogi, Nidhi},
  booktitle={2024 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)}, 
  title={Actionable Cyber Threat Intelligence Using Knowledge Graphs and Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={100-111},
  abstract={Cyber threats are constantly evolving. Extracting actionable insights from unstructured Cyber Threat Intelligence (CTI) data is essential to guide cybersecurity decisions. Increasingly, organizations like Microsoft, Trend Micro, and CrowdS trike are using generative AI to facilitate CTI extraction. This paper addresses the challenge of automating the extraction of actionable CTI using advancements in Large Language Models (LLMs) and Knowledge Graphs (KGs). We explore the application of state-of-the-art open-source LLMs, including the Llama 2 series, Mistral 7B Instruct, and Zephyr for extracting meaningful triples from CTI texts. Our methodology evaluates techniques such as prompt engineering, the guidance framework, and fine-tuning to optimize information extraction and structuring. The extracted data is then utilized to construct a KG, offering a structured and queryable representation of threat intelligence. Experimental results demonstrate the effectiveness of our approach in extracting relevant information, with guidance and fine-tuning showing superior performance over prompt engineering. However, while our methods prove effective in small-scale tests, applying LLMs to large-scale data for KG construction and Link Prediction presents ongoing challenges.},
  keywords={Large language models;Refining;Knowledge graphs;Organizations;Predictive models;Ontologies;Cyber threat intelligence;Cyber Threat Intelligence;Large Language Models;Knowledge Graphs;Threat Prediction},
  doi={10.1109/EuroSPW61312.2024.00018},
  ISSN={2768-0657},
  month={July},}@INPROCEEDINGS{10410334,
  author={Hoogendoorn, Tjalling and Arachchige, Jeewanie Jayasinghe and Bukhsh, Faiza A.},
  booktitle={2023 International Conference on Frontiers of Information Technology (FIT)}, 
  title={Survey of Explainability within Process Mining: A case study of BPI challenge 2020}, 
  year={2023},
  volume={},
  number={},
  pages={43-48},
  abstract={The need for explainability in Business process management is tremendously increasing, especially in the age of generative AI. The number of published articles on explainable AI (XAI) has skyrocketed for five years. AI impacts the decision-making process in business analytics. Process mining as a sub-discipline of data science can play a role in explainable business decision-making. Process mining exhibits its intention in process discovery, performance measures of processes, and process improvements based on the event logs. Although the accuracy of the outcome of process mining models has been investigated at a certain level, the explainability of those is possible through the discretization of the analytic steps. As an initial step in exploring the explainability of process mining, this research conducts a technical analysis of 37 research papers submitted to the Business Process Intelligence (BPI) Challenge 2020. The main focus of this analysis aims to answer the question, "How and why a process model is produced? " To make a foundation for the research question, the notion of explainability is explored based on an Explainable AI ontology. Due to the small sample size, the study cannot identify clear trends of explainability in process mining. However, the results conclude that explainability depends on the process model’s transparency and reproducibility. Moreover, further research with a large sample size is required to understand the discrete factors impacting decision-making in business process management.},
  keywords={Analytical models;Explainable AI;Decision making;Market research;Business process management;Data mining;Business;Data Science;Process Mining;Explainability;Transparency},
  doi={10.1109/FIT60620.2023.00018},
  ISSN={2473-7569},
  month={Dec},}@INPROCEEDINGS{10718742,
  author={Datta, Arkajit and Verma, Tushar and Chawla, Rajat and S, Mukunda N. and Bhola, Ishaan},
  booktitle={2024 29th International Conference on Automation and Computing (ICAC)}, 
  title={AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI Automation}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  abstract={In recent advancements within the domain of Large Language Models (LLMs), there has been a notable emergence of agents capable of addressing Robotic Process Automation (RPA) challenges through enhanced cognitive capabilities and sophisticated reasoning. This development heralds a new era of scalability and human-like adaptability in goal attainment. In this context, we introduce AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical techniques to facilitate autonomous navigation and task execution on web interfaces, thereby obviating the necessity for predefined scripts or manual intervention. Our engine empowers agents to comprehend and implement complex workflows, adapting to dynamic web environments with unparalleled efficiency. Our methodology synergizes cognitive functionalities with robotic automation, endowing AUTONODE with the ability to learn from experience. We have integrated an exploratory module, DoRA (Discovery and mapping Operation for graph Retrieval Agent), which is instrumental in constructing a knowledge graph that the engine utilizes to optimize its actions and achieve objectives with minimal supervision. The versatility and efficacy of AUTONODE are demonstrated through a series of experiments, highlighting its proficiency in managing a diverse array of web-based tasks, ranging from data extraction to transaction processing. The implementation of our paper can be accessed at :https://github.com/TransformerOptimus/AutoNode},
  keywords={YOLO;Intelligent automation;Scalability;Large language models;Refining;Manuals;Knowledge graphs;Robustness;Engines;Graphical user interfaces;Self-Operating Computer;Generative AI;LLMs;Transformers;Vision-Transformers;Graphs;Reinforcement Learning},
  doi={10.1109/ICAC61394.2024.10718742},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{10825154,
  author={Kumar, Neha Mohan and Lisa, Fahmida Tasnim and Islam, Sheikh Rabiul},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Prompt Chaining-Assisted Malware Detection: A Hybrid Approach Utilizing Fine-Tuned LLMs and Domain Knowledge-Enriched Cybersecurity Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1672-1677},
  abstract={As malware threats continue to evolve in complexity, developing accurate and explainable detection systems is crucial for robust cybersecurity. This paper introduces a hybrid malware classification system that leverages fine-tuned large language models (LLMs) and an enriched cybersecurity knowledge graph (KG) to enhance both detection accuracy and interpretability. The system processes two types of input data—network packets and memory dumps, and classifies applications as benign or malign. Fine-tuned Llama models provide initial classifications, along with reasoning, which are further refined through prompt chaining using a knowledge graph populated with MITRE ATT&CK data and SecureBERT embeddings. The KG facilitates contextual reasoning, resulting in more accurate detection and informed decision-making. Experimental results demonstrate classification accuracies of 91.2% for network packet data and 94.35% for memory dump data, despite challenges related to LLM hallucinations and output parsing.},
  keywords={Adaptation models;Accuracy;Large language models;Refining;Knowledge graphs;Malware;Cognition;Data models;Reliability;Computer security;malware detection;explainability;knowledge graph;chain-prompting;large language models},
  doi={10.1109/BigData62323.2024.10825154},
  ISSN={2573-2978},
  month={Dec},}@ARTICLE{9091129,
  author={Jia, Yongnan and Min, Gaochen and Xu, Cong and Li, Xisheng and Zhang, Dezheng},
  journal={IEEE Access}, 
  title={A Knowledge Driven Dialogue Model With Reinforcement Learning}, 
  year={2020},
  volume={8},
  number={},
  pages={131741-131749},
  abstract={In recent decades, many researchers pay a lot of attention on generating informative responses in end-to-end neural dialogue systems. In order to output the responses with knowledge and fact, many works leverage external knowledge to guide the process of response generation. However, human dialogue is not a simple sequence to sequence task but a process heavily relying on their background knowledge about the topic. Thus, the key of generating informative responses is leveraging the appropriate knowledge associated with current topic. This paper focus on addressing incorporating the appropriate knowledge in response generation. We adopt the reinforcement learning to select the most proper knowledge as the input information of the response generation part. Then we design an end-to-end dialogue model consisting of the knowledge decision part and the response generation part. The proposed model is able to effectively complete the knowledge driven dialogue task with specific topic. Our experiments clearly demonstrate the superior performance of our model over other baselines.},
  keywords={Learning (artificial intelligence);Task analysis;Knowledge engineering;Decision making;Information retrieval;Computational modeling;Cognition;Dialogue model;policy gradient;knowledge graph;transformer network},
  doi={10.1109/ACCESS.2020.2993924},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10837958,
  author={Okuhara, Fumika and Egami, Shusaku and Sei, Yuichi and Tahara, Yasuyuki and Ohsuga, Akihiko},
  booktitle={2024 IEEE 7th International Conference on Computer and Communication Engineering Technology (CCET)}, 
  title={Enhancing Panoramic Competency Through Link Prediction in Question Knowledge Graphs using a Language Representation Model}, 
  year={2024},
  volume={},
  number={},
  pages={267-272},
  abstract={Recently, panoramic knowledge has been required. On the other hand, multiple choice questions are suitable for efficient self-learning. Therefore, the purpose of this study is to create multiple choice questions that can reinforce learners' panoramic knowledge. Specifically, we proposed a method for automatically generating multiple choice questions that use Linked Data to present relevant information to give respondents an overall picture of relevant knowledge. There is some research on the methods that generated questions by extracting small subgraphs from the knowledge graphs consisting of entities(words) and relations(links) between the entities and hiding target words (correct answer words). In this study, our goal is to enhance the panoramic of the subgraphs of a specified size by using the link prediction method to complement edges and represent relationships not present in the knowledge graph when generating questions targeted at specific fields. The method of complementing edges involves first inputting two words as subject and object in Knowledge Graph to calculate the cosine similarity using a pretrained language model based on Wikipedia and Wikidata, then predicting the links as a predicate that should be complemented, and finally generating subgraphs by using the Graph Database added the complemented edges. For this study, we generated questions in the field of history, and since history requires temporal and spatial panoramic knowledge, words related to these aspects were focused on and complemented the relationships between them. As a result, 2,746 relationships were complemented by the proposed method in the subgraphs, and the subgraphs contained more words to learn (words found in textbooks that need to be learned) in a specific field compared to those generated using existing methods.},
  keywords={Hands;Databases;Linked data;Computational modeling;Knowledge graphs;Encyclopedias;Predictive models;History;Online services;Panoramic Knowledge;Linked Data;Knowledge Graph;Automatic Generate Question;Educational Application},
  doi={10.1109/CCET62233.2024.10837958},
  ISSN={2836-5992},
  month={Aug},}@INPROCEEDINGS{10597753,
  author={Helali, Mossad and Monjazeb, Niki and Vashisth, Shubham and Carrier, Philippe and Helal, Ahmed and Cavalcante, Antonio and Ammar, Khaled and Hose, Katja and Mansour, Essam},
  booktitle={2024 IEEE 40th International Conference on Data Engineering (ICDE)}, 
  title={KGLiDS: A Platform for Semantic Abstraction, Linking, and Automation of Data Science}, 
  year={2024},
  volume={},
  number={},
  pages={179-192},
  abstract={In recent years, we have witnessed the growing interest from academia and industry in applying data science technologies to analyze large amounts of data. In this process, a myriad of artifacts (datasets, pipeline scripts, etc.) are created. However, there has been no systematic attempt to holistically collect and exploit all the knowledge and experiences that are implicitly contained in those artifacts. Instead, data scientists recover information and expertise from colleagues or learn via trial and error. Hence, this paper presents a scalable platform, KGLiDS, that employs machine learning and knowledge graph technologies to abstract and capture the semantics of data science artifacts and their connections. Based on this information, KGLiDS enables various downstream applications, such as data discovery and pipeline automation. Our comprehensive evaluation covers use cases in data discovery, data cleaning, transformation, and AutoML. It shows that KGLiDS is significantly faster with a lower memory footprint than the state-of-the-art systems while achieving comparable or better accuracy.},
  keywords={Automation;Accuracy;Systematics;Semantics;Pipelines;Machine learning;Knowledge graphs;Linked Data Science;Knowledge Graphs;Graph Neural Networks;Data Integration;Data Discovery},
  doi={10.1109/ICDE60146.2024.00021},
  ISSN={2375-026X},
  month={May},}@INPROCEEDINGS{10800474,
  author={Li, Zehao and Zhong, Ling and Han, Xinyi},
  booktitle={2024 4th International Conference on Electronic Information Engineering and Computer Science (EIECS)}, 
  title={Research on the Entity Relationship Extraction Method of Large Model Chinese Electronic Medical Record With Low-Moment Features}, 
  year={2024},
  volume={},
  number={},
  pages={1043-1046},
  abstract={The entity relationship extraction of Chinese electronic medical records is used to construct the knowledge graph in the medical field, and it plays an important role in serving downstream tasks. However, due to the fuzzy boundary of Chinese electronic medical records, the complex relationship between medical texts, and the high density of entities, the task of extracting the physical relationships of medical texts is not accurate enough. To solve this problem, this paper adopts a large language model ChatGLM2-6B based on (Generative Language Model) architecture and Quantized Low-rank Adaptation of Large Language Models (QLoRA) for Chinese electronic medical record entity relationship extraction. By leveraging the powerful language understanding capabilities of ChatGLM2-6B and the efficient fine-tuning characteristics of QLoRA, the low-moment features are integrated to achieve accurate and efficient Chinese electronic medical record entity relationship extraction, which provides strong support for medical data analysis and clinical decision-making.},
  keywords={Knowledge engineering;Analytical models;Accuracy;Large language models;Computational modeling;Decision making;Knowledge graphs;Feature extraction;Data mining;Electronic medical records;knowledge graph;relationship extraction;large language models;low-moment features},
  doi={10.1109/EIECS63941.2024.10800474},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10266269,
  author={Sierra-Múnera, Alejandro and Westphal, Jan and Krestel, Ralf},
  booktitle={2023 ACM/IEEE Joint Conference on Digital Libraries (JCDL)}, 
  title={Efficient Ultrafine Typing of Named Entities}, 
  year={2023},
  volume={},
  number={},
  pages={205-214},
  abstract={Ultrafine named entity typing (UFET) refers to the assignment of predefined labels to entity mentions in a given context. In contrast to traditional named entity typing, the number of potential labels is in the thousands and one mention can have more than one assigned type. Previous approaches either depend on large training datasets, or require inefficient encoding of all input-type combinations. Therefore, there is a need for investigating the efficiency during training and prediction of entity typing models in the ultrafine-grained setting, considering its distinctively bigger search space, compared to the coarse- and fine-grained tasks. To efficiently solve UFET, we propose Decent, a lightweight model that encodes, using a pretrained language model, the input sentences separately from the type labels. Additionally, we make use of negative oversampling to speed up the training while improving the generalization of unseen types. Using an openly available UFET dataset, we evaluated the classification and runtime performance of Decent and observed that training and prediction runtime is orders of magnitude faster than the current state-of-the-art approaches, while maintaining a competitive classification performance.},
  keywords={Training;Vocabulary;Runtime;Limiting;Computational modeling;Semantics;Predictive models;ultrafine enity typing;named entity recognition},
  doi={10.1109/JCDL57899.2023.00038},
  ISSN={2575-8152},
  month={June},}@ARTICLE{8540089,
  author={Zhou, Bo and Maines, Curtis and Tang, Stephen and Shi, Qi and Yang, Po and Yang, Qiang and Qi, Jun},
  journal={IEEE Transactions on Computational Social Systems}, 
  title={A 3-D Security Modeling Platform for Social IoT Environments}, 
  year={2018},
  volume={5},
  number={4},
  pages={1174-1188},
  abstract={Social Internet-of-Things (SIoT) environment comprises not only smart devices but also the humans who interact with these IoT devices. The benefits of such system are overshadowed due to the cyber security issues. A novel approach is required to understand the security implication under such a dynamic environment while taking both the social and technical aspects into consideration. This paper addressed such challenges and proposed a 3-D security modeling platform that can capture and model the security requirements in the SIoT environment. The modeling process is graphical notation based and works as a security extension to the Business Process Model and Notation. Still, it utilizes the latest 3-D game technology; thus, the security extensions are generated through the third dimension. Consequently, the introduction of security extensions will not increase the complexity of the original SIoT scenario, while keeping all the key information on the same platform. Together with the proposed security ontology, these comprehensive security notations created a unique platform that aims at addressing the ever complicated security issues in the SIoT environment.},
  keywords={Ontologies;Security;Complexity theory;Internet of Things;Process modeling;Business process;game technology;notation;security modeling;social Internet of Things (SIoT)},
  doi={10.1109/TCSS.2018.2878921},
  ISSN={2329-924X},
  month={Dec},}@INPROCEEDINGS{9087557,
  author={Matveev, Anton and Makhnytkina, Olesia and Lizunova, Inna and Vinogradova, Taisiia and Chirkovskii, Artem and Svischev, Aleksei and Mamaev, Nikita},
  booktitle={2020 26th Conference of Open Innovations Association (FRUCT)}, 
  title={A Virtual Dialogue Assistant for Conducting Remote Exams}, 
  year={2020},
  volume={},
  number={},
  pages={284-290},
  abstract={In this paper, we demonstrate issues and possible solutions to building an Artificial Intelligence Dialogue Assistant for human-machine communication. We specialize it for conducting written exams at online education platforms, talk about the main logical components of the system: knowledge base, question encoder, question generation module, question analysis module. As a knowledge base we consider text fragments representing parts of the course of text fragments representing parts of a course and is a source for a format ontology; and is also sourced for neural network generation of fact-based questions in question generation module and building dependency trees for answer evaluation in question analysis module.},
  keywords={Technological innovation;Knowledge based systems;Buildings;Neural networks;Ontologies;Solids;Complexity theory},
  doi={10.23919/FRUCT48808.2020.9087557},
  ISSN={2305-7254},
  month={April},}@INPROCEEDINGS{8282005,
  author={D'Haro, Luis Fernando and Niculescu, Andreea I. and Cai, Caixia and Nair, Suraj and Banchs, Rafael E. and Knoll, Alois and Li, Haizhou},
  booktitle={2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)}, 
  title={An integrated framework for multimodal human-robot interaction}, 
  year={2017},
  volume={},
  number={},
  pages={076-082},
  abstract={Recent research progresses in speech recognition, text-to-speech, natural language understanding, or dialog management components are improving the way humans interact with advanced robot machines. However, far from being solved, we are just starting the process of creating meaningful multimodal platforms that can allow operators to use and control industrial robots through spoken dialogue. This paper describes our ongoing efforts on creating a modular platform that combines different technologies to cover typical requirements in an industrial setting, i.e. robust speech recognition, low level skill functions to operate the robot, recommendations and validation procedures to setup parameters, combination of audio-visual information for challenging environments, integration of domain-knowledge by means of an ontology, a flexible definition of the dialog model and natural language rules, as well as a test and control interface to quickly check the functionality of each module during development and operation. All platform modules are intercommunicated by the ROS operative system which allows the integration of external plugins and modules easily. Finally, a preliminary user study with IT experts simulating a welding task has been doing giving us clues on what should be the focus of our next developments.},
  keywords={Task analysis;Robots;Ontologies;Welding;Speech recognition;Speech;Natural languages},
  doi={10.1109/APSIPA.2017.8282005},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10148526,
  author={Zhang, Xiang and Yu, Bruce X.B. and Liu, Yan and Chen, Gong and Ng, George Wing-Yiu and Chia, Nam-Hung and So, Eric Hang-Kwong and So, Sze-Sze and Cheung, Victor Kai-Lam},
  booktitle={2022 IEEE International Conference on Teaching, Assessment and Learning for Engineering (TALE)}, 
  title={Conversational System for Clinical Communication Training Supporting User-defined Tasks}, 
  year={2022},
  volume={},
  number={},
  pages={396-403},
  abstract={Effective clinical communication is essential for delivering safe and high-quality patient care, especially in emergent cases. Standard communication protocols have been developed to improve communication accuracy and efficiency. However, traditional training and evaluation require substantial manpower and time, which can be infeasible during public crises when training is most needed. This research aims to facilitate autonomous, low-cost, adaptive clinical communication training via artificial intelligence (AI)-powered techniques. We propose a conversational system for clinical communication training supporting user-defined tasks. Two data augmentation (DA) methods, term replacement and context expansion, are proposed to allow non-professional users to create Al models with a small number of samples. Equipped with biomedical ontology and pre-trained language models, our system is able to simulate clinical communication scenarios, provide timely evaluation, and adapt to new tasks with minimal editing. Various experiments demonstrate that our proposed algorithms can achieve satisfactory performance using a small amount of training data. Real-world practice in local hospitals shows that our system can provide expert-level evaluation and deliver effective clinical communication training.},
  keywords={Training;Adaptation models;Protocols;Biological system modeling;Training data;Ontologies;Data models;clinical communication;human-computer interaction;autonomous communication training;conversational system},
  doi={10.1109/TALE54877.2022.00071},
  ISSN={2470-6698},
  month={Dec},}@INPROCEEDINGS{10196264,
  author={Okazaki, Sho and Shirafuji, Shouhei and Yasui, Toshinori and Ota, Jun},
  booktitle={2023 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)}, 
  title={A Framework to Support Failure Cause Identification in Manufacturing Systems through Generalization of Past FMEAs}, 
  year={2023},
  volume={},
  number={},
  pages={858-865},
  abstract={This study proposes a framework for inferring the causes of failures occurring in manufacturing systems from past Failure Mode and Effect Analyses (FMEAs) conducted on other systems to assist in inspecting and maintaining the systems. Among various manufacturing systems, a framework to search past FMEAs and the corresponding causes of the failure requires solving the following problems. First, the difference in products, equipment, and wording to represent them make it difficult to search the similar failure phenomenon from FMEAs. Secondly, the causes of failure highly depend on the process flow of the system until the failure occurs. Therefore, it is also hard to find appropriate failure causes from FMEAs without reflecting on the process. The framework solves the first issue by generalizing descriptions in past FMEAs based on structured concepts of manufacturing systems in an ontology before inference of causes to address. Furthermore, the framework analyzes the correspondence of the process flows between the target manufacturing system and past FMEAs using a process order model generated by SysML diagrams to solve the second issue. The comparison between the causes inferred by the proposed framework and by skilled experts for three typical failures in the manufacturing system and the interview with them about the plausibility of the inference results showed that more than 73 % of them were valid.},
  keywords={Analytical models;Mechatronics;Ontologies;Maintenance engineering;Search problems;Cognition;Data models},
  doi={10.1109/AIM46323.2023.10196264},
  ISSN={2159-6255},
  month={June},}@INPROCEEDINGS{10655250,
  author={Quan, Ruijie and Wang, Wenguan and Ma, Fan and Fan, Hehe and Yang, Yi},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Clustering for Protein Representation Learning}, 
  year={2024},
  volume={},
  number={},
  pages={319-329},
  abstract={Protein representation learning is a challenging task that aims to capture the structure and function of proteins from their amino acid sequences. Previous methods largely ignored the fact that not all amino acids are equally important for protein folding and activity. In this article, we propose a neural clustering framework that can automatically discover the critical components of a protein by considering both its primary and tertiary structure information. Our framework treats a protein as a graph, where each node represents an amino acid and each edge represents a spatial or sequential connection between amino acids. We then apply an iterative clustering strategy to group the nodes into clusters based on their 1D and 3D positions and assign scores to each cluster. We select the highest-scoring clusters and use their medoid nodes for the next iteration of clustering, until we obtain a hierarchical and informative representation of the protein. We evaluate on four protein-related tasks: protein fold classification, enzyme reaction classification, gene ontology term prediction, and enzyme commission number prediction. Experimental results demonstrate that our method achieves state-of-the-art performance.},
  keywords={Proteins;Representation learning;Enzymes;Computer vision;Three-dimensional displays;Ontologies;Amino acids;Protein Representation Learning;Clustering},
  doi={10.1109/CVPR52733.2024.00038},
  ISSN={2575-7075},
  month={June},}@ARTICLE{10508456,
  author={Yang, Puhai and Huang, Heyan and Shi, Shumin and Mao, Xian-Ling},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={STN4DST: A Scalable Dialogue State Tracking Based on Slot Tagging Navigation}, 
  year={2024},
  volume={32},
  number={},
  pages={2494-2507},
  abstract={Dialogue state tracking plays a key role in tracking user intentions in task-oriented dialogue systems. Traditional dialogue state tracking methods usually rely on selecting slot values from a fixed ontology to represent the dialogue state. In recent years, more flexible open vocabulary based approaches have become the mainstream focus which are mainly divided into two categories: generative methods and span extraction methods. Among them, the span extraction method is favored for its outstanding ability to predict unknown slot values. However, the span extraction method only focuses on the predicted slot values, but ignores other potential slot values in the utterance, which leads to insufficient semantic understanding of the utterance and difficulty in dealing with complex utterance scenarios, such as more or longer unknown slot values. To tackle the above drawbacks, in this paper, we propose a novel scalable dialogue state tracking method, which employs slot tagging to locate all potential slot values in the utterances and jointly learns slot pointers to select the predicted slot value from them. Specifically, our STN4DST (Slot Tagging Navigation for Dialogue State Tracking) model not only adopts the above joint learning strategy, which we call slot tagging navigation, to extract slot values from utterances, but also uses previous dialogue states as dialogue contexts to track the change of slot values, and introduces appendix slot values to predict special slot values that cannot be extracted. Extensive experiments show that in the open vocabulary setting, STN4DST achieves the state-of-the-art joint goal accuracy of 85.4% and 96.5% on Sim-M and Sim-R datasets with a large number of unknown slot values, and is also comparable to other state-of-the-art models in the absence of token-level slot annotations for all potential slot values.},
  keywords={Tagging;Navigation;Speech processing;Vocabulary;Semantics;Predictive models;Ontologies;Task-oriented dialogue system;dialogue state tracking;scalable DST;unknown slot value},
  doi={10.1109/TASLP.2024.3393733},
  ISSN={2329-9304},
  month={},}@ARTICLE{10288434,
  author={Tara, Andrei and Turesson, Hjalmar K. and Natea, Nicolae and Kim, Henry M.},
  journal={IEEE Access}, 
  title={An Evaluation of Storage Alternatives for Service Interfaces Supporting a Decentralized AI Marketplace}, 
  year={2023},
  volume={11},
  number={},
  pages={116919-116931},
  abstract={Given the exploding interest in generative AI and the concern that a few companies like Microsoft will monopolize access to such models, we address this centralization risk in the context of a DApp that matches buyers and sellers of various AI services. A key question for a decentralized marketplace is where and how to store the metadata that specifies the services’ properties in human and machine-readable formats. Having one or a few actors controlling access to that data constitutes undesirable centralization. We explore data storage alternatives to ensure decentralization, equitable match-making, and efficiency. Classifying decentralized storage alternatives as simple peer-to-peer replication, replication governed by a permissionless consensus, and replication governed by a private consensus, we select an exemplar for each category: IPFS, Tendermint Cosmos and Hyperledger Fabric. We conduct experiments on performance and find that read and write speeds are fastest for IPFS, about two times slower for Tendermint and slowest for Hyperledger. Writing using IPFS and Tendermint takes significantly longer than reading, and finally, specifically with IPFS, write speeds strongly depend on configuration. Given these results and the properties of the storage technologies, we conclude that simple peer-to-peer storage is the best option for the proposed AI marketplace.},
  keywords={Artificial intelligence;Ontologies;Peer-to-peer computing;Blockchains;Distributed ledger;Resource description framework;Fabrics;Decentralized applications;Storage management;Blockchain;decentralized AI decentralized storage;distributed ontology;semantic models},
  doi={10.1109/ACCESS.2023.3326418},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10191094,
  author={Liu, Tengfei and Hu, Yongli and Chen, Puman and Sun, Yanfeng and Yin, Baocai},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Zero-Shot Text Classification with Semantically Extended Textual Entailment}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={Zero-shot text classification (0SHOT-TC) aims to detect classes that the model never seen in the training set, and has attracted much attention in the research community of Natural Language Processing (NLP). The emergence of pre-trained language models has fostered the progress of 0SHOT-TC, which turns the task into a textual entailment problem of binary classification. It learns an entailment relatedness (yes/no) between the given sentence (premise) and each category (hypothesis) separately. However, the hypothesis generation paradigms need to be further studied, since the label itself or the label descriptions have limited ability to fully express the category space. Conversely, humans can easily extend a set of words describing the categories to be classified. In this paper, we propose a novel zero-shot text classification method called Semantically Extended Textual Entailment (SETE), which imitates the human's ability in knowledge extension. In the proposed method, three semantic extension methods are used to enrich the categories through a combination of static knowledge (e.g. expert knowledge, knowledge graph) and dynamic knowledge (e.g. language models), and the textual entailment model is finally used for 0SHOT-TC. The experimental results on the benchmarks show that our approach significantly outperforms the current methods in both generalized and non-generalized 0SHOT-TC.},
  keywords={Knowledge engineering;Training;Vocabulary;Text categorization;Semantics;Neural networks;Benchmark testing;Zero-shot text classification;semantic extension;text entailment},
  doi={10.1109/IJCNN54540.2023.10191094},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10236713,
  author={Liang, Meng and Shi, Yao},
  booktitle={2023 5th International Conference on Natural Language Processing (ICNLP)}, 
  title={Named Entity Recognition Method Based on BERT-whitening and Dynamic Fusion Model}, 
  year={2023},
  volume={},
  number={},
  pages={191-197},
  abstract={In the context of Natural Language Processing (NLP), Named Entity Recognition (NER) plays a crucial role in tasks like entity relationship extraction and knowledge graph construction. The accuracy of Chinese NER heavily relies on the representation of word embeddings. However, traditional word representation methods like word2vec suffer from word ambiguity and singular word vectors. Similarly, BERT-based word embeddings also exhibit anisotropy. To tackle these challenges, we propose a novel NER method that leverages BERT-whitening and dynamic fusion of BERT’s output from different layers. The dynamic fusion module calculates a weighted sum of BERT’s output across multiple layers, while the whitening module applies a whitening operation to eliminate the anisotropy of word embeddings. By integrating these modules, our model effectively captures the characteristics of input words, providing robust support for subsequent decoding. We evaluate our approach on the CLUENER2020 Chinese fine-grained named entity recognition dataset. Experimental results demonstrate that our method outperforms the traditional BERT-BiLSTM-CRF model without external resources and data expansion, leading to significant improvements in performance.},
  keywords={Training;Anisotropic magnetoresistance;Heuristic algorithms;Knowledge graphs;Natural language processing;Data models;Decoding;NER;word embedding;BERT;anisotropy;whitening},
  doi={10.1109/ICNLP58431.2023.00041},
  ISSN={},
  month={March},}@INPROCEEDINGS{10423971,
  author={Jiang, Yan and Zhang, Zhihou and He, Lingfeng and Gong, Tianyi and Du, Jiawen and Yin, Xinyu},
  booktitle={2023 6th International Conference on Data Science and Information Technology (DSIT)}, 
  title={Application of DA-Bi-SRU and Improved RoBERTa Model in Entity Relationship Extraction for High-Speed Train Bogie}, 
  year={2023},
  volume={},
  number={},
  pages={89-96},
  abstract={Due to the large number of professional terms and complex entity relationships in the field of high-speed train (HST) bogie, the accuracy of entity relationship extraction is low. In order to improve the efficiency and accuracy of entity relationship extraction in high-speed train bogie domain, we propose a novel entity relationship extraction model for the domain of high-speed train (HST) bogie with the aim of improving the efficiency and accuracy of entity relationship extraction. The proposed model is based on RoBERTa-wwm (A Robustly Optimized BERT Pretraining Approach with Whole Word Masking) and DA-Bi-SRU (Double-Attention-Based Bidirectional Simple Recurrent Unit). To facilitate this, we construct a new bogie relation extraction dataset comprising of 25,000 statements collected from literature and professional annotations. The RoBERTa-wwm is employed to obtain dynamic word vectors from the input statements and optimized using the bogie dataset. Subsequently, a Bi-SRU model based on dual attention mechanism is developed to capture bidirectional semantic information and contextual semantic linkage in a rapid manner. Our experiments show that the RoBERTa-wwm-DA-Bi-SRU model outperforms Bi-LSTM and RNN methods with a prediction accuracy of 88.53% and an F1 value of 86.60%. Our proposed model thus demonstrates the potential to accurately extract entity relationships in the bogie knowledge graph of high-speed trains, simplifying the construction process.},
  keywords={Semantics;Knowledge graphs;Predictive models;Data science;Data models;Data mining;Information technology;Improved BERT;Whole Word Masking;Bi-SRU;Attention mechanism;Deep learning;Entity relationship extraction;High-speed train bogie},
  doi={10.1109/DSIT60026.2023.00023},
  ISSN={},
  month={July},}@INPROCEEDINGS{10142258,
  author={Wang, Juan and Peng, Bitao and Tang, Jing},
  booktitle={2023 5th International Conference on Communications, Information System and Computer Engineering (CISCE)}, 
  title={Research on Named Entity Recognition in Judicial Field Based on ERNIE-Gram}, 
  year={2023},
  volume={},
  number={},
  pages={228-233},
  abstract={Named Entity Recognition is a key and fundamental task in natural language processing and can benefit many downstream tasks such as knowledge graph construction, question answering system, machine reading, etc. In view of the contradiction between the rapidly increasing of judgement documents and the low efficiency of manual analysis in the judicial field, we propose a NER model by using a combination of ERNIE-Gram, BiGRU, and CRF. ERNIE-Gram adopts multi-granularity n-gram language learning mechanism to learn the semantic of n-grams more adequately. Thus, we first employ the ERNIE-Gram to capture rich language representation, then we feed them into the BiGRU to obtain more important text features, finally, we use the CRF to decode and output optimal labeling sequence. We conduct experiments on an open dataset of the 2021 “Challenge of AI in Law” information extraction subtask and compare our model with currently familiar models for NER task. Experimental results demonstrate that our proposed model achieves a F1-score of 87.01%, and outperforms all the baseline models.},
  keywords={Learning systems;Text recognition;Computational modeling;Semantics;Knowledge graphs;Manuals;Information retrieval;judicial field;name entity recognition;pretrained language models;explicitly n-gram masked language modeling},
  doi={10.1109/CISCE58541.2023.10142258},
  ISSN={2833-2423},
  month={April},}@ARTICLE{9483677,
  author={Cao, Yixin and Kuang, Jun and Gao, Ming and Zhou, Aoying and Wen, Yonggang and Chua, Tat-Seng},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Learning Relation Prototype From Unlabeled Texts for Long-Tail Relation Extraction}, 
  year={2023},
  volume={35},
  number={2},
  pages={1761-1774},
  abstract={Relation Extraction (RE) is a vital step to complete Knowledge Graph (KG) by extracting entity relations from texts. However, it usually suffers from the long-tail issue. The training data mainly concentrates on a few types of relations, leading to the lack of sufficient annotations for the remaining types of relations. In this paper, we propose a general approach to learn relation prototypes from unlabeled texts, to facilitate the long-tail relation extraction by transferring knowledge from the relation types with sufficient training data. We learn relation prototypes as an implicit factor between entities, which reflects the meanings of relations as well as their proximities for transfer learning. Specifically, we construct a co-occurrence graph from texts, and capture both first-order and second-order entity proximities for embedding learning. Based on this, we further optimize the distance from entity pairs to corresponding prototypes, which can be easily adapted to almost arbitrary RE frameworks. Thus, the learning of infrequent or even unseen relation types will benefit from semantically proximate relations through pairs of entities and large-scale textual information. We have conducted extensive experiments on two publicly available datasets: New York Times and Google Distant Supervision. Compared with eight state-of-the-art baselines, our proposed model achieves significant improvements (4.1 percent F1 on average). Further results on long-tail relations demonstrate the effectiveness of the learned relation prototypes. We further conduct an ablation study to investigate the impacts of varying components, and apply it to four basic relation extraction models to verify the generalization ability. Finally, we analyze several example cases to give intuitive impressions as qualitative analysis. Our codes and data can be found in https://github.com/CrisJk/PA-TRP.},
  keywords={Prototypes;Training;Training data;Data mining;Annotations;Urban areas;Transfer learning;Relation extraction;long-tail;knowledge graph;prototype learning},
  doi={10.1109/TKDE.2021.3096200},
  ISSN={1558-2191},
  month={Feb},}@INPROCEEDINGS{10385748,
  author={Zhou, Zongzhen and Yang, Tao and Hu, Kongfa},
  booktitle={2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Traditional Chinese Medicine Epidemic Prevention and Treatment Question-Answering Model Based on LLMs}, 
  year={2023},
  volume={},
  number={},
  pages={4755-4760},
  abstract={Background: Epidemic diseases in Traditional Chinese Medicine (TCM) constitute an essential part of Chinese medical science. TCM has accumulated rich theoretical and practical experiences in the prevention and treatment of epidemic diseases, forming the academic system of epidemic febrile disease, providing robust support for epidemic prevention and resistance in TCM. However, the numerous and complex literature on TCM epidemic diseases brings challenges to the organization and discovery of epidemic disease knowledges. Objective: To leverage the powerful knowledge learning ability of state-of-the-art LLMs (LLMs) to address the efficient acquisition and utilization of TCM epidemic disease knowledges. Methods: By collecting content related to epidemic diseases from 194 ancient TCM books, as well as the knowledge graph of TCM epidemic disease prevention and treatment, we built the large TCM epidemic disease model EpidemicCHAT based on the ChatGLM model. To assess the performances of the model, several open-source LLMs were compared in the study. Results: Compared to traditional LLMs, which may fail to answer or produce hallucinations in the field of TCM epidemic diseases, EpidemicCHAT demonstrates superior answering and reasoning abilities. In the evaluation of TCM epidemic disease prescription generation, the model achieved scores of 44.02, 61.10, and 59.40 on the BLEU-4, ROUGE-L, and METEOR metrics, respectively. Conclusion: The EpidemicCHAT model proposed in this study performs excellently in the field of TCM epidemic diseases, which might provide a reference for the construction of TCM LLMs and applications such as TCM auxiliary diagnosis and Chinese herbal prescription generation.},
  keywords={Measurement;Epidemics;Knowledge acquisition;Knowledge graphs;Organizations;Question answering (information retrieval);Meteors;LLMs;traditional Chinese medicine;epidemic;knowledge graph},
  doi={10.1109/BIBM58861.2023.10385748},
  ISSN={2156-1133},
  month={Dec},}@ARTICLE{10246785,
  author={Gupta, Amit and Bhatia, Rajesh},
  journal={Journal of Web Engineering}, 
  title={Knowledge Based Deep Inception Model for Web Page Classification}, 
  year={2021},
  volume={20},
  number={7},
  pages={2131-2168},
  abstract={Web Page Classification is decisive for information retrieval and management task and plays an imperative role for natural language processing (NLP) problems in web engineering. Traditional machine learning algorithms excerpt covet features from web pages whereas deep leaning algorithms crave features as the network goes deeper. Pre-trained models such as BERT attains remarkable achievement for text classification and continue to show state-of-the-art results. Knowledge Graphs can provide rich structured factual information for better language modelling and representation. In this study, we proposed an ensemble Knowledge Based Deep Inception (KBDI) approach for web page classification by learning bidirectional contextual representation using pre-trained BERT incorporating Knowledge Graph embeddings and fine-tune the target task by applying Deep Inception network utilizing parallel multi-scale semantics. Proposed ensemble evaluates the efficacy of fusing domain specific knowledge embeddings with the pre-trained BERT model. Experimental interpretation exhibit that the proposed BERT fused KBDI model outperforms benchmark baselines and achieve better performance in contrast to other conventional approaches evaluated on web page classification datasets.},
  keywords={Machine learning algorithms;Knowledge based systems;Text categorization;Semantics;Web pages;Knowledge graphs;Information retrieval;Web page classification;transfer learning;knowledge graph embedding;pre-trained model},
  doi={10.13052/jwe1540-9589.2075},
  ISSN={1544-5976},
  month={October},}@INPROCEEDINGS{10298416,
  author={Chakraborty, Sarthak and Agarwal, Shubham and Garg, Shaddy and Sethia, Abhimanyu and Pandey, Udit Narayan and Aggarwal, Videh and Saini, Shiv},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, 
  title={ESRO: Experience Assisted Service Reliability against Outages}, 
  year={2023},
  volume={},
  number={},
  pages={255-267},
  abstract={Modern cloud services are prone to failures due to their complex architecture, making diagnosis a critical process. Site Reliability Engineers (SREs) spend hours leveraging multiple sources of data, including the alerts, error logs, and domain expertise through past experiences to locate the root cause(s). These experiences are documented as natural language text in outage reports for previous outages. However, utilizing the raw yet rich semi-structured information in the reports systematically is time-consuming. Structured information, on the other hand, such as alerts that are often used during fault diagnosis, is voluminous and requires expert knowledge to discern. Several strategies have been proposed to use each source of data separately for root cause analysis. In this work, we build a diagnostic service called ESRO that recommends root causes and remediation for failures by utilizing structured as well as semi-structured sources of data systematically. ESRO constructs a causal graph using alerts and a knowledge graph using outage reports, and merges them in a novel way to form a unified graph during training. A retrieval based mechanism is then used to search the unified graph and rank the likely root causes and remediation techniques based on the alerts fired during an outage at inference time. Not only the individual alerts, but their respective importance in predicting an outage group is taken into account during recommendation. We evaluated our model on several cloud service outages of a large SaaS enterprise over the course of ~2 years, and obtained an average improvement of 27% in rouge scores after comparing the likely root causes against the ground truth over state-of-the-art baselines. We further establish the effectiveness of ESRO through qualitative analysis on multiple real outage examples.},
  keywords={Training;Fault diagnosis;Root cause analysis;Natural languages;Software as a service;Knowledge graphs;Reliability engineering;System Monitoring;Cloud Services;Causal Graph;Knowledge Graph},
  doi={10.1109/ASE56229.2023.00131},
  ISSN={2643-1572},
  month={Sep.},}@ARTICLE{10736645,
  author={Mi, Li and Dai, Xianjie and Castillo-Navarro, Javiera and Tuia, Devis},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={Knowledge-Aware Text–Image Retrieval for Remote Sensing Images}, 
  year={2024},
  volume={62},
  number={},
  pages={1-13},
  abstract={Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide. By using text as information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only. For this reason, as a matching-based task, cross-modal text–image retrieval often suffers from information asymmetry between text and images. To address this challenge, we propose a Knowledge-aware Text–Image Retrieval (KTIR) method for remote sensing images. By mining relevant information from an external knowledge graph, KTIR enriches the text scope available in the search query and alleviates the information gaps between text and images for better matching. Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pretrained vision–language models to remote sensing applications. Experimental results on three commonly used remote sensing text–image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods.},
  keywords={Remote sensing;Feature extraction;Lakes;Commonsense reasoning;Adaptation models;Visualization;Image retrieval;Boats;Sensors;Knowledge graphs;Knowledge graph;remote sensing;text–image retrieval},
  doi={10.1109/TGRS.2024.3486977},
  ISSN={1558-0644},
  month={},}@INPROCEEDINGS{9313210,
  author={Yang, Songchun and Zheng, Xiangwen and Tong, Fan and Mao, Huajian and Zhao, Dongsheng},
  booktitle={2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={A Novel Algorithm of Expansion Term Selection and Weight Assignment for Query Expansion of Chinese EMR Retrieval}, 
  year={2020},
  volume={},
  number={},
  pages={2139-2146},
  abstract={The techniques of information retrieval eliminate the difficulty of finding specific information in electronic medical records (EMR), and the methods of query expansion (QE) improve the recall of EMR retrieval. However, most existing QE methods of EMR retrieval can't get high-quality expansion terms and corresponding weights for Chinese EMR retrieval because of low-quality sources and improper calculation methods. In this paper, we propose a novel algorithm of expansion term selection and weight assignment for QE of Chinese EMR retrieval based on the clinical needs and unique characteristics of Chinese medical terms. The algorithm first selects expansion terms from a high quality Chinese medical knowledge graph and standard medical term sets, which can ensure the quality of expansion terms. Then it assigns weights to the selected expansion terms based on semantic similarities and manually designed expansion categories that reflect the opinions of medical experts. Experiment results show that our algorithm gets higher-quality expansion terms and more rational weights compared with four benchmark algorithms, using Precision at 10, Recall, Mean Average Precision and Binary Preference-based Measure as evaluation metrics, which implies that our algorithm can significantly improve the effectiveness of QE.},
  keywords={Semantics;Training;Standards;Medical diagnostic imaging;Compounds;Internet;Diseases;Information Retrieval;Query Expansion;Knowledge Graph;Word2Vec;Chinese Electronic Medical Record},
  doi={10.1109/BIBM49941.2020.9313210},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8995179,
  author={Liu, Chunfeng and Zhang, Yan and Yu, Mei and Li, Xuewei and Zhao, Mankun and Xu, Tianyi and Yu, Jian and Yu, Ruiguo},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)}, 
  title={Text-Enhanced Knowledge Representation Learning Based on Gated Convolutional Networks}, 
  year={2019},
  volume={},
  number={},
  pages={308-315},
  abstract={Knowledge representation learning (KRL), which transforms both the entities and relations into continuous low dimensional continuous vector space, has attracted considerable research. Most of existing knowledge graph (KG) completion models only considers the structural representation of triples, but do not consider the important text information about entity descriptions in the knowledge base. We propose a text-enhanced KG model based on gated convolution network (GConvTE), which can learn entity descriptions and symbol triples jointly by feature fusion. Specifically, each triple (head entity, relation, tail entity) is represented as a 3-column structural embedding matrix, a 3-column textual embedding matrix and a 3-column joint embedding matrix where each column vector represents a triple element. Textual embeddings are obtained by bidirectional gated recurrent unit with attention (A-BGRU) encoding entity descriptions and joint embeddings are obtained by the combination of textual embeddings and structural embeddings. Extending feature dimension in embedding layer, these three matrixs are concatenated into 3-channel feature block to be fed into convolution layer, where the gated unit is added to selectively output the joint features maps. These feature maps are concatenated and then multiplied with a weight vector via a dot product to return a score. The experimental results show that our model GConvTE achieves better link performance than previous state-of-art embedding models on two benchmark datasets.},
  keywords={representation learning;knowledge graph completion;joint representation;multidimensional feature;gated convolution network},
  doi={10.1109/ICTAI.2019.00051},
  ISSN={2375-0197},
  month={Nov},}@INPROCEEDINGS{10738062,
  author={Fortuna, Carolina and Hanžel, Vid and Bertalanič, Blaz},
  booktitle={2024 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)}, 
  title={Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin}, 
  year={2024},
  volume={},
  number={},
  pages={8-14},
  abstract={Domain specific digital twins, representing a digital replica of various segments of the smart grid, are foreseen as able to model, simulate, and control the respective segments. At the same time, knowledge-based digital twins, coupled with AI, may also empower humans to understand aspects of the system through natural language interaction in view of planning and policy making. This paper is the first to assess and report on the potential of Retrieval Augmented Generation (RAG) question answers related to household electrical energy measurement aspects leveraging a knowledge-based energy digital twin. Relying on the recently published electricity consumption knowledge graph that actually represents a knowledge-based digital twin, we study the capabilities of ChatGPT, Gemini and Llama in answering electricity related questions. Furthermore, we compare the answers with the ones generated through a RAG techniques that leverages an existing electricity knowledge-based digital twin. Our findings illustrate that the RAG approach not only reduces the incidence of incorrect information typically generated by LLMs but also significantly improves the quality of the output by grounding responses in verifiable data. This paper details our methodology, presents a comparative analysis of responses with and without RAG, and discusses the implications of our findings for future applications of AI in specialized sectors like energy data analysis.},
  keywords={Data analysis;Electricity;Knowledge based systems;Natural languages;Chatbots;Robustness;Question answering (information retrieval);Digital twins;Smart grids;Planning;retrieval augmented generation;large language models;knowledge-based digital twin;knowledge graph;house-holds},
  doi={10.1109/SmartGridComm60555.2024.10738062},
  ISSN={2474-2902},
  month={Sep.},}@INPROCEEDINGS{10740424,
  author={Colliani, Felice Paolo and Futia, Giuseppe and Garifo, Giovanni and Vetrò, Antonio and De Martin, Juan Carlos},
  booktitle={2024 IEEE 18th International Conference on Application of Information and Communication Technologies (AICT)}, 
  title={Towards Named Entity Disambiguation with Graph Embeddings}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={Extracting structured knowledge from scientific literature is crucial for helping professionals make well-informed decisions. This paper presents an approach to distilling knowledge from biomedical documents within the context of Named Entity Disambiguation (NED). The proposed method leverages a joint representation of biomedical entities, combining pre-trained language models with graph machine learning techniques. A Siamese Neural Network (SNN) is trained to optimize this joint representation by integrating the contextual text embeddings of entity mentions with the graph embeddings of corresponding canonical entities in a biomedical Knowledge Graph (KG). During the inference phase, the SNN model assigns a score to this joint representation to disambiguate the target entity among a set of candidates. To the best of our knowledge, this is the first NED method in the biomedical domain that incorporates graph embeddings using a neural model. We empirically evaluated the effectiveness of our approach against well-known biomedical datasets, such as MedMentions and BC5CDR. The results demonstrate a promising direction in utilizing the relational knowledge captured by graph embeddings for the NED task.},
  keywords={Biological system modeling;Neural networks;Machine learning;Knowledge graphs;Information and communication technology;Named Entity Disambiguation;Graph Embeddings;Knowledge graph},
  doi={10.1109/AICT61888.2024.10740424},
  ISSN={2472-8586},
  month={Sep.},}@INPROCEEDINGS{9825235,
  author={Ji, Pengfei and Song, Dandan},
  booktitle={2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)}, 
  title={A Dual Knowledge Aggregation Network for Cross-Domain Sentiment Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1-7},
  abstract={Cross-domain sentiment analysis (CDSA) is an essential subtask of sentiment analysis. It aims to utilize rich source domain data to conquer the data-hungry problem on target domain. Most existing approaches depending on deep learning mainly concentrate on common features or pivots. However, few of them consider the effect of external Knowledge Graph (KG). In this paper, we propose a Dual Knowledge Aggregation Network for Cross-Domain Sentiment Analysis (DKAN), which leverages prior knowledge from two external KGs. Specifically, DKAN comprises two main parts. One is extracting sentence representation features. The other aims to introduce external knowledge better. Also, we use SenticNet to avoid noise from KG by selecting top-n words and inserting special tokens in sentences. We also conduct empirical analyses on the effectiveness of our model on the Amazon reviews dataset. DKAN achieves promising performance compared with other methods.},
  keywords={Knowledge engineering;Deep learning;Sentiment analysis;Analytical models;Feature extraction;corss-domain;sentiment analysis;knowledge graph;SenticNet},
  doi={10.1109/CVIDLICCEA56201.2022.9825235},
  ISSN={},
  month={May},}@INPROCEEDINGS{10691283,
  author={Sumanathilaka, Deshan and Micallef, Nicholas and Hough, Julian},
  booktitle={2024 IEEE 15th Control and System Graduate Research Colloquium (ICSGRC)}, 
  title={Assessing GPT's Potential for Word Sense Disambiguation: A Quantitative Evaluation on Prompt Engineering Techniques}, 
  year={2024},
  volume={},
  number={},
  pages={204-209},
  abstract={Modern digital communications (including social media content) often contain ambiguous words due to their potential for multiple related interpretations (polysemy). This ambiguity poses challenges for traditional Word Sense Disambiguation (WSD) methods, which struggle with limited data and lack of contextual understanding. These limitations hinder efficient translation, information retrieval, and question-answering systems, thereby restricting the benefits of computational linguistics techniques when applied to digital communication technologies. Our research investigates the use of Large Language Models (LLMs) to improve WSD using various prompt engineering techniques. We propose and evaluate a novel method that combines a knowledge graph, together with Part-of-Speech (POS) tagging and few-shot prompting to guide LLMs. By utilizing prompt augmentation with human-in-loop on few-shot prompt approaches, this work demonstrates a substantial improvement in WSD. This research advances accurate word interpretation in digital communications, leading to important implications for improved translation systems, better search results, and more intelligent question-answering technology.},
  keywords={Accuracy;Social networking (online);Large language models;Knowledge graphs;Tagging;Digital communication;Control systems;Computational linguistics;Prompt engineering;Large Language Models;Word Sense Disambiguation;FEWS sense tags;Few Shot Prompting;Knowledge Graph},
  doi={10.1109/ICSGRC62081.2024.10691283},
  ISSN={2833-1028},
  month={Aug},}@INPROCEEDINGS{9616677,
  author={P, Pavithra C and Mandal, Supriya},
  booktitle={2021 Fourth International Conference on Electrical, Computer and Communication Technologies (ICECCT)}, 
  title={An Overview of Relevant Literature on Different Approaches to Word Sense Disambiguation}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  abstract={WSD (Word Sense Disambiguation) is a common issue in Natural Language Processing (NLP) and Machine Learning technology. In NLP, word sense disambiguation is described as the capacity to detect which meaning of a word is activated by its use in a specific context. WSD is a solution to the uncertainty that occurs when words have different meanings in different contexts. Contextual word meaning plays an important role in various applications such as sentiment analysis, search engine, information extraction, machine translation etc. It is a challenge for these systems to detect and overcome the uncertainty that emerges from the lexical ambiguity. Many studies have been conducted over the decades to propose various approaches to the WSD problem. In this manuscript, a comparative study of three approaches namely LESK algorithm, embedding techniques, and Neural Network techniques based on the text collected from children's story books is performed. We explored an approach that combines Bi-LSTM neural network with Knowledge Graph to predict contextual word meaning. Our study shows that the combined approach accuracy is 80.34approaches},
  keywords={Deep learning;Knowledge engineering;Sentiment analysis;Uncertainty;Neural networks;Knowledge based systems;Search engines;Word Sense Disambiguation;Natural Language Processing;Lesk Algorithms;Embedding Techniques;Neural Network;Bi-LSTM;Knowledge Graph},
  doi={10.1109/ICECCT52121.2021.9616677},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{10417286,
  author={Jayasooriya, Asel and Ahamed, Akeel and Bandara, Yomal and Gavindya, Chamod and Kasthurirathna, Dharshana and Abeywardhana, Lakmini},
  booktitle={2023 5th International Conference on Advancements in Computing (ICAC)}, 
  title={An Integrated Approach to Enhance Legal Information Retrieval of Sri Lankan Supreme Court Verdicts}, 
  year={2023},
  volume={},
  number={},
  pages={316-321},
  abstract={Legal professionals and law students often grapple with the arduous task of locating prior Supreme Court rulings for argument preparation and academic study, leading to a laborious and resource-intensive process. This study introduces an innovative approach to streamline the retrieval of legal information from Sri Lankan Supreme Court verdicts. The method centers on developing a Custom Named Entity Recognition (NER) model, boasting a remarkable accuracy exceeding 90%. This model efficiently extracts crucial legal entities from Supreme Court rulings, readily available on the Ministry of Justice's Supreme Court website. The high accuracy ensures precise entity extraction, followed by systematic organization within a dedicated database. Subsequently, a knowledge graph is formed by linking recorded legal entities, reducing information retrieval time to a mere 179 milliseconds, significantly outperforming existing methods. Moreover, a BART summarization model is crafted to generate concise, accurate, and insightful summaries of Supreme Court decisions, boasting an impressive ROGUE1 score of 85%. This approach revolutionizes legal information retrieval, delivering a user-friendly platform that enhances the identification of cases and fosters a deeper understanding, ultimately elevating the quality of legal research and practice.},
  keywords={Adaptation models;Law;Databases;Information retrieval;Feature extraction;Data mining;Task analysis;BART Model;Knowledge Graph;Legal Information Retrieval;Named Entity Recognition;Supreme Court Judgments},
  doi={10.1109/ICAC60630.2023.10417286},
  ISSN={2837-5424},
  month={Dec},}@INPROCEEDINGS{10650447,
  author={Li, Sirui and Wong, Kok Wai and Zhu, Dengya and Fung, Chun Che},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Enhancing Question Answering through Effective Candidate Answer Selection and Mitigation of Incomplete Knowledge Graphs and over-smoothing in Graph Convolutional Networks}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  abstract={Question answering over knowledge graphs (KGQA) seeks to automatically answer natural language questions by retrieving triples within the knowledge graph (KG). In the context of multi-hop KGQA, reasoning across multiple edges of the KG becomes crucial for obtaining answers. Existing methods align with either the path-searching-based mainstream, emphasizing structural KG analysis, or the subgraph-based mainstream, focusing on semantic KG embeddings. Both streams have two primary challenges: (1) KG incompleteness, where path searching or subgraph construction faces limitations in the absence of links between entities; (2) candidate answer selection, wherein most approaches employ pre-defined searching sizes or heuristics. Many recent studies incorporate Graph Convolutional Network (GCN) to encode KGs, yet they overlook the potential over-smoothing issue inherent in GCNs. The over-smoothing problem arises from the tendency of closely connected nodes to exhibit similar embeddings within the deep convolutional architecture of GCNs. To address these challenges, this paper proposes a two-stage framework named ComPath, leveraging insights from both mainstreams. ComPath utilizes GCN to tackle KG incompleteness and introduces a path analyser to mitigate the over-smoothing issue associated with GCN. Candidate answers are selected using semantic similarity. The ablation studies and comparative experiments on the three KGQA benchmark datasets shown that the proposed ComPath performed better than the other KGQAs.},
  keywords={Graph convolutional networks;Prevention and mitigation;Semantics;Neural networks;Natural languages;Knowledge graphs;Spread spectrum communication;Question Answering;Graph Convolutional Network;Incomplete Knowledge Graph;Over-smoothing;Candidate Selection},
  doi={10.1109/IJCNN60899.2024.10650447},
  ISSN={2161-4407},
  month={June},}@INPROCEEDINGS{10500264,
  author={Thapa, Astha and Patil, Rajvardhan},
  booktitle={SoutheastCon 2024}, 
  title={ChatGPT based ChatBot Application}, 
  year={2024},
  volume={},
  number={},
  pages={157-164},
  abstract={Since the invention of ChatGPT, there have been several applications demonstrating how ChatGPT can be used to solve real world problems. This paper focuses on implementing one such application, where we design ChatBot using ChatGPT API. To carry out the experiments, we choose dataset from a specific domain, and store the data in graph oriented database. Sample training dataset consisting of English and its equivalent Cypher queries were designed for prompt tuning ChatGPT on the local Knowledge-graph. This training dataset helps ChatGPT understand the mapping between the English and their equivalent Cypher queries. Once the mapping is learnt, it becomes easier for ChatGPT to encode or represent a real time (previously unseen) English query fed by the user into its equivalent Cypher query format. In this paper, we demonstrate our results on sample Rope-related dataset scrapped from internet, and conclude with limitations and future work.},
  keywords={Training;Ciphers;Technological innovation;Databases;Knowledge graphs;Chatbots;Real-time systems;Chatbot;Chat Assistant;ChatGPT API;Knowledge Graph;Cypher Query Language;Graph Database},
  doi={10.1109/SoutheastCon52093.2024.10500264},
  ISSN={1558-058X},
  month={March},}@INPROCEEDINGS{10825070,
  author={Li, Tangrui and Zhou, Jun and Wang, Hongzheng},
  booktitle={2024 IEEE International Conference on Big Data (BigData)}, 
  title={Aligning Knowledge Graphs Provided by Humans and Generated by Neural Networks}, 
  year={2024},
  volume={},
  number={},
  pages={3441-3447},
  abstract={In this paper, an approach that extracts knowledge graphs (KGs) from neural networks (NNs) and aligns the generated KGs with human-provided ones is proposed for network optimization or transparency enhancement, which is achieved by leveraging Vector Symbolic Architectures (VSAs). The approach identifies entities and relations of NN’s knowledge along with the training process, which makes it a plug-and-play solution. Experiments on synthetic data showed that the matching method works on middle and small-size KGs, and tests on MNIST demonstrated that the aligned NN-generated KG could be very close to the human-provided ones. Further tests on Text2KGBench showed that the method could produce KGs from embedding generated by backbone large language models (LLM) that aligned well with human-provided labels as well.},
  keywords={Training;Codes;Large language models;Artificial neural networks;Knowledge graphs;Ontologies;Vectors;Optimization;Synthetic data;Software development management;Vector Symbolic Architecture;Knowledge Graph;Knowledge Graph Alignment},
  doi={10.1109/BigData62323.2024.10825070},
  ISSN={2573-2978},
  month={Dec},}@INPROCEEDINGS{10392374,
  author={Gou, Yunhe and Jie, Cao},
  booktitle={2023 IEEE 3rd International Conference on Data Science and Computer Application (ICDSCA)}, 
  title={A lightweight biomedical named entity recognition with pre-trained model}, 
  year={2023},
  volume={},
  number={},
  pages={117-121},
  abstract={Biomedical Named Entity Recognition (BioNER) is a specialized subfield of Named Entity Recognition (NER) that focuses on identifying and classifying named entities in biomedical and clinical texts. The goal of BioNER is to extract essential information, such as genes, proteins, diseases, drugs, et al., from scientific literature, electronic health records (EHRs), biomedical databases, and other biomedical text sources. The recognition and classification of these entities are crucial for various biomedical and healthcare-related tasks, including information retrieval, data integration, knowledge extraction, and drug discovery. Traditional BioNER methods typically involve rule-based approaches or machine learning algorithms, et al. These methods have been widely used before the advent of deep learning and transformer-based models. Bidirectional Encoder Representations from Transformers (BERT) is a groundbreaking transformer-based language model. It has revolutionized various natural language processing (NLP) tasks by capturing contextual information and obtain optimal results in multiple benchmarks. A lightweight BioNER optimized model from traditional BERT (LWNER) is proposed in this study, which can capture contextual information and its knowledge transfer from pre-training on large-scale text corpora without relying heavily on feature engineering and handcrafted rules. Fine-tuning BERT on biomedical-specific data helps adapt the model to the nuances and terminology of the biomedical domain. We conduct the method LWNER on BioCreative dataset, BC2GM, BC4CHEMD, BC5CDR, especially the chemical entity in BC5CDR achieve F1- score 91.3%. We construct an online web tool based on LWNER to identify the arbitrary text from scientific literatures for building knowledge graph.},
  keywords={Adaptation models;Text recognition;Terminology;Biological system modeling;Bidirectional control;Transformers;Encoding;Transformer;BioNER;BERT;BiLSTM},
  doi={10.1109/ICDSCA59871.2023.10392374},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9339166,
  author={Feng, Haojun and Duan, Li and Liu, Shukan and Liu, Sijie},
  booktitle={2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)}, 
  title={Entity Hierarchical Clustering Method Based on Multi-channel and T-SNE Dimension Reduction}, 
  year={2020},
  volume={9},
  number={},
  pages={2155-2159},
  abstract={Named entity clustering is a basic work in the field of natural language processing, which is helpful to excavate the implicit relationship between entities. Most of the existing clustering algorithms are unable to combine various features of entities and have some problems such as poor hierarchical clustering analysis. Based on this, this paper proposes a multi-channel dimensionless entity clustering method and carries out experimental verification. A multi-channel framework is constructed, and channels based on knowledge graph, language model and statistics are respectively set up to express the features of entities in objective knowledge, co-existing relationship and different texts. Network embedding method, BERT model and automatic coding machine are respectively used to convert the entities into vector forms. The t-SNE algorithm is used to reduce the dimension and map it to the two-dimensional space, so that it can be expressed visually in the low-dimensional space. An improved hierarchical clustering method is proposed to cluster entities in two-dimensional space and construct hierarchical clustering trees. Experiments show that the F1 value of this algorithm can reach 78.72% at most under the test set. At the same time, through analysis, the algorithm has a strong ability of expansion and generalization.},
  keywords={Dimensionality reduction;Clustering methods;Scalability;Bit error rate;Clustering algorithms;Feature extraction;Natural language processing;multi-channel;network embedding;BERT;t-SNE;improved hierarchical clustering},
  doi={10.1109/ITAIC49862.2020.9339166},
  ISSN={2693-2865},
  month={Dec},}@INPROCEEDINGS{10577330,
  author={Zhao, Ava and Su, Zhanqi and Fei, Bill and Zhuo, Na and Wang, Hao and Yu, Tianzhou and Li, Zuotian and Qian, Cheryl and Chen, Yingjie Victor},
  booktitle={2023 VAST Challenge}, 
  title={Facilitating Visual Analytics with ChatGPT: 2023 VAST Challenge Award - Application of LLMs to Support VA Process}, 
  year={2023},
  volume={},
  number={},
  pages={5-6},
  abstract={To solve the VAST Challenge 2023 MC3, our team employed a large language model, ChatGPT, to explore the potential of AI -guided visual analytics for the detection of anomalies within a knowledge graph in the context of illegal fishing and marine trade. We employed a systematic and iterative approach, guided by GPT augmentation, that enabled problem understanding, data processing, solution exploration, code writing, and results analysis. By generating and analyzing various graphs, we identified anomalies related to revenue and product services. Further analyses unveiled potential illegal fishing activities and identified instances warranting additional investigation. Overall, our work highlights both the strengths and limitations of ChatGPT in aiding the visual analytics process and emphasizes the importance of human judgment in refining AI-generated outputs.},
  keywords={Systematics;Visual analytics;Scalability;Refining;Collaboration;Manuals;Writing;Visual Analytics;ChatGPT;Assisted Learning;AI Language Model;Knowledge GraphMarine Trade},
  doi={10.1109/VASTChallenge60523.2023.00008},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{10587417,
  author={Fu, Hanyu and Liu, Chunfang and Li, Xiaoli},
  booktitle={2024 36th Chinese Control and Decision Conference (CCDC)}, 
  title={Dynamic Task Planning: An Integrated Approach with Scene Relation Perception and Knowledge Graphs}, 
  year={2024},
  volume={},
  number={},
  pages={1539-1543},
  abstract={Confronted with the limitations of conventional Planning Domain Definition Language (PDDL) in dynamic and unpredictable kitchen environments, this paper introduces a novel task planning methodology that leverages the synergy between advanced scene graphs. This interdisciplinary approach begins with the construction of a detailed task directive-target state dataset, which serves as the foundation for refining the capabilities of VisualBERT, a vision-language model. Through fine-tuning, VisualBERT becomes adept at accurately interpreting complex scene dynamics and anticipating the target states to achieve the task query. This is followed by the creation of an extensive knowledge graph containing important parameters of actions and objects. This knowledge base is instrumental in generating PDDL domain and problem files, taking into account both initial and target states. It plays a crucial role in the flexible subtask sequence generation for dynamic environments and tasks. Our proposed method significantly enhances the adaptability to real-world variability, thereby enabling dynamic task planning within domestic kitchen environments efficiently.},
  keywords={Trajectory planning;Instruments;Refining;Knowledge based systems;Knowledge graphs;Predictive models;Manipulators;task planning;PDDL;vision-language model;target state prediction;sub task sequences},
  doi={10.1109/CCDC62350.2024.10587417},
  ISSN={1948-9447},
  month={May},}@INPROCEEDINGS{9721762,
  author={Sajid, Hira and Kanwal, Javeria and Bhatti, Saeed Ur Rehman and Qureshi, Saad Ali and Basharat, Amna and Hussain, Shujaat and Khan, Kifayat Ullah},
  booktitle={2022 16th International Conference on Ubiquitous Information Management and Communication (IMCOM)}, 
  title={Resume Parsing Framework for E-recruitment}, 
  year={2022},
  volume={},
  number={},
  pages={1-8},
  abstract={Modern approaches to improve networking and communication have given ways to the advancement of recruitment process through the development of e-recruitment recommender systems. The increasing expansion of internet- based recruiting has resulted in a large number of resumes being stored in recruitment systems. Most resumes are prepared in a variety of styles to attract the attention of recruiters, including different font sizes, font colors, and table formats. However, data mining operations such as resume information extraction, automatic profile matching, and applicant ranking are immensely affected by the variety of formats. Rule-based methods, supervised methods and semantics-based methods have been introduced to extract facts from resume accurately, however, these methods heavily depend on large amounts of annotated data that is usually difficult to collect Furthermore, these techniques are time-intensive and bear knowledge incompleteness that strongly affect the accuracy of resume parser. In this paper, we present a resume parsing framework that handles the limitations faced in the previous techniques. At first, the raw text is extracted from resumes and blocks are separated using text block classification. Furthermore, the entities are extracted using named entity recognition and enriched using ontology. The proposed resume parser accurately extracts information from resumes that directly contributes towards the selection of best candidate.},
  keywords={Training;Resumes;Layout;Ontologies;Information retrieval;Feature extraction;Information management;Resume parsing;Data Enrichment;Text Extraction;Ontology;Boolean Naive Bayes},
  doi={10.1109/IMCOM53663.2022.9721762},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{10187511,
  author={Weigand, Hans and Johannesson, Paul},
  booktitle={2023 IEEE 25th Conference on Business Informatics (CBI)}, 
  title={How to Identify your Design Science Research Artifact}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  abstract={Design Science Research (DSR) is about the development and investigation of artifacts in context. However, in many articles that subscribe to a DSR approach, the artifact is not clearly classified and identified. Very little attention has been given in the DSR literature on this topic, so guidelines are lacking. Based on artifact ontology, this paper proposes guidelines for design researchers in the IS domain on how to specify both the artifact and the research objective. For validation, the guidelines have been applied to a range of DSR papers. Our results show that the artifact definition guidelines can add to the precision of the research object specification.},
  keywords={Philosophical considerations;Design methodology;Bibliographies;Ontologies;Data science;Data models;Informatics;Design Science;artifact ontology;research methodology},
  doi={10.1109/CBI58679.2023.10187511},
  ISSN={2378-1971},
  month={June},}@INPROCEEDINGS{8104864,
  author={Yang, Chen-Wei and Vyatkin, Valeriy},
  booktitle={2017 IEEE 15th International Conference on Industrial Informatics (INDIN)}, 
  title={On requirements-driven design of distributed smart grid automation control}, 
  year={2017},
  volume={},
  number={},
  pages={738-745},
  abstract={This paper proposes a novel method of modelling requirements for smart grid automation systems that fits and complements the leading standards used for software development in smart grid: IEC 61850 and IEC 61499. Ontological representation is used to formally model the systems requirements and the integration of the requirement modelling to be used in conjunction with IEC 61850 specifications and IEC 61499 implementation. The proposed model is illustrated on a case study presented by the CIGRE working group where the system requirement in natural language was firstly modelled in ontology, then implemented in IEC 61499. Lastly, the resulting IEC 61499 control system was tested and validated against the system requirement by signal testing of the end control system.},
  keywords={IEC Standards;Ontologies;Unified modeling language;Smart grids;Natural languages;Automation;Timing;IEC 61850;IEC 61499;Smart Grid;Ontology;Requirements Modelling},
  doi={10.1109/INDIN.2017.8104864},
  ISSN={2378-363X},
  month={July},}@ARTICLE{9205800,
  author={Larhrib, Mohamed and Escribano, Miguel and Cerrada, Carlos and Escribano, Juan Jose},
  journal={IEEE Access}, 
  title={Converting OCL and CGMES Rules to SHACL in Smart Grids}, 
  year={2020},
  volume={8},
  number={},
  pages={177255-177266},
  abstract={Models are first-class elements in Model-Driven Engineering (MDE). In this paradigm, the most widespread approaches adopted by the development community are Object-Oriented and ontological, formalized using Unified Modeling Language (UML) and Resource Description Framework (RDF), respectively. However, Object Management Group (OMG) does not provide a specific standard language for validating UML models against Object Constraints Language (OCL) constraints; meanwhile, World Wide Web Consortium (W3C) has defined Shapes Constraint Language (SHACL) as a standard validation language. Although the transformation between UML and RDF can be performed at the structural level, no effort has been made to transform OCL to SHACL. This paper addresses the transformation of OCL and text-based constraints to SHACL shapes in the context of Common Grid Model Exchange Standard (CGMES), a UML-based standard for electric utilities in Europe. This paper presents several contributions to the software engineering community. First, solving the validation problem in a standardized way. Second, facilitating European Network of Transmission System Operators for Electricity (ENTSO-E) the construction of an ontology associated with the CGMES standard. Third, allowing developers to integrate the two complementary approaches. Finally, Promoting the adoption and integration of the ontological approach in the software community.},
  keywords={Unified modeling language;Object oriented modeling;Resource description framework;IEC Standards;Ontologies;Common Information Model (electricity);CIM for ENTSO-E (CGMES);OCL rules;ontology;RDF/RDFS;SHACL standard},
  doi={10.1109/ACCESS.2020.3026941},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10711329,
  author={Höfgen, Josua and Vogel-Heuser, Birgit and Vicaria, Alejandra and Pouzolz, François and Kurzhals, Christian},
  booktitle={2024 IEEE 20th International Conference on Automation Science and Engineering (CASE)}, 
  title={Enhancing Model-Based System Architecting Through Formalized Decision Management}, 
  year={2024},
  volume={},
  number={},
  pages={1053-1060},
  abstract={System architecture decisions are typically informally captured in design documents. This practice leads to a loss of knowledge that impedes later activities like design changes, impact analysis, and reuse. Model-Based Systems Engineering (MBSE) frameworks support the development of increasingly complex systems but must address the problem of capitalization on architectural knowledge. To this end the "Decision Ontology for System Architectures (DOSA)" is developed to provide a formalized data model to capture system architecture decisions. DOSA is developed through a synthesis of decisions observed while developing an architecture model for a preliminary study of a novel satellite navigation system at Airbus Defence and Space. The approach is integrated into an MBSE framework enabling engineers to capture decisions that influence the architecture’s characteristics while developing the system model and imminently trace decision to artifacts of the system architecture. Subsequent visual inspection and formal querying of the decision graph facilitates the analysis of made decisions, and their interrelations.},
  keywords={Knowledge engineering;Visualization;Computer aided software engineering;Atmospheric modeling;Systems architecture;Ontologies;Inspection;Satellite navigation systems;Data models;Complex systems;MBSE;System Architecture;Decision Management;Ontology},
  doi={10.1109/CASE59546.2024.10711329},
  ISSN={2161-8089},
  month={Aug},}@INPROCEEDINGS{10205809,
  author={Nadhila, Fadiah and Alamsyah, Andry},
  booktitle={2023 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)}, 
  title={Mapping Personality Traits to Customer Complaints: Framework for Personalized Customer Service}, 
  year={2023},
  volume={},
  number={},
  pages={96-101},
  abstract={The study establishes utilizing the Big Five Personality framework and a Personality Measurement Platform (PMP) for personality analysis. Moreover, Customer Complaint Ontology (CCOntology) framework implements a Naive Bayes machine learning methodology to evaluate and scrutinize customer complaints. The algorithm works by calculating the probability of each complaint category. This association is measured in percentages, enabling the identification of specific personality traits related to customer complaints through identifying complaint characteristics and areas of concern. The study has found that individuals with neurotic personality traits who encounter customer complaints are often associated with problem categories such as Non-Contract, Privacy, and Contract and are more likely to express strong emotional dissatisfaction with a product or service. Linking customer complaints with their corresponding personalities can be an incredibly effective and innovative strategy for personalized customer service businesses in anticipating their needs and providing tailored recommendations that can improve the likelihood of customers making purchases. This approach involves educating employees on the importance of actively listening to customers, asking relevant questions, and anticipating their needs, ensuring that businesses can enhance customer satisfaction while building a loyal customer base.},
  keywords={Technological innovation;Privacy;Social networking (online);Customer services;Oral communication;Ontologies;Big Data;Big Five Personality;Customer Complaint Ontology (CCOntology);Naive Bayes;Personality Measurement Platform;Personalized Customer Service},
  doi={10.1109/IAICT59002.2023.10205809},
  ISSN={2834-8249},
  month={July},}@INPROCEEDINGS{9849830,
  author={Yang, Danyang and Wan, Fangjie and Zhang, Yonggan},
  booktitle={2022 4th International Conference on Advances in Computer Technology, Information Science and Communications (CTISC)}, 
  title={Named Entity Recognition in XLNet Cyberspace Security Domain Based on Dictionary Embedding}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  abstract={With the increase of network security incidents, network security analysts need to analyze massive log information. The introduction of knowledge graph into the field of network security can facilitate analysis by security analysts. NER (Named Entity Recognition) is the upstream task of knowledge graph construction, and the quality of the NER model determines the quality of the knowledge graph to a certain extent. However, the general domain named entity recognition model cannot extract the entities in the network security domain very well. For this phenomenon, this paper proposes an XLNet-Feature-Att model, which uses XLNet in the embedding layer to embed words into the vector space, the encoding layer uses the improved BILSTM FB structure and the Attention layer, and the decoding layer uses the CRF model to achieve sequence labeling and binding. Finally, the experimental comparison is carried out on the data set in the field of network security, and the F1-score reaches the highest 92.28%.},
  keywords={Knowledge engineering;Dictionaries;Computational modeling;Semantics;Cyberspace;Network security;Feature extraction;network security;named entity recognition;XLNet;BILSTM;CRF},
  doi={10.1109/CTISC54888.2022.9849830},
  ISSN={},
  month={April},}@ARTICLE{10843672,
  author={Feng, Wenyan and Li, Yuhang and Ma, Chunhao and Yu, Lisai},
  journal={IEEE Access}, 
  title={From ChatGPT to Sora: Analyzing Public Opinions and Attitudes on Generative Artificial Intelligence in Social Media}, 
  year={2025},
  volume={13},
  number={},
  pages={14485-14498},
  abstract={This study examines public opinions, emotional tendencies, and psychological linguistic characteristics associated with the launch of OpenAI’s ChatGPT and the advanced video generation model, Sora, by analyzing discussions on the Chinese social media platform Weibo. A total of 24,727 valid user-generated texts (1,762,296 words) were collected and analyzed using Python and its associated APIs. Word co-occurrence network analysis, topic modeling based on Latent Dirichlet Allocation (LDA), and emotional characteristics based on the DLUT Emotion Ontology and psycholinguistic analyses based on the Linguistic Inquiry and Word Count (LIWC) dictionary were employed to explore public views on these generative AI technologies. The findings reveal a shift in public focus over time, from initial excitement about technological advancements to growing interest in commercialization, labor, education, ethics, and global competition. The public’s emotional responses to AI were a mix of excitement and apprehension. The study identifies seven distinct emotional types, providing a nuanced understanding of public psychological reactions, which contrasts with previous binary classifications. This research contributes valuable insights for policymakers, businesses, and researchers, highlighting the public’s evolving acceptance of generative AI technologies.},
  keywords={Chatbots;Artificial intelligence;Generative AI;Social networking (online);Blogs;Psychology;Sentiment analysis;Analytical models;Text mining;Ethics;Generative artificial intelligence;ChatGPT;sora;topic modeling;sentiment analysis;psycholinguistics},
  doi={10.1109/ACCESS.2025.3530683},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10066652,
  author={Hattori, Shotaro and Fujii, Akihiro},
  booktitle={2023 IEEE 17th International Conference on Semantic Computing (ICSC)}, 
  title={Criminal Deduction Using Similarity Analysis Between Mystery Stories}, 
  year={2023},
  volume={},
  number={},
  pages={290-291},
  abstract={The " Knowledge Graph Reasoning Challenge" an annual event in which participants try to infer a mystery novel using Knowledge Graph and other tools, has been attracting a lot of attention. The purpose of this research is to propose and implement a new method using BERT for this challenge. To prove this, we focused on the degree of similarity between the two novels and estimated the culprit. As a result of the analysis focusing on the words and actions of the culprit, we succeeded in extracting the culprit's name. However, the successful extraction was based on a specific condition, and further discussion is needed to pursue more precise accuracy in the future.},
  keywords={Bit error rate;Semantics;Focusing;Knowledge graphs;Cognition;BERT;BERTscore;Natural Language Processing;Similarity},
  doi={10.1109/ICSC56153.2023.00057},
  ISSN={2325-6516},
  month={Feb},}@ARTICLE{7496983,
  author={Alvarez, María Luz and Sarachaga, Isabel and Burgos, Arantzazu and Estévez, Elisabet and Marcos, Marga},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={A Methodological Approach to Model-Driven Design and Development of Automation Systems}, 
  year={2018},
  volume={15},
  number={1},
  pages={67-79},
  abstract={The growing complexity of industrial automation demands the adoption of software engineering principles for improving the development process of control systems. This paper presents a methodological approach to the design and development of complex automation systems relying on model-driven engineering (MDE). A benefit of this approach is the integration of methods and techniques widespread within the automation discipline with modern MDE techniques guiding the designer through the development phases. A second advantage is to add flexibility enough to adapt the steps to the needs of the system under design. Finally, the architecture presented is prepared to be adapted to methodology extensions to cover other aspects of automation systems. The framework is based on domain models that are defined through the development phases using the terminology of the automation field. Using model transformations both documentation about system analysis and design and the skeleton of software units are automatically generated. A proof-of-concept tool has been developed that has been tested on the design of medium-complexity projects to assess the impact of its use with respect to project documentation and maintenance.Note to Practitioners—Control software development can be considered one of the challenges in automation field for achieving leadership in the future economic market. This work presents a model-driven engineering-based approach making use of both automation and software engineering methods and techniques for developing automation control systems. The framework implements the methodology for industrial automation systems ( ${\rm MeiA}_{\bullet }$ ) for guiding developers through the development phases and generates the analysis and design documentation using domain terminology, the design documentation that involves the minimal units of design, and the program organization units in one-to-one correspondence with the minimal units of design. From a practical point of view, it should be highly emphasized that developers of automation projects benefit from more structured designs, reduced number of errors, and improved project documentation.},
  keywords={Automation;Unified modeling language;Software engineering;Documentation;Software;Production;Control systems;Engineering frameworks;IEC 61131-3;industrial automation;methodology for industrial automation systems (  ${\mathrm{MeiA}}_{\bullet}$   );model-driven engineering (MDE);PLCopen XML},
  doi={10.1109/TASE.2016.2574644},
  ISSN={1558-3783},
  month={Jan},}@INPROCEEDINGS{9081794,
  author={Annighoefer, Bjoern and Halle, Martin and Schweiger, Andreas and Reich, Marina and Watkins, Christopher and VanderLeest, Steven H. and Harwarth, Stefan and Deiber, Patrick},
  booktitle={2019 IEEE/AIAA 38th Digital Avionics Systems Conference (DASC)}, 
  title={Challenges and Ways Forward for Avionics Platforms and their Development in 2019}, 
  year={2019},
  volume={},
  number={},
  pages={1-10},
  abstract={Today's air vehicles depend on digital technology. It accounts for more than 30% of their development costs. The number of functions, the lines of code, the degree of autonomy, and the number of vehicles rise. This is why there is a need for cutting-edge technology and development methods. There is a gap between academia's methods and industrial applications due to multi-disciplinary challenges. We summarize the state-of-the-art in avionics, namely avionics platforms, requirements engineering, model-based development, automated verification, emerging technologies, and emerging demands. Experts review the most demanding challenges, research gaps, and promising solutions. They provide recommendations for the enhancement of the cooperation between industry and academia and suggest necessary research topics. This article is an introduction for those who are new to avionics. It is an up-to-date summary, for insiders looking for most promising solutions to their current problems; and it is a guide for those advancing avionics research.},
  keywords={avionics platforms;requirements engineering;model-based development;automated verification;multi-core},
  doi={10.1109/DASC43569.2019.9081794},
  ISSN={2155-7209},
  month={Sep.},}@INPROCEEDINGS{9140236,
  author={Torres, Victoria and Serral, Estefanía and Valderas, Pedro and Pelechano, Vicente and Grefen, Paul},
  booktitle={2020 IEEE 22nd Conference on Business Informatics (CBI)}, 
  title={Modeling of IoT devices in Business Processes: A Systematic Mapping Study}, 
  year={2020},
  volume={1},
  number={},
  pages={221-230},
  abstract={The Internet of Things (IoT) enables to connect the physical world to digital business processes (BP). By using the IoT, a BP can, e.g.: 1) take into account real-world data to take more informed business decisions, and 2) automate and/or improve BP tasks. To achieve these benefits, the integration of IoT and BPs needs to be successful. The first step to this end is to support the modeling of IoT-enhanced BPs. Although numerous researchers have studied this subject, it is unclear what is the current state of the art in terms of current modeling solutions and gaps. In this work, we carry out a Systematic Mapping Study (SMS) to find out how current solutions are modelling IoT into business processes. After studying 600 papers, we identified and analyzed in depth a total of 36 different solutions. In addition, we report on some important issues that should be addressed in the near future, such as, for instance the lack of standardization.},
  keywords={Conferences;Unified modeling language;Internet of Things;Systematics;Task analysis;Computers;Business process modeling;Internet of Things;IoT devices;IoT-enhanced BP;Systematic mapping study},
  doi={10.1109/CBI49978.2020.00031},
  ISSN={2378-1971},
  month={June},}@ARTICLE{9050669,
  author={Bandyszak, Torsten and Daun, Marian and Tenbergen, Bastian and Kuhs, Patrick and Wolf, Stefanie and Weyer, Thorsten},
  journal={IEEE Transactions on Automation Science and Engineering}, 
  title={Orthogonal Uncertainty Modeling in the Engineering of Cyber-Physical Systems}, 
  year={2020},
  volume={17},
  number={3},
  pages={1250-1265},
  abstract={Software-intensive cyber-physical systems (CPS) perform essential tasks such as controlling automated production processes in industrial production plants. The required levels of autonomy, openness, and self-adaptation, as well as the dynamic nature of the context of such CPS, result in challenging tasks for their engineering. During operation, unexpected situations in which the system has insufficient knowledge about the current state of the system itself as well as its context may occur. Engineering CPS, e.g., for industrial production sites, must account for such uncertainties the system will have to cope with during its lifetime in a structured and systematic way. Since the development of CPS requires consideration of different system perspectives, current uncertainty modeling approaches cannot be applied right away, as they do not explicitly consider uncertainty aspects that affect different artifacts. To aid the engineering of CPS, this article presents a model-based approach to document uncertainty. We propose “Orthogonal Uncertainty Models,” which closely integrate with other engineering artifacts from different perspectives, as a means for capturing a dedicated uncertainty viewpoint. Our approach has been evaluated in the industry automation domain. The application shows that the idea of regarding uncertainty within a dedicated perspective is highly beneficial. Particularly, our approach helps to uncover and document uncertainties related to behavioral, functional, and structural properties of a system, as well as uncertainties related to business models that would otherwise possibly remain covert. Note to Practitioners-Identifying and documenting uncertainties, which may occur during operation of a system, is a common problem in engineering processes. Such uncertainties may lead to severe damage, and thus need to be mitigated appropriately. It is crucial to account for these uncertainties during engineering, especially in the early phases. Depending on the specific project characteristics, a multitude of different diagram types are used to model a system. Uncertainties thus reflect in many artifacts, which leads to: 1) redundancies in the specified uncertainty attached to diagram elements and 2) uncertainty information (e.g., about the cause or effect of uncertainty) that is spread across different diagrams. The latter makes it difficult to structure uncertainty information and trace it throughout the engineering process so that uncertainty can be systematically considered. Our approach provides a graphical modeling language that employs a dedicated perspective on uncertainty in separate diagrams that can be linked to any engineering artifact.},
  keywords={Uncertainty;Robot sensing systems;Context modeling;Collaboration;Production;Runtime;Cyber-physical systems;industry automation case study;model-based engineering;orthogonal modeling;uncertainty;uncertainty modeling},
  doi={10.1109/TASE.2020.2980726},
  ISSN={1558-3783},
  month={July},}@INPROCEEDINGS{8501304,
  author={Shakeri Hossein Abad, Zahra and Gervasi, Vincenzo and Zowghi, Didar and Barker, Ken},
  booktitle={2018 5th International Workshop on Artificial Intelligence for Requirements Engineering (AIRE)}, 
  title={ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant Information}, 
  year={2018},
  volume={},
  number={},
  pages={8-14},
  abstract={Requirements elicitation requires extensive knowledge and deep understanding of the problem domain where the final system will be situated. However, in many software development projects, analysts are required to elicit the requirements from an unfamiliar domain, which often causes communication barriers between analysts and stakeholders. In this paper, we propose a requirements ELICitation Aid tool (ELICA) to help analysts better understand the target application domain by dynamic extraction and labeling of requirements-relevant knowledge. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modeling of natural language processing tasks. In addition to the information conveyed through text, ELICA captures and processes non-linguistic information about the intention of speakers such as their confidence level, analytical tone, and emotions. The extracted information is made available to the analysts as a set of labeled snippets with highlighted relevant terms which can also be exported as an artifact of the Requirements Engineering (RE) process. The application and usefulness of ELICA are demonstrated through a case study. This study shows how pre-existing relevant information about the application domain and the information captured during an elicitation meeting, such as the conversation and stakeholders' intentions, can be captured and used to support analysts achieving their tasks.},
  keywords={Tools;Stakeholders;Task analysis;Interviews;Ontologies;Feature extraction;Data mining;Requirements elicitation, Natural language processing, Tool support, Dynamic information extraction},
  doi={10.1109/AIRE.2018.00007},
  ISSN={},
  month={Aug},}@INPROCEEDINGS{9338531,
  author={Fattouch, Najla and Ben Lahmar, Imen and Boukadi, Khouloud},
  booktitle={2020 IEEE 29th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)}, 
  title={IoT-aware Business Process: comprehensive survey, discussion and challenges}, 
  year={2020},
  volume={},
  number={},
  pages={100-105},
  abstract={In the last years, the Internet of Things (IoT) know a huge widespread thanks to the increase of the connected objects number. The IoT technology has several benefits that make it among the proliferation technology. The major advantage of this technology is the communication between devices known as Machine-to-Machine (M2M) communication allowing them to be connected without human intervention. Thanks to this advantage, the technology become able to facilitate the people's lives that it become smoother through a seamless cooperation between virtual objects and physical ones. As well as, the IoT sweep various fields (e.g., industry, health) thanks to its capacity to automate tasks.In this setting, a tremendous number of business managers are interesting to integrate the IoT devices into their Business Processes (BPs), known in literature as IoT-aware BP. This integration gives the opportunity to the business managers to avail from the IoT technology in their process through an enhancement of the business performance and an achievement of the business competitiveness. Thus, several researchers competed to identify approaches and methods to integrate the IoT technology within the BP paradigm. In this paper, we present a review of the different proposed approaches that deal with the integration of the IoT technology within the BP. Furthermore, we give in this paper, a rich comparative analysis based on a set of criteria. Finally, we identify some initiatives and challenges in the IoT-aware BP paradigm.},
  keywords={Performance evaluation;Industries;Machine-to-machine communications;Internet of Things;Time factors;Task analysis;Business;IoT-aware Business Process;Internet of Things;Business Process;Industry 4.0},
  doi={10.1109/WETICE49692.2020.00027},
  ISSN={2641-8169},
  month={Sep.},}@ARTICLE{8540835,
  author={Gómez, Francisco J. and Aguilera Chaves, Miguel and Vanfretti, Luigi and Olsen, Svein Harald},
  journal={IEEE Access}, 
  title={Multi-Domain Semantic Information and Physical Behavior Modeling of Power Systems and Gas Turbines Expanding the Common Information Model}, 
  year={2018},
  volume={6},
  number={},
  pages={72663-72674},
  abstract={Due to the rapid increase of intermittent energy resources (IERs), there is a need to have dispatchable production available to ensure secure operation and increase opportunity for energy system flexibility. Gas turbine-based power plants offer flexible operation that is being improved with new technology advancements. Those plants provide, in general, quick start together with significant ramping capability, which can be exploited to balance IERs. Consequently, to understand the potential source of flexibility, better models for gas turbines are required for power system studies and analysis. In this paper, both the required semantic information and physical behavior models of such multi-domain systems are considered. First, UML class diagrams and RDF schemas based on the common information model standards are used to describe the semantic information of the electrical power grid. An extension that exploits the ISO 15926 standard is proposed herein to derive the multi-domain semantics required by integrated electrical power grid with detailed gas turbine dynamic models. Second, the Modelica language is employed to create the equation-based models, which represent the behavior of a multi-domain physical system. A comparative simulation analysis between the power system domain model and the multi-domain model has been performed. Some differences between the turbine dynamics representation of the commonly used GGOV1 standard model and a more detailed gas turbine model are shown.},
  keywords={Unified modeling language;Mathematical model;Power system dynamics;Semantics;Turbines;Standards;Common Information Model (electricity);CIM;cyber-physical systems;dynamic simulation;equation-based modeling;IEC 61970;information modeling;ISO 15926;Modelica;power systems simulation;power systems modeling},
  doi={10.1109/ACCESS.2018.2882311},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8015307,
  author={Walch, Michael},
  booktitle={2017 IEEE International Conference on Agents (ICA)}, 
  title={Knowledge-driven enrichment of cyber-physical systems for industrial applications using the KbR modelling approach}, 
  year={2017},
  volume={},
  number={},
  pages={84-89},
  abstract={This paper addresses design and engineering of models that connect cyber-physical systems and conceptualizations of industrial applications (IA). For this purpose, the hybrid modelling method SeRoIn is constructed using metamodel-based implementation (MMbI). Details are presented on specific building blocks of the modelling method, following the Knowledge-based Robotics (KbR) approach. Focusing on Knowledge-driven Enrichment (KdE) to integrate models, two experiments are conducted in the robotics section (OMiRob) of the Open Models Laboratory (OMiLAB), which involves concepts for hybrid automata, processes and ontologies. Thereby, intelligent behaviour is formalized, coining the notion of “smart models”. The experiments prove the capability of KdE and suggest to establish focused research in the area of KbR.},
  keywords={Tools;Computational modeling;Ontologies;Business;Service robots;Automata},
  doi={10.1109/AGENTS.2017.8015307},
  ISSN={},
  month={July},}@ARTICLE{9141288,
  author={Huang, Zhao and Zhao, Wei},
  journal={IEEE Access}, 
  title={Combination of ELMo Representation and CNN Approaches to Enhance Service Discovery}, 
  year={2020},
  volume={8},
  number={},
  pages={130782-130796},
  abstract={With the rapid growth of Web services, the demand for discovering the optimal services to satisfy the users' requirements is no longer an easy task. The critical issue in the process of service discovery is to conduct a similarity calculation. To solve such an issue, this study proposes an effective approach that combines the Embeddings from Language Models (ELMo) representation and Convolutional Neural Network (CNN) to obtain a more accurate similarity score for retrieving target Web services. More specifically, first, the study adopts the ELMo model to generate effective word representations for capturing the sufficient information from services and queries. Then, the word representations are used to compose a similarity matrix, which will be taken as the input for the CNN to learn the matching relationships. Finally, the combination of the ELMo representation and CNN is used to address the representation and interaction processes within the matching task to improve the service discovery performance. The results demonstrate the effectiveness of our proposed approach for retrieving better targeted Web services.},
  keywords={Semantics;Syntactics;Task analysis;Ontologies;Linguistics;Service-oriented architecture;Service discovery;ELMo;CNN;service similarity;web service},
  doi={10.1109/ACCESS.2020.3009393},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{8285628,
  author={Bowles, J. and Caminati, M. B. and Cha, S.},
  booktitle={2017 International Symposium on Theoretical Aspects of Software Engineering (TASE)}, 
  title={An integrated framework for verifying multiple care pathways}, 
  year={2017},
  volume={},
  number={},
  pages={1-8},
  abstract={Common chronic conditions are routinely treated following standardised procedures known as clinical pathways. For patients suffering from two or more chronic conditions, referred to as multimorbidities, several pathways have to be applied simultaneously. However, since pathways rarely consider the presence of comorbidities, applying several pathways may lead to potentially harmful (medication) conflicts. This paper proposes an automated framework to detect, highlight and resolve conflicts in the treatments used for patients with multimorbidites. We use BPMN as a modelling language for capturing care guidelines. A BPMN model is transformed into an intermediate formal model capturing the possible unfoldings of the pathway. Putting together the constraint solver Z3 and the theorem prover Isabelle, we combine treatment plans and check the correctness of the approach. We illustrate the approach with an example from the medical domain and discuss future work.},
  keywords={Logic gates;Task analysis;Ontologies;Unified modeling language;Semantics;Guidelines;Computational modeling},
  doi={10.1109/TASE.2017.8285628},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{9688274,
  author={Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hedayatnia, Behnam and Hakkani-Tür, Dilek},
  booktitle={2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)}, 
  title={“How Robust R U?”: Evaluating Task-Oriented Dialogue Systems on Spoken Conversations}, 
  year={2021},
  volume={},
  number={},
  pages={1147-1154},
  abstract={Most prior work in dialogue modeling has been on written conversations mostly because of existing data sets. However, written dialogues are not sufficient to fully capture the nature of spoken conversations as well as the potential speech recognition errors in practical spoken dialogue systems. This work presents a new benchmark on spoken task-oriented conversations, which is intended to study multi-domain dialogue state tracking and knowledge-grounded dialogue modeling. We report that the existing state-of-the-art models trained on written conversations are not performing well on our spoken data, as expected. Furthermore, we observe improvements in task performances when leveraging $n$-best speech recognition hypotheses such as by combining predictions based on individual hypotheses. Our data set enables speech-based benchmarking of task-oriented dialogue systems.},
  keywords={Conferences;Benchmark testing;Data models;Robustness;Task analysis;Automatic speech recognition;spoken dialogue systems;dialogue state tracking;knowledge-grounded dialogue generation},
  doi={10.1109/ASRU51503.2021.9688274},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8588250,
  author={Papazoglou, Michael P.},
  booktitle={2018 Sixth International Conference on Enterprise Systems (ES)}, 
  title={Metaprogramming Environment for Industry 4.0}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Industry 4.0 is blurring the lines between the physical, and digital spheres of global production systems. Industry 4.0 sets the foundations for a completely connected factories that are characterized by the digitization and interconnection of supply chains, production equipment and production lines, and the application of the latest advanced digital information technologies to manufacturing activities. To fully realize the promise of Industry 4.0, disparate manufacturing systems, devices, data and processes need to connect, communicate, and interoperate. This paper has dual purpose. It first introduces a model-based engineering that enables a concurrent, collaborative design process where users examine and define requirements, propose solution architectures, demonstrate and exchange ideas with stakeholders, and consider product feature tradeoffs. Subsequently, it proposes a novel programming paradigm and a flexible environment that helps developers develop design-to-production industrial automation solutions by employing structured higher-level modular software techniques.},
  keywords={Manufacturing;Industries;Solid modeling;Supply chains;Three-dimensional displays;Real-time systems;Industry 4.0;Enterprise Knowledge Engineering;Information integration and interoperability;Model-based development;Digital twins;Meta-programming},
  doi={10.1109/ES.2018.00008},
  ISSN={2572-6609},
  month={Oct},}@ARTICLE{9866769,
  author={Silega, Nemury and Noguera, Manuel and Rogozov, Yuri I. and Lapshin, Vyacheslav S. and González, Tahumara},
  journal={IEEE Access}, 
  title={Transformation From CIM to PIM: a Systematic Mapping}, 
  year={2022},
  volume={10},
  number={},
  pages={90857-90872},
  abstract={Model Driven Architecture (MDA) is the most prominent and accepted methodology based on the Model Driven Development (MDD) principles. MDA includes three abstraction levels: Computer Independent Models (CIM), Platform Independent models (PIM) and Platform specific models (PSM). MDA encourages the automatic transformation of models as a means to increase the speed of the software development process and to prevent human errors. There are plenty of solutions to transform PIMs to PSMs, however the CIM to PIM transformation does not receive a similar attention. In that sense, this paper aims to describe a systematic mapping to analyze the main characteristics of the approaches that deal with the CIM to PIM transformation as well as to discuss research directions stemming out from our analysis. The results of this mapping study could be a valuable information source for the scientific community in order to know the real advances in this topic and to avoid unnecessary effort dealing with problems that have already been addressed. For example, this study yielded the models at the CIM level that have already been transformed into models at the PIM level. Hence, with this information, the researchers could focus their attention on finding solutions to transform those models at CIM level that have not been transformed into models at PIM level. Likewise, this mapping study provides information regarding the technological support of this type of transformation. This information could be useful for those software projects interested to adopt MDA.},
  keywords={Computational modeling;Software;Systematics;Business;Software engineering;Mathematical models;Internet;Model driven architecture (MDA);computer independent models (CIM);platform independent models (PIM);systematic mapping},
  doi={10.1109/ACCESS.2022.3201556},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9447088,
  author={Giachetti, Ronald E. and Vaneman, Warren},
  booktitle={2021 IEEE International Systems Conference (SysCon)}, 
  title={Requirements for a System Model in the Context of Digital Engineering}, 
  year={2021},
  volume={},
  number={},
  pages={1-7},
  abstract={The vision of achieving digital engineering in the US Department of Defense has instigated work on defining the information content and structure of the system model. However, few seem to have asked what are the requirements for the system model? In this paper, we use a requirements process to elicit and define the requirements for the system model. The system model is a digital artifact containing descriptions of all the essential objects, their properties, and the relationships between them for the system-of-interest (SoI). The paper describes the context of the system model in relationships to the other components of model-based systems engineering (MBSE) consisting of a modeling language, schema, model-based process, presentation framework, MBSE tools, and knowledgeable workforce. The paper describes how these components interact to provide effective MBSE. Requirements are stated for each component. The paper additionally derives information requirements for the system model according to the systems engineering process’s information needs by examining the inputs and outputs of each activity in the systems engineering process. Lastly, the paper derives the quality characteristics for the system model from the literature on ontologies, modeling languages, and semiotics. The result is a set of requirements for the system model to support MBSE and the digital thread.},
  keywords={Context;Conferences;Project management;Tools;Ontologies;IEEE Standards;Data models},
  doi={10.1109/SysCon48628.2021.9447088},
  ISSN={2472-9647},
  month={April},}@INPROCEEDINGS{10275523,
  author={Dhouib, Saadia and Huang, Yining and Smaoui, Asma and Bhanja, Tapanta and Gezer, Volkan},
  booktitle={2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)}, 
  title={Papyrus4Manufacturing: A Model-Based Systems Engineering approach to AAS Digital Twins}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={As digital twins gain momentum in their usage in diverse domains, the concept of Asset Administration Shells (AAS) has become very relevant for achieving the digital twin approach, where Administration Shells are the digital representation of physical assets. Being a relatively new concept in the Industrial Internet of Things (IIoT) domain, the tools and approaches for creating and deploying AASs are likewise in infancy. This paper introduces an open-source tool, Papyrus4Manufacturing, which provides a model-based systems engineering approach to the AAS. This toolset supports the creation of AAS digital twins from modeling to automatic deployment and connection to assets using the OPC UA protocol. This paper also includes an evaluation of its usability, as it is put to test with an academic use case.},
  keywords={Protocols;Databases;Memory;Software;Digital twins;Servers;Modeling;Digital Twins;Asset Administration Shell;Model-Based System Engineering;Unified Modelling Language;UML Profiles;Generative Software Engineering;OPC UA;BaSyx;Eclipse Papyrus},
  doi={10.1109/ETFA54631.2023.10275523},
  ISSN={1946-0759},
  month={Sep.},}@INPROCEEDINGS{8612870,
  author={Bouzidi, Aljia and Haddar, Nahla and Ben Abdallah, Mounira and Haddar, Kais},
  booktitle={2018 IEEE/ACS 15th International Conference on Computer Systems and Applications (AICCSA)}, 
  title={Alignment of Business Processes and Requirements Through Model Integration}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={Business process and requirement specification are crucial phases in the software development process. Yet, business and requirement modeling are often carried out separately by different languages and design teams, leading to misaligned models. The degree of misalignment grows continuously with their independent evolution. Thus, the potential of model-driven software development cannot be fully exploited. There is a considerable agreement among researchers about the integration technique roles to bridge the gap between heterogeneous models. Therefore, this approach is based on this technique to align the business world represented by BPMN and the software requirement world represented by the UML use case. We define an integrated meta-model that incorporates all BPMN and use case elements as well as new others to map traceable elements. Further, we define a new diagram that provides a means to visualize and combines their use within an integrated model. Our approach supplements CASE tools with additional information and relationships to maintain the global system consistency. To illustrate it, we implement an editor for designing the proposed diagram, and we apply it in a topical case study.},
  keywords={Unified modeling language;Business;Ontologies;Computational modeling;Software;Semantics;Bridges;meta-model integration;Alignment;business process models;requirements models;BPMN;UML use case},
  doi={10.1109/AICCSA.2018.8612870},
  ISSN={2161-5330},
  month={Oct},}@INPROCEEDINGS{8482078,
  author={Ali, Abbas Raza},
  booktitle={2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)}, 
  title={Cognitive Computing to Optimize IT Services}, 
  year={2018},
  volume={},
  number={},
  pages={54-60},
  abstract={In this paper, the challenges of maintaining a healthy IT operational environment have been addressed by proactively analyzing IT Service Desk tickets, customer satisfaction surveys and social media data. A Cognitive solution goes beyond the traditional structured data analysis solutions by deep analyses of both structured and unstructured text. The salient features of the proposed platform include language identification, translation, hierarchical extraction of the most frequently occurring topics, entities and their relationships, text summarization, sentiments and knowledge extraction from the unstructured text using Natural Language Processing techniques. Moreover, the insights from unstructured text combined with structured data allows the development of various classification, segmentation and time-series forecasting use-cases on incident, problem and change datasets. The text and predictive insights together with raw data are used for visualization and exploration of actionable insights on a rich and interactive dashboard. However, it is hard not only to find these insights using traditional Analytics solutions but it might also take very long time to discover them, especially while dealing with massive amount of unstructured data. By taking actions on these insights, organizations can benefit from significant reduction of ticket volume, reduced operational costs and increased customer satisfaction. In various experiments, on average, up to 18-25 % of yearly ticket volume has been reduced using the proposed approach.},
  keywords={Feature extraction;Cognitive systems;Customer satisfaction;Data visualization;Semantics;Servers;Social network services;Knowledge Extraction;Optimizing IT Services;Cognitive Computing;Topic Clustering;Semantic Text Analytics;Service Desk},
  doi={10.1109/ICCI-CC.2018.8482078},
  ISSN={},
  month={July},}@ARTICLE{10418085,
  author={Sewunetie, Walelign Tewabe and Kovács, László},
  journal={IEEE Access}, 
  title={Exploring Sentence Parsing: OpenAI API-Based and Hybrid Parser-Based Approaches}, 
  year={2024},
  volume={12},
  number={},
  pages={38801-38815},
  abstract={This study focuses on the fundamental process of parsing sentences to create semantic graphs from textual documents. It introduces novel techniques for parsing phrases within semantic graph-based induction, employing both ChatGPT-based and Hybrid parser-based approaches. Through a thorough analysis, the study evaluates the performance of these methods in generating semantic networks from text, particularly in capturing detailed event descriptions and relationships. Results indicate a slight advantage in accuracy for the Hybrid parser-based approach (87%) compared to ChatGPT (85%) in sentence parsing tasks. Furthermore, efficiency analysis reveals that ChatGPT’s response quality varies with prompt sizes, while the Hybrid parser-based method consistently maintains excellent response quality.},
  keywords={Semantics;Chatbots;Adaptation models;Knowledge graphs;Task analysis;Context modeling;Training;Natural language processing;Predictive models;Application of sentence parsing;adverb prediction;ChatGPT;hybrid parser;natural language processing;sentence parsing;semantic graph},
  doi={10.1109/ACCESS.2024.3360480},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{9497473,
  author={Wu, Shouxuan and Lu, Jinzhi and Hu, Zhenchao and Yang, Pengfei and Wang, Guoxin and Kiritsis, Dimitris},
  booktitle={2021 16th International Conference of System of Systems Engineering (SoSE)}, 
  title={Cognitive Thread Supports System of Systems for Complex System Development}, 
  year={2021},
  volume={},
  number={},
  pages={82-87},
  abstract={Model-based Systems Engineering (MBSE) has been widely used in the development of complex systems. The system architectures, organizations, research and development processes using MBSE to design complex systems can be seen as a System of Systems (SoS), which has high complexity and hard to manage. The concept of digital thread is proposed to integrate all the models and data in the SoS. However, lack of cognition ability makes it hard to connect the models and data with human, processes and things in the SoS, which reduces the efficiency of complex system development. In this paper, a new concept named Cognitive Thread is first proposed as digital thread with augmented semantic capabilities for identifying the information of the SoS. Then a cognitive thread construction approach based on Open Services for Lifecycle Collaboration (OSLC) specification and knowledge graphs is proposed to support decision-making and management in the SoS. Finally, the feasibility of the proposed approach is verified through a case study of the advanced driver-assistance system development.},
  keywords={Instruction sets;Semantics;Systems architecture;Organizations;Cognition;Data models;Natural language processing;Digital Thread;Cognitive Thread;OSLC specification;System of Systems;Model-based Systems Engineering},
  doi={10.1109/SOSE52739.2021.9497473},
  ISSN={},
  month={June},}@INPROCEEDINGS{10350365,
  author={Ali, Syed Juned and Gavric, Aleksandar and Proper, Henderik and Bork, Dominik},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Encoding Conceptual Models for Machine Learning: A Systematic Review}, 
  year={2023},
  volume={},
  number={},
  pages={562-570},
  abstract={Conceptual models are essential in Software and Information Systems Engineering to meet many purposes since they explicitly represent the subject domains. Machine Learning (ML) approaches have recently been used in conceptual modeling to realize, among others, intelligent modeling assistance, model transformation, and metamodel classification. These works en-code models in various ways, making the encoded models suitable for applying ML algorithms. The encodings capture the models' structure and/or semantics, making this information available to the ML model during training. Therefore, the choice of the encoding for any ML-driven task is crucial for the ML model to learn the relevant contextual information. In this paper, we report findings from a systematic literature review which yields insights into the current research in machine learning for conceptual modeling (ML4CM). The review focuses on the various encodings used in existing ML4CM solutions and provides insights into i) which are the information sources, ii) how is the conceptual model's structure and/or semantics encoded, iii) why is the model encoded, i.e., for which conceptual modeling task and, iv) which ML algorithms are applied. The results aim to structure the state of the art in encoding conceptual models for ML.},
  keywords={Training;Analytical models;Systematics;Machine learning algorithms;Bibliographies;Semantics;Machine learning;Machine learning;Model-driven engineering;Model Encoding;Systematic Literature Review},
  doi={10.1109/MODELS-C59198.2023.00094},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{9256709,
  author={Batista, Leandro and Monsuez, Bruno},
  booktitle={2020 AIAA/IEEE 39th Digital Avionics Systems Conference (DASC)}, 
  title={The conception of a large-scale Systems Engineering environment}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  abstract={With the rise of artificial intelligence, it is time to shape the systems engineering tooling environment for the future. In the last decade, we have seen several emerging technologies that will potentially have a great impact in complex systems. These new technologies are expected to cause a disruptive impact not only in the products but also in to the tools used across the whole product life cycle. For this reason, is imperative to perform a critical review of the current systems engineering tooling ecosystem. This assessment should also map the open research problems that could prevent the complete integration of the new technologies into the systems engineering framework. This paper proposes a new architecture for a system engineering environment to operate in large scale projects. The objective of this research is twofold: it will first identify the capabilities for the next generation platform, and secondly, it will evaluate how artificial intelligence applications can be integrated in compliance with DO-330. The concept developed by this research will drive tool design recommendations enabling the use of artificial intelligence driven applications in a systems engineering tooling ecosystem.},
  keywords={Tools;Systems engineering and theory;Modeling;Standards;Ecosystems;Complex systems;Vocabulary;Systems Engineering;MBSE;Artificial Intelligence;DO-330},
  doi={10.1109/DASC50938.2020.9256709},
  ISSN={2155-7209},
  month={Oct},}@INPROCEEDINGS{10402219,
  author={Liem, Truc Nguyen and Cao Hoai, Sinh Nguyen and Quoc, Hung Nguyen and Van, Tien Nguyen and Pham Trung, Hieu and Quoc, Trung Nguyen and Hoang, Vinh Truong},
  booktitle={2023 IEEE 15th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={GradTOD - A Unified Dialogue State Tracking Model for Task-Oriented and Open Domain Dialogues}, 
  year={2023},
  volume={},
  number={},
  pages={711-719},
  abstract={The task-oriented dialogue domain system requires classifying intent and replying to a specific goal domain. In the sub-module of Task-oriented, the Dialogue State Tracker (DST) is well-known as a variety processing tracker. However, existing DST models often specialize in only task-oriented domains (ToD), leading to limited performance when applied to scenarios. In this paper, we propose GradTOD, a unified DST model that predicts both two task types, task-oriented dialogue (TOD) and open-domain dialogue (ODD). Our model leverages the recent advances in prompt engineering and conditional generation to perform zero-shot learning. After experiments, GradTOD has achieved an 88.6% and 82.5% score on Joint Goal Accuracy metrics when evaluating the Scheme-Guided Dialogue (SGD) and FusedChat test sets correspondingly, demonstrating the adaption ability for multi-domains.},
  keywords={Measurement;Adaptation models;Zero-shot learning;Computational modeling;Predictive models;Task analysis;Computational intelligence;component;formatting;style;styling;insert},
  doi={10.1109/CICN59264.2023.10402219},
  ISSN={2472-7555},
  month={Dec},}@ARTICLE{10320368,
  author={Quevedo, Ernesto and Cerny, Tomas and Rodriguez, Alejandro and Rivas, Pablo and Yero, Jorge and Sooksatra, Korn and Zhakubayev, Alibek and Taibi, Davide},
  journal={IEEE Access}, 
  title={Legal Natural Language Processing From 2015 to 2022: A Comprehensive Systematic Mapping Study of Advances and Applications}, 
  year={2024},
  volume={12},
  number={},
  pages={145286-145317},
  abstract={The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap. We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field. We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns.},
  keywords={Law;Natural language processing;Task analysis;Systematics;Information retrieval;Surveys;Search problems;Deep learning;Systematic-mapping-study;legal-NLP;deep learning},
  doi={10.1109/ACCESS.2023.3333946},
  ISSN={2169-3536},
  month={},}@ARTICLE{10123130,
  author={Zelina, Petr and Halámková, Jana and Nováček, Vít},
  journal={IEEE Transactions on NanoBioscience}, 
  title={Extraction, Labeling, Clustering, and Semantic Mapping of Segments From Clinical Notes}, 
  year={2023},
  volume={22},
  number={4},
  pages={781-788},
  abstract={This work is motivated by the scarcity of tools for accurate, unsupervised information extraction from unstructured clinical notes in computationally underrepresented languages, such as Czech. We introduce a stepping stone to a broad array of downstream tasks such as summarisation or integration of individual patient records, extraction of structured information for national cancer registry reporting or building of semi-structured semantic patient representations that can be used for computing patient embeddings. More specifically, we present a method for unsupervised extraction of semantically-labeled textual segments from clinical notes and test it out on a dataset of Czech breast cancer patients, provided by Masaryk Memorial Cancer Institute (the largest Czech hospital specialising exclusively in oncology). Our goal was to extract, classify (i.e. label) and cluster segments of the free-text notes that correspond to specific clinical features (e.g., family background, comorbidities or toxicities). Finally, we propose a tool for computer-assisted semantic mapping of segment types to pre-defined ontologies and validate it on a downstream task of category-specific patient similarity. The presented results demonstrate the practical relevance of the proposed approach for building more sophisticated extraction and analytical pipelines deployed on Czech clinical notes.},
  keywords={Task analysis;Semantics;Feature extraction;Ontologies;Nanobioscience;Measurement;Clinical diagnosis;Text categorization;Information retrieval;NLP;EHR;clinical notes;information extraction;text classification},
  doi={10.1109/TNB.2023.3275195},
  ISSN={1558-2639},
  month={Oct},}@INPROCEEDINGS{10595652,
  author={Arrotta, Luca and Bettini, Claudio and Civitarese, Gabriele and Fiori, Michele},
  booktitle={2024 IEEE International Conference on Smart Computing (SMARTCOMP)}, 
  title={ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models}, 
  year={2024},
  volume={},
  number={},
  pages={55-62},
  abstract={Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise, while sharing similar privacy concerns if the reasoning is performed in the cloud. An extensive evaluation using two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort.},
  keywords={Knowledge engineering;Deep learning;Training;Privacy;Computational modeling;Ontologies;Human activity recognition;human activity recognition;context-awareness;large language models},
  doi={10.1109/SMARTCOMP61445.2024.00029},
  ISSN={2693-8340},
  month={June},}@INPROCEEDINGS{10350790,
  author={Majumder, Mainak},
  booktitle={2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={A Domain-Driven Model Generation Framework for Cyber-Physical Production Systems}, 
  year={2023},
  volume={},
  number={},
  pages={172-178},
  abstract={The growing influence of Information Technologies in the manufacturing domain has led to the fourth industrial revolution (Industry 4.0). Cyber-Physical Production System (CPPS) is one of the fundamental concepts of Industry 4.0 that aims to develop an intelligent manufacturing environment by leveraging concepts like the Internet of Things (IoT), cloud computing, virtualization, and Artificial Intelligence (AI). However, the challenges originating from the technological heterogeneity in the manufacturing domain remain primary obstacles towards realising a fully automated CPPS. Among them, semantic heterogeneity in manufacturing information is the most crucial which can be attributed to technology and vendor-specific information modelling mechanisms. A CPPS requires seamless machine-to-machine communication which could be hindered due to the non-interoperability among machine data on a semantic level. Therefore, the primary focus of this thesis work is to understand the semantic interoperability challenges of CPPS and propose solutions to address those challenges. The proposed solution revolves around the development of semantic domain models using the modelling philosophies of Domain-Driven Design (DDD).},
  keywords={Production systems;Machine-to-machine communications;Philosophical considerations;Semantics;Model driven engineering;Fourth Industrial Revolution;Internet of Things;CPPS;Domain-Driven Design (DDD);Information Model;Industry 4.0},
  doi={10.1109/MODELS-C59198.2023.00044},
  ISSN={},
  month={Oct},}@INPROCEEDINGS{8010730,
  author={Kaczmarek-Heß, Monika},
  booktitle={2017 IEEE 19th Conference on Business Informatics (CBI)}, 
  title={A Multilevel Model of Events in Support of Enterprise Agility in the Realm of Enterprise Modeling}, 
  year={2017},
  volume={01},
  number={},
  pages={267-276},
  abstract={One of the challenges faced by enterprises is a need to be agile, i.e., having the ability to detect changes and respond to them efficiently and effectively. These changes are caused by `change drivers', i.e., events occurring within an enterprise itself or within its environment. Many scholars argue that the use of conceptual modeling in general, and enterprise modeling in particular, may help achieve and sustain enterprise agility. This, however, requires enterprise modeling approaches not only to provide an integrated view on the enterprise action system and information system, but also to account for events indicating business-impacting situations and information on their impact on the enterprise. In addition, being agile requires the ability to acquire information from event sources (sensing), to process obtained information, and to support a decision-making process (response). In this paper, we argue that the fulfillment of those postulates demands the application of an alternative language paradigm, namely multilevel modeling. Therefore, we contribute a multilevel model of events developed using an integrated modeling and programming approach.},
  keywords={Ontologies;Information systems;Organizations;Analytical models;Sensors;Programming;enterprise modeling;multilevel modeling;FMMLx;modeling of events},
  doi={10.1109/CBI.2017.58},
  ISSN={2378-1971},
  month={July},}@ARTICLE{9556591,
  author={Marschall, Benedikt and Ochsenkuehn, Daniel and Voigt, Tobias},
  journal={IEEE Journal of Emerging and Selected Topics in Industrial Electronics}, 
  title={Design and Implementation of a Smart, Product-Led Production Control Using Industrial Agents}, 
  year={2022},
  volume={3},
  number={1},
  pages={48-56},
  abstract={In theory, the design of modern production systems in the form of a cyber–physical production system (CPPS) allows more flexibility, simple expandability, quick adaptability, and intelligent production control by the product. Multiagent systems (MASs) are thereby recommended as control solution because of their autonomy and dynamic decentralized architecture. Although their potential use and technical excellence have been proven, the costs of implementation and maintenance still outweigh their supposed advantages. This results in low acceptance and usage of operational MAS in the industry. This article describes topics that need to be considered when designing and implementing an MAS as production control for a CPPS in the context of customized mass production. Generally developed approaches are presented, which were implemented in a commercially developed agent framework and validated on the basis of an industrial use case for a product-led filling process in lot size one. All implemented concepts aim to be reusable in comparable applications across industries. In combination with the MAS-internal testing approach also presented, this should contribute to faster, more cost-effective implementation of reliable MAS solutions and ultimately increase their technical maturity.},
  keywords={Production systems;Production control;XML;Radiofrequency identification;Costs;Companies;Testing;Industrial agents (IAs);cyber–physical product ion system (CPPS);multiagent system (MAS)},
  doi={10.1109/JESTIE.2021.3117121},
  ISSN={2687-9743},
  month={Jan},}@INPROCEEDINGS{10386763,
  author={Jesus, Vitor and Patel, Asma and Kumar, Deepak},
  booktitle={2023 10th International Conference on Behavioural and Social Computing (BESC)}, 
  title={Feasibility of Structured, Machine-Readable Privacy Notices}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  abstract={This paper offers a novel approach to the long standing problem of the interface of humans and online privacy notices. As literature and practice, and even art, for more than a decade have identified, privacy notices are nearly always ignored and "accepted" with little thought, mostly because it is not practical nor user-friendly to depend on reading a long text simply to access, e.g., a news website. Nevertheless, privacy notices are a central element, often mandated by law.We approach the problem by (partially) relieving the human from the task of inspecting such documents. Because they are documents written in natural language, often legal language, we assess the feasibility of representing privacy notices in a machine-readable format. Should this be feasible, automated processing of notices that still respect individual choices could be enabled. To this end, we manually inspected privacy notices under EU/UK's GDPR from common websites, and designed a JSON schema that captures their structure.},
  keywords={Privacy;Social computing;Art;Law;Natural languages;Task analysis},
  doi={10.1109/BESC59560.2023.10386763},
  ISSN={},
  month={Oct},}

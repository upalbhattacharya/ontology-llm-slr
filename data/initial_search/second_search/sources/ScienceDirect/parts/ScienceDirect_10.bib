@article{SULASTRI2024101979,
title = {Transforming towards inclusion-by-design: Information system design principles shaping data-driven financial inclusiveness},
journal = {Government Information Quarterly},
volume = {41},
number = {4},
pages = {101979},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101979},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000716},
author = {Reni Sulastri and Marijn Janssen and Ibo {van de Poel} and Aaron Ding},
keywords = {Digital governance, Inclusion, Lending systems, Value-based requirements, Inclusion by design, System-level transformation},
abstract = {Digitalization and datafication of financial systems result in more efficiency, but might also result in the exclusions of certain groups. Governments are looking for ways to increase inclusions and leave no one behind. For this, they must govern an organizational ecosystem of public and private parties. We derive value-based requirements through a systematic research methodology and iteratively refine design principles for achieving inclusivity goals. This refinement process is enriched by interviews with field experts, leading to the formulation of key Design principles: the essential role of inclusive metrics, leveraging alternative data sources, ensuring transparency in loan processes and the ability for decision contestation, providing tailored credit solutions, and maintaining long-term system sustainability. The government's role is to ensure a level playing field where all parties have equal access to the data. Following the principles ensures that exclusion and discrimination become visible and can be avoided. This study underscores the necessity for system-level transformations, inclusion-by-design, and advocacy for a new system design complemented by regulatory updates, new data integration, inclusive AI, and organizational collaborative shifts. These principles can also be used in different data-driven governance situations.}
}
@article{GHOSH2024729,
title = {Feeding the wrath with myelin},
journal = {Trends in Immunology},
volume = {45},
number = {10},
pages = {729-731},
year = {2024},
note = {Special issue: Neuroimmunology – II},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2024.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1471490624002126},
author = {Sourav Ghosh and Carla V. Rothlin},
abstract = {Kloosterman and colleagues studied molecular and cellular changes during radiation therapy and disease recurrence across molecular subtypes of glioblastoma. They uncovered a distinct immune–cancer cell metabolic crosstalk during proneural/oligodendrocyte progenitor cell-like to mesenchymal-like transition, wherein macrophages feed on cholesterol-rich myelin debris to provide lipids to mesenchymal tumor cells, thereby fueling glioblastoma growth.}
}
@article{HARDINGLARSEN2024108459,
title = {Protein representations: Encoding biological information for machine learning in biocatalysis},
journal = {Biotechnology Advances},
volume = {77},
pages = {108459},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108459},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024001538},
author = {David Harding-Larsen and Jonathan Funk and Niklas Gesmar Madsen and Hani Gharabli and Carlos G. Acevedo-Rocha and Stanislav Mazurenko and Ditte Hededam Welner},
keywords = {Machine learning, Biocatalysis, Protein representations, Enzyme engineering, Representation learning, Protein dynamics, Predictive models},
abstract = {Enzymes offer a more environmentally friendly and low-impact solution to conventional chemistry, but they often require additional engineering for their application in industrial settings, an endeavour that is challenging and laborious. To address this issue, the power of machine learning can be harnessed to produce predictive models that enable the in silico study and engineering of improved enzymatic properties. Such machine learning models, however, require the conversion of the complex biological information to a numerical input, also called protein representations. These inputs demand special attention to ensure the training of accurate and precise models, and, in this review, we therefore examine the critical step of encoding protein information to numeric representations for use in machine learning. We selected the most important approaches for encoding the three distinct biological protein representations — primary sequence, 3D structure, and dynamics — to explore their requirements for employment and inductive biases. Combined representations of proteins and substrates are also introduced as emergent tools in biocatalysis. We propose the division of fixed representations, a collection of rule-based encoding strategies, and learned representations extracted from the latent spaces of large neural networks. To select the most suitable protein representation, we propose two main factors to consider. The first one is the model setup, which is influenced by the size of the training dataset and the choice of architecture. The second factor is the model objectives such as consideration about the assayed property, the difference between wild-type models and mutant predictors, and requirements for explainability. This review is aimed at serving as a source of information and guidance for properly representing enzymes in future machine learning models for biocatalysis.}
}
@article{HASSANI2024102136,
title = {A systematic review of data fusion techniques for optimized structural health monitoring},
journal = {Information Fusion},
volume = {103},
pages = {102136},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102136},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004529},
author = {Sahar Hassani and Ulrike Dackermann and Mohsen Mousavi and Jianchun Li},
keywords = {Structural health monitoring, Raw data fusion, Feature fusion, Decision fusion, Deep learning, Machine learning},
abstract = {Advancements in structural health monitoring (SHM) techniques have spiked in the past few decades due to the rapid evolution of novel sensing and data transfer technologies. This development has facilitated the simultaneous recording of a wide range of data, which could contain abundant damage-related features. Concurrently, the age of omnipresent data started with massive amounts of SHM data collected from large-size heterogeneous sensor networks. The abundance of information from diverse sources needs to be aggregated to enable robust decision-making strategies. Data fusion is the process of integrating various data from heterogeneous sources to produce more useful, accurate, and reliable information about system behavior. This paper reviews recent developments in data fusion techniques applied to SHM systems. The theoretical concepts, applications, benefits, and limitations of current methods and challenges in SHM are presented, and future trends in data fusion methods are discussed. Furthermore, a set of criteria is proposed to evaluate contents and information from original and review papers in this field, and a road map is provided discussing possible future work.}
}
@article{BUTZ2025105948,
title = {Contextualizing predictive minds},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {168},
pages = {105948},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105948},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004172},
author = {Martin V. Butz and Maximilian Mittenbühler and Sarah Schwöbel and Asya Achimova and Christian Gumbsch and Sebastian Otte and Stefan Kiebel},
keywords = {Prediction, Cognitive modeling, Events, Active inference, Learning, Behavior, Free energy, Abstraction, Context inference, Deep learning},
abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.}
}
@article{SORBELLO2023,
title = {Artificial Intelligence–Enabled Software Prototype to Inform Opioid Pharmacovigilance From Electronic Health Records: Development and Usability Study},
journal = {JMIR AI},
volume = {2},
year = {2023},
issn = {2817-1705},
doi = {https://doi.org/10.2196/45000},
url = {https://www.sciencedirect.com/science/article/pii/S2817170523000261},
author = {Alfred Sorbello and Syed Arefinul Haque and Rashedul Hasan and Richard Jermyn and Ahmad Hussein and Alex Vega and Krzysztof Zembrzuski and Anna Ripple and Mitra Ahadpour},
keywords = {electronic health records, pharmacovigilance, artificial intelligence, real world data, EHR, natural language, software application, drug, Food and Drug Administration, deep learning},
abstract = {Background
The use of patient health and treatment information captured in structured and unstructured formats in computerized electronic health record (EHR) repositories could potentially augment the detection of safety signals for drug products regulated by the US Food and Drug Administration (FDA). Natural language processing and other artificial intelligence (AI) techniques provide novel methodologies that could be leveraged to extract clinically useful information from EHR resources.
Objective
Our aim is to develop a novel AI-enabled software prototype to identify adverse drug event (ADE) safety signals from free-text discharge summaries in EHRs to enhance opioid drug safety and research activities at the FDA.
Methods
We developed a prototype for web-based software that leverages keyword and trigger-phrase searching with rule-based algorithms and deep learning to extract candidate ADEs for specific opioid drugs from discharge summaries in the Medical Information Mart for Intensive Care III (MIMIC III) database. The prototype uses MedSpacy components to identify relevant sections of discharge summaries and a pretrained natural language processing (NLP) model, Spark NLP for Healthcare, for named entity recognition. Fifteen FDA staff members provided feedback on the prototype’s features and functionalities.
Results
Using the prototype, we were able to identify known, labeled, opioid-related adverse drug reactions from text in EHRs. The AI-enabled model achieved accuracy, recall, precision, and F1-scores of 0.66, 0.69, 0.64, and 0.67, respectively. FDA participants assessed the prototype as highly desirable in user satisfaction, visualizations, and in the potential to support drug safety signal detection for opioid drugs from EHR data while saving time and manual effort. Actionable design recommendations included (1) enlarging the tabs and visualizations; (2) enabling more flexibility and customizations to fit end users’ individual needs; (3) providing additional instructional resources; (4) adding multiple graph export functionality; and (5) adding project summaries.
Conclusions
The novel prototype uses innovative AI-based techniques to automate searching for, extracting, and analyzing clinically useful information captured in unstructured text in EHRs. It increases efficiency in harnessing real-world data for opioid drug safety and increases the usability of the data to support regulatory review while decreasing the manual research burden.}
}
@article{MARTORELLI2024,
title = {Multiple graphical views for automatically generating SQL for the MycoDiversity DB; making fungal biodiversity studies accessible},
journal = {Biodiversity Data Journal},
volume = {12},
year = {2024},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.12.e119660},
url = {https://www.sciencedirect.com/science/article/pii/S1314283624001763},
author = {Irene Martorelli and Aram Pooryousefi and Haike {van Thiel} and Floris J Sicking and Guus J Ramackers and Vincent Merckx and Fons J Verbeek},
keywords = {fungal distribution, fungal biodiversity, biogeography, environmental DNA, mycodiversity, geospatial maps, information visualisation, dynamic hierarchical data, accessibility, reusability, database, FAIR data, controlled vocabulary terms},
abstract = {Fungi is a highly diverse group of eukaryotic organisms that live under an extremely wide range of environmental conditions. Nowadays, there is a fundamental focus on observing how biodiversity varies on different spatial scales, in addition to understanding the environmental factors which drive fungal biodiversity. Metabarcoding is a high-throughput DNA sequencing technology that has positively contributed to observing fungal communities in environments. While the DNA sequencing data generated from metabarcoding studies are available in public archives, this valuable data resource is not directly usable for fungal biodiversity investigation. Additionally, due to its fragmented storage and distributed nature, it is not immediately accessible through a single user interface. We developed the MycoDiversity DataBase User Interface (https://mycodiversity.liacs.nl) to provide direct access and retrieval of fungal data that was previously inaccessible in the public domain. The user interface provides multiple graphical views of the data components used to reveal fungal biodiversity. These components include reliable geo-location terms, the reference taxonomic scientific names associated with fungal species and the standard features describing the environment where they occur. Direct observation of the public DNA sequencing data in association with fungi is accessible through SQL search queries created by interactively manipulating topological maps and dynamic hierarchical tree views. The search results are presented in configurable data table views that can be downloaded for further use. With the MycoDiversity DataBase User Interface, we make fungal biodiversity data accessible, assisting researchers and other stakeholders in using metabarcoding studies for assessing fungal biodiversity.}
}
@article{ROOD20244520,
title = {Toward a foundation model of causal cell and tissue biology with a Perturbation Cell and Tissue Atlas},
journal = {Cell},
volume = {187},
number = {17},
pages = {4520-4545},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424008298},
author = {Jennifer E. Rood and Anna Hupalowska and Aviv Regev},
abstract = {Summary
Comprehensively charting the biologically causal circuits that govern the phenotypic space of human cells has often been viewed as an insurmountable challenge. However, in the last decade, a suite of interleaved experimental and computational technologies has arisen that is making this fundamental goal increasingly tractable. Pooled CRISPR-based perturbation screens with high-content molecular and/or image-based readouts are now enabling researchers to probe, map, and decipher genetically causal circuits at increasing scale. This scale is now eminently suitable for the deployment of artificial intelligence and machine learning (AI/ML) to both direct further experiments and to predict or generate information that was not—and sometimes cannot—be gathered experimentally. By combining and iterating those through experiments that are designed for inference, we now envision a Perturbation Cell Atlas as a generative causal foundation model to unify human cell biology.}
}
@incollection{GUDIVADA202527,
title = {Chapter 2 - Data analytics: fundamentals},
editor = {Mashrur Chowdhury and Kakan Dey and Amy Apon},
booktitle = {Data Analytics for Intelligent Transportation Systems (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {27-66},
year = {2025},
isbn = {978-0-443-13878-2},
doi = {https://doi.org/10.1016/B978-0-443-13878-2.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443138782000126},
author = {Venkat N. Gudivada},
keywords = {Data analytics, data science, data mining, clustering, classification, machine learning, OLAP},
abstract = {This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, and big data analytics to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open-source tools and resources for developing data analytics systems are listed. The chapter concludes by indicating emerging trends in data analytics.}
}
@incollection{FARISCO2024191,
title = {Chapter Ten - The ethical implications of indicators of consciousness in artificial systems},
editor = {Marcello Ienca and Georg Starke},
series = {Developments in Neuroethics and Bioethics},
publisher = {Academic Press},
volume = {7},
pages = {191-204},
year = {2024},
booktitle = {Brains and Machines: Towards a Unified Ethics of AI and Neuroscience},
issn = {2589-2959},
doi = {https://doi.org/10.1016/bs.dnb.2024.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S2589295924000146},
author = {Michele Farisco},
keywords = {Consciousness, Artificial consciousness, Indicators of consciousness, AI ethics, Moral status},
abstract = {The prospect of artificial consciousness raises theoretical, technical and ethical challenges which converge on the core issue of how to eventually identify and characterize it. In order to provide an answer to this question, I propose to start from a theoretical reflection about the meaning and main characteristics of consciousness. On the basis of this conceptual clarification it is then possible to think about relevant empirical indicators (i.e. features that facilitate the attribution of consciousness to the system considered) and identify key ethical implications that arise. In this chapter, I further elaborate previous work on the topic, presenting a list of candidate indicators of consciousness in artificial systems and introducing an ethical reflection about their potential implications. Specifically, I focus on two main ethical issues: the conditions for considering an artificial system as a moral subject; and the need for a non-anthropocentric approach in reflecting about the science and the ethics of artificial consciousness.}
}
@article{TURON2025100118,
title = {The path to adoption of open source AI for drug discovery in Africa},
journal = {Artificial Intelligence in the Life Sciences},
volume = {7},
pages = {100118},
year = {2025},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2024.100118},
url = {https://www.sciencedirect.com/science/article/pii/S2667318524000254},
author = {Gemma Turon and Miquel Duran-Frigola}
}
@article{EICHELBERGER2025107650,
title = {Industry 4.0/IIoT Platforms for manufacturing systems — A systematic review contrasting the scientific and the industrial side},
journal = {Information and Software Technology},
volume = {179},
pages = {107650},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107650},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002556},
author = {Holger Eichelberger and Christian Sauer and Amir Shayan Ahmadian and Christian Kröher},
keywords = {Industry 4.0, Industrial Internet of Things (IIoT), Cyber–Physical Production Systems (CPPS), Software platforms, Systematic literature review},
abstract = {Context:
IIoT, Industry 4.0 or CPPS software platforms are cornerstones of smart manufacturing production systems. Such platforms integrate machines, IIoT and edge devices, realize distributed (management) functionality and provide the basis for user-defined IIoT applications. Individual instances in research and industrial practice do share commonalities while they also differ significantly.
Objective:
A detailed overview of the platform landscape is fundamental for innovative research. However, actual surveys and literature reviews concentrate on specific aspects and usually focus only on the research works, neglecting specific aspects of the industrial use of IIoT platforms. We aim at a systematic overview of the functionalities and approaches of scientific and industrial IIoT platforms along 16 analysis dimensions and thereby exposing gaps between the focuses of research on IIoT platforms and actual industrial IIoT platforms in use. By doing so we are able to highlight future areas of interest to research as well as indicating potentially over-researched areas which are of less interest in actual industrial IIoT platforms.
Method:
We combine a systematic literature review of scientific IIoT platform research with a systematic analysis of industrial IIoT platforms.
Results:
We start off with 1620 research papers plus 70 from snowballing that we systematically filter down to 36 papers (plus 11 added by a SLR update) providing sufficient information for a data extraction, which we analyze along 16 topics to extract actual capabilities and differences of relevant platform approaches. In a second step, we contrast these results with an analysis of 21 industrial platforms.
Conclusion:
Similar approaches, differences and topics for future are exhibited. In comparison with 21 industrial platforms along the same analysis topics, we distill various commonalities, differences, trends and gaps.}
}
@article{CHAKRABORTY2024100164,
title = {From machine learning to deep learning: Advances of the recent data-driven paradigm shift in medicine and healthcare},
journal = {Current Research in Biotechnology},
volume = {7},
pages = {100164},
year = {2024},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2023.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590262823000461},
author = {Chiranjib Chakraborty and Manojit Bhattacharya and Soumen Pal and Sang-Soo Lee},
keywords = {Deep learning, Machine learning, Artificial intelligence, Medicine and health care},
abstract = {The medicine and healthcare sector has been evolving and advancing very fast. The advancement has been initiated and shaped by the applications of data-driven, robust, and efficient machine learning (ML) to deep learning (DL) technologies. ML in the medical sector is developing quickly, causing rapid progress, reshaping medicine, and improving clinician and patient experiences. ML technologies evolved into data-hungry DL approaches, which are more robust and efficient in dealing with medical data. This article reviews some critical data-driven aspects of machine intelligence in the medical field. In this direction, the article illustrated the recent progress of data-driven medical science using ML to DL in two categories: firstly, the recent development of data science in medicine with the use of ML to DL and, secondly, the chabot technologies in healthcare and medicine, particularly on ChatGPT. Here, we discuss the progress of ML, DL, and the transition requirements from ML to DL. To discuss the advancement in data science, we illustrate prospective studies of medical image data, newly evolved DL interpretation data from EMR or EHR, big data in personalized medicine, and dataset shifts in artificial intelligence (AI). Simultaneously, the article illustrated recently developed DL-enabled ChatGPT technology. Finally, we summarize the broad role of ML and DL in medicine and the significant challenges for implementing recent ML to DL technologies in healthcare. The overview of the data-driven paradigm shift in medicine using ML to DL technologies in the article will benefit researchers immensely.}
}
@article{DANESHFAR2024109288,
title = {Image captioning by diffusion models: A survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109288},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109288},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624014465},
author = {Fatemeh Daneshfar and Ako Bartani and Pardis Lotfi},
keywords = {Image captioning, Diffusion models, Image-to-text, Survey, Implemented artificial intelligence, Application of artificial intelligence},
abstract = {Diffusion models are increasingly favored over traditional approaches like generative adversarial networks (GANs) and auto-regressive transformers due to their remarkable generative capabilities. They demonstrate outstanding performance not solely limited to image generation and manipulation but also in text-related tasks. Despite this, existing surveys tend to concentrate on the utilization of diffusion models solely for image generation, ignoring their potential in image captioning. To address this oversight, our paper provides an exhaustive examination of image-to-text diffusion models within the landscape of artificial intelligence (AI) and generative computing, filling a critical void in the literature. Starting with an overview of basic diffusion model principles, we explore into the enhancements brought by conditioning or guidance and the implemented AI. We then present a taxonomy and review of cutting-edge methods in diffusion-based image captioning. Additionally, we explore applications beyond image-to-text generation, such as image-guided creative generation, text editing, and the application of AI. We also cover existing evaluation metrics, software and libraries, as well as challenges and future directions in the field.}
}
@article{PADMANABHANPOTI2024100086,
title = {Enabling affordances for AI Governance},
journal = {Journal of Responsible Technology},
volume = {18},
pages = {100086},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100086},
url = {https://www.sciencedirect.com/science/article/pii/S266665962400012X},
author = {Siri {Padmanabhan Poti} and Christopher J Stanton},
keywords = {Governance, Explainability, Interpretability, Assurance, Technical debt, Shift-left, Behavior tree},
abstract = {Organizations dealing with mission-critical AI based autonomous systems may need to provide continuous risk management controls and establish means for their governance. To achieve this, organizations are required to embed trustworthiness and transparency in these systems, with human overseeing and accountability. Autonomous systems gain trustworthiness, transparency, quality, and maintainability through the assurance of outcomes, explanations of behavior, and interpretations of intent. However, technical, commercial, and market challenges during the software development lifecycle (SDLC) of autonomous systems can lead to compromises in their quality, maintainability, interpretability and explainability. This paper conceptually models transformation of SDLC to enable affordances for assurance, explanations, interpretations, and overall governance in autonomous systems. We argue that opportunities for transformation of SDLC are available through concerted interventions such as technical debt management, shift-left approach and non-ephemeral artifacts. This paper contributes to the theory and practice of governance of autonomous systems, and in building trustworthiness incrementally and hierarchically.}
}
@article{BADAMI2023102231,
title = {Adaptive search query generation and refinement in systematic literature review},
journal = {Information Systems},
volume = {117},
pages = {102231},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102231},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923000674},
author = {Maisie Badami and Boualem Benatallah and Marcos Baez},
keywords = {Systematic reviews, Query enrichment, Query adaptation, Reinforcement learning, Word embedding},
abstract = {Systematic literature reviews (SLRs) are a central part of evidence-based research, which involves collecting and integrating empirical evidence on specific research questions. A key step in this process is building Boolean search queries, which are at the core of information retrieval systems that support literature search. This involves turning general research aims into specific search terms that can be combined into complex Boolean expressions. Researchers must build and refine search queries to ensure they have sufficient coverage and properly represent the literature. In this paper, we propose an adaptive query generation and refinement pipeline for SLR search that uses reinforcement learning to learn the optimal modifications to a query based on feedback from researchers about its performance. Empirical evaluations with 10 SLR datasets showed our approach achieves comparable performance to queries manually composed by SLR authors. We also investigate the impact of design decisions on the performance of the query generation and refinement pipeline. Specifically, we study the effects of the type of input seed, the use of general versus domain-specific word embedding models, the sampling strategy for relevance feedback, and number of iterations in the refinement process. Our results provide insights into the effects of these choices on the pipeline’s performance.}
}
@article{PAL2023100247,
title = {Automated vision-based construction progress monitoring in built environment through digital twin},
journal = {Developments in the Built Environment},
volume = {16},
pages = {100247},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100247},
url = {https://www.sciencedirect.com/science/article/pii/S2666165923001291},
author = {Aritra Pal and Jacob J. Lin and Shang-Hsien Hsieh and Mani Golparvar-Fard},
abstract = {Effective progress monitoring is ineviTable for completing the construction of building and infrastructure projects successfully. In this digital transformation era, with the data-centric management and control approach, the effectiveness of monitoring methods is expected to improve dramatically. ”Digital Twin,” which creates a bidirectional communication flow between a physical entity and its digital counterpart, is found to be a crucial enabling technology for information-aware decision-making systems in manufacturing and other automotive industries. Recognizing the benefits of this technology in production management in construction, researchers have proposed Digital Twin Construction (DTC). DTC leverages building information modeling technology and processes, lean construction practices, on-site digital data collection mechanisms, and Artificial Intelligence (AI) based data analytics for improving construction production planning and control processes. Progress monitoring, a key component in construction production planning and control, can significantly benefit from DTC. However, some knowledge gaps still need to be filled for the practical implementation of DTC for progress monitoring in the built environment domain. This research reviews the existing vision-based progress monitoring methods, studies the evolution of automated vision-based construction progress monitoring research, and highlights the methodological and technological knowledge gaps that must be addressed for DTC-based predictive progress monitoring. Subsequently, it proposes a framework for closed-loop construction control through DTC. Finally, the way forward for fully automated, real-time construction progress monitoring built upon the DTC concept is proposed.}
}
@article{SCHWOERER2024102936,
title = {‘International, intersectional and interdisciplinary’ – Gender and feminist studies degree descriptions and logics of representation in marketised English higher education},
journal = {Women's Studies International Forum},
volume = {105},
pages = {102936},
year = {2024},
issn = {0277-5395},
doi = {https://doi.org/10.1016/j.wsif.2024.102936},
url = {https://www.sciencedirect.com/science/article/pii/S0277539524000748},
author = {Lili Schwoerer},
keywords = {Gender studies, Commodification, Marketisation, Neoliberal university, Difference, Discourse analysis},
abstract = {This article explores how the academic field of gender and feminist studies in England represents itself, by drawing on a discourse analysis of online descriptions from websites of all gender and feminist studies degree programmes and departments in English universities, all but one of which are graduate degrees. Foregrounding the context of the neoliberal university, in which feminist and gender knowledge is simultaneously marginalised and mainstreamed, the article asks how representations of the field are shaped by the marketisation of higher education. This analysis reveals a disjuncture between two representative logics: while most feminist, gender studies and queer scholarship relies on anti-essentialist epistemologies and ontologies, the dominant logic of representation in contemporary universities understands difference as static and representable. This representability enables and is in turn facilitated by marketisation.}
}
@article{BEKAMIRI2024123536,
title = {PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT},
journal = {Technological Forecasting and Social Change},
volume = {206},
pages = {123536},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123536},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524003329},
author = {Hamid Bekamiri and Daniel S. Hain and Roman Jurowetzki},
keywords = {Technological distance, Patent classification, Deep NLP, Augmented SBERT, Hybrid model, Model explainability},
abstract = {This study presents an efficient approach for utilizing text data to calculate patent-to-patent (p2p) technological similarity and proposes a hybrid framework for leveraging the resulting p2p similarity in applications such as semantic search and automated patent classification. To achieve this, we create embeddings using Sentence-BERT (SBERT) on patent claims. For domain adaptation of the general SBERT model, we implement an augmented approach to fine-tune SBERT using in-domain supervised patent claims data. The study utilizes SBERT's efficiency in creating embedding distance measures to map p2p similarity in large sets of patent data. We demonstrate applications of the framework for the use case of automated patent classification with a simple K Nearest Neighbors (KNN) model that predicts assigned Cooperative Patent Classification (CPC) based on the class assignment of the K patents with the highest p2p similarity. The results show that p2p similarity captures technological features in terms of CPC overlap, and the approach is useful for automatic patent classification based on text data. Moreover, the presented classification framework is simple, and the results are easy to interpret and evaluate by end-users via instance-based explanations. The study performs an out-of-sample model validation, predicting all assigned CPC classes on the subclass (663) level with an F1 score of 66 %, outperforming the current state-of-the-art in text-based multi-label patent classification. The study also discusses the applicability of the presented framework for semantic intellectual property (IP) search, patent landscaping, and technology mapping. Finally, the study outlines a future research agenda to leverage multi-source patent embeddings, evaluate their appropriateness across applications, and improve and validate patent embeddings by creating domain-expert curated Semantic Textual Similarity (STS) benchmark datasets.}
}
@incollection{2024285,
title = {Index},
editor = {Shufei Li and Pai Zheng and Lihui Wang},
booktitle = {Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing},
publisher = {Elsevier},
pages = {285-293},
year = {2024},
isbn = {978-0-443-13943-7},
doi = {https://doi.org/10.1016/B978-0-44-313943-7.00018-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139437000181}
}
@article{DANGSAWANG2024100408,
title = {A machine learning approach for detecting customs fraud through unstructured data analysis in social media},
journal = {Decision Analytics Journal},
volume = {10},
pages = {100408},
year = {2024},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2024.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2772662224000122},
author = {Bundidth Dangsawang and Siranee Nuchitprasitchai},
keywords = {Logistic Regression, Long short-term memory, Gated Recurrent Unit, Unstructured data, Customs duties, Commercial goods},
abstract = {Goods and services are sold through social media by individuals not authorized as legitimate dealers, resulting in lost taxes and customs duties to governments. This study proposes a model called SHIELD for detecting these violations through unstructured data in social media. The process involves collecting 2,373,570 records of commercial goods from social media platforms such as Twitter and Facebook in three phases. In Phase 1, keywords for labeling are collected for text classification. Three categories of results are defined: Red Line for smuggled goods, unpaid duty, prohibited goods, and restricted goods; Green Line for non-commercial goods; and Inspect for goods that cannot be identified from the text and require further investigation. Phase 2 and Phase 3 use keywords to detect smugglers from unstructured social media data for labeling grouped by three algorithms of Logistic Regression (LR), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM), employed to classify imported illegal products. The results of all tests show that the LSTM technique had the best accuracy of 99.44% and the best average F1 score of 90.55%. Using algorithms and techniques such as LR, GRU, and LSTM demonstrates the potential of machine learning and natural language processing in detecting illegal activities and promoting economic security.}
}
@article{YANG2024103823,
title = {Span-level bidirectional retention scheme for aspect sentiment triplet extraction},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103823},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103823},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001821},
author = {Xuan Yang and Tao Peng and Haijia Bi and Jiayu Han},
keywords = {Sentiment parsing, Triplet extraction, Relation detection, Semantic transfer},
abstract = {The objective of the Aspect Sentiment Triplet Extraction (ASTE) task is to identify triplets of (aspect, opinion, sentiment) from user-generated reviews. The current study does not extensively integrate the interaction between word pairs and aspect-opinion pairs during the learning process at the granularity of sentence analysis. Furthermore, the bidirectional inference for the triplet, along with the parallel computing approach for long-span texts, also fail to achieve efficient unification. We introduce a new perspective: Span-level Bidirectional Retention Scheme(SBRS) for Aspect Sentiment Triplet Extraction model. The model comprises two pathways. The first pathway involves extracting effective aspect-opinion pair outcomes via two progressive submodules that operate on words and word pairs at varying scales. Building on the first pathway, the second pathway senses the interaction information of word pairs through bidirectional recursion and combines an efficient parallel computing approach. This combination allows the model to utilize three features – context, semantics, and relationship – to accurately identify the sentimental orientation. Thus, the two pathways facilitate the learning of relation-aware representations of word pairs. We carried out experiments on two public datasets, showing an average enhancement of 3.34% and 1.72% in F1 scores compared to the most recent baselines models, and multiple experiments from diverse angles proved the model’s superiority.}
}
@incollection{2023323,
title = {Index},
editor = {Tung-Hung Su and Jia-Horng Kao},
booktitle = {Artificial Intelligence, Machine Learning, and Deep Learning in Precision Medicine in Liver Diseases},
publisher = {Academic Press},
pages = {323-333},
year = {2023},
isbn = {978-0-323-99136-0},
doi = {https://doi.org/10.1016/B978-0-323-99136-0.20001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991360200019}
}
@article{GETULI2025105897,
title = {Parametric design methodology for developing BIM object libraries in construction site modeling},
journal = {Automation in Construction},
volume = {170},
pages = {105897},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105897},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006332},
author = {Vito Getuli and Alessandro Bruttini and Farzad Rahimian},
keywords = {BIM, Object library, Construction site, Parametric design, Modeling, planning, And product information},
abstract = {The adoption of Building Information Modeling (BIM) in construction site layout planning and activity scheduling faces challenges due to the lack of standardized approaches for digitally reproducing and organizing site elements that meet information requirements of diverse regulatory frameworks and stakeholders' use cases. This paper addresses the question of how to streamline the development of BIM objects for construction site modeling by proposing a vendor-neutral parametric design methodology and introduces a dedicated hierarchical structure for BIM object libraries to support users in their implementation. The methodology includes a six-step process for creating informative content, parametric geometries, and documentation, and is demonstrated through the development and implementation of a construction site BIM object library suitable for the Italian context. This approach fills a gap in BIM object development standards and offers a foundation for future research, benefiting practitioners and industry stakeholders involved in BIM-based site layout modeling and activity planning.}
}
@incollection{SCARCELLO2024,
title = {Artificial Intelligence},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00109-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001093},
author = {Francesco Scarcello and Simona Nisticò and Luigi Palopoli},
keywords = {Agents, Artificial intelligence, Knowledge representation, Logic, Machine learning, Natural language processing, Planning, Reasoning, Robotics, Vision},
abstract = {Artificial Intelligence is going to support every human activity, from communication to healthcare, business, entertaining, and so forth. It is a very active field of research with an uncountable spectrum of applications. A picture of the field is provided, with a brief overview of history, recent achievements, and directions to detailed information on the various facets of AI.}
}
@article{BRANNSTROM2025109325,
title = {Goal-hiding information-seeking dialogues: A formal framework},
journal = {International Journal of Approximate Reasoning},
volume = {177},
pages = {109325},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109325},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24002123},
author = {Andreas Brännström and Virginia Dignum and Juan Carlos Nieves},
keywords = {Formal dialogues, Quantitative argumentation, Information extraction, Human-agent interaction, Theory of mind},
abstract = {We consider a type of information-seeking dialogue between a seeker agent and a respondent agent, where the seeker estimates the respondent to not be willing to share a particular set of sought-after information. Hence, the seeker postpones (hides) its goal topic, related to the respondent's sensitive information, until the respondent is perceived as willing to talk about it. In the intermediate process, the seeker opens other topics to steer the dialogue tactfully towards the goal. Such dialogue strategies, which we refer to as goal-hiding strategies, are common in diverse contexts such as criminal interrogations and medical assessments, involving sensitive topics. Conversely, in malicious online interactions like social media extortion, similar strategies might aim to manipulate individuals into revealing information or agreeing to unfavorable terms. This paper proposes a formal dialogue framework for understanding goal-hiding strategies. The dialogue framework uses Quantitative Bipolar Argumentation Frameworks (QBAFs) to assign willingness scores to topics. An initial willingness for each topic is modified by considering how topics promote (support) or demote (attack) other topics. We introduce a method to identify relations among topics by considering a respondent's shared information. Finally, we introduce a gradual semantics to estimate changes in willingness as new topics are opened. Our formal analysis and empirical evaluation show the system's compliance with privacy-preserving safety properties. A formal understanding of goal-hiding strategies opens up a range of practical applications; For instance, a seeker agent may plan with goal-hiding to enhance privacy in human-agent interactions. Similarly, an observer agent (third-party) may be designed to enhance social media security by detecting goal-hiding strategies employed by users' interlocutors.}
}
@article{JOVEJUNCA2024101043,
title = {Genomic architecture of carcass and pork traits and their association with immune capacity},
journal = {animal},
volume = {18},
number = {1},
pages = {101043},
year = {2024},
issn = {1751-7311},
doi = {https://doi.org/10.1016/j.animal.2023.101043},
url = {https://www.sciencedirect.com/science/article/pii/S1751731123003609},
author = {T. Jové-Juncà and D. Crespo-Piazuelo and O. González-Rodríguez and M. Pascual and C. Hernández-Banqué and J. Reixach and R. Quintanilla and M. Ballester},
keywords = {Carcass quality, Genetic correlations, Immunity, pH, Pig},
abstract = {Carcass and pork traits have traditionally been considered of prime importance in pig breeding programmes. However, the changing conditions in modern farming, coupled with antimicrobial resistance issues, are raising the importance of health and robustness-related traits. Here, we explore the genetic architecture of carcass and pork traits and their relationship with immunity phenotypes in a commercial Duroc pig population. A total of nine traits related to fatness, lean content and meat pH were measured at slaughter (∼190 d of age) in 378 pigs previously phenotyped (∼70 d of age) for 36 immunity-related traits, including plasma concentrations of immunoglobulins, acute-phase proteins, leukocytes subpopulations and phagocytosis. Our study showed medium to high heritabilities and strong genetic correlations between fatness, lean content and meat pH at 24 h postmortem. Genetic correlations were found between carcass and pork traits and white blood cells. pH showed strong positive genetic correlations with leukocytes and eosinophils, and strong negative genetic correlations with haemoglobin, haematocrit and cytotoxic T cell proportion. In addition, genome-wide association studies (GWASs) pointed out four significantly associated genomic regions for lean meat percentages in different muscles, ham fat, backfat thickness, and semimembranosus pH at 24 h. The functional annotation of genes located in these regions reported a total of 14 candidate genes, with BGN, DPP10, LEPR, LEPROT, PDE4B and SLC6A8 being the strongest candidates. After performing an expression GWAS for the expression of these genes in muscle, two signals were detected in cis for the BGN and SLC6A8 genes. Our results indicate a genetic relationship between carcass fatness, lean content and meat pH with a variety of immunity-related traits that should be considered to improve immunocompetence without impairing production traits.}
}
@article{BIMPAS2024110156,
title = {Leveraging pervasive computing for ambient intelligence: A survey on recent advancements, applications and open challenges},
journal = {Computer Networks},
volume = {239},
pages = {110156},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110156},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623006011},
author = {Athanasios Bimpas and John Violos and Aris Leivadeas and Iraklis Varlamis},
keywords = {Artificial intelligence, Ubiquitous computing, Smart environments, Context awareness, Internet of Things, Data mining},
abstract = {The advent of pervasive computing and ambient intelligence has opened up new possibilities for the development of intelligent systems that can adapt to the needs of their users. This paper provides a comprehensive survey of recent advancements in this field, focusing on the different techniques, tools, and applications that have been developed to leverage the benefits of pervasive computing for ambient intelligence. We discuss the various challenges and opportunities that arise when designing such systems and provide a critical evaluation of the current state-of-the-art. In particular, we examine the role of machine learning, data analytics, and context awareness in the development of intelligent systems, and we investigate the impact of these technologies on the quality of life of their users. The paper also presents a discussion on the open challenges in this field, such as ensuring user privacy, security, and trust, designing effective user interfaces, and dealing with the complexity of large-scale systems. Finally, we conclude with an outlook on the future of ambient intelligence and its potential impact on society, and we highlight some of the emerging research directions that are likely to shape the field in the coming years.}
}
@incollection{YOSHINORI2024161,
title = {Chapter Eight - Prediction of mitochondrial targeting signals and their cleavage sites},
editor = {Nils Wiedemann},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {706},
pages = {161-192},
year = {2024},
booktitle = {Mitochondrial Translocases Part A},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2024.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0076687924003628},
author = {Fukasawa Yoshinori and Kenichiro Imai and Paul Horton},
abstract = {In this chapter we survey prediction tools and computational methods for the prediction of amino acid sequence elements which target proteins to the mitochondria. We will primarily focus on the prediction of N-terminal mitochondrial targeting signals (MTSs) and their N-terminal cleavage sites by mitochondrial peptidases. We first give practical details useful for using and installing some prediction tools. Then we describe procedures for preparing datasets of MTS containing proteins for statistical analysis or development of new prediction methods. Following that we lightly survey some of the computational techniques used by prediction tools. Finally, after discussing some caveats regarding the reliability of such methods to predict the effects of mutations on MTS function; we close with a discussion of possible future directions of computer prediction methods related to mitochondrial proteins.}
}
@article{OHSAWA20244843,
title = {Semantic Cells: Evolutional process for item sense disambiguation},
journal = {Procedia Computer Science},
volume = {246},
pages = {4843-4852},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.350},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023755},
author = {Yukio Ohsawa and Dingming Xue and Kaira Sekiguchi},
keywords = {evolutionary computing, word-sense disambiguation, item-sense disambiguation, bio-inspired computation},
abstract = {Previous models for learning the semantic vectors of items and their groups, such as words, sentences, nodes, and graphs, using distributed representation have been based on the assumption that the basic sense of an item corresponds to one vector composed of dimensions corresponding to hidden contexts in the target real world, from which multiple senses of the item are obtained by conforming to lexical databases or adapting to the context. However, there may be multiple senses of an item, which are hardly assimilated and change or evolve dynamically following the contextual shift even within a document or a restricted period. This is a process similar to the evolution or adaptation of a living entity with/to environmental shifts. Setting the scope of disambiguation of items for sensemaking, the author presents a method in which a word or item in the data embraces multiple semantic vectors that evolve via interaction with others, similar to a cell embracing chromosomes crossing over with each other. We obtained a preliminary result: the role of a word that evolves to acquire the largest or lower-middle variance of semantic vectors tends to be explainable by the author of the text.}
}
@article{CHAKRABARTY2024498,
title = {Imaging Analytics using Artificial Intelligence in Oncology: A Comprehensive Review},
journal = {Clinical Oncology},
volume = {36},
number = {8},
pages = {498-513},
year = {2024},
note = {Advances in Imaging for Oncology},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2023.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0936655523003345},
author = {N. Chakrabarty and A. Mahajan},
keywords = {Artificial intelligence, cancer, deep learning, diagnosis-genomic mutations-outcome prediction},
abstract = {The present era has seen a surge in artificial intelligence-related research in oncology, mainly using deep learning, because of powerful computer hardware, improved algorithms and the availability of large amounts of data from open-source domains and the use of transfer learning. Here we discuss the multifaceted role of deep learning in cancer care, ranging from risk stratification, the screening and diagnosis of cancer, to the prediction of genomic mutations, treatment response and survival outcome prediction, through the use of convolutional neural networks. Another role of artificial intelligence is in the generation of automated radiology reports, which is a boon in high-volume centres to minimise report turnaround time. Although a validated and deployable deep-learning model for clinical use is still in its infancy, there is ongoing research to overcome the barriers for its universal implementation and we also delve into this aspect. We also briefly describe the role of radiomics in oncoimaging. Artificial intelligence can provide answers pertaining to cancer management at baseline imaging, saving cost and time. Imaging biobanks, which are repositories of anonymised images, are also briefly described. We also discuss the commercialisation and ethical issues pertaining to artificial intelligence. The latest generation generalist artificial intelligence model is also briefly described at the end of the article. We believe this article will not only enrich knowledge, but also promote research acumen in the minds of readers to take oncoimaging to another level using artificial intelligence and also work towards clinical translation of such research.}
}
@article{2025107047,
title = {List of Editorial Board Members},
journal = {Neural Networks},
volume = {181},
pages = {107047},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107047},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024009766}
}
@article{BERNDT2024622,
title = {A Platform Architecture for Data- and AI-supported human-centred Zero Defect Manufacturing for Sustainable Production⁎⁎The ZERO³ project is funded as a lead project as part of the “Production of the Future – 43rd Call” program of the Austrian Research Promotion Agency (FFG) and the Federal Ministry for Climate Protection, Environment, Energy, Mobility, Innovation and Technology (BMK). – (FFG No.: FO999896399).},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {622-627},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.231},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324016598},
author = {René Berndt and Doriana Cobârzan and Eva Eggeling},
keywords = {Zero defect manufacturing, Sustainable production, Circular economy, Knowledge Management, Decision Support System},
abstract = {The challenge for manufacturing companies lies in efficiently adapting to economic, ecological, and social sustainability while maintaining a competitive edge in the global market, despite the rapid advancements in information, communication, and resource-efficient production processes utilizing robotics and AI-based methods. The key question is: How can knowledge regarding feasibility and implementation possibilities be effectively transferred? In our paper, we propose a platform architecture designed to facilitate knowledge exchange and transfer between technology providers and the manufacturing industry. The platform facilitates the dissemination of innovative methods and technologies, improving collaboration and operational efficiency It will also serve as a repository for insights and experiences from technology implementation, making this knowledge accessible for internal and industry-wide use. The goal is to create a sustainable ecosystem for continuous improvement and competitive advantage in manufacturing, by matching technologies and methodologies to specific production needs using algorithms and tracking sustainability metrics.}
}
@article{FIELDS2025256,
title = {Thoughts and thinkers: On the complementarity between objects and processes},
journal = {Physics of Life Reviews},
volume = {52},
pages = {256-273},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2025.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571064525000089},
author = {Chris Fields and Michael Levin},
keywords = {Active inference, Cognitive light cone, Emergence, Evo/devo/eco, Multiscale competency architecture, Niche construction, Semantics},
abstract = {We argue that “processes versus objects” is not a useful dichotomy. There is, instead, substantial theoretical utility in viewing “objects” and “processes” as complementary ways of describing persistence through time, and hence the possibility of observation and manipulation. This way of thinking highlights the role of memory as an essential resource for observation, and makes it clear that “memory” and “time” are also mutually inter-defined, complementary concepts. We formulate our approach in terms of the Free Energy Principle (FEP) of Friston and colleagues and the fundamental idea from quantum theory that physical interactions can be represented by linear operators. Following Levin (2024) [30], we emphasize that memory is, first and foremost, an interpretative function, from which the idea of memory as a record, at some level of accuracy, of past events is derivative. We conclude that the distinction between objects and processes is always contrived, and always misleading, and that science would be better served by abandoning it entirely.}
}
@article{COLTHER2024100625,
title = {Artificial intelligence: Driving force in the evolution of human knowledge},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100625},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100625},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001641},
author = {Cristian Colther and Jean Pierre Doussoulin},
keywords = {Artificial intelligence, Evolution knowledge, Noosphere, Ethical considerations, Future scenarios},
abstract = {This article proposes that artificial intelligence (AI) is positioned as a key driver of a new evolutionary stage of human knowledge, complementing human intelligence and facilitating the creation and development of sophisticated collective intelligence, defined as the noosphere, understood as the sphere of collective human thought. The study reveals several key insights into the transformative potential of AI, including its capacity to accelerate, mediate, and diffuse human knowledge. It concludes that AI not only catalyzes the existence of the noosphere but also redefines the structures and mechanisms through which human knowledge is expanded and democratized. Additionally, the document presents potential risks and significant ethical, social, and legal challenges of an AI-mediated noosphere, offering recommendations and a research agenda around the topic, and limitations and proposals for improvement to be considered in the future.}
}
@incollection{2024275,
title = {Index},
editor = {Shai Ben-David and Giuseppe Curigliano and David Koff and Barbara Alicja Jereczek-Fossa and Davide {La Torre} and Gabriella Pravettoni},
booktitle = {Artificial Intelligence for Medicine},
publisher = {Academic Press},
pages = {275-281},
year = {2024},
series = {Advanced Studies in Complex Systems: Theory and Applications},
isbn = {978-0-443-13671-9},
doi = {https://doi.org/10.1016/B978-0-443-13671-9.09992-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443136719099920}
}
@article{ZUO2024124884,
title = {Collaborative trajectory representation for enhanced next POI recommendation},
journal = {Expert Systems with Applications},
volume = {256},
pages = {124884},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124884},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424017512},
author = {Jiankai Zuo and Yaying Zhang},
keywords = {Next POI recommendation, Trajectory similarity, Attention mechanism, Representation learning},
abstract = {Point-of-Interest (POI) recommendation stands as the cornerstone within a variety of location-based applications and services that intend to anticipate upcoming movements that users may be interested in. The current state-of-the-art methods have effectively explored spatio-temporal contextual features and users’ long-term and short-term preference patterns. Nevertheless, most existing work lacks the ability to effectively capture group movement patterns from trajectory collaboration. Additionally, they pay close attention to the accuracy of personalized recommendations, neglecting recommendation diversity, which refers to offering broader location options that extend beyond a user’s typical preferences, thereby avoiding excessive homogeneity or repetition. To address these gaps, this study proposes a Collaborative Trajectory Representation model (CTRNext), which enhances the diversity of recommendations while maintaining the precision of personalized preferences. To be specific, we first design two trajectory embedding layers to extract joint semantic interactions and the explicit spatiotemporal context-aware representation. Then, a trajectory semantic similarity calculation module that captures collaborative signals from potentially similar-minded users and eliminates barriers caused by trajectory length is proposed. Next, the implicit correlation and further updated representation between different check-in records are achieved through a multi-head self-attention aggregation module. Finally, we put forward a dual-driven user preference matching module to generate the preference-based next POI recommendation while enhancing diversity. Our approach demonstrates its remarkable recommendation accuracy through extensive experimentation on four real-world datasets, surpassing the performance of state-of-the-art methodologies.}
}
@article{HIMEUR2025102742,
title = {Applications of knowledge distillation in remote sensing: A survey},
journal = {Information Fusion},
volume = {115},
pages = {102742},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102742},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005207},
author = {Yassine Himeur and Nour Aburaed and Omar Elharrouss and Iraklis Varlamis and Shadi Atalla and Wathiq Mansoor and Hussain Al-Ahmad},
keywords = {Knowledge distillation, Model compression, Model and data distillation, Remote sensing, Urban planning and precision agriculture},
abstract = {With the ever-growing complexity of models in the field of remote sensing (RS), there is an increasing demand for solutions that balance model accuracy with computational efficiency. Knowledge distillation (KD) has emerged as a powerful tool to meet this need, enabling the transfer of knowledge from large, complex models to smaller, more efficient ones without significant loss in performance. This review article provides an extensive examination of KD and its innovative applications in RS. KD, a technique developed to transfer knowledge from a complex, often cumbersome model (teacher) to a more compact and efficient model (student), has seen significant evolution and application across various domains. Initially, we introduce the fundamental concepts and historical progression of KD methods. The advantages of employing KD are highlighted, particularly in terms of model compression, enhanced computational efficiency, and improved performance, which are pivotal for practical deployments in RS scenarios. The article provides a comprehensive taxonomy of KD techniques, where each category is critically analyzed to demonstrate the breadth and depth of the alternative options, and illustrates specific case studies that showcase the practical implementation of KD methods in RS tasks, such as instance segmentation and object detection. Further, the review discusses the challenges and limitations of KD in RS, including practical constraints and prospective future directions, providing a comprehensive overview for researchers and practitioners in the field of RS. Through this organization, the paper not only elucidates the current state of research in KD but also sets the stage for future research opportunities, thereby contributing significantly to both academic research and real-world applications.}
}
@article{NOREEN2024109003,
title = {Mono-lingual text reuse detection for the Urdu language at lexical level},
journal = {Engineering Applications of Artificial Intelligence},
volume = {136},
pages = {109003},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624011618},
author = {Ayesha Noreen and Iqra Muneer and Rao Muhammad Adeel Nawab},
keywords = {Text reuse detection, Urdu text reuse detection, Derived, Non-derived, Lexical level},
abstract = {Text reuse is the process of creating new texts from pre-existing ones. In recent years, Urdu Text Reuse Detection (U-TRD) has garnered the attention of researchers due to the ready availability of digital text all over the internet, which can be copied or paraphrased from other sources without proper attribution, making it easier to reuse but challenging to detect. Previous studies have explored the issue of U-TRD at the phrasal, sentence/passage, and document levels, using benchmark corpora and methods. However, the problem of U-TRD has not been investigated at the lexical level in terms of corpora and methods. To address this research gap, our study has developed a large benchmark corpus manually annotated at the lexical level. This corpus consists of 22,184 text pairs categorized into two levels of rewrite: (1) Derived (8,660) and (2) Non-Derived (13,524). Additionally, our research has involved the development, application, evaluation, and comparison of a range of methods, including baseline methods (uni-gram overlap and word embedding-based methods), along with state-of-the-art transformer-based methods and feature-fusion-based methods, using the proposed UTRD-Lex-23 corpus. Our study concludes that one of our proposed feature-fusion methods outperforms all other methods. The model we propose, which combines seven different Sentence Transformers (ST) (each producing 768 dimension vectors) with one uni-gram (at word level) and sixteen different features extracted from four different Word Embedding (WE) based models (yielding 300 dimension vectors), achieves an F1 score of 0.70601 using 10-fold cross validation. To foster and promote research in Urdu (a low-resourced language) proposed corpus will be freely and publicly available for research purposes.}
}
@article{ATMAKURU2025102673,
title = {Artificial intelligence-based suicide prevention and prediction: A systematic review (2019–2023)},
journal = {Information Fusion},
volume = {114},
pages = {102673},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102673},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004512},
author = {Anirudh Atmakuru and Alen Shahini and Subrata Chakraborty and Silvia Seoni and Massimo Salvi and Abdul Hafeez-Baig and Sadaf Rashid and Ru San Tan and Prabal Datta Barua and Filippo Molinari and U Rajendra Acharya},
keywords = {Artificial intelligence, Natural language processing, Machine learning, Mental health, Suicide prevention, Suicide prediction},
abstract = {Suicide is a major global public health concern, and the application of artificial intelligence (AI) methods, such as natural language processing (NLP), machine learning (ML), and deep learning (DL), has shown promise in advancing suicide prediction and prevention efforts. Recent advancements in AI – particularly NLP and DL have opened up new avenues of research in suicide prediction and prevention. While several papers have reviewed specific detection techniques like NLP or DL, there has been no recent study that acts as a one-stop-shop, providing a comprehensive overview of all AI-based studies in this field. In this work, we conduct a systematic literature review to identify relevant studies published between 2019 and 2023, resulting in the inclusion of 156 studies. We provide a comprehensive overview of the current state of research conducted on AI-driven suicide prevention and prediction, focusing on different data types and AI techniques employed. We discuss the benefits and challenges of these approaches and propose future research directions to improve the practical application of AI in suicide research. AI is highly capable of improving the accuracy and efficiency of risk assessment, enabling personalized interventions, and enhancing our understanding of risk and protective factors. Multidisciplinary approaches combining diverse data sources and AI methods can help identify individuals at risk by analyzing social media content, patient histories, and data from mobile devices, enabling timely intervention. However, challenges related to data privacy, algorithmic bias, model interpretability, and real-world implementation must be addressed to realize the full potential of these technologies. Future research should focus on integrating prediction and prevention strategies, harnessing multimodal data, and expanding the scope to include diverse populations. Collaboration across disciplines and stakeholders is essential to ensure that AI-driven suicide prevention and prediction efforts are ethical, culturally sensitive, and person-centered.}
}
@article{MUSTAFA2024101450,
title = {Using social Cognitive theory to reengage dormant users in question and answer Communities: A case study of active StackOverflow participants},
journal = {Electronic Commerce Research and Applications},
volume = {68},
pages = {101450},
year = {2024},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2024.101450},
url = {https://www.sciencedirect.com/science/article/pii/S1567422324000954},
author = {Sohaib Mustafa and Wen Zhang and Muhammad {Mateen Naveed} and Dur e Adan},
keywords = {Low participation, Active participants, Dormant users, Peer recognition, Q&A communities},
abstract = {Online question-and-answer communities are seriously threatened by low user participation. There is currently a rare comprehensive study on the knowledge contribution pattern of consistently active participants and the moderating role of peer recognition, which can help improve low participation and reengage inactive users, despite researchers having examined the various facets of knowledge contribution and made helpful suggestions. As per the self-determination and social cognitive theory, the communal environment impacts peers and imitates role models or reliable sources in their involvement patterns. We have examined StackOverflow’s most reliable active users from 2010 to 2020 using the social cognition and self-determination theories to use the findings to reactivate dormant users. We have used a two-step dynamic system GMM model to get robust and reliable findings. The research discovered that peer repudiation, reputation, and online social interactions favorably affect the contributed knowledge. However, knowledge-seeking and earning virtual badges such as gold and bronze usually negatively impact it. Furthermore, it was revealed that the effect of virtual badges on contributed knowledge was positively moderated by peer recognition. However, peer recognition reduces the benefits of social interaction and reputation on the contributed knowledge. The study’s findings advance the body of knowledge and provide thorough management implications for raising low participation, reengaging inactive users, and cultivating a culture of innovative sharing of knowledge.}
}
@article{ZHANG2024102413,
title = {A survey of route recommendations: Methods, applications, and opportunities},
journal = {Information Fusion},
volume = {108},
pages = {102413},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102413},
url = {https://www.sciencedirect.com/science/article/pii/S156625352400191X},
author = {Shiming Zhang and Zhipeng Luo and Li Yang and Fei Teng and Tianrui Li},
keywords = {Urban computing, Route recommendation, Spatio-temporal learning, Deep learning},
abstract = {Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens’ travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: (1) Methodology-wise. We categorize a large volume of classic methods and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. (2) Application-wise. We present numerous novel applications related to route commendation within urban computing scenarios. (3) We discuss current problems and challenges and envision several promising research directions. We believe that this survey can help relevant researchers quickly familiarize themselves with the current state of route recommendation research and then direct them to future research trends.}
}
@article{ANANIKOV2024100075,
title = {Top 20 influential AI-based technologies in chemistry},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100075},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100075},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000332},
author = {Valentine P. Ananikov},
keywords = {Chemical science, Artificial Intelligence, Digital technologies, Research methodology, Machine Learning, Chemical industry, Digital twins, Blockchain, Large data},
abstract = {The beginning and ripening of digital chemistry is analyzed focusing on the role of artificial intelligence (AI) in an expected leap in chemical sciences to bring this area to the next evolutionary level. The analytic description selects and highlights the top 20 AI-based technologies and 7 broader themes that are reshaping the field. It underscores the integration of digital tools such as machine learning, big data, digital twins, the Internet of Things (IoT), robotic platforms, smart control of chemical processes, virtual reality and blockchain, among many others, in enhancing research methods, educational approaches, and industrial practices in chemistry. The significance of this study lies in its focused overview of how these digital innovations foster a more efficient, sustainable, and innovative future in chemical sciences. This article not only illustrates the transformative impact of these technologies but also draws new pathways in chemistry, offering a broad appeal to researchers, educators, and industry professionals to embrace these advancements for addressing contemporary challenges in the field.}
}
@article{FENZA2024127951,
title = {Robustness of models addressing Information Disorder: A comprehensive review and benchmarking study},
journal = {Neurocomputing},
volume = {596},
pages = {127951},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127951},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224007227},
author = {Giuseppe Fenza and Vincenzo Loia and Claudio Stanzione and Maria {Di Gisi}},
keywords = {Information Disorder, Explainable artificial intelligence, Adversarial attacks, Robustness},
abstract = {Machine learning and deep learning models are increasingly susceptible to adversarial attacks, particularly in critical areas like cybersecurity and Information Disorder. This study provides a comprehensive evaluation of model Robustness against such attacks across key tasks well-assessed in Information Disorder literature: Toxic Speech Detection, Sentiment Analysis, Propaganda Detection, and Hate Speech Detection. Rigorous experiments conducted across 13 models and 12 diverse datasets highlight significant vulnerabilities. The methodological framework implements adversarial attacks that strategically manipulates model inputs based on keyword significance, identified using the LIME method, an advanced explainable AI technique. The evaluation measures Robustness primarily through accuracy of the models and attack success rates. The experiments reveal that current models display inconsistent resistance to adversarial manipulations, underscoring an urgent need for developing more sophisticated defensive strategies. The study sheds light on the critical weaknesses in existing models and charts a course for future research to fortify AI resilience against evolving cyber threats. The findings advocate for a paradigm shift in model training and development to prioritize adversarial Robustness, ensuring that AI systems are equipped to handle real-world adversarial scenarios effectively.}
}
@incollection{2024552,
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {552-598},
year = {2024},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.09001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895090015}
}
@article{LEE2023100174,
title = {High-level implementable methods for automated building code compliance checking},
journal = {Developments in the Built Environment},
volume = {15},
pages = {100174},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100174},
url = {https://www.sciencedirect.com/science/article/pii/S266616592300056X},
author = {Jin-Kook Lee and Kyunghyun Cho and Hyeokjin Choi and Soohyung Choi and Sumin Kim and Seung Hyun Cha},
keywords = {High-level method, Implementable method, Automated building code compliance checking, Building permit, Building information modeling (BIM)},
abstract = {This paper presents an approach for defining high-level implementable methods to improve their low-level rule-checking procedures. This is part of an effort to develop challenging building information modeling (BIM)-enabled applications for building permits through automated code compliance checking. The main approach described here aims to alleviate the time-consuming and error-prone tasks of translating natural language into explicitly defined rules. An explicit expression of design requirements is key to building projects involving collaboration between architects, code experts, and developers. To maximize the generalization, neutralization, and reusability of the given rules in natural (written) language, we propose a series of high-level implementable computer programming methods (operators); these can be beneficial for translating verb phrases in building act sentences with minimal ambiguity as well as representing the peculiar properties of building objects without conflicts. Lastly, we demonstrate its application in code compliance checking by employing KBimCode and the developed rule-checking software.}
}
@incollection{ANJUM2025349,
title = {Chapter 19 - Artificial intelligence and deep learning in single-cell omics data analysis: A case study},
editor = {Khalid Raza},
booktitle = {Deep Learning in Genetics and Genomics},
publisher = {Academic Press},
pages = {349-383},
year = {2025},
isbn = {978-0-443-27523-4},
doi = {https://doi.org/10.1016/B978-0-443-27523-4.00007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044327523400007X},
author = {Zubina Anjum and Waniya Khalid and Gurupriya Takkar and Pakhi Chhetri and Khalid Raza},
keywords = {Biological data analysis, Multiomics, Next-generation sequencing, Single cell},
abstract = {The advent of single-cell genomics has led to a new era of biological exploration, allowing researchers to investigate the intricacies of cellular heterogeneity deeply. The ability of artificial intelligence (AI)-driven models to unveil hidden patterns in single-cell expression data, providing the classification of cell types based on gene expression profiles. Deep learning methods in single-cell analysis offer a comprehensive view of cellular biology. This chapter explores the significant role of AI and deep learning methodologies in the analysis and interpretation of single-cell genomic data. Various deep learning models along with their roles in single-cell analysis are discussed. Furthermore, the chapter highlights the role of AI and deep learning in connecting complex model outputs with biological understanding, reshaping our knowledge of cellular complexity, and driving groundbreaking discoveries in disease biology and personalized medicine. Furthermore, challenges and future research directions have been discussed.}
}
@article{MICHALOWSKI2024104681,
title = {Provision and evaluation of explanations within an automated planning-based approach to solving the multimorbidity problem},
journal = {Journal of Biomedical Informatics},
volume = {156},
pages = {104681},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104681},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000996},
author = {Martin Michalowski and Szymon Wilk and Wojtek Michalowski and Malvika Rao and Marc Carrier},
keywords = {Explainability, Automated planning, Multimorbidity, Clinical decision support},
abstract = {The multimorbidity problem involves the identification and mitigation of adverse interactions that occur when multiple computer interpretable guidelines are applied concurrently to develop a treatment plan for a patient diagnosed with multiple diseases. Solving this problem requires decision support approaches which are difficult to comprehend for physicians. As such, the rationale for treatment plans generated by these approaches needs to be provided.
Objective:
To develop an explainability component for an automated planning-based approach to the multimorbidity problem, and to assess the fidelity and interpretability of generated explanations using a clinical case study.
Methods:
The explainability component leverages the task-network model for representing computer interpretable guidelines. It generates post-hoc explanations composed of three aspects that answer why specific clinical actions are in a treatment plan, why specific revisions were applied, and how factors like medication cost, patient’s adherence, etc. influence the selection of specific actions. The explainability component is implemented as part of MitPlan, where we revised our planning-based approach to support explainability. We developed an evaluation instrument based on the system causability scale and other vetted surveys to evaluate the fidelity and interpretability of its explanations using a two dimensional comparison study design.
Results:
The explainability component was implemented for MitPlan and tested in the context of a clinical case study. The fidelity and interpretability of the generated explanations were assessed using a physician-focused evaluation study involving 21 participants from two different specialties and two levels of experience. Results show that explanations provided by the explainability component in MitPlan are of acceptable fidelity and interpretability, and that the clinical justification of the actions in a treatment plan is important to physicians.
Conclusion:
We created an explainability component that enriches an automated planning-based approach to solving the multimorbidity problem with meaningful explanations for actions in a treatment plan. This component relies on the task-network model to represent computer interpretable guidelines and as such can be ported to other approaches that also use the task-network model representation. Our evaluation study demonstrated that explanations that support a physician’s understanding of the clinical reasons for the actions in a treatment plan are useful and important.}
}
@article{BERGER2024106003,
title = {Towards reusable building blocks for agent-based modelling and theory development},
journal = {Environmental Modelling & Software},
volume = {175},
pages = {106003},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224000641},
author = {Uta Berger and Andrew Bell and C. Michael Barton and Emile Chappin and Gunnar Dreßler and Tatiana Filatova and Thibault Fronville and Allen Lee and Emiel {van Loon} and Iris Lorscheid and Matthias Meyer and Birgit Müller and Cyril Piou and Viktoriia Radchuk and Nicholas Roxburgh and Lennart Schüler and Christian Troost and Nanda Wijermans and Tim G. Williams and Marie-Christin Wimmler and Volker Grimm},
keywords = {Individual-based modelling, Theory development, Complex adaptive systems, Software engineering, Best practices},
abstract = {Despite the increasing use of standards for documenting and testing agent-based models (ABMs) and sharing of open access code, most ABMs are still developed from scratch. This is not only inefficient, but also leads to ad hoc and often inconsistent implementations of the same theories in computational code and delays progress in the exploration of the functioning of complex social-ecological systems (SES). We argue that reusable building blocks (RBBs) known from professional software development can mitigate these issues. An RBB is a submodel that represents a particular mechanism or process that is relevant across many ABMs in an application domain, such as plant competition in vegetation models, or reinforcement learning in a behavioural model. RBBs need to be distinguished from modules, which represent entire subsystems and include more than one mechanism and process. While linking modules faces the same challenges as integrating different models in general, RBBs are “atomic” enough to be more easily re-used in different contexts. We describe and provide examples from different domains for how and why building blocks are used in software development, and the benefits of doing so for the ABM community and to individual modellers. We propose a template to guide the development and publication of RBBs and provide example RBBs that use this template. Most importantly, we propose and initiate a strategy for community-based development, sharing and use of RBBs. Individual modellers can have a much greater impact in their field with an RBB than with a single paper, while the community will benefit from increased coherence, facilitating the development of theory for both the behaviour of agents and the systems they form. We invite peers to upload and share their RBBs via our website - preferably referenced by a DOI (digital object identifier obtained e.g. via Zenodo). After a critical mass of candidate RBBs has accumulated, feedback and discussion can take place and both the template and the scope of the envisioned platform can be improved.}
}
@incollection{SUN20241,
title = {Chapter 1 - History of graph computing and graph databases},
editor = {Ricky Sun},
booktitle = {The Essential Criteria of Graph Databases},
publisher = {Elsevier},
pages = {1-32},
year = {2024},
isbn = {978-0-443-14162-1},
doi = {https://doi.org/10.1016/B978-0-443-14162-1.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443141621000040},
author = {Ricky Sun},
keywords = {Big data, Graph thinking, Graph database, Graph computing, Knowledge graph, Network analysis},
abstract = {This chapter introduces the core concept throughout this book—graph thinking. Concrete and visualized real-world examples were given in the first section to facilitate the readers to understand the depth and breadth of the concept, and how to put it to work. The first section is completed with a review of the historical development of graph theory and technologies. The second section gives an overview of how data processing technologies and frameworks have evolved from relational databases to big-data frameworks and eventually to graph databases, and insights into their differences. The final section focuses on introducing the amazing and unprecedented capabilities of graph databases, again, with real-world practical examples. This section ended with a comparison of graph computing and graph databases, hoping to clarify any potential confusion between the two topics.}
}
@article{SHENG2024569,
title = {Artificial intelligence for diabetes care: current and future prospects},
journal = {The Lancet Diabetes & Endocrinology},
volume = {12},
number = {8},
pages = {569-595},
year = {2024},
issn = {2213-8587},
doi = {https://doi.org/10.1016/S2213-8587(24)00154-2},
url = {https://www.sciencedirect.com/science/article/pii/S2213858724001542},
author = {Bin Sheng and Krithi Pushpanathan and Zhouyu Guan and Quan Hziung Lim and Zhi Wei Lim and Samantha Min Er Yew and Jocelyn Hui Lin Goh and Yong Mong Bee and Charumathi Sabanayagam and Nick Sevdalis and Cynthia Ciwei Lim and Chwee Teck Lim and Jonathan Shaw and Weiping Jia and Elif Ilhan Ekinci and Rafael Simó and Lee-Ling Lim and Huating Li and Yih-Chung Tham},
abstract = {Summary
Artificial intelligence (AI) use in diabetes care is increasingly being explored to personalise care for people with diabetes and adapt treatments for complex presentations. However, the rapid advancement of AI also introduces challenges such as potential biases, ethical considerations, and implementation challenges in ensuring that its deployment is equitable. Ensuring inclusive and ethical developments of AI technology can empower both health-care providers and people with diabetes in managing the condition. In this Review, we explore and summarise the current and future prospects of AI across the diabetes care continuum, from enhancing screening and diagnosis to optimising treatment and predicting and managing complications.}
}
@article{RAJ2024201309,
title = {Classify Alzheimer genes association using Naïve Bayes algorithm},
journal = {Human Gene},
volume = {41},
pages = {201309},
year = {2024},
issn = {2773-0441},
doi = {https://doi.org/10.1016/j.humgen.2024.201309},
url = {https://www.sciencedirect.com/science/article/pii/S2773044124000536},
author = {Sushrutha Raj and Anchal Vishnoi and Alok Srivastava},
keywords = {Disease gene associations, Alzheimer's candidate genes, Machine learning, Text mining, Text classification, Cross validation},
abstract = {Background
Alzheimer's disease, the most common form of dementia, accounts for 60–80% of cases and its prevalence is projected to increase as aging populations grow. By 2050, the number of individuals with Alzheimer's and dementia worldwide is expected to reach 152 million. Genetics plays a significant role, contributing to about 70% of the overall risk, underscoring the importance of understanding the genetic basis for developing targeted interventions. This study presents a system that combines text mining and machine learning techniques to identify and prioritize prospective candidate genes for Alzheimer's and further classifies them into three association classes with weights.
Methods
The machine learning-based classifier was trained over a meticulously curated gold standard dataset and then rigorously validated utilizing a 10-fold cross-validation method, demonstrating its consistency across all the folds of the data. This developed ensemble learning system categorizes PubMed abstracts into three distinct groups: Yes, No, and Ambiguous using text mining and a Bayesian classification algorithm. The system further predicts disease-gene associations over unknown disease-specific prediction data by using the developed classifier.
Results
With an average accuracy of 87.33% and confidence level of 90.10% +/− 0.142, the protocol effectively extracted 2031 associated genes, of which 1162, 489 and 1439 belong to positive, negative and ambiguous classes respectively at the threshold of 0.9. In comparison between the established disease gene databases, our system identified 915 positive genes that had not been previously reported. One can use these positive genes for in-depth understanding and ambiguous genes for further exploration of their association with Alzheimer's disease.
Conclusions
The system's ability to generate accurate predictions demonstrates its robustness and provides valuable insights into the genetic factors of Alzheimer's disease. Consequently, this study contributes to existing knowledge and paves the way for future research in this field.}
}
@article{QIAN2024104453,
title = {Evaluating resilience of urban lifelines against flooding in China using social media data},
journal = {International Journal of Disaster Risk Reduction},
volume = {106},
pages = {104453},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104453},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924002152},
author = {Jiale Qian and Yunyan Du and Fuyuan Liang and Jiawei Yi and Nan Wang and Wenna Tu and Sheng Huang and Tao Pei and Ting Ma and Keith Burghardt and Kristina Lerman},
keywords = {Early warning, Flood, Resilience, Rumor spreading, Social media, Urban lifelines},
abstract = {Urban lifelines are the backbone of fundamental services that require stability to enable all other aspects of society to function during natural hazards. However, few studies have focused on measuring lifeline performance and resilience to hazards, especially from the public perspective. In this study, taking flood as the research object, we developed an enhanced urban lifelines classification scheme that integrates the Federal Emergency Management Agency (FEMA)'s lifeline framework with the co-occurrence relationships of keywords derived from 430 million Weibo posts. Leveraging this framework, we formulated two key indicators: Public Concern Ratio and Public Emotion Ratio, to evaluate the resilience of urban lifelines during the 2017 and 2020 flood in China. The results demonstrate the robust resilience of urban lifelines against flooding, with notable improvements over time. The study also identifies certain vulnerable lifelines, notably in the ability of early warning and control of rumor spreading, which often lead to an increase in social media posts expressing negative emotions during flooding. These areas are pinpointed for necessary enhancements. Employing a data-driven methodology, the study provides a novel and insightful approach to assessing urban lifeline resilience against flooding.}
}
@incollection{2025247,
title = {Index},
editor = {Himanshu Arora},
booktitle = {Artificial Intelligence in Urologic Malignancies},
publisher = {Academic Press},
pages = {247-258},
year = {2025},
isbn = {978-0-443-15504-8},
doi = {https://doi.org/10.1016/B978-0-443-15504-8.00014-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443155048000144}
}
@article{WU2025107203,
title = {Multi-knowledge informed deep learning model for multi-point prediction of Alzheimer’s disease progression},
journal = {Neural Networks},
volume = {185},
pages = {107203},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107203},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025000826},
author = {Kai Wu and Hong Wang and Feiyan Feng and Tianyu Liu and Yanshen Sun},
keywords = {Alzheimer’s disease (AD) progression, Multi-point prediction, Multi-knowledge informed, Dual path feature extraction, Visual feature},
abstract = {The diagnosis of Alzheimer’s disease (AD) based on visual features-informed by clinical knowledge has achieved excellent results. Our study endeavors to present an innovative and detailed deep learning framework designed to accurately predict the progression of Alzheimer’s disease. We propose Mul-KMPP, a Multi-Knowledge Informed Deep Learning Model for Multi-Point Prediction of AD progression, intended to facilitate precise assessments of AD progression in older adults. Firstly, we designed a dual-path methodology to capture global and local brain characteristics for visual feature extraction (utilizing MRIs). Then, we developed a diagnostic module before the prediction module, leveraging AAL (Anatomical Automatic Labeling) knowledge. Following this, predictions are informed by clinical insights. For this purpose, we devised a new composite loss function, including diagnosis loss, prediction loss, and consistency loss of the two modules. To validate our model, we compiled a dataset comprising 819 samples and the results demonstrate that our Mul-KMPP model achieved an accuracy of 86.8%, sensitivity of 86.1%, specificity of 92.1%, and area under the curve (AUC) of 95.9%, significantly outperforming several competing diagnostic methods at every time point. The source code for our model is available at https://github.com/Camelus-to/Mul-KMPP.}
}
@article{DEASY2024379,
title = {Data Science Opportunities To Improve Radiotherapy Planning and Clinical Decision Making},
journal = {Seminars in Radiation Oncology},
volume = {34},
number = {4},
pages = {379-394},
year = {2024},
note = {Future of Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2024.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053429624000638},
author = {Joseph O. Deasy},
abstract = {Radiotherapy aims to achieve a high tumor control probability while minimizing damage to normal tissues. Personalizing radiotherapy treatments for individual patients, therefore, depends on integrating physical treatment planning with predictive models of tumor control and normal tissue complications. Predictive models could be improved using a wide range of rich data sources, including tumor and normal tissue genomics, radiomics, and dosiomics. Deep learning will drive improvements in classifying normal tissue tolerance, predicting intra-treatment tumor changes, tracking accumulated dose distributions, and quantifying the tumor response to radiotherapy based on imaging. Mechanistic patient-specific computer simulations (‘digital twins’) could also be used to guide adaptive radiotherapy. Overall, we are entering an era where improved modeling methods will allow the use of newly available data sources to better guide radiotherapy treatments.}
}
@article{BARBEROAPARICIO2024102035,
title = {Addressing data scarcity in protein fitness landscape analysis: A study on semi-supervised and deep transfer learning techniques},
journal = {Information Fusion},
volume = {102},
pages = {102035},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102035},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003512},
author = {José A. Barbero-Aparicio and Alicia Olivares-Gil and Juan J. Rodríguez and César García-Osorio and José F. Díez-Pastor},
keywords = {Bioinformatics, Machine learning, Transfer learning, Semi-supervised learning, Protein fitness prediction, Small datasets},
abstract = {This paper presents a comprehensive analysis of deep transfer learning methods, supervised methods, and semi-supervised methods in the context of protein fitness prediction, with a focus on small datasets. The analysis includes the exploration of the combination of different data sources to enhance the performance of the models. While deep learning and deep transfer learning methods have shown remarkable performance in situations with abundant data, this study aims to address the more realistic scenario faced by wet lab researchers, where labeled data is often limited. The novelty of this work lies in its examination of deep transfer learning in the context of small datasets and its consideration of semi-supervised methods and multi-view strategies. While previous research has extensively explored deep transfer learning in large dataset scenarios, little attention has been given to its efficacy in small dataset settings or its comparison with semi-supervised approaches. Our findings suggest that deep transfer learning, exemplified by ProteinBERT, shows promising performance in this context compared to the rest of the methods across various evaluation metrics, not only in small dataset contexts but also in large dataset scenarios. This highlights the robustness and versatility of deep transfer learning in protein fitness prediction tasks, even with limited labeled data. The results of this study shed light on the potential of deep transfer learning as a state-of-the-art approach in the field of protein fitness prediction. By leveraging pre-trained models and fine-tuning them on small datasets, researchers can achieve competitive performance surpassing traditional supervised and semi-supervised methods. These findings provide valuable insights for wet lab researchers who face the challenge of limited labeled data, enabling them to make informed decisions when selecting the most effective methodology for their specific protein fitness prediction tasks. Additionally, the study investigated the combination of two different sources of information (encodings) through our enhanced semi-supervised methods, yielding noteworthy results improving their base model and providing valuable insights for further research. The presented analysis contributes to a better understanding of the capabilities and limitations of different learning approaches in small dataset scenarios, ultimately aiding in the development of improved protein fitness prediction methods.}
}
@article{LIU2024102592,
title = {A review of digital twin capabilities, technologies, and applications based on the maturity model},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102592},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102592},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002404},
author = {Yang Liu and Jun Feng and Jiamin Lu and Siyuan Zhou},
keywords = {Digital twin, DT, Maturity model, Digital twin capabilities, Technologies and applications},
abstract = {The advanced stage of Industry 4.0 is characterized by the integration and interaction between physical and virtual spaces, and Digital Twin (DT) technology, congruent with this vision, has garnered extensive attention and has undergone large-scale implementation. Yet, in the practical implementation of Digital Twin projects, several issues persist: ① How to formulate reasonable task objectives and action plans before project implementation? ② how to determine and assess the development level of the digital twin during project implementation? ③ How to evaluate the effectiveness of digital twin after project completion and how to enhance improvement in the next steps? Consequently, a methodological model is urgently needed to evaluate the development process of Digital Twins, offering a benchmark for their design, development, and appraisal. To address these issues, this paper introduces a five-level Digital Twin Maturity Model (DTMM), which systematically aligns DT capabilities, phased objectives, and technical requirements within a unified framework, creating a theoretical system capable of assessing DT’s developmental level and specifying its construction trajectory. Further, this paper catalogs supporting tools aligned with the technical specifications stipulated in DTMM’s functional capabilities, aiding developers in devising implementation strategies. Additionally, it scrutinizes the application status across six DT vertical sectors, conducts maturity evaluations, and confirms the efficacy of the proposed model. The conclusion can be drawn that DT is still in its embryonic phase. This work aspires to assist project managers and public policymakers gain a more objective understanding of Digital Twin, offering references to facilitate their positive development and broader implementation.}
}
@incollection{2025299,
title = {Index},
editor = {Miltiadis D. Lytras and Abdulrahman Housawi and Basim S. Alsaywid and Naif Radi Aljohani},
booktitle = {Next Generation eHealth},
publisher = {Academic Press},
pages = {299-305},
year = {2025},
series = {Next Generation Technology Driven Personalized Medicine And Smart Healthcare},
isbn = {978-0-443-13619-1},
doi = {https://doi.org/10.1016/B978-0-443-13619-1.20001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443136191200019}
}
@article{LEE2024108219,
title = {GraphRec-based Korean expert recommendation using author contribution index and the paper abstracts in marine},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108219},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108219},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003774},
author = {Jeong-Wook Lee and Jae-Hoon Kim},
keywords = {Recommendation system, Marine expert recommendation system, Graph neural networks},
abstract = {Expert recommendation systems recommend specialized experts in a particular field to users based on the knowledge of those experts. However, these systems are limited by the number of experts available and the potential for subjective evaluation, which may result in inappropriate recommendations. Furthermore, we explore the evolution from traditional to deep learning-based recommendation systems, emphasizing graph-based recommendation systems. Nonetheless, deep learning-based systems require large amounts of data, and marine expert recommendation training data are scarce. To address these issues, we constructed and utilized marine expert data in this study. The dataset contains abstracts of marine-related papers and information on their authors. Graphs were generated by assessing the similarity among the abstracts, representing them in a graph format indicative of this similarity, and using the author contribution index to depict the relationship between the abstracts and their respective authors. Various similarity methods and abstract embedding techniques were experimentally explored to realize performance optimization. In the experiments, the optimized model achieved a mean absolute error of 0.7556 and a root-mean-squared error of 1.0421. Notably, this study highlights the limitations of traditional evaluation metrics and proposes the averaged mean reciprocal rank as a suitable alternative. This metric facilitates the quantitative evaluation of model performance on newly created data, obviating a comparison model. Finally, applying the newly constructed data to the GraphRec model by using their graphical representation significantly improves the system performance.}
}
@article{CEN2024102032,
title = {Towards interpretable imaging genomics analysis: Methodological developments and applications},
journal = {Information Fusion},
volume = {102},
pages = {102032},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102032},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003482},
author = {Xiaoping Cen and Wei Dong and Wei Lv and Yi Zhao and Fred Dubee and Alexios-Fotios A. Mentis and Dragomirka Jovic and Huanming Yang and Yixue Li},
keywords = {Interpretability, Imaging genomics, Genotype-phenotype association, Precision medicine},
abstract = {Identifying the relationship between imaging features and genetic variation (a term coined“imaging genomics”) offers valuable insight into the pathogenesis of cancer, as well as cognitive and psychiatric disorders. However, how to integrate -omics and imaging data in a biologically meaningful approach to both provide pathobiological underpinning and achieve clinical applications remain challenging. In this review, we aim to discuss the difficulties in combining different sources of data and explaining the genotype-phenotype association, and we attempt to discuss the potential applications of imaging genomics in several (patho)physiological questions to better understand human physiology and disease. Future efforts on deciphering the genotype-phenotype landscapes by interpretable imaging genomics analyses are needed to provide new insights into molecular biology and molecular medicine.}
}
@article{HENRIQUE2024100043,
title = {Trust in artificial intelligence: Literature review and main path analysis},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100043},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000033},
author = {Bruno Miranda Henrique and Eugene Santos},
keywords = {Artificial intelligence, Trust, Trust calibration, Literature review, Main path analysis},
abstract = {Artificial Intelligence (AI) is present in various modern systems, but it is still subject to acceptance in many fields. Medical diagnosis, autonomous driving cars, recommender systems and robotics are examples of areas in which some humans distrust AI technology, which ultimately leads to low acceptance rates. Conversely, those same applications can have humans who over rely on AI, acting as recommended by the systems with no criticism regarding the risks of a wrong decision. Therefore, there is an optimal balance with respect to trust in AI, achieved by calibration of expectations and capabilities. In this context, the literature about factors influencing trust in AI and its calibration is scattered among research fields, with no objective summaries of the overall evolution of the theme. In order to close this gap, this paper contributes a literature review of the most influential papers on the subject of trust in AI, selected by quantitative methods. It also proposes a Main Path Analysis of the literature, highlighting how the theme has evolved over the years. As results, researchers will find an overview on trust in AI based on the most important papers objectively selected and also tendencies and opportunities for future research.}
}
@article{MOSQUERA2024107492,
title = {Understanding the landscape of software modelling assistants for MDSE tools: A systematic mapping},
journal = {Information and Software Technology},
volume = {173},
pages = {107492},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107492},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000971},
author = {David Mosquera and Marcela Ruiz and Oscar Pastor and Jürgen Spielberger},
keywords = {Modelling assistance, Model-driven development, Systematic mapping, State of the practice, Low code, No-code},
abstract = {Context
Model Driven Software Engineering (MDSE) and low-code/no-code software development tools promise to increase quality and productivity by modelling instead of coding software. One of the major advantages of modelling software is the increased possibility of involving diverse stakeholders since it removes the barrier of being IT experts to actively participate in software production processes. From an academic and industry point of view, the main question remains: What has been proposed to assist humans in software modelling tasks?
Objective
In this paper, we systematically elucidate the state of the art in assistants for software modelling and their use in MDSE and low-code/no-code tools.
Method
We conducted a systematic mapping to review the state of the art and answer the following research questions: i) how is software modelling assisted? ii) what goals and limitations do existing modelling assistance proposals report? iii) which evaluation metrics and target users do existing modelling assistance proposals consider? For this purpose, we selected 58 proposals from 3.176 screened records and reviewed 17 MDSE and low-code/no-code tools from main market players published by the Gartner Magic Quadrant.
Result
We clustered existing proposals regarding their modelling assistance strategies, goals, limitations, evaluation metrics, and target users, both in research and practice.
Conclusions
We found that both academic and industry proposals recognise the value of assisting software modelling. However, documentation about MDSE assistants’ limitations, evaluation metrics, and target users is scarce or non-existent. With the advent of artificial intelligence, we expect more assistants for MDSE and low-code/no-code software development will emerge, making imperative the need for well-founded frameworks for designing modelling assistants focused on addressing target users’ needs and advancing the state of the art.}
}
@article{SICILIANI2023102284,
title = {AI-based decision support system for public procurement},
journal = {Information Systems},
volume = {119},
pages = {102284},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102284},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001205},
author = {Lucia Siciliani and Vincenzo Taccardi and Pierpaolo Basile and Marco {Di Ciano} and Pasquale Lops},
keywords = {E-procurement, Data analysis, Data visualisation, Natural language processing, Semantic search, Decision support systems},
abstract = {Tenders are powerful means of investment of public funds and represent a strategic development resource. Thus, improving the efficiency of procuring entities and developing evaluation models turn out to be essential to facilitate e-procurement procedures. With this contribution, we introduce our research to create a supporting system for the decision-making and monitoring process during the entire course of investments and contracts. This system employs artificial intelligence techniques based on natural language processing, focused on providing instruments for extracting useful information from both structured and unstructured (i.e., text) data. Therefore, we developed a framework based on a web app that provides integrated tools such as a semantic search engine, a summariser, an open information extraction engine in the form of triples (subject–predicate–object) for tender documents, and dashboards for analysing tender data.}
}
@article{KHALID2024102344,
title = {Repairing raw metadata for metadata management},
journal = {Information Systems},
volume = {122},
pages = {102344},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102344},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924000024},
author = {Hiba Khalid and Esteban Zimányi},
keywords = {Data preparation, Metadata management, Metadata representation, Metadata categorization},
abstract = {With the exponential growth of data production, the generation of metadata has become an integral part of the process. Metadata plays a crucial role in facilitating enhanced data analytics, data integration, and resource management by offering valuable insights. However, inconsistencies arise due to deviations from standards in metadata recording, including missing attribute information, publishing URLs, and provenance. Furthermore, the recorded metadata may exhibit inconsistencies, such as varied value formats, special characters, and inaccurately entered values. Addressing these inconsistencies through metadata preparation can greatly enhance the user experience during data management tasks. This paper introduces MDPrep, a system that explores the usability and applicability of data preparation techniques in improving metadata quality. Our approach involves three steps: (1) detecting and identifying problematic metadata elements and structural issues, (2) employing a keyword-based approach to enhance metadata elements and a syntax-based approach to rectify structural metadata issues, and (3) comparing the outcomes to ensure improved readability and reusability of prepared metadata files.}
}
@article{VELDHUIS2025100708,
title = {Critical Artificial Intelligence literacy: A scoping review and framework synthesis},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100708},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000771},
author = {Annemiek Veldhuis and Priscilla Y. Lo and Sadhbh Kenny and Alissa N. Antle},
keywords = {Artificial intelligence, Critical literacy, AI ethics, AI literacy, Computational empowerment, Literature review},
abstract = {The proliferation of Artificial Intelligence (AI) in everyday life raises concerns for children, other marginalized groups, and the general public. As new AI implementations continue to emerge, it is crucial to enable children to engage critically with AI. Critical literacy objectives and practices can encourage children to question, critique, and transform the social, political, cultural, and ethical implications of AI. As an initial step towards critical AI education, we conducted a 10-year scoping review to identify publications reporting on activities that engage children, between the ages of 5 and 18, to address the critical implications of AI. Our review identifies a wide range of participants, content, and pedagogical approaches. Through framework synthesis guided by an established critical literacy model, we examine the critical literacy learning objectives embedded in the reported activities and propose a critical AI literacy framework. This paper outlines future opportunities for critical AI literacies in the field of child–computer interaction including inspiring new learning activities, encouraging inclusive perspectives, and supporting pragmatic curriculum integration.}
}
@article{DENG2025104132,
title = {Edge-featured multi-hop attention graph neural network for intrusion detection system},
journal = {Computers & Security},
volume = {148},
pages = {104132},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104132},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004371},
author = {Ping Deng and Yong Huang},
keywords = {Multi-hop attention, Graph neural networks, Intrusion detection, Internet of Things},
abstract = {With the development of the Internet, the application of computer technology has rapidly become widespread, driving the progress of Internet of Things (IoT) technology. The attacks present on networks have become more complex and stealthy. However, traditional network intrusion detection systems with singular functions are no longer sufficient to meet current demands. While some machine learning-based network intrusion detection systems have emerged, traditional machine learning methods cannot effectively respond to the complex and dynamic nature of network attacks. Intrusion detection systems utilizing deep learning can better enhance detection capabilities through diverse data learning and training. To capture the topological relationships in network data, using graph neural networks (GNNs) is most suitable. Most existing GNNs for intrusion detection use multi-layer network training, which may lead to over-smoothing issues. Additionally, current intrusion detection solutions often lack efficiency. To mitigate the issues mentioned above, this paper proposes an Edge-featured Multi-hop Attention Graph Neural Network for Intrusion Detection System (EMA-IDS), aiming to improve detection performance by capturing more features from data flows. Our method enhances computational efficiency through attention propagation and integrates node and edge features, fully leveraging data characteristics. We carried out experiments on four public datasets, which are NF-CSE-CIC-IDS2018-v2, NF-UNSW-NB15-v2, NF-BoT-IoT, and NF-ToN-IoT. Compared with existing models, our method demonstrated superior performance.}
}
@article{TROUMPOUKIS2024505,
title = {European AI and EO convergence via a novel community-driven framework for data-intensive innovation},
journal = {Future Generation Computer Systems},
volume = {160},
pages = {505-521},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24003133},
author = {Antonis Troumpoukis and Iraklis Klampanos and Despina-Athanasia Pantazi and Mohanad Albughdadi and Vasileios Baousis and Omar Barrilero and Alexandra Bojor and Pedro Branco and Lorenzo Bruzzone and Andreina Chietera and Philippe Fournand and Richard Hall and Michele Lazzarini and Adrian Luna and Alexandros Nousias and Christos Perentis and George Petrakis and Dharmen Punjani and David Röbl and George Stamoulis and Eleni Tsalapati and Indrė Urbanavičiūtė and Giulio Weikmann and Xenia Ziouvelou and Marcin Ziółkowski and Manolis Koubarakis and Vangelis Karkaletsis},
keywords = {Artificial Intelligence, Earth observation, DIAS, Applications, Case-study, Methodology, Platform},
abstract = {Artificial Intelligence (AI) represents a collection of tools and methodologies that have the potential to revolutionise various aspects of human activity. Earth observation (EO) data, including satellite and in-situ, are essential in a number of high impact applications, ranging from security and energy to agriculture and health. In this paper, we present the AI4Copernicus framework for bridging the two domains within the European context to enable data-centred innovation. In order to achieve this goal, AI4Copernicus has developed and enriches the European AI-on-demand platform with a number of application bootstrapping services and tools to accelerate uptake and innovation, whilst it provides integration over AI-on-Demand services and the Copernicus ecosystem, targeting the highly successful Data and Information Access Service (DIAS) Cloud platforms. More specifically, by employing procedures for onboarding and validating models and tools, and by utilising a host of meticulously reviewed and supervised open calls-enabled projects, and containerisation best-practices, AI4Copernicus deployed and made available several products on DIAS platforms. Moreover, these products and resources have been made available on the AI-on-Demand platform catalogue for discovery, use and further development. The AI4Copernicus framework is being used by a number of business-driven projects and SMEs spanning several application domains. This article provides an overview of the European AI and EO context as well as the AI4Copernicus technological framework and tools offered. Further, we present real world use-cases as well as a community-centred evaluation of our framework based on usage and feedback received from several projects.}
}
@article{ZHANG2025111920,
title = {M3NetFlow: A multi-scale multi-hop graph AI model for integrative multi-omic data analysis},
journal = {iScience},
pages = {111920},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111920},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225001804},
author = {Heming Zhang and S. Peter Goedegebuure and Li Ding and David DeNardo and Ryan C. Fields and Michael Province and Yixin Chen and Philip Payne and Fuhai Li},
abstract = {Summary
Multi-omic data-driven studies are at the forefront of precision medicine by characterizing complex disease signaling systems across multiple views and levels. The integration and interpretation of multi-omic data are critical for identifying disease targets and deciphering disease signaling pathways. However, it remains an open problem due to the complex signaling interactions among many proteins. Herein, we propose a Multi-scale Multi-hop Multi-omic Network Flow model, M3NetFlow, to facilitate both hypothesis-guided and generic multi-omic data analysis tasks. We evaluated M3NetFlow using two independent case studies: 1) uncovering mechanisms of synergy of drug combinations (hypothesis/anchor-target guided multi-omic analysis), and 2) identifying biomarkers of Alzheimer ’s disease (generic multi-omic analysis). The evaluation and comparison results showed M3NetFlow achieved the best prediction accuracy and identified a set of drug combination synergy and disease associated targets. The model can be directly applied to other multi-omic data-driven studies. The code is publicly accessible at: https://github.com/FuhaiLiAiLab/M3NetFlow}
}
@article{WANG2024105602,
title = {Proactive safety hazard identification using visual–text semantic similarity for construction safety management},
journal = {Automation in Construction},
volume = {166},
pages = {105602},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105602},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003388},
author = {Yiheng Wang and Bo Xiao and Ahmed Bouferguene and Mohamed Al-Hussein},
keywords = {Behavior-based safety, Computer vision, Natural language processing, Image captioning, Automatic safety hazard identification},
abstract = {Automated safety management in construction can reduce injuries by identifying hazardous postures, actions, and missing personal protective equipment (PPE). However, existing computer vision (CV) methods have limitations in connecting recognition results to text-based safety rules. To address this issue, this paper presents a multi-modal framework that bridges the gap between construction image monitoring and safety knowledge. The framework includes an image processing module that utilizes CV and dense image captioning techniques, and a text processing module that employs natural language processing for semantic similarity evaluation. Experiments showed a mean average precision of 49.6% in dense captioning and an F1 score of 74.3% in hazard identification. While the proposed framework demonstrates a promising multi-modal approach towards automated safety hazard identification and reasoning, improvements in dataset size and model performance are still needed to enhance its effectiveness in real-world applications.}
}
@article{DAS2024382,
title = {Towards the development of an explainable e-commerce fake review index: An attribute analytics approach},
journal = {European Journal of Operational Research},
volume = {317},
number = {2},
pages = {382-400},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724001826},
author = {Ronnie Das and Wasim Ahmed and Kshitij Sharma and Mariann Hardey and Yogesh K. Dwivedi and Ziqi Zhang and Chrysostomos Apostolidis and Raffaele Filieri},
keywords = {Fake reviews, Amazon, Risk analysis, AI explainability, BERT, Topic model indexing, LIME confidence score},
abstract = {Instruments of corporate risk and reputation assessment tools are quintessentially developed on structured quantitative data linked to financial ratios and macroeconomics. An emerging stream of studies has challenged this norm by demonstrating improved risk assessment and model prediction capabilities through unstructured textual corporate data. Fake online consumer reviews pose serious threats to a business’ competitiveness and sales performance, directly impacting revenue, market share, brand reputation and even survivability. Research has shown that as little as three negative reviews can lead to a potential loss of 59.2 % of customers. Amazon, as the largest e-commerce retail platform, hosts over 85,000 small-to-medium-size (SME) retailers (UK), selling over fifty percent of Amazon products worldwide. Despite Amazon's best efforts, fake reviews are a growing problem causing financial and reputational damage at a scale never seen before. While large corporations are better equipped to handle these problems more efficiently, SMEs become the biggest victims of these scam tactics. Following the principles of attribute (AA) and responsible (RA) analytics, we present a novel hybrid method for indexing enterprise risk that we call the Fake Review Index (RFRI). The proposed modular approach benefits from a combination of structured review metadata and semantic topic index derived from unstructured product reviews. We further apply LIME to develop a Confidence Score, demonstrating the importance of explainability and openness in contemporary analytics within the OR domain. Transparency, explainability and simplicity of our roadmap to a hybrid modular approach offers an attractive entry platform for practitioners and managers from the industry.}
}
@article{ZHU2023100680,
title = {Transformers and their application to medical image processing: A review},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {16},
number = {4},
pages = {100680},
year = {2023},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2023.100680},
url = {https://www.sciencedirect.com/science/article/pii/S1687850723001589},
author = {Dongmei Zhu and Dongbo Wang},
keywords = {Transformer, Image processing, Image classification, Image segmentation, Image reconstruction},
abstract = {Transformers perform well in natural language processing tasks and have made many breakthroughs in computer vision. In medical image processing, transformers are successfully used in image segmentation, classification, reconstruction, and diagnosis. In this paper, we mainly expound on the transformer principle and its application in medical imaging. Specifically, we first introduce the basic principles and model structure of transformers. Then, we summarize the improvement mechanism of the transformer's network including combining the transformer with the Unet network, creating a transformer lightweight variant network, strengthening the cross-fast link mechanism, and building a large model with the transformer as the skeleton. Second, extensive discussion is given to medical image segmentation, reconstruction, classification, and other applications. Finally, the main challenges transformers face in the medical image processing field and future development prospects. Furthermore, we systematically summarize the latest research progress of transformers and their application in medical image processing, which has significant reference value for transformer research in the medical field.}
}
@article{WANG2024111950,
title = {A comprehensive survey on interactive evolutionary computation in the first two decades of the 21st century},
journal = {Applied Soft Computing},
volume = {164},
pages = {111950},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111950},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624007245},
author = {Yanan Wang and Yan Pei},
keywords = {Interactive evolutionary computation, Evolutionary computation, Computational intelligence, Humanized computational intelligence, Human-machine interaction},
abstract = {Interactive evolutionary computation (IEC) has demonstrated significant success in addressing numerous real-world problems that are challenging to quantify mathematically or are inadequately evaluated using conventional computational models. This success arises from IEC’s ability to effectively amalgamate evolutionary computation (EC) algorithms with expert knowledge and user preferences. These problems encompass the creative and personalized generation of products, art, and sound; the design optimization of communication systems, environments, and pharmaceuticals; and expert support in areas such as portfolio selection and hearing aid fitting, among others. Despite significant advancements in IEC over the past two decades, no major comprehensive survey encompassing all aspects of IEC research has been conducted since 2001. This article aims to address this gap by providing a comprehensive survey and an enriched definition and scope of IEC, along with innovative ideas for future research in this field. The proposed IEC definition more clearly reflects the mechanism and current research status of the IEC. Additionally, the survey categorizes IEC research into five distinct directions from a problem-oriented perspective: interactive evolutionary computation algorithms, IEC algorithm improvements, evolutionary multi-objective optimization (EMO) with IEC, human perception studies with IEC, and IEC applications. Each direction is meticulously explored, elucidating its contents and key features, while providing a concise summary of pertinent IEC studies. Finally, the survey investigates several promising future trends in IEC, analyzing them through the lens of these five directions and considering the current perspective of computational intelligence, artificial intelligence, and human-machine interaction.}
}
@article{DUAN2024100145,
title = {Human–robot object handover: Recent progress and future direction},
journal = {Biomimetic Intelligence and Robotics},
volume = {4},
number = {1},
pages = {100145},
year = {2024},
issn = {2667-3797},
doi = {https://doi.org/10.1016/j.birob.2024.100145},
url = {https://www.sciencedirect.com/science/article/pii/S2667379724000032},
author = {Haonan Duan and Yifan Yang and Daheng Li and Peng Wang},
keywords = {Human–robot interactions, Object handover},
abstract = {Human–robot object handover is one of the most primitive and crucial capabilities in human–robot collaboration. It is of great significance to promote robots to truly enter human production and life scenarios and serve human in numerous tasks. Remarkable progressions in the field of human–robot object handover have been made by researchers. This article reviews the recent literature on human–robot object handover. To this end, we summarize the results from multiple dimensions, from the role played by the robot (receiver or giver), to the end-effector of the robot (parallel-jaw gripper or multi-finger hand), to the robot abilities (grasp strategy or motion planning). We also implement a human–robot object handover system for anthropomorphic hand to verify human–robot object handover pipeline. This review aims to provide researchers and developers with a guideline for designing human–robot object handover methods.}
}
@article{LIN2025216436,
title = {Deep learning-assisted methods for accelerating the intelligent screening of novel 2D materials: New perspectives focusing on data collection and description},
journal = {Coordination Chemistry Reviews},
volume = {529},
pages = {216436},
year = {2025},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2025.216436},
url = {https://www.sciencedirect.com/science/article/pii/S0010854525000062},
author = {Yuandong Lin and Ji Ma and Yong-Guang Jia and Chongchong Yu and Jun-Hu Cheng},
keywords = {2D materials, Deep learning, Data collections, Data descriptions, Material screenings},
abstract = {Since the isolation of graphene, the interest in two-dimensional (2D) materials has been steadily growing thanks to their unique chemical and physical properties, as well as their potential for various applications. Deep learning (DL), currently one of the most sophisticated machine learning (ML) models, is emerging as a highly effective tool for intelligently investigating and screening 2D materials. The utilization of abundant data sources, appropriate descriptors, and neural networks enables the prediction of the structural and physicochemical properties of undiscovered 2D materials based on DL. Specifically, high-quality and well-described data plays a crucial role in effective model training, accurate predictions, and the discovery of new 2D materials. It also promotes reproducibility, collaboration, and continuous improvement within this field. This tutorial review is dedicated to an examination of the characterization, prediction, and discovery of 2D materials facilitated by various DL techniques. It focuses on the perspective of data collection and description, aiming to provide a clearer understanding of underlying principles and predicting outcomes. In addition, it also offers insights into future research prospects. The growing acceptance of DL is set to accelerate and transform the study of 2D materials.}
}
@article{XU2024104120,
title = {On implementing autonomous supply chains: A multi-agent system approach},
journal = {Computers in Industry},
volume = {161},
pages = {104120},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104120},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000484},
author = {Liming Xu and Stephen Mak and Maria Minaricova and Alexandra Brintrup},
keywords = {Autonomous supply chain, Multi-agent system, Autonomous agents, Perishable foods, Resilience},
abstract = {Trade restrictions, the COVID-19 pandemic, and geopolitical conflicts have significantly exposed vulnerabilities within traditional global supply chains. These events underscore the need for organisations to establish more resilient and flexible supply chains. To address these challenges, the concept of the autonomous supply chain (ASC), characterised by predictive and self-decision-making capabilities, has recently emerged as a promising solution. However, research on ASCs is relatively limited, with no existing studies specifically focusing on their implementations. This paper aims to address this gap by presenting an implementation of ASC using a multi-agent approach. It presents a methodology for the analysis and design of such an agent-based ASC system (A2SC). This paper provides a concrete case study, the autonomous meat supply chain, which showcases the practical implementation of the A2SC system using the proposed methodology. Additionally, a system architecture and a toolkit for developing such A2SC systems are presented. Despite limitations, this work demonstrates a promising approach for implementing an effective ASC system.}
}
@article{PILARIO2024100163,
title = {Teaching classical machine learning as a graduate-level course in chemical engineering: An algorithmic approach},
journal = {Digital Chemical Engineering},
volume = {11},
pages = {100163},
year = {2024},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2024.100163},
url = {https://www.sciencedirect.com/science/article/pii/S2772508124000255},
author = {Karl Ezra Pilario},
keywords = {Chemical engineering education, Machine learning, Data science, Artificial intelligence, Bridging topics, Roadmap},
abstract = {The demand for engineering graduates with technical skills in data science, machine learning (ML), and artificial intelligence (AI) is now growing. Chemical engineering (ChemE) departments around the world are currently addressing this skills gap by instituting AI or ML elective courses in their program. However, designing such a course is difficult since the issue of which ML models to teach and the depth of theory to be discussed remains unclear. In this paper, we present a graduate-level ML course particularly designed such that students will be able to apply ML for research in ChemE. To achieve this, the course intends to cover a wide selection of ML models with emphasis on their motivations, derivations, and training algorithms, followed by their applications to ChemE-related data sets. We argue that this algorithmic approach to teaching ML can help broaden the capabilities of students since they can judge for themselves which tool to use when, even for problems outside the process industries, or they can modify the methods to test novel ideas. We found that students remain engaged in the mathematical details as long as every topic is properly motivated and the gaps in the required statistical and computer science concepts are filled. Hence, this paper also presents a roadmap of ML topics, their motivations, and bridging topics that can be followed by instructors. Lastly, we report anonymized student feedback on this course which is being offered at the Department of Chemical Engineering, University of the Philippines, Diliman.}
}
@article{LU2025102747,
title = {Multimodal dual perception fusion framework for multimodal affective analysis},
journal = {Information Fusion},
volume = {115},
pages = {102747},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102747},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005256},
author = {Qiang Lu and Xia Sun and Yunfei Long and Xiaodi Zhao and Wang Zou and Jun Feng and Xuxin Wang},
keywords = {Multimodal sentiment analysis, Sarcasm detection, Fake news detection, Multimodal affective analysis, Multimodal dual perception fusion},
abstract = {The misuse of social platforms and the difficulty in regulating post contents have culminated in a surge of negative sentiments, sarcasms, and the rampant spread of fake news. In response, Multimodal sentiment analysis, sarcasm detection and fake news detection based on image and text have attracted considerable attention recently. Due to that these areas share semantic and sentiment features and confront related fusion challenges in deciphering complex human expressions across different modalities, integrating these multimodal classification tasks that share commonalities across different scenarios into a unified framework is expected to simplify research in sentiment analysis, and enhance the effectiveness of classification tasks involving both semantic and sentiment modeling. Therefore, we consider integral components of a broader spectrum of research known as multimodal affective analysis towards semantics and sentiment, and propose a novel multimodal dual perception fusion framework (MDPF). Specifically, MDPF contains three core procedures: (1) Generating bootstrapping language-image Knowledge to enrich origin modality space, and utilizing cross-modal contrastive learning for aligning text and image modalities to understand underlying semantics and interactions. (2) Designing dynamic connective mechanism to adaptively match image-text pairs and jointly employing gaussian-weighted distribution to intensify semantic sequences. (3) Constructing a cross-modal graph to preserve the structured information of both image and text data and share information between modalities, while introducing sentiment knowledge to refine the edge weights of the graph to capture cross-modal sentiment interaction. We evaluate MDPF on three publicly available datasets across three tasks, and the empirical results demonstrate the superiority of our proposed model.}
}
@incollection{2024197,
title = {Index},
editor = {Cecilio Angulo and Alejandro Chacón and Pere Ponsa},
booktitle = {Cognitive Assistant Supported Human-Robot Collaboration},
publisher = {Academic Press},
pages = {197-204},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-22135-4},
doi = {https://doi.org/10.1016/B978-0-44-322135-4.00020-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443221354000201}
}
@article{LIU2025119522,
title = {Intelligent visual analysis of accident behavior and mechanism inherent in ship collision accident data},
journal = {Ocean Engineering},
volume = {315},
pages = {119522},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119522},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824028609},
author = {Tao Liu and Hao Hong and Jihong Chen and Yaqin Zhang and Kejun Zhao and Maowen Liu and Jinxian Weng and Wen Liu},
keywords = {Ship collision accident, Visual analysis, Co-occurrence pattern, Collision risk, Accident similarity, Graph neural network},
abstract = {The analysis of ship collision accident data plays a crucial role in ensuring maritime transportation safety. This paper proposes an intelligent visual analysis-based method focusing on the co-occurrence patterns, collision risk, similarity analysis, and classification of ship collision data. Based on data from 1573 maritime accidents that occurred globally between 2010 and 2023, this paper first designs co-occurrence patterns to identify regularities between accident attributes. Second, a fusion model combining fuzzy comprehensive evaluation and logistic regression is developed to calculate ship collision risk, and the model is applied to reconstruct and visually analyze the collision process. Third, the similarity between two collision accidents is analyzed based on Sentence-BERT algorithm, and a heterogeneous graph neural network model is constructed for accident classification. Finally, all the analysis contents are integrated into the visual analysis system of the ship collision accident data to illustrate the accident behavior and mechanism inherent in ship collision accident data. The rich content within the visualization system provides trainee crew members with a more comprehensive understanding of accidents and serves as a reference for maritime management agencies to analyze similar and related accidents, derive lessons learned, identify high-risk areas, and formulate targeted risk management strategies.}
}
@incollection{CHANG20243,
title = {Chapter 1 - Introduction to artificial intelligence for cardiovascular clinicians},
editor = {Anthony C. Chang and Alfonso Limon},
booktitle = {Intelligence-Based Cardiology and Cardiac Surgery},
publisher = {Academic Press},
pages = {3-120},
year = {2024},
series = {Intelligence-Based Medicine: Subspecialty Series},
isbn = {978-0-323-90534-3},
doi = {https://doi.org/10.1016/B978-0-323-90534-3.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390534300010X},
author = {Anthony C. Chang and Alfonso Limon},
keywords = {Artificial intelligence, Cardiovascular clinicians, Deep learning technology, Human-machine intelligence continuum, Machine learning, Neuroscience},
abstract = {The impressive gains in deep learning (DL) started in 2012 and its successful utilization in image interpretation have led to the current momentum for artificial intelligence (AI) awareness and adoption. In 2016, Google DeepMind's AlphaGo software soundly defeated the best human Go champion Lee Sedol to introduce the capability of DL outside of image interpretation. More recently, there have been impressive exponential advances in natural language processing with transformer tools such as GPT-3, GPT-4, and now ChatGPT. DeepMind and its AlphaFold AI tool has been able to predict the three-dimensional (3D) structure of proteins since 2021 and was Science magazine's “Breakthrough of the Year.” All of these AI accomplishments heralded the recent new era in AI. Major universities with AI departments (such as Stanford, MIT, and Carnegie Mellon) and technology giants (such as IBM, Apple, Facebook, and Microsoft in the United States as well as other large companies such as Baidu, Alibaba, and Tencent [BAT] in China) are all fervidly exploring real-life applications of AI. There is also a movement to democratize AI so that “no-code platforms” can accommodate people who do not know how to code [1].}
}
@article{JU2024106207,
title = {A Comprehensive Survey on Deep Graph Representation Learning},
journal = {Neural Networks},
volume = {173},
pages = {106207},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106207},
url = {https://www.sciencedirect.com/science/article/pii/S089360802400131X},
author = {Wei Ju and Zheng Fang and Yiyang Gu and Zequn Liu and Qingqing Long and Ziyue Qiao and Yifang Qin and Jianhao Shen and Fang Sun and Zhiping Xiao and Junwei Yang and Jingyang Yuan and Yusheng Zhao and Yifan Wang and Xiao Luo and Ming Zhang},
keywords = {Deep learning on graphs, Graph representation learning, Graph neural network, Survey},
abstract = {Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future.}
}
@article{AMIN20201472,
title = {Establishing trustworthiness and authenticity in qualitative pharmacy research},
journal = {Research in Social and Administrative Pharmacy},
volume = {16},
number = {10},
pages = {1472-1482},
year = {2020},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2020.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1551741119309155},
author = {Mohamed Ezzat Khamis Amin and Lotte Stig Nørgaard and Afonso M. Cavaco and Matthew J. Witry and Lisa Hillman and Alina Cernasev and Shane P. Desselle},
abstract = {Spurred by the value it can add, the use of qualitative research methods has been steadily growing by social pharmacy researchers around the globe, either separately or as part of mixed methods research projects. Given this increase, it is important to provide guidance to assist researchers in ensuring quality when employing such methods. This commentary addresses both theoretical fundamentals as well as practical aspects of establishing quality in qualitative social pharmacy research. More specifically, it provides an explanation of each of the criteria of trustworthiness proposed by Lincoln and Guba (credibility, transferability, dependability and confirmability) and different techniques used in establishing them. It also provides a brief overview of authenticity, a more recent and less widely used set of criteria that involve demonstrating fairness, ontological authenticity, educative authenticity, catalytic authenticity, and tactical authenticity. For each of these terms, the commentary provides a definition, how it applies to social pharmacy research, and guidance on when and how to use them. These are accompanied by examples from the pharmacy literature where the criteria have been used. The commentary ends by providing a summary of competing viewpoints of establishing quality in the published literature while inviting the reader to reflect on how the presented criteria would apply to different qualitative research projects.}
}
@article{DAKAKNI2023100179,
title = {Artificial intelligence in the L2 classroom: Implications and challenges on ethics and equity in higher education: A 21st century Pandora's box},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100179},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100179},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000589},
author = {Deema Dakakni and Nehme Safa},
keywords = {Artificial intelligence, Ethical uses of AI, L2 classrooms, Digital technology and L2 learning, Chatbot GPT and ethics, Writing mills},
abstract = {The purpose of this research was to investigate attitudes of both students and teachers concerning Artificial Intelligence (AI) tools in the L2 classroom. The study was a descriptive, qualitative, mixedmethods case study whose data were taken from a purposive, convenient sample at a private, English-speaking university during the Summer Semester 2023 in Beirut, Lebanon. Data collection primarily involved an online survey on Google forms which was given to a sample of 49 students taking a research-based English 202 course of which 46 were completed. Afterwards, six English teachers and six students were chosen based on their voluntary will to participate in individual interviews for the former and semi-structured focus group interviews for the latter. The findings revealed that approximately 85% of students did indeed use AI unethically to get ideas for their assignments, assist them in their projects' “blue-prints” or do their assignments/projects altogether. The findings also revealed that a “love/hate” relationship seemed to dictate students' relationships with AI, where students did indeed make use of AI but were distrusting of it for privacy and equity concerns. Finally, findings also revealed that most of the interviewed instructors' readiness to undergo training for AI was more to monitor students' potential misuse of it. The article purposes a suggestive revamping of course learning objectives due to students' inclinations to misuse AI to do their coursework with 89.4% of students willing to use AI to complete their coursework should university punitive measures be removed; furthermore, the article equally proposes future research investigating the impact and use of AI in the higher educational classroom on student performance and that it be used with a “grain of salt” as it may unleash a Pandora's box of future generations graduating without the necessary know-how in delicate professions of medicine, nursing, engineering, architecture among others.}
}
@article{TRAPPEY201738,
title = {Exploring 4G patent and litigation informatics in the mobile telecommunications industry},
journal = {World Patent Information},
volume = {50},
pages = {38-51},
year = {2017},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0172219016300692},
author = {Charles V. Trappey and Amy J.C. Trappey},
keywords = {Mobile telecommunications, Patents, Knowledge e-discovery, Formal concept analysis, Ontology},
abstract = {Patent informatics are often analysed for IP protections, particularly in high-tech industries. This research develops a computer-supported generic methodology for discovering evolutions and linkages between litigations and disputed patents. The IP litigations in mobile telecommunications are used as the case study. An ontology framework representing the 4G domain knowledge is defined first. Then, a modified formal concept analysis (MFCA) approach is developed to discover the evolutionary linkages of legal cases and their disputed patents. In addition to citation-based patent analysis, this research provides a new approach in identifying legal and technical evolutions for future R&D planning and IP strategies.}
}
@article{ZHANG2024104613,
title = {iCORPP: Interleaved commonsense reasoning and probabilistic planning on robots},
journal = {Robotics and Autonomous Systems},
volume = {174},
pages = {104613},
year = {2024},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104613},
url = {https://www.sciencedirect.com/science/article/pii/S092188902300252X},
author = {Shiqi Zhang and Piyush Khandelwal and Peter Stone},
keywords = {Integrated Reasoning and Planning, Commonsense reasoning, Planning under uncertainty, Autonomous Robots, Markov Decision Processes, POMDPs},
abstract = {Robot sequential decision-making in the real world is a challenge because it requires the robots to simultaneously reason about the current world state and dynamics, while planning actions to accomplish complex tasks. On the one hand, declarative languages and reasoning algorithms support representing and reasoning with commonsense knowledge. But these algorithms are not good at planning actions toward maximizing cumulative reward over a long, unspecified horizon. On the other hand, probabilistic planning frameworks, such as Markov decision processes (MDPs) and partially observable MDPs (POMDPs), support planning to achieve long-term goals under uncertainty. But they are ill-equipped to represent or reason about knowledge that is not directly related to actions. In this article, we present an algorithm, called iCORPP, to simultaneously estimate the current world state, reason about world dynamics, and construct task-oriented controllers. In this process, robot decision-making problems are decomposed into two interdependent (smaller) subproblems that focus on reasoning to “understand the world” and planning to “achieve the goal” respectively. The developed algorithm has been implemented and evaluated both in simulation and on real robots using everyday service tasks, such as indoor navigation, and dialog management. Results show significant improvements in scalability, efficiency, and adaptiveness, compared to competitive baselines including handcrafted action policies.}
}
@article{CHEN2025109698,
title = {High-order complementary cloud application programming interface recommendation with logical reasoning for incremental development},
journal = {Engineering Applications of Artificial Intelligence},
volume = {140},
pages = {109698},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109698},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624018566},
author = {Zhen Chen and Denghui Xie and Xiaolong Wang and Dianlong You and Limin Shen},
keywords = {Recommendation systems, Cloud application programming interface recommendation, Complementary recommendation, Logical reasoning},
abstract = {Cloud application programming interface, as the best carrier for service delivery, data exchange, and capability replication, has been an indispensable element of innovation in today’s app-driven world. However, it is difficult for developers to select the suitable one when facing the sea of cloud application programming interfaces. Existing researches focus on generating single-function and high-quality recommendation lists, while ignoring developers’ needs for high-order complementary cloud application programming interfaces in incremental development. In this paper, we present a high-order complementary cloud application programming interface recommendation with logical reasoning. Firstly, we conduct data analysis to demonstrate the necessity of recommending high-order complementary cloud application programming interfaces and the existence of substitute noise. Secondly, a logical reasoning network is designed using projection, intersection, and negation three logic operators, wherein high-order complementary relations are mined and substitute noises are eliminated. Then, the cloud application programming interface base vector that is complementary but not substitute to the query set is generated, and Kullback–Leibler divergence is subsequently introduced to generate complementary recommendation results. Finally, experimental results demonstrate the superiority of our approach in low-, high-, and hybrid-order complementary recommendation scenarios, and there is a significant increase in hit rate, normalize discounted cumulative gain, mean reciprocal rank, and substitute degree by 11.43%/4.86%, 10.08%/4.28%, 7.50%/2.67%, and 36.33%/32.35% on ProgrammableWeb and Huawei AppGallery datasets respectively. The proposed approach is not only more likely to produce diversified results that meet developers’ needs but also help providers better formulate pricing strategies to achieve combined sales and improve revenue.}
}
@article{ZHONG202352,
title = {Enhancing head and neck tumor management with artificial intelligence: Integration and perspectives},
journal = {Seminars in Cancer Biology},
volume = {95},
pages = {52-74},
year = {2023},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2023.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X23001086},
author = {Nian-Nian Zhong and Han-Qi Wang and Xin-Yue Huang and Zi-Zhan Li and Lei-Ming Cao and Fang-Yi Huo and Bing Liu and Lin-Lin Bu},
keywords = {Artificial intelligence, Head and neck tumor, Machine learning, Neural network, Deep learning, Computer vision, Prediction model},
abstract = {Head and neck tumors (HNTs) constitute a multifaceted ensemble of pathologies that primarily involve regions such as the oral cavity, pharynx, and nasal cavity. The intricate anatomical structure of these regions poses considerable challenges to efficacious treatment strategies. Despite the availability of myriad treatment modalities, the overall therapeutic efficacy for HNTs continues to remain subdued. In recent years, the deployment of artificial intelligence (AI) in healthcare practices has garnered noteworthy attention. AI modalities, inclusive of machine learning (ML), neural networks (NNs), and deep learning (DL), when amalgamated into the holistic management of HNTs, promise to augment the precision, safety, and efficacy of treatment regimens. The integration of AI within HNT management is intricately intertwined with domains such as medical imaging, bioinformatics, and medical robotics. This article intends to scrutinize the cutting-edge advancements and prospective applications of AI in the realm of HNTs, elucidating AI’s indispensable role in prevention, diagnosis, treatment, prognostication, research, and inter-sectoral integration. The overarching objective is to stimulate scholarly discourse and invigorate insights among medical practitioners and researchers to propel further exploration, thereby facilitating superior therapeutic alternatives for patients.}
}
@article{DWIVEDI2024102750,
title = {“Real impact”: Challenges and opportunities in bridging the gap between research and practice – Making a difference in industry, policy, and society},
journal = {International Journal of Information Management},
volume = {78},
pages = {102750},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102750},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223001317},
author = {Yogesh K. Dwivedi and Anand Jeyaraj and Laurie Hughes and Gareth H. Davies and Manju Ahuja and Mousa Ahmed Albashrawi and Adil S. Al-Busaidi and Salah Al-Sharhan and Khalid Ibrahim Al-Sulaiti and Levent Altinay and Shem Amalaya and Sunil Archak and María Teresa Ballestar and Shonil A. Bhagwat and Anandhi Bharadwaj and Amit Bhushan and Indranil Bose and Pawan Budhwar and Deborah Bunker and Alexandru Capatina and Lemuria Carter and Ioanna Constantiou and Crispin Coombs and Tom Crick and Csaba Csáki and Yves Darnige and Rahul Dé and Rick Delbridge and Rameshwar Dubey and Robin Gauld and Ravi Kumar Gutti and Marié Hattingh and Arve Haug and Leeya Hendricks and Airo Hino and Cathy H.C. Hsu and Netta Iivari and Marijn Janssen and Ikram Jebabli and Paul Jones and Iris Junglas and Abhishek Kaushik and Deepak Khazanchi and Mitsuru Kodama and Sascha Kraus and Vikram Kumar and Christian Maier and Tegwen Malik and Machdel Matthee and Ian P. McCarthy and Marco Meier and Bhimaraya Metri and Adrian Micu and Angela-Eliza Micu and Santosh K. Misra and Anubhav Mishra and Tonja Molin-Juustila and Leif Oppermann and Nicholas O’Regan and Abhipsa Pal and Neeraj Pandey and Ilias O. Pappas and Andrew Parker and Kavita Pathak and Daniel Pienta and Ariana Polyviou and Ramakrishnan Raman and Samuel Ribeiro-Navarrete and Paavo Ritala and Michael Rosemann and Suprateek Sarker and Pallavi Saxena and Daniel Schlagwein and Hergen Schultze and Chitra Sharma and Sujeet Kumar Sharma and Antonis Simintiras and Vinay Kumar Singh and Hanlie Smuts and John Soldatos and Manoj Kumar Tiwari and Jason Bennett Thatcher and Cristina Vanberghen and Ákos Varga and Polyxeni Vassilakopoulou and Viswanath Venkatesh and Giampaolo Viglia and Tim Vorley and Michael Wade and Paul Walton},
keywords = {Academic impact, Implications for practice, Relevance, Research benefits, Research contribution, Research impact},
abstract = {Achieving impact from academic research is a challenging, complex, multifaceted, and interconnected topic with a number of competing priorities and key performance indicators driving the extent and reach of meaningful and measurable benefits from research. Academic researchers are incentivised to publish their research in high-ranking journals and academic conferences but also to demonstrate the impact of their outputs through metrics such as citation counts, altmetrics, policy and practice impacts, and demonstrable institutional decision-making influence. However, academic research has been criticized for: its theoretical emphasis, high degree of complexity, jargon-heavy language, disconnect from industry and societal needs, overly complex and lengthy publishing timeframe, and misalignment between academic and industry objectives. Initiatives such as collaborative research projects and technology transfer offices have attempted to deliver meaningful impact, but significant barriers remain in the identification and evaluation of tangible impact from academic research. This editorial focusses on these aspects to deliver a multi-expert perspective on impact by developing an agenda to deliver more meaningful and demonstrable change to how “impact” can be conceptualized and measured to better align with the aims of academia, industry, and wider society. We present the 4D model - Design, Deliver, Disseminate, and Demonstrate - to provide a structured approach for academia to better align research endeavors with practice and deliver meaningful, tangible benefits to stakeholders.}
}
@article{GROVER2024477,
title = {Global Workforce and Access: Demand, Education, Quality},
journal = {Seminars in Radiation Oncology},
volume = {34},
number = {4},
pages = {477-493},
year = {2024},
note = {Future of Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1053429624000547},
author = {Surbhi Grover and Laurence Court and Sheldon Amoo-Mitchual and John Longo and Danielle Rodin and Aba Anoa Scott and Yolande Lievens and Mei Ling Yap and May Abdel-Wahab and Peter Lee and Ekaterina Harsdorf and Jamal Khader and Xun Jia and Manjit Dosanjh and Ahmed Elzawawy and Taofeeq Ige and Miles Pomper and David Pistenmaa and Patricia Hardenbergh and Daniel G Petereit and Michele Sargent and Kristin Cina and Benjamin Li and Yavuz Anacak and Chuck Mayo and Sainikitha Prattipati and Nwamaka Lasebikan and Katharine Rendle and Donna O'Brien and Eugenia Wendling and C. Norman Coleman},
abstract = {There has long existed a substantial disparity in access to radiotherapy globally. This issue has only been exacerbated as the growing disparity of cancer incidence between high-income countries (HIC) and low and middle-income countries (LMICs) widens, with a pronounced increase in cancer cases in LMICs. Even within HICs, iniquities within local communities may lead to a lack of access to care. Due to these trends, it is imperative to find solutions to narrow global disparities. This requires the engagement of a diverse cohort of stakeholders, including working professionals, non-governmental organizations, nonprofits, professional societies, academic and training institutions, and industry. This review brings together a diverse group of experts to highlight critical areas that could help reduce the current global disparities in radiation oncology. Advancements in technology and treatment, such as artificial intelligence, brachytherapy, hypofractionation, and digital networks, in combination with implementation science and novel funding mechanisms, offer means for increasing access to care and education globally. Common themes across sections reveal how utilizing these new innovations and strengthening collaborative efforts among stakeholders can help improve access to care globally while setting the framework for the next generation of innovations.}
}
@article{OFFENHUBER2023264,
title = {Reconsidering Representation in College Design Curricula},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {264-282},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
author = {Dietmar Offenhuber and Joy Mountford},
keywords = {Representation, Data, Models, Maps, Visualization, Sensory modalities},
abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.}
}
@article{SHARMA2024111934,
title = {A survey on machine learning techniques applied to source code},
journal = {Journal of Systems and Software},
volume = {209},
pages = {111934},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111934},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003291},
author = {Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro},
keywords = {Machine learning for software engineering, Source code analysis, Deep learning, Datasets, Tools},
abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{STEPHENSON20233156,
title = {Physical Laboratory Automation in Synthetic Biology},
journal = {ACS Synthetic Biology},
volume = {12},
number = {11},
pages = {3156-3169},
year = {2023},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.3c00345},
url = {https://www.sciencedirect.com/science/article/pii/S2161506323003285},
author = {Ashley Stephenson and Lauren Lastra and Bichlien Nguyen and Yuan-Jyue Chen and Jeff Nivala and Luis Ceze and Karin Strauss},
keywords = {automation, synthetic biology, standardization, robotics, microfluidics, design-build-test-learn},
abstract = {Synthetic Biology has overcome many of the early challenges facing the field and is entering a systems era characterized by adoption of Design-Build-Test-Learn (DBTL) approaches. The need for automation and standardization to enable reproducible, scalable, and translatable research has become increasingly accepted in recent years, and many of the hardware and software tools needed to address these challenges are now in place or under development. However, the lack of connectivity between DBTL modules and barriers to access and adoption remain significant challenges to realizing the full potential of lab automation. In this review, we characterize and classify the state of automation in synthetic biology with a focus on the physical automation of experimental workflows. Though fully autonomous scientific discovery is likely a long way off, impressive progress has been made toward automating critical elements of experimentation by combining intelligent hardware and software tools. It is worth questioning whether total automation that removes humans entirely from the loop should be the ultimate goal, and considerations for appropriate automation versus total automation are discussed in this light while emphasizing areas where further development is needed in both contexts.
}
}
@article{HARGREAVES2024301844,
title = {DFPulse: The 2024 digital forensic practitioner survey},
journal = {Forensic Science International: Digital Investigation},
volume = {51},
pages = {301844},
year = {2024},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301844},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001719},
author = {Christopher Hargreaves and Frank Breitinger and Liz Dowthwaite and Helena Webb and Mark Scanlon},
keywords = {Digital forensics, Practitioner survey, Challenges, Future directions, Artificial intelligence},
abstract = {This paper reports on the largest survey of digital forensic practitioners to date (DFPulse) conducted from March to May 2024 resulting in 122 responses. The survey collected information about practitioners' operating environments, the technologies they encounter, investigative techniques they use, the challenges they face, the degree to which academic research is accessed and useful to the practitioner community, and their suggested future research directions. The paper includes quantitative and qualitative results from the survey and a discussion of the implications for academia, the improvements that can be made, and future research directions.}
}
@article{ZHAO2023100521,
title = {Toward parallel intelligence: An interdisciplinary solution for complex systems},
journal = {The Innovation},
volume = {4},
number = {6},
pages = {100521},
year = {2023},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2023.100521},
url = {https://www.sciencedirect.com/science/article/pii/S2666675823001492},
author = {Yong Zhao and Zhengqiu Zhu and Bin Chen and Sihang Qiu and Jincai Huang and Xin Lu and Weiyi Yang and Chuan Ai and Kuihua Huang and Cheng He and Yucheng Jin and Zhong Liu and Fei-Yue Wang},
abstract = {The growing complexity of real-world systems necessitates interdisciplinary solutions to confront myriad challenges in modeling, analysis, management, and control. To meet these demands, the parallel systems method rooted in the artificial systems, computational experiments, and parallel execution (ACP) approach has been developed. The method cultivates a cycle termed parallel intelligence, which iteratively creates data, acquires knowledge, and refines the actual system. Over the past two decades, the parallel systems method has continuously woven advanced knowledge and technologies from various disciplines, offering versatile interdisciplinary solutions for complex systems across diverse fields. This review explores the origins and fundamental concepts of the parallel systems method, showcasing its accomplishments as a diverse array of parallel technologies and applications while also prognosticating potential challenges. We posit that this method will considerably augment sustainable development while enhancing interdisciplinary communication and cooperation.}
}
@article{LIU2024102423,
title = {Artificial intelligence for production, operations and logistics management in modular construction industry: A systematic literature review},
journal = {Information Fusion},
volume = {109},
pages = {102423},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102423},
url = {https://www.sciencedirect.com/science/article/pii/S156625352400201X},
author = {Qiurui Liu and Yanfang Ma and Lin Chen and Witold Pedrycz and Mirosław J. Skibniewski and Zhen-Song Chen},
keywords = {Literature review, Bibliometric analysis, Production management, Operations management, Logistics management, Artificial intelligence, Modular construction industry},
abstract = {Artificial intelligence (AI) has garnered significant attention within the modular construction industry, emerging as a prominent frontier development trend. A comprehensive and systematic analysis is required to gain a thorough understanding of the existing literature on the use of AI in the management of production, operations, and logistics within the modular construction industry. This review delves into the various aspects of AI implementation in this sector, adopting a critical perspective. The objective of this paper is to analyze the progress, suitability, and research patterns in the field of AI for the management of productions, operations, and logistics within the modular construction industry. First, a concise overview of AI technologies pertaining to the contemporary research on the production, operations and logistics management of the modular construction industry is provided. Second, a bibliometric analysis is performed to provide a comprehensive overview of the existing publications pertaining to this subject matter. Subsequently, this paper presents literature reviews and outlines future directions for each component, specifically AI in the context of production management, operations management, and logistics management within the modular construction industry. The review provides a valuable knowledge base and roadmap to guide future research and development efforts in AI-enhanced modular construction management.}
}
@article{LAREYRE202357,
title = {Comprehensive Review of Natural Language Processing (NLP) in Vascular Surgery},
journal = {EJVES Vascular Forum},
volume = {60},
pages = {57-63},
year = {2023},
issn = {2666-688X},
doi = {https://doi.org/10.1016/j.ejvsvf.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666688X23000758},
author = {Fabien Lareyre and Bahaa Nasr and Arindam Chaudhuri and Gilles {Di Lorenzo} and Mathieu Carlier and Juliette Raffort},
keywords = {Artificial Intelligence, Literature search, Natural Language Processing, Vascular surgery},
abstract = {Objective
The use of Natural Language Processing (NLP) has attracted increased interest in healthcare with various potential applications including identification and extraction of health information, development of chatbots and virtual assistants. The aim of this comprehensive literature review was to provide an overview of NLP applications in vascular surgery, identify current limitations, and discuss future perspectives in the field.
Data sources
The MEDLINE database was searched on April 2023.
Review methods
The database was searched using a combination of keywords to identify studies reporting the use of NLP and chatbots in three main vascular diseases. Keywords used included Natural Language Processing, chatbot, chatGPT, aortic disease, carotid, peripheral artery disease, vascular, and vascular surgery.
Results
Given the heterogeneity of study design, techniques, and aims, a comprehensive literature review was performed to provide an overview of NLP applications in vascular surgery. By enabling identification and extraction of information on patients with vascular diseases, such technology could help to analyse data from healthcare information systems to provide feedback on current practice and help in optimising patient care. In addition, chatbots and NLP driven techniques have the potential to be used as virtual assistants for both health professionals and patients.
Conclusion
While Artificial Intelligence and NLP technology could be used to enhance care for patients with vascular diseases, many challenges remain including the need to define guidelines and clear consensus on how to evaluate and validate these innovations before their implementation into clinical practice.}
}
@article{SANTOS2025107568,
title = {Software solutions for newcomers’ onboarding in software projects: A systematic literature review},
journal = {Information and Software Technology},
volume = {177},
pages = {107568},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107568},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001733},
author = {Italo Santos and Katia Romero Felizardo and Igor Steinmacher and Marco A. Gerosa},
keywords = {Systematic literature review, Software projects, Open source software, Onboarding, Turnover, Tool, Newcomers, Novices},
abstract = {Context:
Newcomers joining an unfamiliar software project face numerous barriers; therefore, effective onboarding is essential to help them engage with the team and develop the behaviors, attitudes, and skills needed to excel in their roles. However, onboarding can be a lengthy, costly, and error-prone process. Software solutions can help mitigate these barriers and streamline the process without overloading senior members.
Objective:
This study aims to identify the state-of-the-art software solutions for onboarding newcomers.
Methods:
We conducted a systematic literature review (SLR) to answer six research questions.
Results:
We analyzed 32 studies about software solutions for onboarding newcomers and yielded several key findings: (1) a range of strategies exists, with recommendation systems being the most prevalent; (2) most solutions are web-based; (3) solutions target a variety of onboarding aspects, with a focus on process; (4) many onboarding barriers remain unaddressed by existing solutions; (5) laboratory experiments are the most commonly used method for evaluating these solutions; and (6) diversity and inclusion aspects primarily address experience level.
Conclusion:
We shed light on current technological support and identify research opportunities to develop more inclusive software solutions for onboarding. These insights may also guide practitioners in refining existing platforms and onboarding programs to promote smoother integration of newcomers into software projects.}
}
@article{REDDY20244081,
title = {A Fusion Model for Personalized Adaptive Multi-Product Recommendation System Using Transfer Learning and Bi-GRU},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {4081-4107},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.057071},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008361},
author = {Buchi Reddy Ramakantha Reddy and Ramasamy Lokesh Kumar},
keywords = {Personalized recommendation systems, transfer learning, bidirectional gated recurrent units (Bi-GRU), performance metrics, adaptive systems, product reviews},
abstract = {Traditional e-commerce recommendation systems often struggle with dynamic user preferences and a vast array of products, leading to suboptimal user experiences. To address this, our study presents a Personalized Adaptive Multi-Product Recommendation System (PAMR) leveraging transfer learning and Bi-GRU (Bidirectional Gated Recurrent Units). Using a large dataset of user reviews from Amazon and Flipkart, we employ transfer learning with pre-trained models (AlexNet, GoogleNet, ResNet-50) to extract high-level attributes from product data, ensuring effective feature representation even with limited data. Bi-GRU captures both spatial and sequential dependencies in user-item interactions. The innovation of this study lies in the innovative feature fusion technique that combines the strengths of multiple transfer learning models, and the integration of an attention mechanism within the Bi-GRU framework to prioritize relevant features. Our approach addresses the classic recommendation systems that often face challenges such as cold start along with data sparsity difficulties, by utilizing robust user and item representations. The model demonstrated an accuracy of up to 96.9%, with precision and an F1-score of 96.2% and 96.97%, respectively, on the Amazon dataset, significantly outperforming the baselines and marking a considerable advancement over traditional configurations. This study highlights the effectiveness of combining transfer learning with Bi-GRU for scalable and adaptive recommendation systems, providing a versatile solution for real-world applications.}
}
@incollection{2023503,
title = {Index},
editor = {Anthony C. Chang and Alfonso Limon},
booktitle = {Intelligence-Based Cardiology and Cardiac Surgery},
publisher = {Academic Press},
pages = {503-513},
year = {2023},
series = {Intelligence-Based Medicine: Subspecialty Series},
isbn = {978-0-323-90534-3},
doi = {https://doi.org/10.1016/B978-0-323-90534-3.20001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905343200012}
}
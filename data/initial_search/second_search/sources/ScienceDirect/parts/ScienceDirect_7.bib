@article{LI2025308,
title = {Trustworthy AI for human-centric smart manufacturing: A survey},
journal = {Journal of Manufacturing Systems},
volume = {78},
pages = {308-327},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002747},
author = {Dongpeng Li and Shimin Liu and Baicun Wang and Chunyang Yu and Pai Zheng and Weihua Li},
keywords = {Industry 5.0, Human-centric smart manufacturing, Human–machine symbiosis, Trustworthy AI},
abstract = {Human-centric smart manufacturing (HCSM) envisions a symbiotic relationship between humans and machines, leveraging human capability and Artificial Intelligence (AI)’s precision and computational power to achieve mutual enhancement. Trustworthy AI (TAI) is a promising enabler in this transition, ensuring that the integration of AI technologies within manufacturing scenarios is safe, transparent, and participatory. This paper systematically reviews TAI within the context of HCSM by adopting a progressive 3-layer framework. This framework aligns with the developmental stages of HCSM and includes basic safety (protection), advancing to explainability, accountability, and uncertainty awareness (perception), and culminating in continuous updating with human involvement (participation). The review explores the role of TAI across key stages of the product lifecycle, demonstrating how TAI can empower humans and highlighting current advancements while identifying ongoing challenges. The paper concludes by discussing future directions and offering insights for developing TAI-integrated HCSM.}
}
@article{ALI2024100954,
title = {Cognitive systems and interoperability in the enterprise: A systematic literature review},
journal = {Annual Reviews in Control},
volume = {57},
pages = {100954},
year = {2024},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2024.100954},
url = {https://www.sciencedirect.com/science/article/pii/S1367578824000233},
author = {Jana Al Haj Ali and Ben Gaffinet and Hervé Panetto and Yannick Naudet},
keywords = {Cognition, Cognitive systems, Cognitive cyber–physical systems, Cognitive Digital Twin, Cognitive interoperability},
abstract = {The transition from automated processes to mechanisms that manifest intelligence through cognitive abilities such as memorisation, adaptability and decision-making in uncertain contexts, has marked a turning point in the field of industrial systems, particularly in the development of cyber–physical systems and digital twins. This evolution, supported by advances in cognitive science and artificial intelligence, has opened the way to a new era in which systems are able to adapt and evolve autonomously, while offering more intuitive interaction with human users. This article proposes a systematic literature review to gather and analyse current research on Cognitive Cyber–Physical Systems (CCPS), Cognitive Digital Twins (CDT), and cognitive interoperability, which are pivotal in a contemporary Cyber–Physical Enterprise (CPE). From this review, we first seek to understand how cognitive capabilities that are traditionally considered as human traits have been defined and modelled in cyber–physical systems and digital twins in the context of Industry 4.0/5.0, and what cognitive functions they implement. We explore their theoretical foundations, in particular in relation to cognitive psychology and humanities definitions and theories. Then we analyse how interoperability between cognitive systems has been considered, leading to cognitive interoperability, and we highlight the role of knowledge representation and reasoning.}
}
@article{REN2024112595,
title = {Self-labeling in multivariate causality and quantification for adaptive machine learning},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112595},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112595},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124012292},
author = {Yutian Ren and Aaron Haohua Yen and G.P. Li},
keywords = {Adaptive learning, Self-supervised learning, Machine learning, Causality inspired learning, Causal time delay, Noisy label},
abstract = {Adaptive machine learning (ML) aims to allow ML models to adapt to ever-changing environments with potential concept drift after model deployment. Traditionally, adaptive ML requires a new dataset to be manually labeled to tailor deployed models to altered data distributions. Recently, an interactive causality based self-labeling method was proposed to autonomously associate causally related data streams for domain adaptation, showing promising results compared to traditional feature similarity-based semi-supervised learning. Several unanswered research questions remain, including self-labeling’s compatibility with multivariate causality and the quantitative analysis of the auxiliary models used in the self-labeling. The auxiliary models, the interaction time model (ITM) and the effect state detector (ESD), are vital to the success of self-labeling. This paper further develops the self-labeling framework and its theoretical foundations to address these research questions. A framework for the application of self-labeling to multivariate causal graphs is proposed using four basic causal relationships, and the impact of non-ideal ITM and ESD performance is analyzed. A simulated experiment is conducted based on a multivariate causal graph, validating the proposed theory.}
}
@incollection{ZHENG2024,
title = {Machine Learning in Bioinformatics},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00166-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001664},
author = {Huiru Zheng and Jyotsna Talreja Wassan and Haiying Wang},
keywords = {Algorithms, Bioinformatics, Deep learning, Generative AI, Genomics, Machine learning, Natural language processing, Proteomics},
abstract = {The unprecedented growth in scale and type of biological data has attracted the use of machine learning in bioinformatics to build informative models for understanding the underlying biological processes. In this encyclopedia chapter, we aim to provide readers with a general introduction to a few key machine learning models useful in bioinformatics, including the most recently developed techniques of deep neural networks. The chapter discusses how different machine-learning models may be suited to varied biological data. Furthermore, applications and recommendations of machine learning in the field of bioinformatics are highlighted in the chapter.}
}
@article{HASSAN2024128058,
title = {Unfolding Explainable AI for Brain Tumor Segmentation},
journal = {Neurocomputing},
volume = {599},
pages = {128058},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008294},
author = {Muhammad Hassan and Ahmed Ameen Fateh and Jieqiong Lin and Yijiang Zhuang and Guisen Lin and Hairui Xiong and Zhou You and Peiwu Qin and Hongwu Zeng},
keywords = {Segmentation, Brain Tumor, Machine Learning, Deep Learning, Explainable AI, Neuro-Symbolic Learning},
abstract = {Brain tumor segmentation (BTS) has been studied from handcrafted engineered features to conventional machine learning (ML) methods, followed by the cutting-edge deep learning approaches. Each recent approach has attempted to overcome the challenges of previous methods and brought conveniences in efficacy, throughput, computation, explainability, investigation, and interpretability. Recently, deep learning (DL) algorithms show excellent performance regarding diverse fields, including image process, computer vision, health analytics, autonomous vehicles, and natural language processes; however, ultimately impediment in making the artificial intelligence explainable and interpretable to clinicians while dealing with critical health informatics and radiomics. Besides the sophisticated deep learning models for brain tumor segmentation, notorious notions like explainability, investigation, trust, and interpretability of DL raised significant concerns for clinicians in their domains. Among many DL methods, the neuro-symbolic learning (NSL) concept has gained more attention as it can contribute to explainable and interpretable AI. In the current study, we survey the prominent approaches, from handcrafted engineering conventional ML to deep learning algorithms, highlight the challenges in DL algorithms, and propose NSL architectures for BTS. Compared to existing surveys, our study not only outlines handcrafted to DL methods for BTS but also proposed explainable and interpretable pipelines appropriate for clinical practices. Our study can better facilitate novice learners in explainable AI and propose efficient, robust, interpretable DL models to facilitate the diagnosis, prognosis, and treatment of BTS.}
}
@article{SANTOSA2024112437,
title = {S3PaR: Section-based Sequential Scientific Paper Recommendation for paper writing assistance},
journal = {Knowledge-Based Systems},
volume = {303},
pages = {112437},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112437},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010712},
author = {Natasha Christabelle Santosa and Xin Liu and Hyoil Han and Jun Miyazaki},
keywords = {Recommender systems, Sequential recommendation, Dynamic user interest, Scientific paper recommendation, Graph neural networks, Attention mechanisms},
abstract = {A scientific paper recommender system (RS) is very helpful for literature searching in that it (1) helps novice researchers explore their own field and (2) helps experienced researchers explore new fields outside their area of expertise. However, existing RSs usually recommend relevant papers based on users’ static interests, i.e., papers they cited in their past publication(s) or reading histories. In this paper, we propose a novel recommendation task based on users’ dynamic interests during their paper-writing activity. This dynamism is revealed in (for example) the topic shift while writing the Introduction vs. Related Works section. In solving this task, we developed a new pipeline called “Section-based Sequential Scientific Paper Recommendation (S3PaR)”, which recommends papers based on the context of the given user’s currently written paper section. Our experiments demonstrate that this unique task and our proposed pipeline outperform existing standard RS baselines.}
}
@article{PANCHENDRARAJAN2024124097,
title = {Synergizing machine learning & symbolic methods: A survey on hybrid approaches to natural language processing},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124097},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124097},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009631},
author = {Rrubaa Panchendrarajan and Arkaitz Zubiaga},
keywords = {Hybrid NLP, Machine learning, Symbolic methods, Hybrid approaches, Natural language processing},
abstract = {The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning. Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges and future directions, offering a roadmap for future research avenues.}
}
@article{WANG2024123781,
title = {What can rhetoric bring us? Incorporating rhetorical structure into neural related work generation},
journal = {Expert Systems with Applications},
volume = {251},
pages = {123781},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123781},
url = {https://www.sciencedirect.com/science/article/pii/S095741742400647X},
author = {Pancheng Wang and Shasha Li and Jintao Tang and Ting Wang},
keywords = {Natural language processing, Related work generation, Multi-document summarization, Rhetorical structure analysis},
abstract = {The ever-increasing volume of research literature poses challenges for researchers in keeping up with related works in their fields. Automating the generation of related work section holds promise for saving time and effort. However, current models often fall short of producing coherent and logically correct related work with multiple sentences, a phenomenon we refer to as rhetorical structure chaos. Rhetorical structure describes how adjacent spans of units are connected to each other, and logically correct rhetorical structure is essential for a well-structured related work. Hence, to tackle the rhetorical structure chaos issue, this paper explicitly incorporates rhetorical structure information into related work generation. Firstly, we conduct the first rhetorical structure analysis for related work section, which provides insights into understanding the organization and arrangement of contents within related work. Then, based on two preliminary studies on rhetorical structure, we present a novel related work generation model called RSGen, which incorporates rhetorical structure at both the encoding and decoding stages. The encoding stage is facilitated with a rhetorical structure-based graph encoder, while the decoding process is guided by a rhetorical plan — ordered sequence of rhetorical functions of the related work. We conduct extensive experiments on three related work generation datasets to evaluate the performance of our model. The results show that our approach achieves state-of-the-art performance on ROUGE metrics. An ablation study and more analyses further highlight the remarkable efficacy of introducing rhetorical structure into both the encoding and decoding stages.}
}
@article{SUN2025101707,
title = {Knowledge-aware audio-grounded generative slot filling for limited annotated data},
journal = {Computer Speech & Language},
volume = {89},
pages = {101707},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101707},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000901},
author = {Guangzhi Sun and Chao Zhang and Ivan Vulić and Paweł Budzianowski and Philip C. Woodland},
keywords = {Slot filling, Spoken language understanding, Audio-grounding, Contextual biasing, Knowledge base, Generative model, Limited data, Few-shot, Zero-shot},
abstract = {Manually annotating fine-grained slot-value labels for task-oriented dialogue (ToD) systems is an expensive and time-consuming endeavour. This motivates research into slot-filling methods that operate with limited amounts of labelled data. Moreover, the majority of current work on ToD is based solely on text as the input modality, neglecting the additional challenges of imperfect automatic speech recognition (ASR) when working with spoken language. In this work, we propose a Knowledge-Aware Audio-Grounded generative slot filling framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for ToD with speech input. KA2G achieves robust and data-efficient slot filling for speech-based ToD by (1) framing it as a text generation task, (2) grounding text generation additionally in the audio modality, and (3) conditioning on available external knowledge (e.g. a predefined list of possible slot values). We show that combining both modalities within the KA2G framework improves the robustness against ASR errors. Further, the knowledge-aware slot-value generator in KA2G, implemented via a pointer generator mechanism, particularly benefits few-shot and zero-shot learning. Experiments, conducted on the standard speech-based single-turn SLURP dataset and a multi-turn dataset extracted from a commercial ToD system, display strong and consistent gains over prior work, especially in few-shot and zero-shot setups.}
}
@article{KILICOGLU2024104588,
title = {Semantics-enabled biomedical literature analytics},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104588},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104588},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000066},
author = {Halil Kilicoglu and Faezeh Ensan and Bridget McInnes and Lucy Lu Wang}
}
@article{STANCIU2025108526,
title = {Decoding a decade. Trends and evolution in learning analytics: A comprehensive synthesis},
journal = {Computers in Human Behavior},
volume = {165},
pages = {108526},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108526},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224003947},
author = {Ionut Dorin Stanciu and Ángel Hernández-García and Miguel Ángel Conde and Nicolae Nistor},
keywords = {Learning analytics, Topic modeling, Latent Dirichlet Allocation, Gibbs sampling, Machine learning},
abstract = {This article concludes the special issue “Learning Analytics 10 Years After: A Retrospective and Research Agenda” with a summary of the contributed studies. To assess their representativeness of the last decade's learning analytics research, a literature analysis was performed based on topic extraction. Following PRISMA guidelines, 3897 journal articles and conference papers in Learning Analytics were analyzed with Latent Dirichlet Allocation with Gibbs sampling to uncover common topics. Nine primary topics emerged: skills assessment and program evaluation; adoption of learning analytics in higher education; educational tool design and teacher support; student engagement in online courses; predictive modeling in education; technology integration in education; social learning and collaborative knowledge building; data mining in educational research; and online learning environments and student behavior. Time and publication type significantly influenced topic presence. The articles selected for this special issue spanned the most frequent publication themes, demonstrating their representativeness. Furthermore, the review underscores the importance of integrating methodology with educational theory, as highlighted in the authoritative review that opens the issue, paving the way for continued advancements.}
}
@article{ZHIOUA2024e148,
title = {NEW APPROACH OF ARTIFICIAL INTELLIGENCE FOR FERTILITY TREATMENT KNOWLEDGE ACCESS AND DECISION-MAKING},
journal = {Fertility and Sterility},
volume = {122},
number = {4, Supplement },
pages = {e148-e149},
year = {2024},
note = {80th Scientific Congress of the American Society for Reproductive Medicine},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2024.07.537},
url = {https://www.sciencedirect.com/science/article/pii/S0015028224011543},
author = {Kais Zhioua and Marouen Braham}
}
@article{THEODOSIOU20243247,
title = {BioTextQuest v2.0: An evolved tool for biomedical literature mining and concept discovery},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3247-3253},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024002757},
author = {Theodosios Theodosiou and Konstantinos Vrettos and Ismini Baltsavia and Fotis Baltoumas and Nikolas Papanikolaou and Andreas Ν. Antonakis and Dimitrios Mossialos and Christos A. Ouzounis and Vasilis J. Promponas and Makrina Karaglani and Ekaterini Chatzaki and Sven Brandau and Georgios A. Pavlopoulos and Evangelos Andreakos and Ioannis Iliopoulos},
keywords = {Biomedical literature mining, Concept discovery},
abstract = {The process of navigating through the landscape of biomedical literature and performing searches or combining them with bioinformatics analyses can be daunting, considering the exponential growth of scientific corpora and the plethora of tools designed to mine PubMed(®) and related repositories. Herein, we present BioTextQuest v2.0, a tool for biomedical literature mining. BioTextQuest v2.0 is an open-source online web portal for document clustering based on sets of selected biomedical terms, offering efficient management of information derived from PubMed abstracts. Employing established machine learning algorithms, the tool facilitates document clustering while allowing users to customize the analysis by selecting terms of interest. BioTextQuest v2.0 streamlines the process of uncovering valuable insights from biomedical research articles, serving as an agent that connects the identification of key terms like genes/proteins, diseases, chemicals, Gene Ontology (GO) terms, functions, and others through named entity recognition, and their application in biological research. Instead of manually sifting through articles, researchers can enter their PubMed-like query and receive extracted information in two user-friendly formats, tables and word clouds, simplifying the comprehension of key findings. The latest update of BioTextQuest leverages the EXTRACT named entity recognition tagger, enhancing its ability to pinpoint various biological entities within text. BioTextQuest v2.0 acts as a research assistant, significantly reducing the time and effort required for researchers to identify and present relevant information from the biomedical literature.}
}
@article{GAN2024102786,
title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102786},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004348},
author = {Lu Gan and Wenbo Chu and Guofa Li and Xiaolin Tang and Keqiang Li},
keywords = {Intelligent transportation systems, Autonomous vehicles, Survey, Large models, Efficient deployment techniques},
abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.}
}
@article{MARINI2024103303,
title = {Multimodal representations of biomedical knowledge from limited training whole slide images and reports using deep learning},
journal = {Medical Image Analysis},
volume = {97},
pages = {103303},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103303},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002287},
author = {Niccolò Marini and Stefano Marchesin and Marek Wodzinski and Alessandro Caputo and Damian Podareanu and Bryan Cardenas Guevara and Svetla Boytcheva and Simona Vatrano and Filippo Fraggetta and Francesco Ciompi and Gianmaria Silvello and Henning Müller and Manfredo Atzori},
keywords = {Computational pathology, Multimodal Learning, Medical ontology, Natural language processing, Colon cancer},
abstract = {The increasing availability of biomedical data creates valuable resources for developing new deep learning algorithms to support experts, especially in domains where collecting large volumes of annotated data is not trivial. Biomedical data include several modalities containing complementary information, such as medical images and reports: images are often large and encode low-level information, while reports include a summarized high-level description of the findings identified within data and often only concerning a small part of the image. However, only a few methods allow to effectively link the visual content of images with the textual content of reports, preventing medical specialists from properly benefitting from the recent opportunities offered by deep learning models. This paper introduces a multimodal architecture creating a robust biomedical data representation encoding fine-grained text representations within image embeddings. The architecture aims to tackle data scarcity (combining supervised and self-supervised learning) and to create multimodal biomedical ontologies. The architecture is trained on over 6,000 colon whole slide Images (WSI), paired with the corresponding report, collected from two digital pathology workflows. The evaluation of the multimodal architecture involves three tasks: WSI classification (on data from pathology workflow and from public repositories), multimodal data retrieval, and linking between textual and visual concepts. Noticeably, the latter two tasks are available by architectural design without further training, showing that the multimodal architecture that can be adopted as a backbone to solve peculiar tasks. The multimodal data representation outperforms the unimodal one on the classification of colon WSIs and allows to halve the data needed to reach accurate performance, reducing the computational power required and thus the carbon footprint. The combination of images and reports exploiting self-supervised algorithms allows to mine databases without needing new annotations provided by experts, extracting new information. In particular, the multimodal visual ontology, linking semantic concepts to images, may pave the way to advancements in medicine and biomedical analysis domains, not limited to histopathology.}
}
@article{KLIMOVA20231,
title = {Strategic Trends in Artificial Intelligence Through Impact of Computational Science: What Young Scientists Should Expect},
journal = {Procedia Computer Science},
volume = {229},
pages = {1-7},
year = {2023},
note = {12th International Young Scientists Conference in Computational Science, YSC2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923019919},
author = {Alexandra Klimova and Denis Nasonov and Alexander Hvatov and Nikolay O. Nikitin and Sergey V. Ivanov and Anna V. Kalyuzhnaya and Alexander Boukhanovsky},
keywords = {Artificial Intelligence, Computational Science, Trends, Impact, Young Scientists},
abstract = {This volume presents selected papers of the 12th Young Scientists Conference in Computational Science (YSC'2023). ITMO University annually organises the event with various academic partners to disseminate current trends in Artificial Intelligence and Computational science among young researchers. In this paper, we present our view on major trends and challenges today in front of scientific and industrial society in this promising area.}
}
@article{LI2025106005,
title = {Towards worker-centric construction scene understanding: Status quo and future directions},
journal = {Automation in Construction},
volume = {171},
pages = {106005},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106005},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525000457},
author = {Huimin Li and Hui Deng and Yichuan Deng},
keywords = {Scene understanding, Construction site, Computer vision, Image captioning, Construction worker},
abstract = {Construction scene understanding is the process of perceiving, analyzing, and interpreting three-dimensional dynamic scenes observed through sensor networks, which is usually real-time. The purpose is to understand the construction scene by analyzing the geometric and semantic features of the objects and their relationships. Construction scene understanding is a basic technology for construction automation and Human-Robot Interaction. Although there are several reviews on this topic, existing reviews do not cover the latest technological advances and lack systematic elaboration for construction scene understanding. Therefore, this paper reviews research on construction scene understanding conducted over the past fifteen years, summarizing five key scene elements: signals, pixels, objects, relationships, and events, along with the richness of semantic information. It also outlines advancements in perception and reasoning abilities in this domain. In addition, this review proposes five research trends and seven future directions to provide some inspiration for researchers.}
}
@article{LIU2024111818,
title = {The fusion of fuzzy theories and natural language processing: A state-of-the-art survey},
journal = {Applied Soft Computing},
volume = {162},
pages = {111818},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111818},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624005921},
author = {Ming Liu and Hongjun Zhang and Zeshui Xu and Kun Ding},
keywords = {Fuzzy theory, Natural language processing, Fusion, Artificial intelligence},
abstract = {Recent years have witnessed a drastic surge in natural language processing (NLP), which is a popular research orientation in artificial intelligence. In contrast to precise numbers, human language is very complex and diverse, with millions of expressions, both spoken and written. It is due to this ambiguity and imprecision that most of the problems in NLP relating to cognition, translation, and understanding are non-trivial. Fuzzy theory, which accepts the fact that ambiguity exists, aims to address and actively quantify conceptual vagueness into messages that can be processed by computers. Following the thread of recent studies, we systematically review the fusion of fuzzy theory and NLP technologies from the aspects of commonly used fuzzy theories in NLP, the NLP tasks fuzzy theories are applied to, the application fields of fusion and the basic paradigms of fusion. Towards the end of this paper, we delineate the constraints and obstacles encountered in current researches, while also endeavoring to suggest avenues for enhancement that may serve as a reference for subsequent scholarly inquiry.}
}
@incollection{HARRER2024345,
title = {Chapter 40 - Artificial intelligence drives the digital transformation of pharma},
editor = {Chayakrit Krittanawong},
booktitle = {Artificial Intelligence in Clinical Practice},
publisher = {Academic Press},
pages = {345-372},
year = {2024},
isbn = {978-0-443-15688-5},
doi = {https://doi.org/10.1016/B978-0-443-15688-5.00049-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443156885000498},
author = {Stefan Harrer and Jeffrey Menard and Michael Rivers and Darren V.S. Green and Joel Karpiak and Jeliazko R. Jeliazkov and Maxim V. Shapovalov and Diego {del Alamo} and Matt C. Sternke},
keywords = {Drug discovery, drug design, drug development, biopharma manufacturing, artificial intelligence, data analytics, pharma, digital health, digital transformation},
abstract = {The evolution of AI has reached a point at which first commoditized analytical tools can be integrated into real-world workflows for assisting human practitioners in the health and life sciences sector by making better and faster decisions. Pharma is embracing this development by exploring the use of AI technology not only for increasing the efficiency and capabilities of drug discovery, design, and development but also for addressing new markets and adopting new business models in digital health. Since 2018 several big pharma enterprises have built substantial in-house capabilities in data science, AI/ML research and development, struck partnerships with big tech, EMR, cloud and analytics vendors, and expanded their data and AI footprint through acquisitions of innovative start- and scale-ups. We shed a light on market and technology drivers for these developments by explaining selected high-impact use cases of AI for the drug development cycle: discovery of new drug targets, custom-design of novel drug compounds, design and execution of more efficient clinical trials, as well as automation and optimization of drug manufacturing, distribution, and marketing channels. Transporting these applications into the digital health sector, we describe why and how pharma explores the use of AI, data, and cloud technology to bring new diagnostic, prognostic, personalized treatment, and prevention schemes for health, disease, and wellness management to patients and caregivers. Focus is given to the following topics: digital therapeutics, digital diagnostics and pathology, telehealth and virtual care, patient monitoring, and interoperable and securely linked electronic health data management platforms. Technology and market trends affecting the near and long-term future of the role of AI as driver of the digital transformation of pharma are provided accompanied by a comprehensive list of technical and strategic articles and market research analyses.}
}
@article{SUTRIAWAN2024200360,
title = {Review of ambiguity problem in text summarization using hybrid ACA and SLR},
journal = {Intelligent Systems with Applications},
volume = {22},
pages = {200360},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200360},
url = {https://www.sciencedirect.com/science/article/pii/S266730532400036X},
author = {Sutriawan Sutriawan and Supriadi Rustad and Guruh Fajar Shidik and Pujiono Pujiono and Muljono Muljono},
keywords = {Systematic_literature_review, Text_summarization, Ambiguity_problem, SLR, Hybrid_ACA_&_SLR},
abstract = {Text summarization is the process of creating a text summary that contains important information from a text document. In recent years, significant progress has been made in the field of text summarization research, along with the challenges that drive research progress in the field at large. The development of textual data has sparked great interest in text summarization research, which is thoroughly reviewed in this survey study. Text summarization research improvements continue to be made to date with various approaches, such as abstractive and extractive. The abstractive approach uses an intermediate representation of the input document to produce a summary that may differ from the original text. The extractive approach means that key sentences are extracted from the source document and combined to form a summary. Despite the various methodologies and approaches recommended, the summaries produced still contain ambiguities that can be interpreted with different meanings, resulting in errors in defining ambiguities, uncertainty in measuring the quality of summaries, difficulty in modeling linguistic context, difficulty in representing semantic meanings, and difficulty in specifying types of ambiguities. This research survey offers a comprehensive exploration of text summarization research, covering challenges, classifications, approaches, preprocessing methods, features, techniques, and evaluation methods, meeting future research needs. The results provide an overview of the state of the art of recent research developments in the topic of ambiguity resolution in text summarization, such as trends in research topics and approaches or techniques used in addressing ambiguity problems in text summarization.}
}
@article{SOUDEEP2024105882,
title = {Enhancing road traffic flow in sustainable cities through transformer models: Advancements and challenges},
journal = {Sustainable Cities and Society},
volume = {116},
pages = {105882},
year = {2024},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.105882},
url = {https://www.sciencedirect.com/science/article/pii/S2210670724007066},
author = {Shahriar Soudeep and Most. {Lailun Nahar Aurthy} and Jamin Rahman Jim and M.F. Mridha and Md Mohsin Kabir},
keywords = {Predictive modeling, Transformer models, Sustainable cities, Traffic flow prediction, Urban planning, Traffic management},
abstract = {Efficient traffic flow is crucial for sustainable cities, as it directly impacts energy consumption, pollution levels, and overall quality of life. The integration of superficial intelligence, particularly transformer models, plays a significant role in enhancing the predictive capabilities for traffic management, thereby supporting sustainable urban development. In this survey, we explored the application of transformer models to predict and optimize traffic flow in sustainable cities. These models leverage advanced machine learning to capture intricate spatiotemporal patterns,thereby providing valuable insights for urban planners and traffic management centers. By systematically reviewing the literature, we emphasize the importance of transformer models in urban planning and sustainable resource use. Our study demonstrates how transformer models can learn complex spatiotemporal patterns from traffic data by incorporating both real-time and historical data to enhance prediction accuracy. This improved predictive capability aids the development of smart cities by reducing traffic congestion, facilitating smoother movement for city dwellers and tourists, and ultimately contributing to the sustainability goals of urban areas. This comprehensive review highlights the transformative potential of predictive modeling using transformer models, underscoring their critical role in optimizing urban infrastructure and promoting sustainable city development.}
}
@article{MA2025104156,
title = {A multi-view projection-based object-aware graph network for dense captioning of point clouds},
journal = {Computers & Graphics},
volume = {126},
pages = {104156},
year = {2025},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.104156},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324002917},
author = {Zijing Ma and Zhi Yang and Aihua Mao and Shuyi Wen and Ran Yi and Yongjin Liu},
keywords = {Point clouds, 3D dense captioning, Multimodel, Graph network},
abstract = {3D dense captioning has received increasing attention in the multimodal field of 3D vision and language. This task aims to generate a specific descriptive sentence for each object in the 3D scene, which helps build a semantic understanding of the scene. However, due to inevitable holes in point clouds, there are often incorrect objects in the generated descriptions. Moreover, most existing models use KNN to construct relation graphs, which are not robust and have poor adaptability to different scenes. They cannot represent the relationship between the surrounding objects well. To address these challenges, in this paper, we propose a novel multi-level mixed encoding model for accurate 3D dense captioning of objects in point clouds. To handle holes in point clouds, we extract multi-view projection image features of objects based on our key observation that a hole in an object seldom exists in all projection images from different view angles. Then, the image features are fused with object detection features as the input of subsequent modules. Moreover, we combine KNN and DBSCAN clustering algorithms to construct a graph G and fuse their output features subsequently, which ensures the robustness of the graph structure for accurately describing the relationships between objects. Specifically, DBSCAN clusters are formed based on density, which alleviates the problem of using a fixed K value in KNN. Extensive experiments conducted on ScanRefer and Nr3D datasets demonstrate the effectiveness of our proposed model.}
}
@article{GRENCZUK20245545,
title = {AI-Supported Translation Tools for Legal Texts: A Comparative Analysis},
journal = {Procedia Computer Science},
volume = {246},
pages = {5545-5554},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.707},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027686},
author = {Andrzej Greńczuk and Iwona Chomiak-Orsa and Katarzyna Tryczyńska},
keywords = {Artificial intelligence, legal translation, legal act, machine translation},
abstract = {One of the effects of globalization is the increase in the intensity and importance of international cooperation. The context of internationalization of the functioning of organizations and international contracts has influenced the need to popularize translation services. In the case of everyday language or basic communication processes, the lack of clarity and an appropriate level of quality of translations between any language of the world can cause minor problems and communication problems. However, in the case of contracts, political protocol or legal regulations, the quality of translation processes between languages is very important. Despite the high popularity of IT translation tools, there is still a need for the services of professional, traditional translators, especially when translation processes involve highly specialized vocabulary or highly formalized studies, such as legal regulations. The aim of the article is a comparative analysis of two tools using LLM in the processes of translating legal texts into less popular languages, such as Dutch and Polish. In order to assess the possibility and quality of translation of popular translators such as DeepL and Google Translate, the authors used a scientific experiment in which a sworn translator from Dutch took part, assessing the quality and unambiguity of the translations made by IT tools.}
}
@article{ZHAO2025104044,
title = {Towards human-like questioning: Knowledge base question generation with bias-corrected reinforcement learning from human feedback},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104044},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104044},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004035},
author = {Runhao Zhao and Jiuyang Tang and Weixin Zeng and Yunxiao Guo and Xiang Zhao},
keywords = {Knowledge base question generation, Reinforcement learning from human feedback, Human-like style, Sycophancy and confirmation biases},
abstract = {Knowledge Base Question Generation (KBQG) aims to output natural language questions based on a Knowledge Base (KB) and the target answers. However, existing KBQG solutions neglect the authenticity of the generated questions, and the questions exhibit low diversity and lack of human questioning style. To address this challenge, we for the first time introduce reinforcement learning from human feedback (RLHF) to cope with KBQG and propose a Bias-corrected RLHF framework, i.e., BC-RLHF, to reduce the sycophancy and confirmation biases existing in current RLHF paradigm. Specifically, we begin by training a cold-start question generation model, QG-SFT. We then design a high-quality feedback mechanism to train two models, i.e., the human-like question judge and multi-granular question quality judge, which are adept at evaluating the quality of generated questions. Subsequently, we incorporate these judges and human feedback into the reinforcement learning framework, designing a bias-corrected reinforcement learning model to optimize and train the final question generation model. Consequently, the generated questions exhibit a more human-like style while ensuring fluency and accuracy. Extensive experiments demonstrate the effectiveness of our proposed method in this challenging task, achieving relative performance gains of 24.50%, 4.21% and 2.16% on three KBQG datasets, respectively, outperforming existing state-of-the-art methods.}
}
@article{JURISICA2024102006,
title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {38},
number = {4},
pages = {102006},
year = {2024},
note = {Genetics of Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2024.102006},
url = {https://www.sciencedirect.com/science/article/pii/S1521694224000779},
author = {I Jurisica},
keywords = {Precision medicine, Rheumatoid arthritis, Artificial intelligence, Integrative computational biology},
abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.}
}
@article{CHANG2024108258,
title = {The path from task-specific to general purpose artificial intelligence for medical diagnostics: A bibliometric analysis},
journal = {Computers in Biology and Medicine},
volume = {172},
pages = {108258},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108258},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524003421},
author = {Chuheng Chang and Wen Shi and Youyang Wang and Zhan Zhang and Xiaoming Huang and Yang Jiao},
keywords = {Artificial intelligence, Bibliometric analysis, Diagnostics, Medicine, Network visualization},
abstract = {Artificial intelligence (AI) has revolutionized many fields, and its potential in healthcare has been increasingly recognized. Based on diverse data sources such as imaging, laboratory tests, medical records, and electrophysiological data, diagnostic AI has witnessed rapid development in recent years. A comprehensive understanding of the development status, contributing factors, and their relationships in the application of AI to medical diagnostics is essential to further promote its use in clinical practice. In this study, we conducted a bibliometric analysis to explore the evolution of task-specific to general-purpose AI for medical diagnostics. We used the Web of Science database to search for relevant articles published between 2010 and 2023, and applied VOSviewer, the R package Bibliometrix, and CiteSpace to analyze collaborative networks and keywords. Our analysis revealed that the field of AI in medical diagnostics has experienced rapid growth in recent years, with a focus on tasks such as image analysis, disease prediction, and decision support. Collaborative networks were observed among researchers and institutions, indicating a trend of global cooperation in this field. Additionally, we identified several key factors contributing to the development of AI in medical diagnostics, including data quality, algorithm design, and computational power. Challenges to progress in the field include model explainability, robustness, and equality, which will require multi-stakeholder, interdisciplinary collaboration to tackle. Our study provides a holistic understanding of the path from task-specific, mono-modal AI toward general-purpose, multimodal AI for medical diagnostics. With the continuous improvement of AI technology and the accumulation of medical data, we believe that AI will play a greater role in medical diagnostics in the future.}
}
@article{DEROSARIO2023,
title = {Applications of Natural Language Processing for the Management of Stroke Disorders: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/48693},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000248},
author = {Helios {De Rosario} and Salvador Pitarch-Corresa and Ignacio Pedrosa and Marina Vidal-Pedrós and Beatriz {de Otto-López} and Helena García-Mieres and Lydia Álvarez-Rodríguez},
keywords = {stroke, natural language processing, artificial intelligence, scoping review, scoping, review methods, review methodology, NLP, cardiovascular, machine learning, deep learning},
abstract = {Background
Recent advances in natural language processing (NLP) have heightened the interest of the medical community in its application to health care in general, in particular to stroke, a medical emergency of great impact. In this rapidly evolving context, it is necessary to learn and understand the experience already accumulated by the medical and scientific community.
Objective
The aim of this scoping review was to explore the studies conducted in the last 10 years using NLP to assist the management of stroke emergencies so as to gain insight on the state of the art, its main contexts of application, and the software tools that are used.
Methods
Data were extracted from Scopus and Medline through PubMed, using the keywords “natural language processing” and “stroke.” Primary research questions were related to the phases, contexts, and types of textual data used in the studies. Secondary research questions were related to the numerical and statistical methods and the software used to process the data. The extracted data were structured in tables and their relative frequencies were calculated. The relationships between categories were analyzed through multiple correspondence analysis.
Results
Twenty-nine papers were included in the review, with the majority being cohort studies of ischemic stroke published in the last 2 years. The majority of papers focused on the use of NLP to assist in the diagnostic phase, followed by the outcome prognosis, using text data from diagnostic reports and in many cases annotations on medical images. The most frequent approach was based on general machine learning techniques applied to the results of relatively simple NLP methods with the support of ontologies and standard vocabularies. Although smaller in number, there has been an increasing body of studies using deep learning techniques on numerical and vectorized representations of the texts obtained with more sophisticated NLP tools.
Conclusions
Studies focused on NLP applied to stroke show specific trends that can be compared to the more general application of artificial intelligence to stroke. The purpose of using NLP is often to improve processes in a clinical context rather than to assist in the rehabilitation process. The state of the art in NLP is represented by deep learning architectures, among which Bidirectional Encoder Representations from Transformers has been found to be especially widely used in the medical field in general, and for stroke in particular, with an increasing focus on the processing of annotations on medical images.}
}
@article{DAROCHAFRANCO2024100708,
title = {Managing and controlling digital role-playing game elements: A current state of affairs},
journal = {Entertainment Computing},
volume = {51},
pages = {100708},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000764},
author = {Artur de Oliveira {da Rocha Franco} and Windson Viana {de Carvalho} and José Wellington Franco {da Silva} and José Gilvan Rodrigues Maia and Miguel Franklin {de Castro}},
keywords = {RPG, Artificial intelligence, Procedural content generation},
abstract = {Role-playing games (RPGs) are interactive gaming experiences designed and driven by the intricate development of narratives and characters. However, these games also highlight elements, such as combat, exploration, creativity, and crafting. Game developers helped to shape the market by incorporating RPGs into digital media; however, they suffered restrictions in their game design and storytelling caused by technological limitations. Fortunately, academia has studied these challenges and mitigated these latter issues to better understand and connect digital RPGs with their analog versions. Our research offers a comprehensive overview of studies centered on technologies for managing, generating, and controlling digital RPG elements. We aim to characterize Artificial Intelligence research within this domain and elucidate the diverse techniques employed. In this context, we examined 72 papers about Procedural Content Generation (PCG) in RPG, which were identified by mixing a database search with a snowballing method. In this paper, we provide an overview of PCG techniques investigated within the realm of RPGs and present the trending approaches for future developments in the field.}
}
@article{LIU2024112041,
title = {Event extraction as machine reading comprehension with question-context bridging},
journal = {Knowledge-Based Systems},
volume = {299},
pages = {112041},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112041},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124006750},
author = {Liu Liu and Ming Liu and Shanshan Liu and Kun Ding},
keywords = {Event extraction, Natural language processing, Graph neural network, Machine reading comprehension},
abstract = {Most existing methods regard event extraction as the classification task. They not only heavily rely on named entity recognition, causing error propagation, but are also inefficient in low resource scenarios. To deal with above challenges, we propose an improved machine reading comprehension (MRC) approach, namely MRCBEE. Firstly, a new paradigm is applied to redefine event extraction as MRC task by designing question templates for event detection and argument extraction tasks. Specially, Question-Context Bridging is a new graph structure drawn to reconstruct the semantic relationship between the template and the text, in order to strengthen the prompt role of question templates. Then, a cross-domain attention module is designed to integrate both the semantic feature and global feature of words. A new GNN based on gated mechanism is proposed to capture the global feature and filter the information of invalid neighbors. Finally, the results of our experiments show that MRCBEE achieves better performance than the state-of-the-art methods on ACE2005 and ERE datasets. And it outperforms prior methods in low resource and complex text scenarios.}
}
@article{SUKHOBOKOV2024101279,
title = {A universal knowledge model and cognitive architectures for prototyping AGI},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101279},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000731},
author = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
keywords = {Cognitive architecture, AGI, Metagraph, Archigraph, Universal knowledge model, Machine consciousness, Machine subconsciousness, Machine reflection, Machine worldview},
abstract = {The article identified 56 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a reference cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph model are used, constructed as a development of annotated metagraphs. As other components, the reference cognitive architecture being developed includes following modules: machine consciousness, machine subconsciousness, interaction with the external environment, a goal management, an emotional control, social interaction, reflection, ethics, worldview, learning, monitoring, statement problems, solving problems, self-organization and meta learning. Based on the composition of the proposed reference architecture modules, existing cognitive architectures containing the following modules were analyzed: machine consciousness, machine subconsciousness, reflection, worldview.}
}
@article{PEREZPEREZ2024103960,
title = {Insights into wheat science: A bibliometric review using unsupervised machine learning techniques},
journal = {Journal of Cereal Science},
volume = {118},
pages = {103960},
year = {2024},
issn = {0733-5210},
doi = {https://doi.org/10.1016/j.jcs.2024.103960},
url = {https://www.sciencedirect.com/science/article/pii/S0733521024001188},
author = {Martín Pérez-Pérez and Miguel Ribeiro and Florentino Fdez-Riverola and Gilberto Igrejas},
keywords = {Wheat, Literature analysis, Knowledge extraction, Machine learning, LLM, Clustering},
abstract = {Wheat (Triticum spp.) has been one of the most important cereal crops, serving as a source of protein and energy in the human diet. It remains a vital component of global food security, with extensive scientific literature dedicated to its study, although the large volume of literature often hinders global analysis. In this study, different unsupervised machine learning techniques, such as K-Nearest Neighbors (KNN) and Uniform Manifold Approximation and Projection (UMAP), text mining analyses, including word embeddings and statistical word analysis, and graph analysis methodologies, were applied to gain a deeper understanding of the wheat literature. The proposed bibliometric analysis was conducted and integrated with the Journal Citation Reports (JCR) to identify major wheat research trends in the PubMed literature. This analysis examined the evolution of these trends over time, evaluated the geographical distribution, impact, and research domains, and assessed author collaboration networks and the evolving relevance of different countries. Research on disease resistance, genetic modification, and dietary impact demonstrates a consistent increase in output, while interest in topics related to overcoming salt stress and enhancing animal feed appears to be diminishing. Interestingly, research on wheat germ agglutinin saw a surge in interest during the late 2000s, stabilizing thereafter. These trends underscore the dynamic nature of wheat research, driven by evolving priorities and technological advancements, particularly in genetics and omics tools. Moreover, the increasing significance of China in wheat research, including its size, impact, and networking, alongside longstanding leaders such as the United States, signals a shifting landscape in global wheat research.}
}
@article{YANG2024167263,
title = {Autophagy and machine learning: Unanswered questions},
journal = {Biochimica et Biophysica Acta (BBA) - Molecular Basis of Disease},
volume = {1870},
number = {6},
pages = {167263},
year = {2024},
issn = {0925-4439},
doi = {https://doi.org/10.1016/j.bbadis.2024.167263},
url = {https://www.sciencedirect.com/science/article/pii/S0925443924002527},
author = {Ying Yang and Zhaoying Pan and Jianhui Sun and Joshua Welch and Daniel J. Klionsky},
keywords = {Artificial intelligence, Lysosome, Macroautophagy, Stress},
abstract = {Autophagy is a critical conserved cellular process in maintaining cellular homeostasis by clearing and recycling damaged organelles and intracellular components in lysosomes and vacuoles. Autophagy plays a vital role in cell survival, bioenergetic homeostasis, organism development, and cell death regulation. Malfunctions in autophagy are associated with various human diseases and health disorders, such as cancers and neurodegenerative diseases. Significant effort has been devoted to autophagy-related research in the context of genes, proteins, diagnosis, etc. In recent years, there has been a surge of studies utilizing state of the art machine learning (ML) tools to analyze and understand the roles of autophagy in various biological processes. We taxonomize ML techniques that are applicable in an autophagy context, comprehensively review existing efforts being taken in this direction, and outline principles to consider in a biomedical context. In recognition of recent groundbreaking advances in the deep-learning community, we discuss new opportunities in interdisciplinary collaborations and seek to engage autophagy and computer science researchers to promote autophagy research with joint efforts.}
}
@article{TANG2024110839,
title = {Cyber threat indicators extraction based on contextual knowledge prompt},
journal = {Computer Networks},
volume = {254},
pages = {110839},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110839},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624006716},
author = {Hailiang Tang and Dawei Lin and Wanyu Li and Wenxiao Zhang and Jun Zhao},
keywords = {Cyber threat intelligence, Cyber security, IOC extraction, Interpretability, Social data},
abstract = {Extracting Indicators of Compromise (IOC) from security-related social data (e.g., security blogs, hacker forums) is crucial for predicting cyber risks and mitigating cyber attacks proactively. However, existing IOC extraction approaches suffer from two serious limitations. First, they fail to learn the multiculti-granular and fine-grained IOC features, resulting in high false positives. Second, current methods cannot incorporate symbolic rules and contextual knowledge, resulting in poor interpretability. In this paper, we propose AIIOC, an Accurate and Interpretable I O C extraction model based on contextual knowledge prompts. Particularly, AIIOC first proposes a multi-granularity attention mechanism to learn fine-grained IOC features and boost the accuracy of IOC identification. Additionally, AIIOC designs a novel sequence labeling method that integrates symbolic rules and contextual knowledge prompts, which can encode symbolic rules and contextual semantics of IOC in trainable recurrent neural networks to improve both accuracy and interpretability. Experimental results on two real-world datasets verify that AIIOC outperforms state-of-the-art methods and showcases promising interpretability by incorporating symbolic rules and contextual knowledge prompts.}
}
@article{DU2025102755,
title = {Natural language processing in finance: A survey},
journal = {Information Fusion},
volume = {115},
pages = {102755},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102755},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005335},
author = {Kelvin Du and Yazhi Zhao and Rui Mao and Frank Xing and Erik Cambria},
keywords = {Natural language processing, Finance, Financial sentiment analysis, Financial narrative processing, Financial forecasting, Portfolio management, Question answering, Risk management, Regulatory compliance, ESG and sustainable finance, Explainable AI, Digital assets},
abstract = {This survey presents an in-depth review of the transformative role of Natural Language Processing (NLP) in finance, highlighting its impact on ten major financial applications: (1) financial sentiment analysis, (2) financial narrative processing, (3) financial forecasting, (4) portfolio management, (5) question answering, virtual assistant and chatbot, (6) risk management, (7) regulatory compliance monitoring, (8) Environmental, Social, Governance (ESG) and sustainable finance, (9) explainable artificial intelligence (XAI) in finance and (10) NLP for digital assets. With the integration of vast amounts of unstructured financial data and advanced NLP techniques, the study explores how NLP enables data-driven decision-making and innovation in the financial sector, alongside the limitations and challenges. By providing a comprehensive analysis of NLP applications combining both academic and industrial perspectives, this study postulates the future trends and evolution of financial services. It introduces a unique review framework to understand the interaction of financial data and NLP technologies systematically and outlines the key drivers, transformations, and emerging areas in this field. This survey targets researchers, practitioners, and professionals, aiming to close their knowledge gap by highlighting the significance and future direction of NLP in enhancing financial services.}
}
@article{ISAEE2025121643,
title = {Addressing the challenges of open n-ary relation extraction with a deep learning-driven approach},
journal = {Information Sciences},
volume = {692},
pages = {121643},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121643},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524015573},
author = {Mitra Isaee and Afsaneh Fatemi and Mohammadali Nematbakhsh},
keywords = {Natural language processing, Open relation extraction, Open-domain text, N-ary relation, Entity embedding, SpanBERT},
abstract = {Open relation extraction is a critical task in natural language processing aimed at automatically extracting relations between entities in open-domain corpora. Most existing systems focus on extracting binary relations (relations between two entities) while extracting more complex n-ary relations (involving more than two entities) remains a significant challenge. Additionally, many previous systems rely on hand-crafted patterns and natural language processing tools, which result in error accumulation and reduced accuracy. The current study proposes a novel approach to open n-ary relation extraction that leverages recent advancements in deep learning architectures. This approach addresses the limitations of existing open relation extraction systems, particularly their reliance on hand-crafted patterns and their focus on binary relations. It utilizes SpanBERT to capture relational patterns from text data directly and introduces entity embedding vectors to create distinct representations of entities within sentences. These vectors enhance the proposed system’s understanding of the entities within the input sentence, leading to more accurate relation extraction. Notably, the proposed system in the present study achieves an F1-score of 89.79 and 92.67 on the LSOIE-wiki and OpenIE4 datasets, outperforming the best existing models by over 12% and 10%, respectively. These results highlight the effectiveness of the proposed approach in addressing the challenges of open n-ary relation extraction.}
}
@article{LAVALLE2025102410,
title = {A methodology for the systematic design of storytelling dashboards applied to Industry 4.0},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102410},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102410},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000059},
author = {Ana Lavalle and Alejandro Maté and Maribel Yasmina Santos and Pedro Guimarães and Juan Trujillo and Antonina Santos},
keywords = {Analytical requirements, Big data, Data visualization, Storytelling dashboards, Industrial processes},
abstract = {Dashboards are popular tools for presenting key insights to decision-makers by translating large volumes of data into clear information. However, while individual visualizations may effectively answer specific questions, they often fail to connect in a way that conveys the overall narrative, leaving decision-makers without a cohesive understanding of the area under analysis. This paper presents a novel methodology for the systematic design of holistic dashboards, moving from analytical requirements to storytelling dashboards. Our approach ensures that all visualizations are aligned with the analytical goals of decision-makers. It includes several key steps: capturing analytical requirements through the i* framework; structuring and refining these requirements into a tree model to reflect the decision-maker’s mental analysis; identifying and preparing relevant data; capturing the key concepts and relationships for the composition of the cohesive storytelling dashboard through a novel storytelling conceptual model; finally, implementing and integrating the visualizations into the dashboard, ensuring coherence and alignment with the decision-maker’s needs. Our methodology has been applied in real-world industrial environments. We evaluated its impact through a controlled experiment. The findings show that storytelling dashboards significantly improve data interpretation, reduce misinterpretations, and enhance the overall user experience compared to traditional dashboards.}
}
@article{ZHANG2025128695,
title = {Cross-attention multi-perspective fusion network based fake news censorship},
journal = {Neurocomputing},
volume = {611},
pages = {128695},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128695},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014668},
author = {Weishan Zhang and Mingli Zhang and Zhicheng Bao and Zhenqi Wang},
keywords = {Fake news censorship, Internal perspectives, News environment, Cross-attention, LLMs},
abstract = {Current fake news censorship models mostly use only one single semantic perspective, which contains insufficient information and may result in biases. However, news inherently encompasses multiple perspectives, including internal elements such as semantics, emotion, and style, as well as the news environment which may offer valuable information for news censorship. Combining multiple perspectives helps mitigate the biases and provides comprehensive information for news censorship. Thus, we propose the Cross-Attention Multi-Perspective Fusion Network Based Fake News Censorship (MPFNC). In this model, we design an Internal Multi-Perspective Fusion Network to integrate the internal perspectives of news, including semantic, emotion, and style, and equip it with Cross-Attention to capture the correlations between these perspectives. Additionally, we construct the news environment based on the fuzzy cluster and apply Cross-Attention to uncover associations with the internal perspectives. Compared to the current SOTA models, our model achieves a 3.04% and 3.86% improvement in F1 score on Chinese and English datasets, respectively. On fake news datasets generated by LLMs, our model also outperforms the best-performing LLM by 10% in accuracy.}
}
@article{HU2024103742,
title = {DLRGeoTweet: A comprehensive social media geocoding corpus featuring fine-grained places},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103742},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103742},
url = {https://www.sciencedirect.com/science/article/pii/S030645732400102X},
author = {Xuke Hu and Tobias Elßner and Shiyu Zheng and Helen Ngonidzashe Serere and Jens Kersten and Friederike Klan and Qinjun Qiu},
keywords = {Annotated twitter corpus, Geoparsing, Geocoding, Toponym resolution, Toponym disambiguation, Fine-grained places},
abstract = {Every day, many short text messages on social media are generated in response to real-world events, providing a valuable resource for various domains such as emergency response and traffic management. Since exact coordinates of social media posts are rarely attached by users, accurately recognizing and resolving fine-grained place names, such as home addresses and Points of Interest, from these posts is crucial for understanding the precise locations of critical events, such as rescue requests. This task, known as geoparsing, involves toponym recognition and toponym resolution or geocoding. However, existing social media datasets for evaluating geoparsing approaches often lack sufficient fine-grained place names with associated geo-coordinates or linked to gazetteers, making evaluating, comparing, and training geocoding methods for such locations challenging. Moreover, the absence of supportive annotation tools compounds this challenge. To address these gaps, we implemented a lightweight Python tool leveraging Nominatim. Using this tool, we annotated a comprehensive X (formerly Twitter) geocoding corpus called DLRGeoTweet. The corpus underwent a rigorous cross-validation process to guarantee its quality. This corpus includes a total of 7,364 tweets and 12,510 places, of which 6,012 are fine-grained. It comprises two global datasets encompassing worldwide events and three local datasets related to local events such as the 2017 Hurricane Harvey. The annotation process spanned over ten months and required approximately 1000 person-hours to complete. We then evaluate 15 latest and representative geocoding approaches, including many deep learning-based, on DLRGeoTweet. The results highlight the inherent challenges in resolving fine-grained places accurately. Despite increasing access constraints to Twitter data, our corpus’s focus on short, informal text makes it a valuable resource for geocoding across multiple social media platforms.}
}
@article{FEHER2025109492,
title = {Learning to generate and evaluate fact-checking explanations with transformers},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109492},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109492},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624016506},
author = {Darius Feher and Abdullah Khered and Hao Zhang and Riza Batista-Navarro and Viktor Schlegel},
keywords = {Explainable fact checking, Natural language generation, Automated natural language generation evaluation, Explainability, Fact verification},
abstract = {In an era increasingly dominated by digital platforms, the spread of misinformation poses a significant challenge, highlighting the need for solutions capable of assessing information veracity. Our research contributes to the field of Explainable Artificial Antelligence (XAI) by developing transformer-based fact-checking models that contextualise and justify their decisions by generating human-accessible explanations. Importantly, we also develop models for automatic evaluation of explanations for fact-checking verdicts across different dimensions such as (self)-contradiction, hallucination, convincingness and overall quality. By introducing human-centred evaluation methods and developing specialised datasets, we emphasise the need for aligning Artificial Intelligence (AI)-generated explanations with human judgements. This approach not only advances theoretical knowledge in XAI but also holds practical implications by enhancing the transparency, reliability and users’ trust in AI-driven fact-checking systems. Furthermore, the development of our metric learning models is a first step towards potentially increasing efficiency and reducing reliance on extensive manual assessment. Based on experimental results, our best performing generative model achieved a Recall-Oriented Understudy for Gisting Evaluation-1 (ROUGE-1) score of 47.77 demonstrating superior performance in generating fact-checking explanations, particularly when provided with high-quality evidence. Additionally, the best performing metric learning model showed a moderately strong correlation with human judgements on objective dimensions such as (self)-contradiction and hallucination, achieving a Matthews Correlation Coefficient (MCC) of around 0.7.}
}
@article{THO2024104674,
title = {Improving biomedical Named Entity Recognition with additional external contexts},
journal = {Journal of Biomedical Informatics},
volume = {156},
pages = {104674},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104674},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000923},
author = {Bui Duc Tho and Minh-Tien Nguyen and Dung Tien Le and Lin-Lung Ying and Shumpei Inoue and Tri-Thanh Nguyen},
keywords = {Biomedical Named Entity Recognition, Information extraction, Transformers, External contexts},
abstract = {Objective:
Biomedical Named Entity Recognition (bio NER) is the task of recognizing named entities in biomedical texts. This paper introduces a new model that addresses bio NER by considering additional external contexts. Different from prior methods that mainly use original input sequences for sequence labeling, the model takes into account additional contexts to enhance the representation of entities in the original sequences, since additional contexts can provide enhanced information for the concept explanation of biomedical entities.
Methods:
To exploit an additional context, given an original input sequence, the model first retrieves the relevant sentences from PubMed and then ranks the retrieved sentences to form the contexts. It next combines the context with the original input sequence to form a new enhanced sequence. The original and new enhanced sequences are fed into PubMedBERT for learning feature representation. To obtain more fine-grained features, the model stacks a BiLSTM layer on top of PubMedBERT. The final named entity label prediction is done by using a CRF layer. The model is jointly trained in an end-to-end manner to take advantage of the additional context for NER of the original sequence.
Results:
Experimental results on six biomedical datasets show that the proposed model achieves promising performance compared to strong baselines and confirms the contribution of additional contexts for bio NER.
Conclusion:
The promising results confirm three important points. First, the additional context from PubMed helps to improve the quality of the recognition of biomedical entities. Second, PubMed is more appropriate than the Google search engine for providing relevant information of bio NER. Finally, more relevant sentences from the context are more beneficial than irrelevant ones to provide enhanced information for the original input sequences. The model is flexible to integrate any additional context types for the NER task.}
}
@article{GAO2024723,
title = {Artificial Intelligence in manufacturing: State of the art, perspectives, and future directions},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {723-749},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.04.101},
url = {https://www.sciencedirect.com/science/article/pii/S000785062400115X},
author = {Robert X. Gao and Jörg Krüger and Marion Merklein and Hans-Christian Möhring and József Váncza},
keywords = {Artificial intelligence, Smart manufacturing, Machine learning},
abstract = {Inspired by the natural intelligence of humans and bio-evolution, Artificial Intelligence (AI) has seen accelerated growth since the beginning of the 21st century. Successful AI applications have been broadly reported, with Industry 4.0 providing a thematic platform for AI-related research and development in manufacturing. This paper highlights applications of AI in manufacturing, ranging from production system design and planning to process modeling, optimization, quality assurance, maintenance, automated assembly and disassembly. In addition, the paper presents an overview of representative manufacturing problems and matching AI solutions, and a perspective of future research to leverage AI towards the realization of smart manufacturing.}
}
@article{FRIEDMAN2024385,
title = {Editorial},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {10},
number = {4},
pages = {385-388},
year = {2024},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S240587262400100X},
author = {Ken Friedman and Yongqi Lou and Jin Ma}
}
@article{NICOLSON2023102633,
title = {Improving chest X-ray report generation by leveraging warm starting},
journal = {Artificial Intelligence in Medicine},
volume = {144},
pages = {102633},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102633},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723001471},
author = {Aaron Nicolson and Jason Dowling and Bevan Koopman},
keywords = {Chest X-ray report generation, Image captioning, Multi-modal learning, Warm starting},
abstract = {Automatically generating a report from a patient’s Chest X-rays (CXRs) is a promising solution to reducing clinical workload and improving patient care. However, current CXR report generators—which are predominantly encoder-to-decoder models—lack the diagnostic accuracy to be deployed in a clinical setting. To improve CXR report generation, we investigate warm starting the encoder and decoder with recent open-source computer vision and natural language processing checkpoints, such as the Vision Transformer (ViT) and PubMedBERT. To this end, each checkpoint is evaluated on the MIMIC-CXR and IU X-ray datasets. Our experimental investigation demonstrates that the Convolutional vision Transformer (CvT) ImageNet-21K and the Distilled Generative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for warm starting the encoder and decoder, respectively. Compared to the state-of-the-art (M2 Transformer Progressive), CvT2DistilGPT2 attained an improvement of 8.3% for CE F-1, 1.8% for BLEU-4, 1.6% for ROUGE-L, and 1.0% for METEOR. The reports generated by CvT2DistilGPT2 have a higher similarity to radiologist reports than previous approaches. This indicates that leveraging warm starting improves CXR report generation. Code and checkpoints for CvT2DistilGPT2 are available at https://github.com/aehrc/cvt2distilgpt2.}
}
@article{WEI20243299,
title = {Graph Convolutional Networks Embedding Textual Structure Information for Relation Extraction},
journal = {Computers, Materials and Continua},
volume = {79},
number = {2},
pages = {3299-3314},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.047811},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824002613},
author = {Chuyuan Wei and Jinzhe Li and Zhiyuan Wang and Shanshan Wan and Maozu Guo},
keywords = {Relation extraction, graph convolutional neural networks, dependency tree, dynamic structure attention},
abstract = {Deep neural network-based relational extraction research has made significant progress in recent years, and it provides data support for many natural language processing downstream tasks such as building knowledge graph, sentiment analysis and question-answering systems. However, previous studies ignored much unused structural information in sentences that could enhance the performance of the relation extraction task. Moreover, most existing dependency-based models utilize self-attention to distinguish the importance of context, which hardly deals with multiple-structure information. To efficiently leverage multiple structure information, this paper proposes a dynamic structure attention mechanism model based on textual structure information, which deeply integrates word embedding, named entity recognition labels, part of speech, dependency tree and dependency type into a graph convolutional network. Specifically, our model extracts text features of different structures from the input sentence. Textual Structure information Graph Convolutional Networks employs the dynamic structure attention mechanism to learn multi-structure attention, effectively distinguishing important contextual features in various structural information. In addition, multi-structure weights are carefully designed as a merging mechanism in the different structure attention to dynamically adjust the final attention. This paper combines these features and trains a graph convolutional network for relation extraction. We experiment on supervised relation extraction datasets including SemEval 2010 Task 8, TACRED, TACREV, and Re-TACED, the result significantly outperforms the previous.}
}
@article{BASOLE2024114133,
title = {Complex business ecosystem intelligence using AI-powered visual analytics},
journal = {Decision Support Systems},
volume = {178},
pages = {114133},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114133},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623002087},
author = {Rahul C. Basole and Hyunwoo Park and C. David Seuss},
keywords = {Business ecosystem, Artificial intelligence, Text mining, Complex networks, Interactive visualization},
abstract = {Business ecosystems are complex, dynamic systems characterized by a multitude of entities, including companies, ventures, and technologies, as well as activities and trends. Understanding the state of business ecosystems is an increasingly critical strategic imperative for many decision makers, but it is a resource-intensive activity as relevant information sources are dispersed, often highly unstructured, and not integrated or curated to deliver actionable insights. In this research, we present the design and implementation of an interactive visual analytic system that integrates artificial intelligence and graph visualization techniques to augment decision makers’ understanding of the complex public narrative associated with business ecosystems entities. Our system is driven by a real-time content engine of 100,000+ global data sources including press releases, news articles, industry reports, analyst blogs in multiple languages organized across several domain-specific repositories. Following a user-specified query, the engine extracts both domain-agnostic and domain-specific entities and concepts for each document in the result set. We then model and visualize the resulting data as a dynamic, multipartite network and implement graph pruning algorithms and interactive data controls to enable users to interactively explore and discover the underlying business ecosystem from multiple perspectives. We illustrate and discuss the value of our system using representative use cases. Our study makes multiple contributions to visual decision support theory and practice, including mining unstructured data, constructing and interacting with knowledge graphs, and designing visual analytic tools for ecosystem intelligence. We conclude the study with implications and future research opportunities.}
}
@article{CHU2025126378,
title = {GeoSMIE: An event extraction framework for Document-Level spatial morphological information extraction},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126378},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126378},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424032457},
author = {Deping Chu and Bo Wan and Huizhu Ni and Hong Li and Zhuo Tan and Yan Dai and Zijing Wan and Tao Tang and Shunping Zhou},
keywords = {Spatial information extraction, Spatial morphological information, Chinese geological text, Event extraction},
abstract = {Spatial morphological information (SMI) in geological texts provides critical insights into the formation, localization, and distribution of geological bodies. However, SMI is often scattered across multiple sentences or coexists in complex forms within the same document, making it challenging to extract using traditional methods. In this paper, we address this gap by formalizing SMI extraction as an event extraction task and proposed a novel Geological body SMI Extraction model, GeoSMIE. Our approach is innovative in two key ways: first, we implement a no-trigger-word annotation strategy to capture both descriptive and digital SMI, ensuring that SMI without explicit morphological triggers is not missed. Second, we design dual graph neural networks (GNNs) to handle long-distance dependencies and complex interactions between scattered arguments across sentences. To validate its effectiveness, we compared GeoSMIE to state-of-the-art models on the constructed dataset. For SMI extraction, GeoSMIE outperformed the optimal baseline by 0.4%, 2.2%, and 1.5% for accuracy, recall, and Micro-F1 score, respectively. This work provides an innovative idea for extracting complex spatial information from geoscience texts.}
}
@article{LIN2025102795,
title = {Has multimodal learning delivered universal intelligence in healthcare? A comprehensive survey},
journal = {Information Fusion},
volume = {116},
pages = {102795},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102795},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005736},
author = {Qika Lin and Yifan Zhu and Xin Mei and Ling Huang and Jingying Ma and Kai He and Zhen Peng and Erik Cambria and Mengling Feng},
keywords = {Multimodal fusion, Intelligent healthcare, Multimodal learning, Foundation model, Medical vision-language},
abstract = {The rapid development of artificial intelligence has constantly reshaped the field of intelligent healthcare and medicine. As a vital technology, multimodal learning has increasingly garnered interest because of data complementarity, comprehensive information fusion, and great application potential. Currently, numerous researchers are dedicating their attention to this field, conducting extensive studies and constructing abundant intelligent systems. Naturally, an open question arises that has multimodal learning delivered universal intelligence in healthcare? To answer this question, we adopt three unique viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey of the current progress of medical multimodal learning from the perspectives of datasets, task-oriented methods, and universal foundation models. Based on them, we further discuss the proposed question from five issues to explore the real impacts of advanced techniques in healthcare, from data and technologies to performance and ethics. The answer is that current technologies have NOT achieved universal intelligence and there remains a significant journey to undertake. Finally, in light of the above reviews and discussions, we point out ten potential directions for exploration to promote multimodal fusion technologies in the domain, towards the goal of universal intelligence in healthcare.}
}
@article{ZHANG2024105632,
title = {Knowledge management for off-site construction},
journal = {Automation in Construction},
volume = {166},
pages = {105632},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105632},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003686},
author = {Zhen Zhang and Yang Zou and Brian H.W. Guo and Johannes Dimyadi and Roy Davies and Lixin Jiang},
keywords = {Off-site construction (OSC), Knowledge management (KM), Artificial intelligence (AI), Systematic literature review},
abstract = {Off-site construction (OSC) is expected to boost productivity, shorten construction time, and reduce labour and material wastage. Despite these benefits, most OSC projects have not fully achieved these advantages, where a primary obstacle lies in the limited management of OSC knowledge. However, there is still no holistic understanding of the integration of KM in the OSC context. Therefore, this paper explores the latest development in KM for OSC through a systematic literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and template analysis. The review is based on 66 screened and assessed journal articles from all years to 2024 with a particular focus on KM and OSC. Through the quantitative and qualitative analysis, this study groups four main research themes including KM for OSC design, KM for OSC project management, knowledge-based OSC decision-making, and the management of OSC knowledge. The results are discussed to gain a systematic understanding of key OSC knowledge domains, investigate the integration of KM for OSC, and explore future research needs including emerging artificial intelligence (AI) technologies.}
}
@article{RONG2025102819,
title = {Pred-ID: Future event prediction based on event type schema mining by graph induction and deduction},
journal = {Information Fusion},
volume = {117},
pages = {102819},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102819},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005979},
author = {Huan Rong and Zhongfeng Chen and Zhenyu Lu and Xiao-ke Xu and Kai Huang and Victor S. Sheng},
keywords = {Event intelligence analysis, Event graph mining, Graph generation, Event prediction},
abstract = {In the field of information management, effective event intelligence management is crucial for its development. With the continuous evolution of events, predicting future events has become a key task in information management. Event Prediction aims to predict upcoming events based on given contextual information. This requires modeling events and their relationships in the context to infer the structure of future events. However, the existing event prediction methods ignore that the event graph schema based on core events can provide more knowledge about history and future for event prediction through induction and deduction, so as to achieve accurate event prediction. In addressing this issue, we directed our focus towards Event Schema Induction. Inspired by it, we propose the Pred-ID model, designed to build event evolutionary pattern through Inductive Event Graph Generation, Deductive Event Graph Expansion, and Graph Fusion for Event Prediction. Specifically, in the Inductive Event Graph Generation phase, Pred-ID extracts the event core subgraph and event developmental trends from the instance event graph, learning the global structure and uncovering the main processes of event development. Then, in the Deductive Event Graph Expansion phase, by expanding future event node and stretching the main processes of event development into future directions, Pred-ID obtains deductive results, so as to construct the event evolutionary pattern. Finally, in the Graph Fusion for Event Prediction phase, aligning and merging the event evolutionary pattern with the instance event graph enables collaborative prediction of future events. The experimental results indicate that our proposed Pred-ID achieves optimal performance in event evolutionary pattern generation and event prediction tasks.}
}
@article{ALONSOBARRIUSO2024109028,
title = {Recommendation system of scientific articles from discharge summaries},
journal = {Engineering Applications of Artificial Intelligence},
volume = {136},
pages = {109028},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109028},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624011862},
author = {Adrián {Alonso Barriuso} and Alberto Fernández-Isabel and Isaac {Martín de Diego} and Alfonso Ardoiz and J.F. {J. Viseu Pinheiro}},
keywords = {Medical text processing, Diagnosis retrieval, Computational semantics, Recommendation system, Scientific relevance},
abstract = {Medical professionals are often overwhelmed by the amount of patients they have to care for, leaving little time available to keep up to date in their respective specialities. They usually find it challenging to keep up with the vast amount of medical literature and identify the most relevant articles for their practice, especially those related to their patient’s specific conditions. Therefore, a system that proactively supports healthcare professionals in selecting relevant articles related to the characteristics of the patients is crucial. This paper presents Medical Expert Linguist for Evaluating Nosology and Diagnosis Information (MELENDI) to tackle this issue. It is a recommendation system that effectively and efficiently recommends pertinent medical articles to healthcare professionals based on their patients’ diagnoses. It combines a semantic similarity model generated using the content of discharge summaries, with a relevance estimator produced by analysing scientific publications. To test the system, 1,000,000 abstracts were obtained from PubMed and 10 discharge reports from ’Medical Information Mart for Intensive Care (MIMIC-III) were used. A group of 5 medical specialists has been involved in the system’s evaluation. These evaluations demonstrated good overall performance, supporting the implementation of the system in a real-world environment, such as a hospital information system.}
}
@article{WU2025128999,
title = {MixEI: Mixing explicit and implicit commonsense knowledge in open-domain dialogue response generation},
journal = {Neurocomputing},
volume = {618},
pages = {128999},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128999},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224017703},
author = {Sixing Wu and Jiong Yu and Wei Zhou},
keywords = {Dialogue generation, Commonsense knowledge, Knowledge alignment, Implicit knowledge externalization},
abstract = {The inadequate awareness of real-world knowledge often causes machines to produce generic responses, such as ‘I think so.’, which may bring the degression of user interests. Consequently, enriching knowledge awareness and fabricating informative responses are long-standing challenges in open-domain dialogue systems. Previous studies have shown incorporating everyday commonsense knowledge can significantly enhance the open-domain dialogue response models. Nonetheless, previous works can only use explicit or implicit knowledge. Unlike them, this work presents a novel MixEI to leverage both explicit and implicit commonsense knowledge in dialogue generation. MixEI uses Dual-Way Knowledge Alignment and MixEI-Ranker to retrieve a set of contextually relevant commonsense facts as the explicit background knowledge and identify implicit knowledge labels by selecting clue facts that can tightly connect the dialogue context. MixEI uses BART as the backbone. After jointly encoding the background knowledge and dialogue history, MixEI first tries to externalize the implicit clue knowledge; then, the response decoding can seek information from both explicit and implicit knowledge. Extensive experiments on Chinese Weibo and English Reddit have verified the superior performance of the proposed MixEI-Ranker and MixEI.}
}
@article{VISWESWARAN2024104713,
title = {Fairness and inclusion methods for biomedical informatics research},
journal = {Journal of Biomedical Informatics},
volume = {158},
pages = {104713},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104713},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400131X},
author = {Shyam Visweswaran and Yuan Luo and Mor Peleg}
}
@article{YANG2025,
title = {Robust Automated Harmonization of Heterogeneous Data Through Ensemble Machine Learning: Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/54133},
url = {https://www.sciencedirect.com/science/article/pii/S229196942500016X},
author = {Doris Yang and Doudou Zhou and Steven Cai and Ziming Gan and Michael Pencina and Paul Avillach and Tianxi Cai and Chuan Hong},
keywords = {ensemble learning, semantic learning, distribution learning, variable harmonization, machine learning, cardiovascular health study, intracohort comparison, intercohort comparison, gold standard labels},
abstract = {Background
Cohort studies contain rich clinical data across large and diverse patient populations and are a common source of observational data for clinical research. Because large scale cohort studies are both time and resource intensive, one alternative is to harmonize data from existing cohorts through multicohort studies. However, given differences in variable encoding, accurate variable harmonization is difficult.
Objective
We propose SONAR (Semantic and Distribution-Based Harmonization) as a method for harmonizing variables across cohort studies to facilitate multicohort studies.
Methods
SONAR used semantic learning from variable descriptions and distribution learning from study participant data. Our method learned an embedding vector for each variable and used pairwise cosine similarity to score the similarity between variables. This approach was built off 3 National Institutes of Health cohorts, including the Cardiovascular Health Study, the Multi-Ethnic Study of Atherosclerosis, and the Women’s Health Initiative. We also used gold standard labels to further refine the embeddings in a supervised manner.
Results
The method was evaluated using manually curated gold standard labels from the 3 National Institutes of Health cohorts. We evaluated both the intracohort and intercohort variable harmonization performance. The supervised SONAR method outperformed existing benchmark methods for almost all intracohort and intercohort comparisons using area under the curve and top-k accuracy metrics. Notably, SONAR was able to significantly improve harmonization of concepts that were difficult for existing semantic methods to harmonize.
Conclusions
SONAR achieves accurate variable harmonization within and between cohort studies by harnessing the complementary strengths of semantic learning and variable distribution learning.}
}
@article{EDGELL2024100075,
title = {A monstrous matter: The three faces of artificial creativity},
journal = {Journal of Creativity},
volume = {34},
number = {1},
pages = {100075},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100075},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000013},
author = {Robert A. Edgell},
keywords = {Artificial intelligence, Creativity, Matters of concern, Trust, Creative value, Creative personal identity},
abstract = {Through a focus on artificial creativity (AC), creativity and innovation researchers, practitioners, and educators are beginning to demystify the phenomenon's liminality by exploring and contesting the potential affordances, constraints, and pitfalls brought about by the deployment of powerful AI models for creative endeavors. For the creativity community, AC as a sociotechnical network has become a deeply consternating and contested monster. Given the recency of AC, there has been little theorizing yet. My critical self-reflection paper seeks to understand the community's concerns and, thereby, to discern theoretical insights that conceptually contribute towards a theory of AC. Drawing on autoethnography, I identified three distinct perceived matters of concern represented by anthropomorphic personalities or faces of AC: Trickster, Surveyor, and Harbinger. The findings reveal that the Trickster is the most monstrous and disconcerting face of AC. It may be prankish or deceptive, but can also be beneficent and supportive. While the Surveyor provides surveillance, measurement, and calculation, the Harbinger announces competing future visions, one of utopian hope and the other of dystopian despair. I conclude by discussing the implications of three underlying theoretical variables: trust, creative value, and creative personal identity.}
}
@article{CAO2025102403,
title = {Textual data augmentation using generative approaches - Impact on named entity recognition tasks},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102403},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102403},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001277},
author = {Danrun Cao and Nicolas Béchet and Pierre-François Marteau and Oussama Ahmia},
keywords = {Data augmentation, Named entity recognition, Feature engineering, Word embeddings, Model robustness, Generative model, Call for tenders},
abstract = {Industrial applications of Named Entity Recognition (NER) are usually confronted with small and imbalanced corpora. This could harm the performance of trained and finetuned recognition models, especially when they encounter unknown data. In this study we develop three generation-based data enrichment approaches, in order to increase the number of examples of underrepresented entities. We compare the impact of enriched corpora on NER models, using both non-contextual (fastText) and contextual (Bert-like) embedding models to provide discriminant features to a biLSTM-CRF used as an entity classifier. The approach is evaluated on a contract renewal detection task applied to a corpus of calls for tenders. The results show that the proposed data enrichment procedure effectively improves the NER model’s effectiveness when applied on both known and unknown data.}
}
@article{MAI2025104368,
title = {Towards the next generation of Geospatial Artificial Intelligence},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {136},
pages = {104368},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104368},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225000159},
author = {Gengchen Mai and Yiqun Xie and Xiaowei Jia and Ni Lao and Jinmeng Rao and Qing Zhu and Zeping Liu and Yao-Yi Chiang and Junfeng Jiao},
keywords = {Geospatial Artificial Intelligence, Heterogeneity-aware GeoAI, Knowledge-Guided GeoAI, Spatial representation learning, Geo-Foundation Models, Fairness-aware GeoAI, Privacy-aware GeoAI, Interpretable and explainable GeoAI},
abstract = {Geospatial Artificial Intelligence (GeoAI), as the integration of geospatial studies and AI, has become one of the fastest-developing research directions in spatial data science and geography. This rapid change in the field calls for a deeper understanding of the recent developments and envision where the field is going in the near future. In this work, we provide a quantitative analysis of the GeoAI literature from the spatial, temporal, and semantic aspects. We briefly discuss the history of AI and GeoAI by highlighting some pioneering work. Then we discuss the current landscape of GeoAI by selecting five representative subdomains including remote sensing, urban computing, Earth system science, cartography, and geospatial semantics. Finally, we highlight several unique future research directions of GeoAI which are classified into two groups: GeoAI method development challenges and GeoAI Ethics challenges. Topics include heterogeneity-aware GeoAI, knowledge-guided GeoAI, spatial representation learning, geo-foundation models, fairness-aware GeoAI, privacy-aware GeoAI, as well as interpretable and explainable GeoAI. We hope our review of GeoAI’s past, present, and future is comprehensive and can enlighten the next generation of GeoAI research.}
}
@article{ALONSO2025100509,
title = {A novel approach for job matching and skill recommendation using transformers and the O*NET database},
journal = {Big Data Research},
volume = {39},
pages = {100509},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000048},
author = {Rubén Alonso and Danilo Dessí and Antonello Meloni and Diego {Reforgiato Recupero}},
keywords = {Information extraction, Transformers, Online enrolling process, Natural language processing, Course recommendation},
abstract = {Today we have tons of information posted on the web every day regarding job supply and demand which has heavily affected the job market. The online enrolling process has thus become efficient for applicants as it allows them to present their resumes using the Internet and, as such, simultaneously to numerous organizations. Online systems such as Monster.com, OfferZen, and LinkedIn contain millions of job offers and resumes of potential candidates leaving to companies with the hard task to face an enormous amount of data to manage to select the most suitable applicant. The task of assessing the resumes of candidates and providing automatic recommendations on which one suits a particular position best has, therefore, become essential to speed up the hiring process. Similarly, it is important to help applicants to quickly find a job appropriate to their skills and provide recommendations about what they need to master to become eligible for certain jobs. Our approach lies in this context and proposes a new method to identify skills from candidates' resumes and match resumes with job descriptions. We employed the O*NET database entities related to different skills and abilities required by different jobs; moreover, we leveraged deep learning technologies to compute the semantic similarity between O*NET entities and part of text extracted from candidates' resumes. The ultimate goal is to identify the most suitable job for a certain resume according to the information there contained. We have defined two scenarios: i) given a resume, identify the top O*NET occupations with the highest match with the resume, ii) given a candidate's resume and a set of job descriptions, identify which one of the input jobs is the most suitable for the candidate. The evaluation that has been carried out indicates that the proposed approach outperforms the baselines in the two scenarios. Finally, we provide a use case for candidates where it is possible to recommend courses with the goal to fill certain skills and make them qualified for a certain job.}
}
@article{TRAJANOV2023714,
title = {Review of Natural Language Processing in Pharmacology},
journal = {Pharmacological Reviews},
volume = {75},
number = {4},
pages = {714-738},
year = {2023},
issn = {0031-6997},
doi = {https://doi.org/10.1124/pharmrev.122.000715},
url = {https://www.sciencedirect.com/science/article/pii/S0031699724007762},
author = {Dimitar Trajanov and Vangel Trajkovski and Makedonka Dimitrieva and Jovana Dobreva and Milos Jovanovik and Matej Klemen and Aleš Žagar and Marko Robnik-Šikonja},
abstract = {Natural language processing (NLP) is an area of artificial intelligence that applies information technologies to process the human language, understand it to a certain degree, and use it in various applications. This area has rapidly developed in the past few years and now employs modern variants of deep neural networks to extract relevant patterns from large text corpora. The main objective of this work is to survey the recent use of NLP in the field of pharmacology. As our work shows, NLP is a highly relevant information extraction and processing approach for pharmacology. It has been used extensively, from intelligent searches through thousands of medical documents to finding traces of adversarial drug interactions in social media. We split our coverage into five categories to survey modern NLP: methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries. We split each of the five categories into appropriate subcategories, describe their main properties and ideas, and summarize them in a tabular form. The resulting survey presents a comprehensive overview of the area, useful to practitioners and interested observers.
Significance Statement
The main objective of this work is to survey the recent use of NLP in the field of pharmacology in order to provide a comprehensive overview of the current state in the area after the rapid developments that occurred in the past few years. The resulting survey will be useful to practitioners and interested observers in the domain.}
}
@article{HUANG2025100526,
title = {Generative spatial artificial intelligence for sustainable smart cities: A pioneering large flow model for urban digital twin},
journal = {Environmental Science and Ecotechnology},
volume = {24},
pages = {100526},
year = {2025},
issn = {2666-4984},
doi = {https://doi.org/10.1016/j.ese.2025.100526},
url = {https://www.sciencedirect.com/science/article/pii/S2666498425000043},
author = {Jeffrey Huang and Simon Elias Bibri and Paul Keel},
keywords = {Sustainable smart cities, Generative artificial intelligence, Generative spatial artificial intelligence, Foundation models, Large flow model, Urban digital twin, Urban planning and design},
abstract = {Rapid urbanization, alongside escalating resource depletion and ecological degradation, underscores the critical need for innovative urban development solutions. In response, sustainable smart cities are increasingly turning to cutting-edge technologies—such as Generative Artificial Intelligence (GenAI), Foundation Models (FMs), and Urban Digital Twin (UDT) frameworks—to transform urban planning and design practices. These transformative tools provide advanced capabilities to analyze complex urban systems, optimize resource management, and enable evidence-based decision-making. Despite recent progress, research on integrating GenAI and FMs into UDT frameworks remains scant, leaving gaps in our ability to capture complex urban flows and multimodal dynamics essential to achieving environmental sustainability goals. Moreover, the lack of a robust theoretical foundation and real-world operationalization of these tools hampers comprehensive modeling and practical adoption. This study introduces a pioneering Large Flow Model (LFM), grounded in a robust foundational framework and designed with GenAI capabilities. It is specifically tailored for integration into UDT systems to enhance predictive analytics, adaptive learning, and complex data management functionalities. To validate its applicability and relevance, the Blue City Project in Lausanne City is examined as a case study, showcasing the ability of the LFM to effectively model and analyze urban flows—namely mobility, goods, energy, waste, materials, and biodiversity—critical to advancing environmental sustainability. This study highlights how the LFM addresses the spatial challenges inherent in current UDT frameworks. The LFM demonstrates its novelty in comprehensive urban modeling and analysis by completing impartial city data, estimating flow data in new locations, predicting the evolution of flow data, and offering a holistic understanding of urban dynamics and their interconnections. The model enhances decision-making processes, supports evidence-based planning and design, fosters integrated development strategies, and enables the development of more efficient, resilient, and sustainable urban environments. This research advances both the theoretical and practical dimensions of AI-driven, environmentally sustainable urban development by operationalizing GenAI and FMs within UDT frameworks. It provides sophisticated tools and valuable insights for urban planners, designers, policymakers, and researchers to address the complexities of modern cities and accelerate the transition towards sustainable urban futures.}
}
@article{EGUIA2024,
title = {Clinical Decision Support and Natural Language Processing in Medicine: Systematic Literature Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/55315},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006149},
author = {Hans Eguia and Carlos Luis Sánchez-Bocanegra and Franco Vinciarelli and Fernando Alvarez-Lopez and Francesc Saigí-Rubió},
keywords = {artificial intelligence, AI, natural language processing, clinical decision support system, CDSS, health recommender system, clinical information extraction, electronic health record, systematic literature review, patient, treatment, diagnosis, health workers},
abstract = {Background
Ensuring access to accurate and verified information is essential for effective patient treatment and diagnosis. Although health workers rely on the internet for clinical data, there is a need for a more streamlined approach.
Objective
This systematic review aims to assess the current state of artificial intelligence (AI) and natural language processing (NLP) techniques in health care to identify their potential use in electronic health records and automated information searches.
Methods
A search was conducted in the PubMed, Embase, ScienceDirect, Scopus, and Web of Science online databases for articles published between January 2000 and April 2023. The only inclusion criteria were (1) original research articles and studies on the application of AI-based medical clinical decision support using NLP techniques and (2) publications in English. A Critical Appraisal Skills Programme tool was used to assess the quality of the studies.
Results
The search yielded 707 articles, from which 26 studies were included (24 original articles and 2 systematic reviews). Of the evaluated articles, 21 (81%) explained the use of NLP as a source of data collection, 18 (69%) used electronic health records as a data source, and a further 8 (31%) were based on clinical data. Only 5 (19%) of the articles showed the use of combined strategies for NLP to obtain clinical data. In total, 16 (62%) articles presented stand-alone data review algorithms. Other studies (n=9, 35%) showed that the clinical decision support system alternative was also a way of displaying the information obtained for immediate clinical use.
Conclusions
The use of NLP engines can effectively improve clinical decision systems’ accuracy, while biphasic tools combining AI algorithms and human criteria may optimize clinical diagnosis and treatment flows.
Trial Registration
PROSPERO CRD42022373386; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=373386}
}
@article{YOUSIF2023100372,
title = {Exploring deep learning approaches for video captioning: A comprehensive review},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {6},
pages = {100372},
year = {2023},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2023.100372},
url = {https://www.sciencedirect.com/science/article/pii/S277267112300267X},
author = {Adel Jalal Yousif and Mohammed H. Al-Jammas},
keywords = {Evaluation metrics, Video captioning, Video description, Computer vision, Deep learning},
abstract = {While humans can easily describe visual data at varying levels of detail, the same task presents a significant challenge for machines. This challenge becomes even more complex when dealing with video data. The process of understanding a video and generating descriptive text for it is known as video captioning. Video captioning requires not only understanding the visual content but also producing human-like descriptions that accurately capture its semantics. Achieving this level of understanding requires the collaborative efforts of both the computer vision and natural language processing research communities. The captions produced through video captioning serve as valuable resources that can be further leveraged for various applications such as video search, accessibility for visually impaired people, and human-robot interaction. Deep learning strategies have emerged as powerful tools in addressing the complexities of video captioning. By leveraging large scale annotated video caption datasets and sophisticated neural network architectures, deep learning approaches have made significant advances in this challenging task. In the existing literature, numerous techniques, benchmark datasets, and evaluation metrics have been developed, emphasizing the necessity for a comprehensive examination to concentrate research efforts in this rapidly evolving field. This paper provides a survey of deep learning based methods for video captioning, highlighting their key components, challenges, and recent advancements.}
}
@article{DELIMA2025100810,
title = {An AI-powered approach to the semiotic reconstruction of narratives},
journal = {Entertainment Computing},
volume = {52},
pages = {100810},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100810},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001782},
author = {Edirlei Soares {de Lima} and Margot M.E. Neggers and Bruno Feijó and Marco A. Casanova and Antonio L. Furtado},
keywords = {Computational narratology, Interactive story composition, Deconstruction, Semiotic relations, Semiotic reconstruction, Book narratives, Movie narratives, Storyboards, Artificial intelligence, Intelligent agents},
abstract = {This article presents a novel and highly interactive process to generate natural language narratives based on our ongoing work on semiotic relations, providing four criteria for composing new narratives from existing stories. The wide applicability of this semiotic reconstruction process is suggested by a reputed literary scholar’s deconstructive claim that new narratives can often be shown to be a tissue of previous narratives. Along, respectively, three semiotic axes – syntagmatic, paradigmatic, and meronymic – existing stories can yield new stories by the combination, imitation, or expansion of an iconic scene; lastly, a new story may emerge through reversal via an antithetic consideration, i.e., through the adoption of opposite values. Targeting casual users, we present a fully operational prototype with a simple and user-friendly interface that incorporates an AI agent, namely ChatGPT. The prototype, in a coauthor capacity, generates context-compatible sequences of events in storyboard format using backward-chaining abductive reasoning (employing Stable Diffusion to draw scene illustrations), conforming as much as possible to the user’s authorial instructions. The extensive repertoire of book and movie summaries available to the AI agent obviates the need to manually supply laborious and error-prone context specifications. A user study was conducted to evaluate user experience and satisfaction with the generated narratives. The preliminary findings suggest that our approach has the potential to enhance story quality while offering a positive user experience.}
}
@article{HERR2024100341,
title = {Estimating prevalence of rare genetic disease diagnoses using electronic health records in a children’s hospital},
journal = {Human Genetics and Genomics Advances},
volume = {5},
number = {4},
pages = {100341},
year = {2024},
issn = {2666-2477},
doi = {https://doi.org/10.1016/j.xhgg.2024.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2666247724000812},
author = {Kate Herr and Peixin Lu and Kessi Diamreyan and Huan Xu and Eneida Mendonca and K. Nicole Weaver and Jing Chen},
keywords = {rare genetic diseases, natural language processing, bioinformatics, Orphanet, electronic health record, genetic testing},
abstract = {Summary
Rare genetic diseases (RGDs) affect a significant number of individuals, particularly in pediatric populations. This study investigates the efficacy of identifying RGD diagnoses through electronic health records (EHRs) and natural language processing (NLP) tools, and analyzes the prevalence of identified RGDs for potential underdiagnosis at Cincinnati Children’s Hospital Medical Center (CCHMC). EHR data from 659,139 pediatric patients at CCHMC were utilized. Diagnoses corresponding to RGDs in Orphanet were identified using rule-based and machine learning-based NLP methods. Manual evaluation assessed the precision of the NLP strategies, with 100 diagnosis descriptions reviewed for each method. The rule-based method achieved a precision of 97.5% (95% CI: 91.5%, 99.4%), while the machine-learning-based method had a precision of 73.5% (95% CI: 63.6%, 81.6%). A manual chart review of 70 randomly selected patients with RGD diagnoses confirmed the diagnoses in 90.3% (95% CI: 82.0%, 95.2%) of cases. A total of 37,326 pediatric patients were identified with 977 RGD diagnoses based on the rule-based method, resulting in a prevalence of 5.66% in this population. While a majority of the disorders showed a higher prevalence at CCHMC compared with Orphanet, some diseases, such as 1p36 deletion syndrome, indicated potential underdiagnosis. Analyses further uncovered disparities in RGD prevalence and age of diagnosis across gender and racial groups. This study demonstrates the utility of employing EHR data with NLP tools to systematically investigate RGD diagnoses in large cohorts. The identified disparities underscore the need for enhanced approaches to guarantee timely and accurate diagnosis and management of pediatric RGDs.}
}
@article{TAO2024127795,
title = {KFEX-N : A table-text data question-answering model based on knowledge-fusion encoder and EX-N tree decoder},
journal = {Neurocomputing},
volume = {593},
pages = {127795},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127795},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224005666},
author = {Ye Tao and Jiawang Liu and Hui Li and Wenqian Cao and Xiugong Qin and Yunlong Tian and Yongjie Du},
keywords = {Question answering, Tabular and textual, Numerical reasoning, Domain knowledge, Natural language processing},
abstract = {Answering questions about hybrid data combining tables and text is challenging. Recent research has employed encoder-tree decoder frameworks to simulate the reasoning process of arithmetic expressions for generating answers. However, this approach overlooks the inherent diversity of expressions; there might be multiple valid reasoning paths, leading to a decrease in the accuracy of inferred expression trees. Moreover, encoders, lacking rich domain knowledge, struggle to capture deep relationships between questions and supporting evidence; this limitation results in models making errors when selecting operation units. In this paper, we propose a Knowledge-Fusion encoder and EX-N tree decoder table-text data question-answering model(KFEX-N). During the encoding process, the integration of traditional encoders with cross-fusion encoders forms a knowledge-fusion encoder, endowing the model with rich domain knowledge and enhancing its understanding of the operational units required to answer questions. Additionally, we propose an EX-N tree decoder. It reduces the diversity of inference paths through a constrained structure and mitigates the occurrence of final answer errors resulting from decoding errors. We validate our model using publicly available Table-Text QA datasets (TAT-QA and Fin-QA) and achieve state-of-the-art performance.}
}
@article{ZABALALOPEZ2024226,
title = {A survey of data-centric technologies supporting decision-making before deploying military assets},
journal = {Defence Technology},
volume = {42},
pages = {226-246},
year = {2024},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2024.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S221491472400182X},
author = {Alexandra Zabala-López and Mario Linares-Vásquez and Sonia Haiduc and Yezid Donoso},
keywords = {Data-centric technologies, Military, Analytics, Machine learning, Data science, Artificial intelligence},
abstract = {In a time characterized by the availability of vast amounts of data, the effective utilization of information is critical for timely decision-making in military operations. However, processing large amounts of data requires computational resources and time. Therefore, decision makers have used data-centric technologies to take advantage of public and private data sources to support military operations. This survey explores the integration and application of data-centric technologies, such as data analytics, data science, and machine learning, to optimize decision-making workflows within military contexts supporting the deployment of military assets and resources. To address the information gap, this article presents a literature review, specifically a survey. Our survey examines the use of the mentioned technologies to process and analyze information that contributes to the phases of situational awareness, and planning in military environments. We then introduce a taxonomy of the approaches associated with implementing these technologies in military scenarios. Furthermore, we discuss relevant factors for the seamless integration of data-centric technologies into military decision-making processes, and reveal the importance of specialized personnel, architectures, and cybersecurity issues in the task of developing prototypes and models. The findings of this paper aim to provide valuable insights for military institutions, offering a deeper understanding of the use of data-centric technologies as innovative practices to enhance the effectiveness of military decision-making.}
}
@article{TRAPPEY2021120511,
title = {An intelligent patent recommender adopting machine learning approach for natural language processing: A case study for smart machinery technology mining},
journal = {Technological Forecasting and Social Change},
volume = {164},
pages = {120511},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120511},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520313378},
author = {Amy Trappey and Charles V. Trappey and Alex Hsieh},
keywords = {Natural language processing, Patent recommendation, Word embedding, Technology mining and trend analysis},
abstract = {Recommendation systems are widely applied in many fields, such as online customized product searches and customer-centric advertisements. This research develops the methodology for a patent recommender to discover semantically relevant patents for further technology mining and trend analysis. The proposed recommender adopts machine learning (ML) algorithms for natural language processing (NLP) to represent patent documents in vector space and to enable semantic analyses of the patent documents. The ML approach of neural network (NN) language models, trained by domain patent documents (text) as a training set, convert patent documents into vectors and, thus, can identify semantically similar patents using document similarity measures. In particular, the proposed recommender is deployed to in-depth case studies for advanced patent recommendations. The case domain of smart machinery is used to better enable smart manufacturing by incorporating innovative technologies, such as intelligent sensors, intelligent controllers, and intelligent decision making. The research uses six sub-domains in smart machinery technologies as the case studies to verify the superior accuracy and efficacy of the recommender system and methodologies.}
}
@article{CUI2025126128,
title = {Research on the mechanism of organizing and managing mainstream integrated media information resources in the era of big data},
journal = {Expert Systems with Applications},
volume = {266},
pages = {126128},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126128},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029956},
author = {Jindong Cui and Chenyu Li and Chenrui Bao and Guoli Qu},
keywords = {Mainstream integrated media, Information organization, Management Mechanism, Big data},
abstract = {As mainstream integrated media assumes increasing importance in guiding public opinion, achieving efficient organization and management of its information resources within the context of big data and the information economy has become a fundamental cornerstone for its development and value creation. This paper focuses on the content and dissemination characteristics of mainstream integrated media information, constructs a model for its organization and management, and examines various aspects such as decentralized information collection, multimodal resource processing, semantic feature extraction, information unit construction and association, information chain traceability, and simulation results. Additionally, it proposes management strategies and countermeasures from a comprehensive perspective of the entire information organization chain. The developed organization and management mechanism aligns with the evolving requirements of mainstream integrated media in the big data era, providing a foundation for the deep utilization and enhancement of information value.}
}
@article{WANG2025102968,
title = {A homogeneous multimodality sentence representation for relation extraction},
journal = {Information Fusion},
volume = {118},
pages = {102968},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102968},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000417},
author = {Kai Wang and Yanping Chen and WeiZhe Yang and Yongbin Qin},
keywords = {Natural language processing, Relation extraction, Homogeneous multimodality, Semantic structures},
abstract = {Deep neural networks enable a sentence to be transformed into different multimodalities such as a token sequence representation (a one-dimensional semantic representation) or a semantic plane (a two-dimensional semantic representation). Sequence representation has the advantage of learning sequential dependencies of a sentence. Semantic plane is built by organizing all spans of a sentence, which is effective in resolving complicated sentence semantic structures. The two representations are derived from a homogeneous resource (the same sentence), but they are separately used in related works. In this paper, a homogeneous multimodality sentence representation is proposed to make full use of semantic information in a sentence. We construct a homomodality model, which is composed of three components: a sequential encoder to generate sequential modality, a plane encoder to build plane modality, and a multimodality fusion component aligning homogeneous multimodalities for learning a multi-granularity semantic representation. Our model is evaluated on four public datasets to support the relation extraction task. Compared with related works, it achieves state-of-the-art performance on all datasets. Analytical experiments show that fusing homogeneous multimodalities is effective in making full use of sentence information for advancing the discriminability of a deep neural network.}
}
@article{LIU2024509,
title = {Integration of data science with product design towards data-driven design},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {509-532},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0007850624001252},
author = {Ang Liu and Stephen Lu and Fei Tao and Nabil Anwer},
keywords = {Product design, Data science, Data-driven design},
abstract = {This paper aims to investigate the scientific integration of data science with product design towards data-driven design (D3). Data science has potential to facilitate design decision-making through insight extraction, predictive analytics, and automatic decisions. A systematic scoping review is conduced to converge various D3 applications in four dimensions: the design dimension about design operations, the data dimension about popular data sources and common data-related challenges, the method dimension about the methodological foundations, and the social/ethical dimension about social/ethical considerations and implications. Based on the state-of-the-art, this paper also highlights potential future research avenues in this dynamic field.}
}
@article{BRANNSTROM2024101203,
title = {A formal understanding of computational empathy in interactive agents},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101203},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101203},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001377},
author = {Andreas Brännström and Joel Wester and Juan Carlos Nieves},
keywords = {Computational empathy, Conversational agents, Human–agent interaction, Knowledge engineering},
abstract = {Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agents’ empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definition—an ontology—of empathy is developed. We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactions—be it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors.}
}
@article{RAJENDRAN2024100913,
title = {Learning across diverse biomedical data modalities and cohorts: Challenges and opportunities for innovation},
journal = {Patterns},
volume = {5},
number = {2},
pages = {100913},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100913},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923003227},
author = {Suraj Rajendran and Weishen Pan and Mert R. Sabuncu and Yong Chen and Jiayu Zhou and Fei Wang},
abstract = {Summary
In healthcare, machine learning (ML) shows significant potential to augment patient care, improve population health, and streamline healthcare workflows. Realizing its full potential is, however, often hampered by concerns about data privacy, diversity in data sources, and suboptimal utilization of different data modalities. This review studies the utility of cross-cohort cross-category (C4) integration in such contexts: the process of combining information from diverse datasets distributed across distinct, secure sites. We argue that C4 approaches could pave the way for ML models that are both holistic and widely applicable. This paper provides a comprehensive overview of C4 in health care, including its present stage, potential opportunities, and associated challenges.}
}
@article{WANG2025103085,
title = {Implementation path and reference model for Multilateral Data Circulation System (MDCS) in Datacentric Product-Service System (DPSS): from an industrial practice survey},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103085},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007365},
author = {Chengjun Wang and Xinguo Ming and Xinming Gao and Xianyu Zhang},
keywords = {Datacentric product-service systems, Data circulation, Data sovereignty, Data privacy},
abstract = {With the digital transformation of enterprises and the development of digital infrastructure (smart sensors, 5G/6G, IoT, Industrial Internet, etc.), large amounts of data are generated in various stages of the product life cycle. The value of data in the Product-Service System is becoming prominent. However, through literature review and industrial practice survey, it has been observed that there is a lack of systematic investigation into the processes of data circulation and utilization within PSS. Additionally, within the existing research on data circulation, scholars focus on partial points such as data privacy computing, data sharing and data transaction, lacking an overall reference model for the data circulation in the Product-Service System and the path of implementing a multilateral data circulation platform in the industry. This paper aims to use the industrial practice survey method, based on the literature review, to propose the Datacentric Product-Service System (DPSS) for the first time, and study the main processes of data circulation in the DPSS. The study of the reference model and industrial implementation path of the multilateral data circulation system that meets the industry’s needs in the Datacentric Product-Service System. It provides a reference for the government and industry to design, implement and regulate the domain data circulation platform. In addition, the proposed data circulation system reference model and implementation path can enhance the value symbiosis among enterprises and increase industry benefits.}
}
@article{ZHANG2025104048,
title = {Graph structure prefix injection transformer for multi-modal entity alignment},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104048},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104048},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004072},
author = {Yan Zhang and Xiangyu Luo and Jing Hu and Miao Zhang and Kui Xiao and Zhifei Li},
keywords = {Multi-modal knowledge graphs, Multi-modal entity alignment, Contrastive learning},
abstract = {Multi-modal entity alignment aims to integrate corresponding entities across different MMKGs. However, previous studies have not adequately considered the impact of graph structural heterogeneity on EA tasks. Different MMKGs typically exhibit variations in graph structural features, leading to distinct structural representations of the same entity relationships. Additionally, the topological structure of the graph also differs. To tackle these challenges, we introduce GSIEA, the MMEA framework that integrates structural prefix injection and modality fusion. Different from other methods that directly fuse structural data with multi-modal features to perform the alignment task, GSIEA separately processes structural data and multi-modal data such as images and attributes, incorporating a prefix injection interaction module within a multi-head attention mechanism to optimize the utilization of multi-modal information and minimize the impact of graph structural differences. Additionally, GSIEA employs a convolutional enhancement module to extract fine-grained multi-modal features and computes cross-modal weights to achieve feature fusion. We conduct experimental evaluations on two public datasets, containing 12,846 and 11,199 entity pairs, respectively, demonstrating that GSIEA outperforms baseline models, with an average improvement of 3.26% in MRR and a maximum gain of 12.5%. Furthermore, the average improvement in Hits@1 is 4.96%, with a maximum increase of 16.92%. The code of our model is stored at https://github.com/HubuKG/GSIEA.}
}
@article{YU2025104074,
title = {Exploring long- and short-term knowledge state graph representations with adaptive fusion for knowledge tracing},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104074},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104074},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000160},
author = {Ganfeng Yu and Zhiwen Xie and Guangyou Zhou and Zhuo Zhao and Jimmy Xiangji Huang},
keywords = {Knowledge tracing, Knowledge states, Graph representation learning, Temporal graph prediction, Educational AI},
abstract = {Knowledge Tracing (KT) is an important research area in online education that focuses on predicting future academic performance based on students’ historical exercise records. The key to solving the KT problem lies in assessing students’ knowledge states through their responses to concept-related exercises. However, analyzing exercise records from a single perspective does not provide a comprehensive model of student knowledge. The truth is that students’ knowledge states often exhibit long- and short-term phenomena, corresponding to long-term knowledge systems and short-term real-time learning, both of which are closely related to learning quality and preferences. Existing studies have often neglected the learning preferences implied by long-term knowledge states and their impact on student performance. Therefore, we introduce a hybrid knowledge tracing model that utilizes both long- and short-term knowledge state representations (L-SKSKT). It enhances KT by fusing these two types of knowledge state representations and measuring their impact on learning quality. L-SKSKT includes a graph construction method designed to model students’ long- and short-term knowledge states. In addition, L-SKSKT incorporates a knowledge state graph embedding model that can effectively capture long- and short-term dependencies, generating corresponding knowledge state representations. Furthermore, we propose a fusion mechanism to integrate these representations and trace their impact on learning outcomes. Extensive empirical results on four benchmark datasets show that our approach achieves the best performance for KT, and beats various strong baselines with a large margin.}
}
@article{2024iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {244},
pages = {iii-v},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(24)03028-X},
url = {https://www.sciencedirect.com/science/article/pii/S187705092403028X}
}
@incollection{TOZZI2024355,
title = {4.15 - Advanced Mechanics of Hard Tissue Using Imaging-Based Measurements and Artificial Intelligence},
editor = {Vadim Silberschmidt},
booktitle = {Comprehensive Mechanics of Materials (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {355-380},
year = {2024},
isbn = {978-0-323-90647-0},
doi = {https://doi.org/10.1016/B978-0-323-90646-3.00046-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323906463000460},
author = {Gianluca Tozzi and Markus J. Buehler},
keywords = {Bone, Computed tomography, Deep learning, Diffusion models., Digital volume correlation, Hard tissue, Imaging, Mechanics, Neural networks, Tooth, X-ray},
abstract = {This chapter focuses on the use of advanced imaging-based techniques to assess the complex mechanical patterns of hard tissue, with a specific focus on bone and tooth. Hard tissues are complex hierarchical materials, requiring constant technological advancement to capture their structure-function relationship at different dimensional levels. Experimental techniques such as digital volume correlation, mainly based on X-ray tomography, greatly contributed to deepen understanding of hard tissues local deformation with investigations ranging from clinical computed tomography (CT) to nanoCT. In recent years, the advent of artificial intelligence proposed novel methodologies encompassing image classification and imaging-based mechanical predictions, which have the potential to revolutionize the field of hard tissue mechanics. These approaches and their integration are illustrated with various examples.}
}
@article{XIE2025128888,
title = {MAGO: Multi-Knowledge Aware and Global Strategy Sequence Optimizing Network for Emotional Support Conversation},
journal = {Neurocomputing},
volume = {618},
pages = {128888},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128888},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401659X},
author = {Qijun Xie and Wei Peng},
keywords = {Emotional Support Conversation, Global strategy selection, Multi-Knowledge Aware Integrator, Response generation},
abstract = {Emotional Support Conversation (ESConv) task aims to mitigate the emotional distress of help-seekers by providing supportive responses, which is facilitated by two main sub-tasks: selecting an appropriate supportive strategy and generating effective supportive responses to the help-seeker. However, the existing methodologies exhibit two primary shortcomings. Firstly, they primarily utilize commonsense knowledge such as using the COMET, ignoring the conceptual facts which can enhance the user-utterance understanding and improve the supportive response generation quality. Secondly, they rely excessively on dialogue history or user emotional feedbacks for strategy selection, neglecting the broader characteristics and interrelationships among various strategies globally. In this paper, we introduce the Multi-Knowledge Aware and Global Strategy Sequence Optimizing Network (MAGO). MAGO employs a Multi-Knowledge Aware Integrator to enrich the context understanding and improve response generation from the perspective of commonsense knowledge and conceptual facts. Additionally, MAGO globally incorporates the estimation of optimal strategy tag sequence by using a Strategy-Constrained Conditional Random Field (SCRF). Experimental results demonstrate MAGO’s superior performance in both strategy prediction (significantly increasing accuracy by 18%) and response generation, and analyses illustrate the importance of selecting strategies globally.}
}
@article{DOLHA20243246,
title = {Experiments with natural language queries on RDF vs. XML-serialized BPMN diagrams},
journal = {Procedia Computer Science},
volume = {246},
pages = {3246-3255},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.315},
url = {https://www.sciencedirect.com/science/article/pii/S187705092402338X},
author = {Damaris Naomi Dolha and Robert Andrei Buchmann},
keywords = {BPMN, RDF, prompt engineering, process queries},
abstract = {Our study reports a comparative analysis of natural language interactions with BPMN models, specifically contrasting semantic graphs generated from diagrams by the RDF export of Bee-Up, against the traditional standard BPMN 2.0 XML export from standard-compliant tools (in our case, SAP Signavio Process Transformation Suite). Utilizing varied prompt engineering techniques, the study evaluates the efficacy of GPT-based services from OpenAI in interpreting the nuanced semantic network structures of BPMN-as-RDF and the standard control flow hierarchical decomposition of BPMN-as-XML. Although image-based multi-modal interpretation of BPMN diagrams is also available in such services, our work is motivated by the fact that most BPM systems deliver structured serializations and not images through their APIs; moreover, any data stored in diagrams cannot be scrutinized by computer vision capabilities, being set as annotations in most tools. By exploring both the challenges and effectiveness of utilizing natural language in interacting with BPMN models, the analysis underscores the ability of RDF to mediate semantic richness and open-ended extension of procedural knowledge compared to the closed-world of the XML interchange schemas. Diagrammatic environments are thus encouraged to pursue this as a potential convergence between different means of knowledge representation.}
}
@article{XIAO2024105874,
title = {Automated daily report generation from construction videos using ChatGPT and computer vision},
journal = {Automation in Construction},
volume = {168},
pages = {105874},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105874},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006101},
author = {Bo Xiao and Yifan Wang and Yongpan Zhang and Chen Chen and Amos Darko},
keywords = {Construction daily report generation, Computer vision, ChatGPT, Construction management, Project documentation},
abstract = {Daily reports are important in construction management, informing project teams about status, enabling timely resolutions of delays and budget issues, and serving as official records for disputes and litigation. However, current practices are manual and time-consuming, requiring engineers to physically visit sites for observations. To fill this gap, this paper proposes an automated framework to generate daily construction reports from on-site videos by integrating ChatGPT and computer vision (CV)-based methods. The framework utilizes CV methods to analyze video footage and extract relevant productivity and activity information, which is then fed into ChatGPT using proper prompts to generate daily reports. A web application is developed to implement and validate the framework on a real construction site in Hong Kong, generating daily reports over a month. This research enhances construction management by significantly reducing documentation efforts through generative artificial intelligence, with potential applications in jobsite safety management, quality reporting, and stakeholder communication.}
}
@article{WU2024715,
title = {A review of deep learning methods for ligand based drug virtual screening},
journal = {Fundamental Research},
volume = {4},
number = {4},
pages = {715-737},
year = {2024},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2024.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S2667325824001043},
author = {Hongjie Wu and Junkai Liu and Runhua Zhang and Yaoyao Lu and Guozeng Cui and Zhiming Cui and Yijie Ding},
keywords = {Virtual screening, Deep learning, Drug discovery, Drug-target interaction, Drug-target affinity},
abstract = {Drug discovery is costly and time consuming, and modern drug discovery endeavors are progressively reliant on computational methodologies, aiming to mitigate temporal and financial expenditures associated with the process. In particular, the time required for vaccine and drug discovery is prolonged during emergency situations such as the coronavirus 2019 pandemic. Recently, the performance of deep learning methods in drug virtual screening has been particularly prominent. It has become a concern for researchers how to summarize the existing deep learning in drug virtual screening, select different models for different drug screening problems, exploit the advantages of deep learning models, and further improve the capability of deep learning in drug virtual screening. This review first introduces the basic concepts of drug virtual screening, common datasets, and data representation methods. Then, large numbers of common deep learning methods for drug virtual screening are compared and analyzed. In addition, a dataset of different sizes is constructed independently to evaluate the performance of each deep learning model for the difficult problem of large-scale ligand virtual screening. Finally, the existing challenges and future directions in the field of virtual screening are presented.}
}
@article{SHAMAY2025906,
title = {Mastering the complexities of cancer nanomedicine with text mining, AI and automation},
journal = {Journal of Controlled Release},
volume = {379},
pages = {906-919},
year = {2025},
issn = {0168-3659},
doi = {https://doi.org/10.1016/j.jconrel.2025.01.057},
url = {https://www.sciencedirect.com/science/article/pii/S0168365925000665},
author = {Yosi Shamay},
abstract = {In this contribution to the Orations - New Horizons of the Journal of Controlled Release, I present a personal perspective on the complexities of cancer nanomedicine and the approaches to master them. This oration draws mainly from my lab's journey to explore three transformative approaches to master complexities in the field: (1) leveraging text mining to construct dynamic knowledge bases for hypothesis generation in cell-specific drug delivery, (2) introducing the concept of meta-synergy to further optimize and classify multi-drug combinations across dimensions such as chemical loading, pharmacodynamics, and pharmacokinetics (3) utilizing automation to accelerate nanoparticle discovery with advanced screening methodologies such as aggregation-induced emission (AIE). I argue that by embracing complexity in nanomedicine, we can manifest new therapeutic possibilities, paving the way for more effective, precise, and adaptive treatment strategies.}
}
@incollection{GREENBERG2025349,
title = {15 - Ethics for artificial agents: Toward commensurate capability and self-regulation},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {349-371},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000018},
author = {Ariel M. Greenberg},
keywords = {Advance directive, Artificial intelligence, Autonomous systems, Design policy, Duties, Machine ethics, Moral judgment and decision-making},
abstract = {In this chapter, we tackle the charge to make artificial agent self-regulation on par with their expanding capabilities. We begin by offering research and development schemes focused on installing in machines the perception, knowledge, and reasoning required to support ethics sensitivity and to produce agentic behavior compliant with the principles of nonmaleficence, beneficence, and responsibility. Next, we review evolutionary, psychological, and neuroscientific accounts of the phenomenology and emergence of moral judgment and decision-making that speak to the natural coupling of capability and self-regulation. Finally, we discuss sweeping themes of endogeneity, generality, mentalization, legibility, and duty responsiveness meant to guide how we ought to design an appropriate balance of capability and self-regulation in artificial agents.}
}
@article{MAGNUS2024167,
title = {Towards a GPT-Based Lean Manufacturing Consultant for Manufacturing Optimization},
journal = {Procedia CIRP},
volume = {130},
pages = {167-176},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.072},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012253},
author = {Christian S. Magnus and Moritz Venschott},
keywords = {generative pre-trained transformer, deep learning, smart factory, process optimization},
abstract = {The ever-present competition between industrial companies, both domestically and globally, puts manufacturing companies under constant pressure to optimize. Lean Manufacturing approaches have proven their worth in this environment and continue to hold great promise in the age of digitalization in Smart Factories. However, due to a lack of internal expertise and human resources, very few companies succeed in realizing the hidden potential across all processes. Additional expertise and human resources are often bought in from consultants. Generative Pre-Trained Transformers (GPTs) are a class of deep learning models that process and generate text using text-to-text conversational interfaces. They use large amounts of pre-existing text data to learn complex patterns, semantic relationships, and contextual nuances. Simple conversations with ChatGPT show that it already has some knowledge about Lean Manufacturing. Further investigations are necessary to check, if it is capable to help manufacturing companies with Value Stream Analyses, Line Balancing, etc. to create conversational systems that guide users through the necessary analyses and optimization steps. This paper presents the results of a study on the applicability of OpenAI‘s ChatGPT-4 as a substitute for external consulting services in the described area. It shows exemplary use-cases in manufacturing, the structure and results of an experimental study, and the challenges to be solved when using ChatGPT-4 as Lean Manufacturing consultant.}
}
@article{BUNNE20247045,
title = {How to build the virtual cell with artificial intelligence: Priorities and opportunities},
journal = {Cell},
volume = {187},
number = {25},
pages = {7045-7063},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424013321},
author = {Charlotte Bunne and Yusuf Roohani and Yanay Rosen and Ankit Gupta and Xikun Zhang and Marcel Roed and Theo Alexandrov and Mohammed AlQuraishi and Patricia Brennan and Daniel B. Burkhardt and Andrea Califano and Jonah Cool and Abby F. Dernburg and Kirsty Ewing and Emily B. Fox and Matthias Haury and Amy E. Herr and Eric Horvitz and Patrick D. Hsu and Viren Jain and Gregory R. Johnson and Thomas Kalil and David R. Kelley and Shana O. Kelley and Anna Kreshuk and Tim Mitchison and Stephani Otte and Jay Shendure and Nicholas J. Sofroniew and Fabian Theis and Christina V. Theodoris and Srigokul Upadhyayula and Marc Valer and Bo Wang and Eric Xing and Serena Yeung-Levy and Marinka Zitnik and Theofanis Karaletsos and Aviv Regev and Emma Lundberg and Jure Leskovec and Stephen R. Quake},
keywords = {cell biology, AI, ML, virtual cell},
abstract = {Summary
Cells are essential to understanding health and disease, yet traditional models fall short of modeling and simulating their function and behavior. Advances in AI and omics offer groundbreaking opportunities to create an AI virtual cell (AIVC), a multi-scale, multi-modal large-neural-network-based model that can represent and simulate the behavior of molecules, cells, and tissues across diverse states. This Perspective provides a vision on their design and how collaborative efforts to build AIVCs will transform biological research by allowing high-fidelity simulations, accelerating discoveries, and guiding experimental studies, offering new opportunities for understanding cellular functions and fostering interdisciplinary collaborations in open science.}
}
@article{LI2024100266,
title = {A systematic review of the first year of publications on ChatGPT and language education: Examining research on ChatGPT’s use in language learning and teaching},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100266},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100266},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000699},
author = {Belle Li and Victoria L. Lowell and Chaoran Wang and Xiangning Li},
keywords = {Systematic review, Artificial intelligence (AI), ChatGPT, Language learning},
abstract = {This systematic review aims to explore published research on the use of ChatGPT in language learning between November 2022 and November 2023, outlining the types of papers, methodologies adopted, publishing journals, major research trends, topics of interest, and existing gaps demanding attention. The PRISMA framework was utilized to capture the latest published articles, selecting 36 articles that met the inclusion criteria. Findings extracted from this review include (1) authors worldwide contribute to this topic, with Asia and North America leading; (2) the wide distribution across various journals underscores the interdisciplinary nature of this research topic, such as computer science, psychology, linguistics, education, and other social sciences; (3) empirical research dominates the literature that is published, with the majority focusing on higher education and ethical considerations. Other findings include that ChatGPT plays multifaceted roles, supporting self-directed language learning, content generation, and teacher workflows. Research gaps include the need for diversified scopes, longitudinal studies, exploration of stakeholders’ perceptions, and assessments of feedback quality.}
}
@article{OWEN2024,
title = {AI for Analyzing Mental Health Disorders Among Social Media Users: Quarter-Century Narrative Review of Progress and Challenges},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/59225},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007933},
author = {David Owen and Amy J Lynham and Sophie E Smart and Antonio F Pardiñas and Jose {Camacho Collados}},
keywords = {mental health, depression, anxiety, schizophrenia, social media, natural language processing, narrative review},
abstract = {Background
Mental health disorders are currently the main contributor to poor quality of life and years lived with disability. Symptoms common to many mental health disorders lead to impairments or changes in the use of language, which are observable in the routine use of social media. Detection of these linguistic cues has been explored throughout the last quarter century, but interest and methodological development have burgeoned following the COVID-19 pandemic. The next decade may see the development of reliable methods for predicting mental health status using social media data. This might have implications for clinical practice and public health policy, particularly in the context of early intervention in mental health care.
Objective
This study aims to examine the state of the art in methods for predicting mental health statuses of social media users. Our focus is the development of artificial intelligence–driven methods, particularly natural language processing, for analyzing large volumes of written text. This study details constraints affecting research in this area. These include the dearth of high-quality public datasets for methodological benchmarking and the need to adopt ethical and privacy frameworks acknowledging the stigma experienced by those with a mental illness.
Methods
A Google Scholar search yielded peer-reviewed articles dated between 1999 and 2024. We manually grouped the articles by 4 primary areas of interest: datasets on social media and mental health, methods for predicting mental health status, longitudinal analyses of mental health, and ethical aspects of the data and analysis of mental health. Selected articles from these groups formed our narrative review.
Results
Larger datasets with precise dates of participants’ diagnoses are needed to support the development of methods for predicting mental health status, particularly in severe disorders such as schizophrenia. Inviting users to donate their social media data for research purposes could help overcome widespread ethical and privacy concerns. In any event, multimodal methods for predicting mental health status appear likely to provide advancements that may not be achievable using natural language processing alone.
Conclusions
Multimodal methods for predicting mental health status from voice, image, and video-based social media data need to be further developed before they may be considered for adoption in health care, medical support, or as consumer-facing products. Such methods are likely to garner greater public confidence in their efficacy than those that rely on text alone. To achieve this, more high-quality social media datasets need to be made available and privacy concerns regarding the use of these data must be formally addressed. A social media platform feature that invites users to share their data upon publication is a possible solution. Finally, a review of literature studying the effects of social media use on a user’s depression and anxiety is merited.}
}
@article{REALENOSEI2024103264,
title = {From vision to text: A comprehensive review of natural image captioning in medical diagnosis and radiology report generation},
journal = {Medical Image Analysis},
volume = {97},
pages = {103264},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103264},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524001890},
author = {Gabriel Reale-Nosei and Elvira Amador-Domínguez and Emilio Serrano},
keywords = {Medical image captioning, Natural image captioning, Diagnostic captioning, Radiology report generation, Survey, State-of-the-art review},
abstract = {Natural Image Captioning (NIC) is an interdisciplinary research area that lies within the intersection of Computer Vision (CV) and Natural Language Processing (NLP). Several works have been presented on the subject, ranging from the early template-based approaches to the more recent deep learning-based methods. This paper conducts a survey in the area of NIC, especially focusing on its applications for Medical Image Captioning (MIC) and Diagnostic Captioning (DC) in the field of radiology. A review of the state-of-the-art is conducted summarizing key research works in NIC and DC to provide a wide overview on the subject. These works include existing NIC and MIC models, datasets, evaluation metrics, and previous reviews in the specialized literature. The revised work is thoroughly analyzed and discussed, highlighting the limitations of existing approaches and their potential implications in real clinical practice. Similarly, future potential research lines are outlined on the basis of the detected limitations.}
}
@article{AMOR2024102764,
title = {Digital regulatory compliance checking for the construction industry},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102764},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102764},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004129},
author = {Robert Amor and Bimal Kumar and Richard Watson}
}
@article{2024iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {246},
pages = {iii-xxxiv},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(24)03167-3},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924031673}
}
@article{VALCAMONICO2025110834,
title = {A systematic procedure for the analysis of maintenance reports based on a taxonomy and BERT attention mechanism},
journal = {Reliability Engineering & System Safety},
volume = {257},
pages = {110834},
year = {2025},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2025.110834},
url = {https://www.sciencedirect.com/science/article/pii/S0951832025000377},
author = {Dario Valcamonico and Piero Baraldi and July Bias Macêdo and Márcio Das Chagas Moura and Jonathan Brown and Stéphane Gauthier and Enrico Zio},
keywords = {Maintenance, Natural Language Processing, BERT, DBSCAN, Freight transport trains},
abstract = {This work proposes a systematic procedure for analyzing maintenance reports to support maintenance decision-making for a fleet of similar systems. The proposed procedure allows achieving three objectives: (1) grouping maintenance interventions, (2) identifying common characteristics in the maintenance interventions, and (3) recognizing occurrences of rare events of maintenance intervention. Specifically, the attention mechanism of Bidirectional Encoder Representation from Transformer (BERT) and the Density Based Spatial Clustering Applications with Noise (DBSCAN) methods are combined to group maintenance interventions according to their similarity of stated features. A taxonomy of the words used in the textual reports to state the maintenance interventions is developed to systematically identify common features of the clusters, such as the involved components, their working state, the occurred failures or malfunctions, the performed maintenance actions and the personnel that has performed the intervention. The proposed procedure is applied to a repository of reports of maintenance interventions performed on mechanical and electric components of traction systems of a fleet of trains. The obtained results show that it can effectively support decision-making on the maintenance of traction systems.}
}
@article{EVANS20241509,
title = {Developments and applications of the OPTIMADE API for materials discovery, design, and data exchange††Electronic supplementary information (ESI) available: Copy of Table 1 with web links. See DOI: https://doi.org/10.1039/d4dd00039k},
journal = {Digital Discovery},
volume = {3},
number = {8},
pages = {1509-1533},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00039k},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001219},
author = {Matthew L. Evans and Johan Bergsma and Andrius Merkys and Casper W. Andersen and Oskar B. Andersson and Daniel Beltrán and Evgeny Blokhin and Tara M. Boland and Rubén {Castañeda Balderas} and Kamal Choudhary and Alberto {Díaz Díaz} and Rodrigo {Domínguez García} and Hagen Eckert and Kristjan Eimre and María Elena {Fuentes Montero} and Adam M. Krajewski and Jens Jørgen Mortensen and José Manuel {Nápoles Duarte} and Jacob Pietryga and Ji Qi and Felipe de Jesús {Trejo Carrillo} and Antanas Vaitkus and Jusong Yu and Adam Zettel and Pedro Baptista {de Castro} and Johan Carlsson and Tiago F. T. Cerqueira and Simon Divilov and Hamidreza Hajiyani and Felix Hanke and Kevin Jose and Corey Oses and Janosh Riebesell and Jonathan Schmidt and Donald Winston and Christen Xie and Xiaoyu Yang and Sara Bonella and Silvana Botti and Stefano Curtarolo and Claudia Draxl and Luis Edmundo {Fuentes Cobas} and Adam Hospital and Zi-Kui Liu and Miguel A. L. Marques and Nicola Marzari and Andrew J. Morris and Shyue Ping Ong and Modesto Orozco and Kristin A. Persson and Kristian S. Thygesen and Chris Wolverton and Markus Scheidgen and Cormac Toher and Gareth J. Conduit and Giovanni Pizzi and Saulius Gražulis and Gian-Marco Rignanese and Rickard Armiento},
abstract = {The Open Databases Integration for Materials Design (OPTIMADE) application programming interface (API) empowers users with holistic access to a growing federation of databases, enhancing the accessibility and discoverability of materials and chemical data. Since the first release of the OPTIMADE specification (v1.0), the API has undergone significant development, leading to the v1.2 release, and has underpinned multiple scientific studies. In this work, we highlight the latest features of the API format, accompanying software tools, and provide an update on the implementation of OPTIMADE in contributing materials databases. We end by providing several use cases that demonstrate the utility of the OPTIMADE API in materials research that continue to drive its ongoing development.}
}
@article{MOKOS2025112231,
title = {Model-based safety analysis of requirement specifications},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112231},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112231},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002759},
author = {Konstantinos Mokos and Panagiotis Katsaros and Preben Bohn},
keywords = {Model-based design, Requirements formalization, Ontology-based specification, Formal verification, Safety analysis},
abstract = {Model-based design primarily aims to establish a communication framework throughout a system’s design. Moreover, models with formal semantics allow verification based on rigorous methods, including the analysis of system safety. However, building formal models is a tedious manual process and cannot be easily applied to real problems. A key gap that hinders automation of model development is that there is no systematic way to connect system requirements with the activity of model-based design. In this article, we introduce a workflow to tackle this gap and ultimately automate the analysis of system safety using formal methods. We extend our previous work on boilerplate-based specification of system requirements with ontological semantics towards specifying FDIR (Failure, Detection, Isolation, Recovery) requirements. The workflow is centered around the automated generation of a model skeleton in SLIM, a component-based formal modeling language, from a set of ontology-based requirement specifications. Our approach has been implemented into a dedicated tool, which not only provides visualization of the ontology relations, but also supports traceability of the analysis findings back to the requirements specification. Finally, we provide results on the safety analysis of a real star-tracker system based on a SLIM model derived by minimally changing the auto-generated model skeleton.}
}
@article{SARKER2024101110,
title = {Multi-aspect rule-based AI: Methods, taxonomy, challenges and directions towards automation, intelligence and transparent cybersecurity modeling for critical infrastructures},
journal = {Internet of Things},
volume = {25},
pages = {101110},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101110},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524000520},
author = {Iqbal H. Sarker and Helge Janicke and Mohamed Amine Ferrag and Alsharif Abuadbba},
keywords = {Cybersecurity, Rule-based Modeling, Explainable AI, Responsible AI, Machine learning, Security data analytics, Knowledge discovery, Data-driven decision-making, Automation, Intelligence, Transparency, Trustworthiness, Critical infrastructures},
abstract = {Critical infrastructure (CI) typically refers to the essential physical and virtual systems, assets, and services that are vital for the functioning and well-being of a society, economy, or nation. However, the rapid proliferation and dynamism of today’s cyber threats in digital environments may disrupt CI functionalities, which would have a debilitating impact on public safety, economic stability, and national security. This has led to much interest in effective cybersecurity solutions regarding automation and intelligent decision-making, where AI-based modeling is potentially significant. In this paper, we take into account “Rule-based AI” rather than other black-box solutions since model transparency, i.e., human interpretation, explainability, and trustworthiness in decision-making, is an essential factor, particularly in cybersecurity application areas. This article provides an in-depth study on multi-aspect rule based AI modeling considering human interpretable decisions as well as security automation and intelligence for CI. We also provide a taxonomy of rule generation methods by taking into account not only knowledge-driven approaches based on human expertise but also data-driven approaches, i.e., extracting insights or useful knowledge from data, and their hybridization. This understanding can help security analysts and professionals comprehend how systems work, identify potential threats and anomalies, and make better decisions in various real-world application areas. We also cover how these techniques can address diverse cybersecurity concerns such as threat detection, mitigation, prediction, diagnosis for root cause findings, and so on in different CI sectors, such as energy, defence, transport, health, water, agriculture, etc. We conclude this paper with a list of identified issues and opportunities for future research, as well as their potential solution directions for how researchers and professionals might tackle future generation cybersecurity modeling in this emerging area of study.}
}
@article{NAITHANI2025102686,
title = {Editorial overview: Genome studies and molecular genetics 2024},
journal = {Current Opinion in Plant Biology},
volume = {83},
pages = {102686},
year = {2025},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2024.102686},
url = {https://www.sciencedirect.com/science/article/pii/S1369526624001778},
author = {Sushma Naithani and Leena Tripathi}
}
@article{DENG2025105224,
title = {Does ChatGPT enhance student learning? A systematic review and meta-analysis of experimental studies},
journal = {Computers & Education},
volume = {227},
pages = {105224},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105224},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524002380},
author = {Ruiqi Deng and Maoli Jiang and Xinlu Yu and Yuyan Lu and Shasha Liu},
keywords = {Teaching/learning strategies, Improve classroom teaching, Elementary education, Secondary education, Post-secondary education},
abstract = {Chat Generative Pre-Trained Transformer (ChatGPT) has generated excitement and concern in education. While cross-sectional studies have highlighted correlations between ChatGPT use and learning performance, they fall short of establishing causality. This review examines experimental studies on ChatGPT's impact on student learning to address this gap. A comprehensive search across five databases identified 69 articles published between 2022 and 2024 for analysis. The findings reveal that ChatGPT interventions are predominantly implemented at the university level, cover various subject areas focusing on language education, are integrated into classroom environments as part of regular educational practices, and primarily involve direct student use of ChatGPT. Overall, ChatGPT improves academic performance, affective-motivational states, and higher-order thinking propensities; it reduces mental effort and has no significant effect on self-efficacy. However, methodological limitations, such as the lack of power analysis and concerns regarding post-intervention assessments, warrant cautious interpretation of results. This review presents four propositions from the findings: (1) distinguish between the quality of ChatGPT outputs and the positive effects of interventions on academic performance by shifting from well-defined problems in post-intervention assessments to more complex, project-based assessments that require skill demonstration, adopting proctored assessments, or incorporating metrics such as originality alongside quality; (2) evaluate long-term impacts to determine whether the positive effects on affective-motivational states are sustained or merely owing to novelty effect; (3) prioritise objective measures to complement subjective assessments of higher-order thinking; and (4) use power analysis to determine adequate sample sizes to avoid Type II errors and provide reliable effect size estimates. This review provides valuable insights for researchers, instructors, and policymakers evaluating the effectiveness of generative AI integration in educational practice.}
}
@article{LUO2025125827,
title = {ChatGPT based contrastive learning for radiology report summarization},
journal = {Expert Systems with Applications},
volume = {267},
pages = {125827},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125827},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424026940},
author = {Zhenjie Luo and Zuowei Jiang and Mingyang Wang and Xiaoyan Cai and Dehong Gao and Libin Yang},
keywords = {Text summarization, Radiology reports, ChatGPT, Contrastive learning},
abstract = {Automatically Impression Generation (AIG) can conclude essential information of the “Findings” section, thus facilitating more effective communication between radiographers and physicians. Different from general abstractive summarization, AIG is more challenging for data-driven neural models. This may be due to two critical issues: the serious data bias and the shallow and rough use of available data. To alleviate data bias problem and make best use of available data, we propose a novel ChatGPT based Contrastive Learning (CCL) approach. Specifically, CCL progressively learns to generate impressions to address the above problems. Firstly, we input “findings” and its target “impression” into T5 for fine-tuning, namely WarmUp; Secondly, we propose CCL to alleviate exposure bias by adding its self-generation as negative samples, and treat all negative samples differently according to their sample quality. This can make full use of the available limited data; Thirdly, we select the non-dominant data in data bias by assessing model and input them into ChatGPT for data augmentation. By iteratively doing the second step and third step, CCL can gradually improve its summarization performance on radiology reports. Obtained results on the public OpenI and MIMIC-CXR datasets show effectiveness of our CCL.22https://github.com/jzw1234/Chataug-CCL.}
}
@article{TRAPPEY201819,
title = {Construction and validation of an ontology-based technology function matrix: Technology mining of cyber physical system patent portfolios},
journal = {World Patent Information},
volume = {55},
pages = {19-24},
year = {2018},
note = {Advanced Analytics of Intellectual Property Information for TechMining},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0172219018300139},
author = {Amy J.C. Trappey and Charles V. Trappey and Usharani Hareesh Govindarajan and Allen C.C. Jhuang},
keywords = {Technology function matrix (TFM), Cyber Physical System (CPS), Patent analysis, Patent portfolio},
abstract = {This research develops a computer-supported ontology-based Technology Function Matrix (TFM) construction method, called eTFM, as an approach to reduce technology mining man-power and enhance the accuracy and consistency of patent analysis results. The paper addresses a rarely discussed issue of the TFM validation. The proposed validation approach compares the TFMs construction based on both on the domain ontology and the International Patent Classification (IPC) classes. The research demonstrates the methodology's practical applications using the patent analysis case of cyber physical system (CPS), an essential core technology enabling advanced manufacturing and Industry 4.0.}
}
@article{BALASUBRAMANIAN2025100545,
title = {A cognitive platform for collecting cyber threat intelligence and real-time detection using cloud computing},
journal = {Decision Analytics Journal},
volume = {14},
pages = {100545},
year = {2025},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2025.100545},
url = {https://www.sciencedirect.com/science/article/pii/S2772662225000013},
author = {Prasasthy Balasubramanian and Sadaf Nazari and Danial Khosh Kholgh and Alireza Mahmoodi and Justin Seby and Panos Kostakos},
keywords = {Cyber threat intelligence, Machine learning operations, Classification, Indicators of compromise, Bidirectional encoder representations from transformers (BERT), Longformer},
abstract = {The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks. However, for most organizations, collecting actionable CTI remains both a technical bottleneck and a black box. While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild. This study proposes an efficient platform capable of processing compute-intensive data pipelines, based on cloud computing, for real-time detection, collection, and sharing of CTI from various online sources. We developed a prototype platform (TSTEM) with a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, Elasticsearch, Logstash, and Kibana (ELK), Kafka, and Machine Learning Operations (MLOps) to autonomously search, extract, and index indicators of compromise (IOCs) in the wild. Moreover, the provisioning, monitoring, and management of the platform are achieved through infrastructure as code (IaC). Custom focus-crawlers collect web content, processed by a first-level classifier to identify potential IOCs. Relevant content advances to a second level for further examination. State-of-the-art natural language processing (NLP) models are used for classification and entity extraction, enhancing the IOC extraction methodology. Our results indicate these models exhibit high accuracy (exceeding 98%) in classification and extraction tasks, achieving this performance within less than a minute. The system’s effectiveness is due to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification with low false positives.}
}
@article{GALDO2024195,
title = {Artificial intelligence in paediatrics: Current events and challenges},
journal = {Anales de Pediatría (English Edition)},
volume = {100},
number = {3},
pages = {195-201},
year = {2024},
issn = {2341-2879},
doi = {https://doi.org/10.1016/j.anpede.2024.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S2341287924000383},
author = {Brais Galdo and Carla Pazos and Jerónimo Pardo and Alfonso Solar and Daniel Llamas and Enrique Fernández-Blanco and Alejandro Pazos},
keywords = {Artificial intelligence, 7P medicine, Machine learning, Paediatrics, Personalized medicine, Inteligencia artificial, Medicina de las 7P, Aprendizaje máquina, Pediatría, Medicina personalizada},
abstract = {This article examines the use of artificial intelligence (AI) in the field of paediatric care within the framework of the 7P medicine model (Predictive, Preventive, Personalized, Precise, Participatory, Peripheral and Polyprofessional). It highlights various applications of AI in the diagnosis, treatment and management of paediatric diseases as well as the role of AI in prevention and in the efficient management of health care resources and the resulting impact on the sustainability of public health systems. Successful cases of the application of AI in the paediatric care setting are presented, placing emphasis on the need to move towards a 7P health care model. Artificial intelligence is revolutionizing society at large and has a great potential for significantly improving paediatric care.
Resumen
Se examina el uso de la inteligencia artificial (IA) en el campo de la atención a la salud pediátrica dentro del marco de la "Medicina de las 7P" (Predictiva, Preventiva, Personalizada, Precisa, Participativa, Periférica y Poliprofesional). Se destacan diversas aplicaciones de la IA en el diagnóstico, el tratamiento y el control de enfermedades pediátricas, así como su papel en la prevención y en la gestión eficiente de los recursos médicos con su repercusión en la sostenibilidad de los sistemas públicos de salud. Se presentan casos de éxito de la aplicación de la IA en el ámbito pediátrico y se hace un gran énfasis en la necesidad de caminar hacia la Medicina de las 7P. La IA está revolucionando la sociedad en general ofreciendo un gran potencial para mejorar significativamente el cuidado de la salud en pediatría.}
}
@article{ZHANG20243652,
title = {Foundation model for generalist remote sensing intelligence: Potentials and prospects},
journal = {Science Bulletin},
volume = {69},
number = {23},
pages = {3652-3656},
year = {2024},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2024.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S2095927324006510},
author = {Mi Zhang and Bingnan Yang and Xiangyun Hu and Jianya Gong and Zuxun Zhang}
}
@article{DOSSANTOS2024108422,
title = {Machine learning applied to digital phenotyping: A systematic literature review and taxonomy},
journal = {Computers in Human Behavior},
volume = {161},
pages = {108422},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108422},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224002905},
author = {Marília Pit {dos Santos} and Wesllei Felipe Heckler and Rodrigo Simon Bavaresco and Jorge Luis Victória Barbosa},
keywords = {Systematic literature review, Digital phenotyping, Digital phenotype, Machine learning},
abstract = {Health conditions, encompassing both physical and mental aspects, hold an influence that extends beyond the individual. These conditions affect personal well-being, relationships, and financial stability. Innovative strategies in healthcare, such as digital phenotyping, are strategic to mitigate these impacts. By merging diverse data sources, digital phenotyping seeks a comprehensive understanding of health, well-being, and behavioral conditions. Machine learning can enhance the analysis of these data, improving the comprehension of health and well-being. Therefore, this paper presents a systematic literature review on machine learning and digital phenotyping, examining the research field by filtering 2,860 articles from eleven databases published up to November 2023. The analysis focused on 124 articles to answer six research questions addressing machine learning techniques, data, devices, ontologies, and research challenges. This work presents a taxonomy for mapping explored areas in digital phenotyping and another for organizing machine learning techniques used in digital phenotyping research. The review found increased publications in 2023, indicating a growing interest in the field. The main challenges arise from the studies’ small participant samples and imbalanced datasets, limiting the generalizability of the results to broader populations and the choice of ML methods. Furthermore, the reliance on self-reported data can introduce potential inaccuracies due to recall and reporting biases. Beyond self-reports, authors explored different data types, including physiological, clinical, contextual, smartphone-based, and multimedia. Despite using video recordings in controlled experiments, studies have yet to investigate this method within intelligent environments. Researchers also analyzed neurophysiological phenotypes, suggesting the potential for interventions based on these characteristics.}
}
@article{LUO2024102678,
title = {Learning multimodal adaptive relation graph and action boost memory for visual navigation},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102678},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102678},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003264},
author = {Jian Luo and Bo Cai and Yaoxiang Yu and Aihua Ke and Kang Zhou and Jian Zhang},
keywords = {Action boost memory, Knowledge graph, Reinforcement learning, Visual navigation, Visual transformer network},
abstract = {The task of visual navigation (VN) is steering the agent find target object only using visual perceptions. Previous works largely exploit multimodal information (e.g. visual and training memory) to improve the environmental perception ability, while making less effort to leverage interchange information. Besides, multimodal fusion tends to ignore the data dependencies (prefer a part of the modal data) as well as the supervision of the action. In this work, we present a novel multimodal graph learning (MGL) structure for VN, which consists of three parts. (1) the multimodal fusion exploits the rich information across spatial, RGB, and depth information about objects’ place, as well as semantic information about their categories, (2) adaptive relation graph (ARG) is dynamically built using object detectors, which encodes multimodal fusion and adapt to a novel environment. It embeds its navigation history and other useful task-oriented structural information, thus make the agent own the association ability and make advisable informed decisions and (3) action boost module (ABM) aims to assist the agent make intelligent decisions, which predicts more accurate action using beneficial training experience. Our agent can foresight what the goal state may look like and how to get closer towards that state. These combinations of the “what” and the “how” allow the agent to navigate to the target object effectively. We validate our approach on the AI2-THOR dataset. It reports 24.2% and 23.7% increase in SPL(Success weighted by Per Length) and SR(Success Rate) compared with baselines, respectively. Code and datasets can be found in https://github.com/luosword/ABM_VN.}
}
@article{LE2025167680,
title = {An in-depth review of AI-powered advancements in cancer drug discovery},
journal = {Biochimica et Biophysica Acta (BBA) - Molecular Basis of Disease},
volume = {1871},
number = {3},
pages = {167680},
year = {2025},
issn = {0925-4439},
doi = {https://doi.org/10.1016/j.bbadis.2025.167680},
url = {https://www.sciencedirect.com/science/article/pii/S0925443925000250},
author = {Minh Huu Nhat Le and Phat Ky Nguyen and Thi Phuong Trang Nguyen and Hien Quang Nguyen and Dao Ngoc Hien Tam and Han Hong Huynh and Phat Kim Huynh and Nguyen Quoc Khanh Le},
keywords = {AI-driven drug discovery, cancer genomics, Computational drug design, Bioinformatics, Precision oncology, cancer therapeutics},
abstract = {The convergence of artificial intelligence (AI) and genomics is redefining cancer drug discovery by facilitating the development of personalized and effective therapies. This review examines the transformative role of AI technologies, including deep learning and advanced data analytics, in accelerating key stages of the drug discovery process: target identification, drug design, clinical trial optimization, and drug response prediction. Cutting-edge tools such as DrugnomeAI and PandaOmics have made substantial contributions to therapeutic target identification, while AI's predictive capabilities are driving personalized treatment strategies. Additionally, advancements like AlphaFold highlight AI's capacity to address intricate challenges in drug development. However, the field faces significant challenges, including the management of large-scale genomic datasets and ethical concerns surrounding AI deployment in healthcare. This review underscores the promise of data-centric AI approaches and emphasizes the necessity of continued innovation and interdisciplinary collaboration. Together, AI and genomics are charting a path toward more precise, efficient, and transformative cancer therapeutics.}
}
@article{BARNES20242072,
title = {This month in The Journal},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {10},
pages = {2072-2073},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2024.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0002929724003380},
author = {Alyson B. Barnes and Kylee L. Spencer}
}
@article{ORDONEZ2024102346,
title = {Data engineering and modeling for artificial intelligence},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102346},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102346},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000703},
author = {Carlos Ordonez and Wojciech Macyna and Ladjel Bellatreche}
}
@article{HEYDER2023101772,
title = {Ethical management of human-AI interaction: Theory development review},
journal = {The Journal of Strategic Information Systems},
volume = {32},
number = {3},
pages = {101772},
year = {2023},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2023.101772},
url = {https://www.sciencedirect.com/science/article/pii/S0963868723000185},
author = {Teresa Heyder and Nina Passlack and Oliver Posegga},
keywords = {Artificial intelligence, Ethics, Human-AI interaction, Theoretical review, Sociomateriality},
abstract = {AI-based technologies have changed the nature of the symbiosis between humans and AI, and so strategic management of human-AI interaction in organizations requires deeper ethical considerations. Aligning AI with human values requires a systematic understanding of the ethical management of human-AI interaction. We conduct a theoretical review, from a sociotechnical perspective, and analyze ethical management of human-AI interaction through the lens of sociomateriality. Our systematic approach helps explain and clarify the interdependencies between two ethical perspectives – duty and virtue ethics – in sociotechnical systems. We also provide a theoretical framework that leads to seven avenues for future research.}
}
@article{BACK202423,
title = {Accelerated chemical science with AI},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {23-33},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00213f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000858},
author = {Seoin Back and Alán Aspuru-Guzik and Michele Ceriotti and Ganna Gryn'ova and Bartosz Grzybowski and Geun Ho Gu and Jason Hein and Kedar Hippalgaonkar and Rodrigo Hormázabal and Yousung Jung and Seonah Kim and Woo Youn Kim and Seyed Mohamad Moosavi and Juhwan Noh and Changyoung Park and Joshua Schrier and Philippe Schwaller and Koji Tsuda and Tejs Vegge and O. Anatole {von Lilienfeld} and Aron Walsh},
abstract = {In light of the pressing need for practical materials and molecular solutions to renewable energy and health problems, to name just two examples, one wonders how to accelerate research and development in the chemical sciences, so as to address the time it takes to bring materials from initial discovery to commercialization. Artificial intelligence (AI)-based techniques, in particular, are having a transformative and accelerating impact on many if not most, technological domains. To shed light on these questions, the authors and participants gathered in person for the ASLLA Symposium on the theme of ‘Accelerated Chemical Science with AI’ at Gangneung, Republic of Korea. We present the findings, ideas, comments, and often contentious opinions expressed during four panel discussions related to the respective general topics: ‘Data’, ‘New applications’, ‘Machine learning algorithms’, and ‘Education’. All discussions were recorded, transcribed into text using Open AI's Whisper, and summarized using LG AI Research's EXAONE LLM, followed by revision by all authors. For the broader benefit of current researchers, educators in higher education, and academic bodies such as associations, publishers, librarians, and companies, we provide chemistry-specific recommendations and summarize the resulting conclusions.}
}
@incollection{LAMURIAS2024,
title = {Text Mining for Bioinformatics Using Biomedical Literature},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000178},
author = {Andre Lamurias and Diana F. Sousa and Francisco M. Couto},
keywords = {Biomedical literature, Distant supervision, Event extraction, Machine Learning, Named entity recognition, Normalization, Relation extraction, Text mining},
abstract = {Biomedical literature is a large and rich source of information for various applications. Text mining tools aim at extracting information from the literature in an efficient manner since processing scientific texts is a complex task given the formal and highly specialized language. Text mining tools tackle these challenges using different approaches, such as rule-based methods and machine learning algorithms including deep learning. This document overviews the current biomedical text mining tools by describing their approaches, tasks (e.g., Named Entity Recognition, Relation Extraction, Event Extraction, Question Answering), available corpora, toolkits and applications, and community challenges.}
}
@article{SONG202416844,
title = {AI empowering traditional Chinese medicine?},
journal = {Chemical Science},
volume = {15},
number = {41},
pages = {16844-16886},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc04107k},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024015359},
author = {Zhilin Song and Guanxing Chen and Calvin Yu-Chian Chen},
abstract = {For centuries, Traditional Chinese Medicine (TCM) has been a prominent treatment method in China, incorporating acupuncture, herbal remedies, massage, and dietary therapy to promote holistic health and healing. TCM has played a major role in drug discovery, with over 60% of small-molecule drugs approved by the FDA from 1981 to 2019 being derived from natural products. However, TCM modernization faces challenges such as data standardization and the complexity of TCM formulations. The establishment of comprehensive TCM databases has significantly improved the efficiency and accuracy of TCM research, enabling easier access to information on TCM ingredients and encouraging interdisciplinary collaborations. These databases have revolutionized TCM research, facilitating advancements in TCM modernization and patient care. In addition, advancements in AI algorithms and database data quality have accelerated progress in AI for TCM. The application of AI in TCM encompasses a wide range of areas, including herbal screening and new drug discovery, diagnostic and treatment principles, pharmacological mechanisms, network pharmacology, and the incorporation of innovative AI technologies. AI also has the potential to enable personalized medicine by identifying patterns and correlations in patient data, leading to more accurate diagnoses and tailored treatments. The potential benefits of AI for TCM are vast and diverse, promising continued progress and innovation in the field.}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{LIU2025125701,
title = {A cascaded retrieval-while-reasoning multi-document comprehension framework with incremental attention for medical question answering},
journal = {Expert Systems with Applications},
volume = {265},
pages = {125701},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125701},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424025685},
author = {Jiandong Liu and Jianfeng Ren and Ruibin Bai and Zibo Zhang and Zheng Lu},
keywords = {Medical QA, Multi-document MRC, Cascaded network, Incremental attention, Retrieval-while-reasoning},
abstract = {Clinical Machine Reading Comprehension (MRC) is challenging due to the need for medical expertise and comprehensive reasoning chains for diagnosis. This paper introduces a novel cascade retrieval-while-reasoning framework for clinical MRC that incrementally retrieves and processes multiple supporting documents to effectively address the complexities of answering medical questions. The proposed cascade system is designed in such a way that easier questions are processed by shallower network layers with fewer documents, while more difficult ones are handled by deeper layers with more documents. In the proposed system, a retriever is designed to provide knowledge documents incrementally according to the comprehended difficulty level of each question, which interacts with the reader via query updating for each retrieval and modifies its search direction to better exploit the knowledge bank. To handle the supporting information from incrementally retrieved documents, a progressive attention mechanism is designed to extract cross-document features for better reasoning. The attentional information from multiple supporting documents is then aggregated for the final decision. The proposed method is compared with state-of-the-art models for medical MRC tasks on a large medical QA dataset. Experimental results show that the proposed model effectively combines multiple knowledge documents to solve challenging real-world clinical diagnosis problems. It significantly outperforms the previously best-performing model by 1.84%, reaching an accuracy of 63.00%.}
}
@article{GRAVES2024100051,
title = {Modeling morality and spirituality in artificial chaplains},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100051},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100051},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000112},
author = {Mark Graves}
}
@article{HAN2024121052,
title = {LegalAsst: Human-centered and AI-empowered machine to enhance court productivity and legal assistance},
journal = {Information Sciences},
volume = {679},
pages = {121052},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121052},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524009666},
author = {Wenjuan Han and Jiaxin Shen and Yanyao Liu and Zhan Shi and Jinan Xu and Fangxu Hu and Hao Chen and Yan Gong and Xueli Yu and Huaqing Wang and Zhijing Liu and Yajie Yang and Tianshui Shi and Mengyao Ge},
keywords = {Tools to assist in the trial, Explainable process, Multi-level inference, Traceable decision, Controllable judgment, Artificial intelligence technology},
abstract = {We propose autonomous software (namely, LegalAsst ) as a step toward an AI-empowered but human-centered machine focused on enhancing court productivity and legal assistance. LegalAsst aims to provide explainable, traceable, and controllable legal assistance and references for lawyers, judges, government officials, and the general public. To achieve this goal, it collates, processes, distills, and visualizes the whole judgment procedure. It streamlines and semi-automates the judgment procedure through case analysis, legislation analysis, and judicial decision-making. Specifically, to make laws and cases easier to navigate and understand, we incorporate structured representations to perform them. Then based on structured representations, we take a step further by introducing a decision-tree-based judgment, making the entire judging process visible and tractable. Our system not only tracks the procedural aspects of judgments but also incorporates modification capabilities, enabling the consideration of the most up-to-date legislation and societal factors to generate more adaptable judgment outcomes.1}
}
@article{SADLEK2025103956,
title = {Severity-based triage of cybersecurity incidents using kill chain attack graphs},
journal = {Journal of Information Security and Applications},
volume = {89},
pages = {103956},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103956},
url = {https://www.sciencedirect.com/science/article/pii/S2214212624002588},
author = {Lukáš Sadlek and Muhammad Mudassar Yamin and Pavel Čeleda and Basel Katt},
keywords = {Kill chain, Attack graph, Incident severity, Incident triage, MITRE ATT&CK, Cyber crisis},
abstract = {Security teams process a vast number of security events. Their security analysts spend considerable time triaging cybersecurity alerts. Many alerts reveal incidents that must be handled first and escalated to the more experienced staff to allow appropriate responses according to their severity. The current state requires an automated approach, considering contextual relationships among security events, especially detected attack tactics and techniques. In this paper, we propose a new graph-based approach for incident triage. First, it generates a kill chain attack graph from host and network data. Second, it creates sequences of detected alerts that could represent ongoing multi-step cyber attacks and matches them with the attack graph. Last, it assigns severity levels to the created sequences of alerts according to the most advanced kill chain phases that were used and the criticality of assets. We implemented the approach using the MulVAL attack graph generator and generation rules for MITRE ATT&CK techniques. The evaluation was accomplished in a testbed where multi-step attack scenarios were executed. Classification of sequences of alerts based on computed match scores obtained 0.95 area under the receiver operating characteristic curve in a feasible time. Moreover, a threshold exists for classifying 80% of positive sequences correctly and only a small percentage of negative sequences wrongly. Therefore, the approach selects malicious sequences of alerts and significantly improves incident triage.}
}
@article{CAO2024561,
title = {Artificial intelligence in metal forming},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {561-587},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.04.102},
url = {https://www.sciencedirect.com/science/article/pii/S0007850624001161},
author = {Jian Cao and Markus Bambach and Marion Merklein and Mojtaba Mozaffar and Tianju Xue},
keywords = {Artificial intelligence, Machine learning, Material characterization, Process design, Process control},
abstract = {Forming processes are known for their intricacies in prediction and control due to the complex loading conditions and material flow. This paper will first introduce the AI algorithms used or having potential to be used in forming, and then investigate the state-of-the-art advances of AI-based technologies in forming processes with four main pillars of process simulation, process design and optimization, in-situ process control, and qualification and certification of forming processes and formed products. Future directions of AI in forming for both academic research and industrial applications will be proposed to leverage digitalization and data science to explore new solutions in forming processes.}
}
@article{THOMAS20241060,
title = {One-shot relation retrieval in news archives: adapting N-way K-shot relation Classification for efficient knowledge extraction},
journal = {Procedia Computer Science},
volume = {246},
pages = {1060-1069},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.525},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025705},
author = {Hugo Thomas and Guillaume Gravier and Pascale Sébillot},
keywords = {relation extraction, relation retrieval, few-shot learning, N-way K-shot learning},
abstract = {One-shot relation retrieval is the knowledge extraction task that consists in searching in a textual dataset for all occurrences of a relation of interest, named the source relation, characterized by a single example—a relation being a link between a pair of entities in an utterance. Performing this task on large datasets requires an intelligent system to automate the process, for instance when exploring news archives for press review or business intelligence. We propose a framework that leverages the representation learning capabilities of N-way K-shot models for few-shot relation Classification and extends these models to enable one-shot retrieval with a rejection class. At evaluation time, one-shot relation retrieval is performed in a N-way K-shot setting where 1 of the N ways (or relations) is the source relation and the N-1 others are distractors, i.e., relations modeling a rejection class. We benchmark this framework and investigate the influence of the number and the choice of distractors on the standard TACREV and FewRel datasets. Experimental results demonstrate the effectiveness of our approach to address this highly challenging task, however with high variability primarily induced by the type of the source relation. Experiments also highlight a sound strategy for the choice of distractors—a large number of distractors at an intermediate distance from the embedding of the source relation in the latent space learned by the model—, which provides a competing trade-of between recall and precision. This strategy is globally optimal but can however be surpassed on certain source relations by others, depending on the characteristics of the source relation, paving the way for future work. We finally show the substantial benefit of two-shot retrieval over one-shot retrieval, which sheds light on the design of actual intelligent applications leveraging one- or few-shot relation retrieval.}
}
@article{XIA2025129031,
title = {MPE3: Learning meta-prompt with entity-enhanced semantics for few-shot named entity recognition},
journal = {Neurocomputing},
volume = {620},
pages = {129031},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129031},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224018022},
author = {Yuwei Xia and Zhao Tong and Liang Wang and Qiang Liu and Shu Wu and Xiaoyu Zhang},
keywords = {Information extraction, Few-shot named entity recognition, Prompt-learning, Meta-learning},
abstract = {Recently, prompt-tuning has been proven to be surprisingly effective on few-shot tasks. Intuitively, some studies explore Few-shot Named Entity Recognition (NER) based on prompt-tuning. However, how to properly initialize and effectively learn the prompt under limited training conditions still remains significantly challenging for few-shot NER. To meet these challenges, we propose a novel Meta-Prompt with Entity-Enhanced semantics for Few-shot NER, MPE3 for brevity. Specifically, we first explore the importance of the named entities’ semantics in the few-shot NER task. And we propose to construct prompts with entity-enhanced semantics which contain much useful prior knowledge for identifying named entities. Furthermore, to address the issue of inadequate training more substantially, we aim to train a meta-prompt that can be more effective and adaptive for few-shot NER scenarios. To achieve this, we divide the training data into many source-domain agnostic meta-tasks tailored to the characteristics of the NER problem for training. And we specially design a prompt meta-learner for training these meta-tasks. This training strategy succeeds in guiding prompts to optimize in a better direction for few-shot scenarios by the learned meta-knowledge from each meta-task. We conduct extensive experiments on three NER datasets under two different few-shot settings. Our method outperforms the current state-of-the-art model by 5.60%∼13.34% and 2.41%∼7.34% on average in the two different few-shot settings respectively, which validates the effectiveness and superiority of our model.}
}
@article{MAIER20241623,
title = {Simulation Discovery and Semi-Automatic Scenario Generation for Evaluation of Turbulence in Production Systems},
journal = {Procedia CIRP},
volume = {130},
pages = {1623-1631},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.292},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124014495},
author = {Julian B. Maier and Eduardo Colangelo and Theresa-Franziska Hinrichsen and Dinh Khoi Tran and Hans-Hermann Wiendahl and Marco F. Huber},
keywords = {Resilience, Digital Twins, Model Discovery, Scenario Generation},
abstract = {Production systems have always faced changes and disruptions, which require dynamic decision-making to adjust existing plans to the unfolding reality. The interdependence of highly interconnected supply chain networks further adds to this volatility. Given this complexity, mainly caused by ambiguity and the systems’ dynamic, achieving transparency to make decisions in the context of production planning and control is challenging. Simulation models can help assess the outcome of different scenarios through experiments. However, building simulation models by hand requires extensive manual effort and expert knowledge of simulation tools. Although often partly automated, simulation experiments still require the exertion of simulation engineers to be conducted on a large scale. Moreover, the created models are often static and require additional resources to be updated in order to reflect changes in the physical system. To reduce this effort, the authors propose a concept combining the automatic discovery of simulation models from execution data with the semi-automatic generation of scenarios. This facilitates logistical risk analysis and prediction by evaluating the consequences of possible disruptive events. The concept aims to enable domain experts to use digital twins in large-scale virtual scenario evaluation, which is fundamental for increasing the agility of manufacturing systems by speeding up decision processes.}
}
@article{ARROTTA2025126178,
title = {Multi-subject human activities: A survey of recognition and evaluation methods based on a formal framework},
journal = {Expert Systems with Applications},
volume = {267},
pages = {126178},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126178},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030458},
author = {Luca Arrotta and Gabriele Civitarese and Xi Chen and Julien Cumin and Claudio Bettini},
keywords = {Multi-subject HAR, Group activity recognition, Human activity recognition, Ambient intelligence},
abstract = {Human Activity Recognition (HAR) in smart environments is a well-explored research domain, given its diverse applications which include healthcare, surveillance, building management, and many more. While the majority of HAR research focuses on recognizing the activities of a single subject, in real-world scenarios smart environments are often populated by multiple subjects that may be engaged in both independent and joint activities. This gives rise to the challenge of Multi-Subject HAR, which is an open and complex problem. This survey paper aims to offer researchers and practitioners a comprehensive analysis of Multi-Subject HAR, encompassing its potential applications, sensing solutions, methods, datasets, evaluation metrics, and ongoing challenges. In addition to presenting the latest research works in this area and identifying open issues, our major contributions consist of a comprehensive problem formalization and a thorough discussion of the evaluation metrics to assess different dimensions of multi-subject HAR systems.}
}
@article{JIAO2025124343,
title = {The secrets to high-level green technology innovation of China's waste power battery recycling enterprises},
journal = {Journal of Environmental Management},
volume = {375},
pages = {124343},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2025.124343},
url = {https://www.sciencedirect.com/science/article/pii/S0301479725003196},
author = {Jianling Jiao and Yuqin Chen and Jingjing Li and Shanlin Yang},
keywords = {Green technology innovation, Waste power battery recycling, PSR, Bayesian network, GPT-4},
abstract = {Green technology innovation (GTI) in China's waste power battery recycling (WPBR) sector is a key driver for sustainable resource management, environmental protection, and economic prosperity. Using the PSR-BN-GPT-4 model and multi-source data, this study explores China's WPBRenterprises' high-level GTI mechanism. The research concludes that (1) Compared to traditional expert knowledge, the Bayesian network model based on GPT-4 exhibits superior causal reasoning capability. (2) The current level of GTI in China's WPBR industry is relatively low, with the probability of high-level GTI being only 19%. (3) Key factors identified include incentives like R&D investment, bottlenecks such as green finance policy tools, and hindrances like government procurement policy tools. (4) “Supporting Infrastructure Policy Tools - Recycling Outlets Number - Market Potential -Green Technology Innovation” and “Green Finance Policy Tools - R&D Investment - Green Technology Innovation” are two critical paths for enhancing the high-level development of GTI in WPBR enterprises. The study offers valuable insights for governmental, industrial, and corporate decision-making regarding GTI in battery recycling.}
}
@article{ZHAO2024102552,
title = {Multimodal Aspect-Based Sentiment Analysis: A survey of tasks, methods, challenges and future directions},
journal = {Information Fusion},
volume = {112},
pages = {102552},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102552},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003300},
author = {Tianyu Zhao and Ling-ang Meng and Dawei Song},
keywords = {Multimodal sentiment analysis, Multimodal Named Entity Recognition, Multimodal Aspect Based Sentiment Analysis, Multimodal Category Based Sentiment Analysis},
abstract = {With the development of social media, users increasingly tend to express their sentiments (broadly including sentiment polarities, emotions and sarcasm, etc.) associated with fine-grained aspects (e.g., entities) in multimodal content (mostly encompassing images and texts). Consequently, automated recognition of sentiments within multimodal content over different aspects, namely Multimodal Aspect-Based Sentiment Analysis (MABSA), has recently become an emergent research area. This paper assesses the state-of-the-art methods in MABSA based on a systematic taxonomy over different subtasks of MABSA. It compiles advanced models for each task and offers a concise overview of popular datasets and evaluation standards. Finally, we discuss the limitations of current research and highlight promising future research directions.}
}
@article{YU2025126356,
title = {A non-autoregressive Chinese-Braille translation approach with CTC loss optimization},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126356},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126356},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424032238},
author = {Hailong Yu and Wei Su and Yi Yang and Lei liu and Yongna Yuan and Yingchun Xie and Tianyuan Huang},
keywords = {Chinese-Braille translation, Non-autoregressive translation, CTC loss},
abstract = {The rise of Neural Machine Translation (NMT) models opens doors for translating Chinese text into Braille, improving information access for visually impaired individuals. However, current NMT models, often based on encoder–decoder architectures, utilize sequential rather than parallel processing in the decoder. This autoregressive decoding hinders architectures like the Transformer from fully leveraging their training speed advantages during inference. While the Transformer excels in parallel training, its inference time complexity remains O(T2), where T represents sequence length. This bottleneck becomes particularly significant when translating Braille, known for its long character sequences. We propose a non-autoregressive Chinese-to-Braille translation model that solely employs the encoder architecture along with Connectionist Temporal Classification (CTC) loss to generate complete Braille sequences simultaneously. This approach significantly improves inference speed, achieving a substantial acceleration compared to autoregressive models during inference with a time complexity of O(1). Remarkably, alongside increased inference speed, translation accuracy also improves. By incorporating a pre-training technique, our method achieves a remarkable BLEU Score of 95.10% with a limited dataset of only 2k Chinese-Braille training pairs.}
}
@article{YU2024,
title = {Coastal Zone Information Model: A comprehensive architecture for coastal digital twin by integrating data, models, and knowledge},
journal = {Fundamental Research},
year = {2024},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2024.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S2667325824002619},
author = {Zhaoyuan Yu and Pei Du and Lin Yi and Wen Luo and Dongshuang Li and Binru Zhao and Longhui Li and Zhuo Zhang and Jun Zhang and Jiyi Zhang and Wenchao Ma and Changchun Huang and Shuo Li and Xiaolu Yan and Guonian Lv and Linwang Yuan},
keywords = {Coastal zone information model, Coastal digitization, Coastal knowledge cognition, Data and model integration, Coastal digital twin},
abstract = {The coastal zone represents a critical intersection of naturally ecological and socio-economic processes. The abundance of data, models, and knowledge derived from various sources in coastal zones facilitates us to integrate them to better understand the evolution of coastal environments. This paper proposes a comprehensive framework of Coastal Zone Information Model (CZIM) to integrate multi-domain coastal information. The core idea of CZIM is to integrate multi-discipline coastal data, models, and knowledge for standardized governance, so as to carry, express, and apply coastal information by the digital system approaching the coastal digital twin. The CZIM framework includes four aspects: coastal data governance, model integration, knowledge engineering, and system construction. We perform a detailed literature review to illustrate the demands and challenges related to those four. The components of each aspect and their interlinks are introduced subsequently, and the future challenges of constructing coastal digital twins relying on CZIM are discussed. CZIM aims to strengthen the ability to organize, manage and apply refined coastal information to support more efficient, scientific, and intelligent decision-making in response to gradually volatile forces from both human activities and natural events, now and in the future. This paper provides a valuable reference for the next generation of coastal digitization in the target of the coastal digital twin.}
}
@article{MARINO2024101072,
title = {RummaGEO: Automatic mining of human and mouse gene sets from GEO},
journal = {Patterns},
volume = {5},
number = {10},
pages = {101072},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101072},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002319},
author = {Giacomo B. Marino and Daniel J.B. Clarke and Alexander Lachmann and Eden Z. Deng and Avi Ma’ayan},
keywords = {transcriptomics, gene expression, gene set enrichment analysis, data mining, data integration, signature search, LINCS, CFDE, ARCHS4, RNA sequencing},
abstract = {Summary
The Gene Expression Omnibus (GEO) has millions of samples from thousands of studies. While users of GEO can search the metadata describing studies, there is a need for methods to search GEO at the data level. RummaGEO is a gene expression signature search engine for human and mouse RNA sequencing perturbation studies extracted from GEO. To develop RummaGEO, we automatically identified groups of samples and computed differential expressions to extract gene sets from each study. The contents of RummaGEO are served for gene set, PubMed, and metadata search. Next, we analyzed the contents of RummaGEO to identify patterns and perform global analyses. Overall, RummaGEO provides a resource that is enabling users to identify relevant GEO studies based on their own gene expression results. Users of RummaGEO can incorporate RummaGEO into their analysis workflows for integrative analyses and hypothesis generation.}
}
@article{FARAHANI2025126508,
title = {Chart question answering with multimodal graph representation learning and zero-shot classification},
journal = {Expert Systems with Applications},
volume = {270},
pages = {126508},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126508},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001307},
author = {Ali Mazraeh Farahani and Peyman Adibi and Mohammad Saeed Ehsani and Hans-Peter Hutter and Alireza Darvishy},
keywords = {VQA, Chart question answering, Graph neural network, Graph attention, Graph isomorphism, Zero-shot learning},
abstract = {Chart Question Answering (CQA) is a special case of Visual Question Answering (VQA) that aims to generate accurate answers for questions related to charts, graphs, and other data representations. When exploring charts, one often asks complex reasoning questions involving logical and arithmetic operations, as well as referring to visual features. Previous approaches to CQA have applied general VQA methods, but their performance have been poor. In contrast, recent large models have focused on using image-based attention mechanisms for question answering while neglecting the inherent structure of charts. We propose a Graph Neural Network (GNN)-based model for CQA that leverages chart structure and relationships between chart components and question words to improve performance. By incorporating positional and structural encodings, our approach captures spatial relationships and chart topology, enhancing the model’s understanding of visual data. Using Graph Attention Network (GAT) and Graph Isomorphism Network (GIN), we effectively model relationships in graph-based data, offering a resource-efficient solution to CQA given hardware limitations. To address the Out of Vocabulary (OOV) problem, we integrate zero-shot learning (ZSL) for better generalization to unseen words and concepts. Additionally, Hard Negative Mining (HNM) enhances robustness by training the model to distinguish closely related answers, improving performance on complex and underrepresented question types. We have conducted intensive experiments on three publicly available challenging datasets (FigureQA, DVQA, and PlotQA), and analyzed different aspects of the proposed model. We demonstrate that our proposed approach surpasses competitive methods, particularly excelling in addressing structural questions, as expected from a graph-based topology capturing approach. Furthermore, we have conducted a series of examinations on various aspects of our proposed method by comprehensively analyzing the components within the answering pipeline. Additionally, we critically evaluated methods that utilize a classification approach for answer generation and demonstrate why their reported high accuracies are not necessarily reasonable. The resulting performance of the proposed method on the benchmarks are quite promising.}
}
@article{YANG2025107117,
title = {A discrete convolutional network for entity relation extraction},
journal = {Neural Networks},
volume = {184},
pages = {107117},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107117},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024010463},
author = {Weizhe Yang and Yongbin Qin and Kai Wang and Ying Hu and Ruizhang Huang and Yanping Chen},
keywords = {Relation extraction, Discrete convolution, Semantic structure, Deep learning, Natural language processing},
abstract = {Relation extraction independently verifies all entity pairs in a sentence to identify predefined relationships between named entities. Because these entity pairs share the same contextual features of a sentence, they lead to a complicated semantic structure. To distinguish semantic expressions between relation instances, manually designed rules or elaborate deep architectures are usually applied to learn task-relevant representations. In this paper, a discrete convolutional network is proposed to incorporate discrete linguistic interactions and deep feature weighting. This network applies a discretization strategy to fix parameters of convolutional kernels into ternary values. Then, these discretized kernels are used to learn discrete semantic structures from vectorized token representations. Our approach leverages the ability of discrete CNNs to capture discrete linguistic patterns of a sentence, thereby maintaining model expressiveness and improving performance in the relation extraction task. Furthermore, our method has the advantages of reducing the overfitting problem caused by depending on prior knowledge and decreasing the computational complexity by reducing the number of trainable parameters. Our model is evaluated on five widely used benchmark datasets. It achieves state-of-the-art performance, outperforming all compared related works. Experimental results also demonstrate that, compared with traditional CNN networks, it achieves an average improvement of 14.66% in F1-score and accelerates training by an average of 17.46%, highlighting the efficiency and effectiveness of our model in the relation extraction task.}
}
@article{FORNARI2025101477,
title = {Digital Twins of Business Processes: A Research Manifesto},
journal = {Internet of Things},
volume = {30},
pages = {101477},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101477},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524004189},
author = {Fabrizio Fornari and Ivan Compagnucci and Massimo {Callisto De Donato} and Yannis Bertrand and Harry H. Beyel and Emilio Carrión and Marco Franceschetti and Wolfgang Groher and Joscha Grüger and Emre Kilic and Agnes Koschmider and Francesco Leotta and Chiao-Yun Li and Giovani Lugaresi and Lukas Malburg and Juergen Mangler and Massimo Mecella and Oscar Pastor and Uwe Riss and Ronny Seiger and Estefania Serral and Victoria Torres and Pedro Valderas},
keywords = {Digital Twin, Business process, Internet of Things},
abstract = {Modern organizations necessitate continuous business processes improvement to maintain efficiency, adaptability, and competitiveness. In the last few years, the Internet of Things, via the deployment of sensors and actuators, has heavily been adopted in organizational and industrial settings to monitor and automatize physical processes influencing and enhancing how people and organizations work. Such advancements are now pushed forward by the rise of the Digital Twin paradigm applied to organizational processes. Advanced ways of managing and maintaining business processes come within reach as there is a Digital Twin of a business process - a virtual replica with real-time capabilities of a real process occurring in an organization. Combining business process models with real-time data and simulation capabilities promises to provide a new way to guide day-to-day organization activities. However, integrating Digital Twins and business processes is a non-trivial task, presenting numerous challenges and ambiguities. This manifesto paper aims to contribute to the current state of the art by clarifying the relationship between business processes and Digital Twins, identifying ongoing research and open challenges, thereby shedding light on and driving future exploration of this innovative interplay.}
}
@article{ZHENG2025,
title = {Machine Memory Intelligence: Inspired by Human Memory Mechanisms},
journal = {Engineering},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S2095809925000293},
author = {Qinghua Zheng and Huan Liu and Xiaoqing Zhang and Caixia Yan and Xiangyong Cao and Tieliang Gong and Yong-Jin Liu and Bin Shi and Zhen Peng and Xiaocen Fan and Ying Cai and Jun Liu},
keywords = {Machine memory intelligence, Neural mechanism, Associative representation, Continual learning, Collaborative reasoning},
abstract = {Large models, exemplified by ChatGPT, have reached the pinnacle of contemporary artificial intelligence (AI). However, they are plagued by three inherent drawbacks: excessive training data and computing power consumption, susceptibility to catastrophic forgetting, and a deficiency in logical reasoning capabilities within black-box models. To address these challenges, we draw insights from human memory mechanisms to introduce “machine memory,” which we define as a storage structure formed by encoding external information into a machine-representable and computable format. Centered on machine memory, we propose the brand-new machine memory intelligence (M2I) framework, which encompasses representation, learning, and reasoning modules and loops. We explore the key issues and recent advances in the four core aspects of M2I, including neural mechanisms, associative representation, continual learning, and collaborative reasoning within machine memory. M2I aims to liberate machine intelligence from the confines of data-centric neural networks and fundamentally break through the limitations of existing large models, driving a qualitative leap from weak to strong AI.}
}
@article{LEEWIS2025107627,
title = {Improving operational decision-making through decision mining - utilizing method engineering for the creation of a decision mining method},
journal = {Information and Software Technology},
volume = {179},
pages = {107627},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107627},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002325},
author = {Sam Leewis and Koen Smit and Bas {van den Boom} and Johan Versendaal},
keywords = {Decision making, Decision mining, Method engineering, Decision mining method, Systematic literature review},
abstract = {Context
This study addresses the challenge of enhancing the efficiency and agility of decision support software supporting both operational decision-making and software production teams developing decision support software. It centers on creating a method that assists in mining decisions, checking decisions on conformance, and improving decisions, which supports software production teams in developing decision support software.
Objective
The primary objective is to develop an explicit, clear, and structured approach for discovering, checking, and improving decisions using decision support software. The study aims to create a blueprint for software production teams to develop Decision Mining (DM) software, in line with recent advancements in the field. Additionally, it seeks to provide a consolidated, methodical overview of activities and deliverables in the DM research field.
Method
The research employs method engineering principles to construct a method for DM that leverages the existing body of knowledge by utilizing a Systematic Literature Review (SLR). The study focuses on developing individual building blocks and method fragments incorporated into seven DM scenarios.
Results
The study led to the creation of a Decision Mining Method (DMM), which includes 138 method fragments grouped into eleven categories. These fragments were systematically merged to form a comprehensive DMM. The method encapsulates the complexity of DM and provides practical applicability in real-world scenarios, highlighted by the identification of seven distinct scenarios in DM phases. The study also conducted the first SLR in the DM field, providing a comprehensive overview of current practices and outcomes.
Conclusion
The study helps in advancing the DM field by creating a structured approach and a comprehensive method for DM, aligning with recent developments in the field. It successfully aggregated the fragmented DM domain into a cohesive methodological overview, crucial for future research. The study also lays out a detailed agenda for future research, focusing on expanding and validating the DMM, incorporating cross-disciplinary insights, and addressing the challenges in machine learning within DM. The future research directions aim to refine and broaden the applicability of the DMM, ensuring its effectiveness in diverse practical contexts and contributing to a more holistic and comprehensive approach to decision mining.}
}
@article{GAN2024110794,
title = {A survey of dialogic emotion analysis: Developments, approaches and perspectives},
journal = {Pattern Recognition},
volume = {156},
pages = {110794},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110794},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005454},
author = {Chenquan Gan and Jiahao Zheng and Qingyi Zhu and Yang Cao and Ye Zhu},
keywords = {Dialogic emotion analysis, Natural language process, Dialogic artificial intelligence},
abstract = {Dialogic emotion analysis is an emerging and important research field in natural language processing. It aims to understand and process emotions in various forms of dialogue, such as human-human conversations, human–machine interactions, and chatbot responses. However, dialogic emotion analysis faces many challenges, such as the diversity of dialogue genres, the complexity of emotional expressions, and the difficulty of capturing the emotional needs of dialogue participants. Moreover, the current dialogue systems lack the ability to analyze emotions effectively and appropriately in different dialogue contexts. Therefore, a comprehensive review of the existing research on dialogic emotion analysis is needed. This survey aims to review dialogic emotion analysis methods based on natural language processing from 2017 to 2024. The review process follows the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). We summarize the research methods and emphasize their main research contributions. In addition, we also discuss current research trends and possible future research directions, as well as the impact of personal traits on emotions and potential ethical issues.}
}
@article{GHIO2024102687,
title = {Democratizing academic research with Artificial Intelligence: The misleading case of language},
journal = {Critical Perspectives on Accounting},
volume = {98},
pages = {102687},
year = {2024},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2023.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1045235423001430},
author = {Alessandro Ghio},
keywords = {Artificial Intelligence, ChatGPT, Communication model, Language, Posthuman, Technology, Translation},
abstract = {This essay questions the use of Artificial Intelligence (AI) models like ChatGPT to enable academics to work in multiple languages. ChatGPT has the potential to dismantle the dominance of English in research communication. Adapting Te Eni's model of communication complexity, I explore the implications of using ChatGPT for non-native English speakers in the development, inputs, process, and impact of research communication. I then relate these technological changes to broader reflections on the relationship between machines and humans and the implications for the future of academic research. I argue that far from democratizing research communication, the proliferation of AI models like ChatGPT is creating new power imbalances and hegemonic positions that raise important ethical concerns for the academic community.}
}
@article{CASHEEKAR2024100632,
title = {A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions},
journal = {Computer Science Review},
volume = {52},
pages = {100632},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100632},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000169},
author = {Avyay Casheekar and Archit Lahiri and Kanishk Rath and Kaushik Sanjay Prabhakar and Kathiravan Srinivasan},
keywords = {Computational intelligence, Artificial intelligence, Chatbots, Conversational agents, ChatGPT},
abstract = {This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI’s ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT’s applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT.}
}
@article{LI2025129232,
title = {Event-level supervised contrastive learning with back-translation augmentation for event causality identification},
journal = {Neurocomputing},
volume = {621},
pages = {129232},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129232},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224020034},
author = {Shunhang Li and Gang Zhou and Jing Chen and Yepeng Sun and Ningbo Huang and Sisi Peng},
keywords = {Event causality identification, Complex causality reasoning, Back-translation, Supervised contrastive learning, Sampling strategy},
abstract = {How to identify event causality from natural language texts is becoming a heated research topic due to the significant role played by causality in AI domain. However, the limited scale and complex causality expressions of existing labeled datasets still impede better event causality identification (ECI) performance, especially in sentence-level ECI. Here, this study proposes the event-level supervised contrastive learning with back-translation augmentation method (BT-ESupCL) to address the aforementioned two obstacles simultaneously. Specifically, an efficient compound back-translation augmentation method with three parallel strategies is designed to mitigate data scarcity accompanied by more expression diversity and lower computational cost. To validly model the intricate causality expressions (in terms of both event semantics and casual structures) and then train a robust model, BT-ESupCL develops novel sampling strategies to construct contrastive pairs with more causality interactions. And, a fine-tuned supervised contrastive loss is devised to cope with imbalanced class distributions and data noise. Comprehensive experiments and analyses are implemented to verify the efficacy of our method. The results demonstrate that our approach achieves more promising performance than prior methods with 1.3% and 2.2% F1-score improvements on EventStoryLine Corpus and CausalTimeBank respectively, and helps to gain more diverse samples, more robustness to noise.}
}
@article{SUN2025129035,
title = {Global Span Semantic Dependency Awareness and Filtering Network for nested named entity recognition},
journal = {Neurocomputing},
volume = {617},
pages = {129035},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129035},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401806X},
author = {Yunlei Sun and Xiaoyang Wang and Haosheng Wu and Miao Hu},
keywords = {Named entity recognition, Span-based methods, Semantic dependency},
abstract = {Span-based methods for nested named entity recognition (NER) are effective in handling the complexities of nested entities with hierarchical structures. However, these methods often overlook valid semantic dependencies among global spans, resulting in a partial loss of semantic information. To address this issue, we propose the Global Span Semantic Dependency Awareness and Filtering Network (GSSDAF). Our model begins with BERT for initial sentence encoding. Following this, a span semantic representation matrix is generated using a multi-head biaffine attention mechanism. We introduce the Global Span Dependency Awareness (GSDA) module to capture valid semantic dependencies among all spans, and the Local Span Dependency Enhancement (LSDE) module to selectively enhance key local dependencies. The enhanced span semantic representation matrix is then decoded to classify the spans. We evaluated our model on seven public datasets. Experimental results demonstrate that our model effectively handles nested NER, achieving higher F1 scores compared to baselines. Ablation experiments confirm the effectiveness of each module. Further analysis indicates that our model can learn valid semantic dependencies between global spans, significantly improving the accuracy of nested entity recognition. Our code is available at https://github.com/Shaun-Wong/GSSDAF.}
}
@article{ASIF2024483,
title = {Robotic disassembly for end-of-life products focusing on task and motion planning: A comprehensive survey},
journal = {Journal of Manufacturing Systems},
volume = {77},
pages = {483-524},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002127},
author = {Mohammed Eesa Asif and Alireza Rastegarpanah and Rustam Stolkin},
keywords = {Electric vehicles, Lithium-ion batteries, Robotic disassembly, Recycling, Circular Economy, Task and motion planning},
abstract = {The rise of mass production and the resulting accumulation of end-of-life (EoL) products present a growing challenge in waste management and highlight the need for efficient resource recovery. In response to this challenge, robotic disassembly has emerged as a vital tool for the circular economy. Combining accuracy, adaptability, and the potential for handling hazardous materials offers a sustainable solution for dismantling complex EoL objects. This comprehensive survey delves into the motivations for robotic disassembly and the pivotal role of task and motion planning (TAMP) in optimising disassembly processes. It analyses the evolution of disassembly strategies, from conventional methods to those driven by cutting-edge artificial intelligence (AI) techniques, for the future of waste management. Additionally, the survey explores several case study applications, focusing on the disassembly of EV lithium-ion batteries. It highlights how TAMP and AI integration can bolster adaptability, safety, and informed decision-making within real-world disassembly challenges. Finally, the review examines promising future research directions in robotics that hold the potential to advance further improvement in robotic disassembly to increase sustainability and the responsible management of EoL products.}
}
@article{WANG2024109588,
title = {Digital evolution and twin miracle of sugarcane breeding},
journal = {Field Crops Research},
volume = {318},
pages = {109588},
year = {2024},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2024.109588},
url = {https://www.sciencedirect.com/science/article/pii/S0378429024003411},
author = {Xiaoding Wang and Qibin Wu and Haitao Zeng and Xu Yang and Xuechao Yang and Xun Yi and Ibrahim Khalil and Youxiong Que},
keywords = {Sugarcane breeding, Smart breeding, Artificial intelligence, Blockchain, Human-Cyber-Physical System, Digital twin},
abstract = {Context
Sugarcane, as an important economic crop, faces challenges such as long breeding cycles, low genetic improvement efficiency, and complex breeding operations.
Method
In order to address these challenges and improve the economic benefits of sugarcane breeding, this paper proposes an innovative smart sugarcane breeding system driven by artificial intelligence (AI), blockchain and digital twin technologies.
Results
The system integrates these technologies within a Human-Cyber-Physical System framework to offer a more efficient, secure, and smart strategy for sugarcane breeding. Firstly, AI processes extensive genetic and phenotypic data to enable precise prediction and optimization of sugarcane traits, resulting in shortened breeding cycles and enhanced efficiency and accuracy in selecting elite sugarcane varieties. Secondly, blockchain technology ensures the security and traceability of breeding data, enhancing the reliability and integrity of the breeding process. Thirdly, digital twin technology enables the real-time circulation of lifelike representations of real-world data among breeding-related workers. The system architecture consists of three layers: a physical layer for data collection, a cyber layer responsible for data analysis, storage and circulation managed by AI, blockchain and digital twin, and a human layer comprised of breeders and stakeholders. This multi-layered approach allows for sophisticated interaction and collaboration between the physical and digital realms, enhancing decision-making and breeding outcomes.
Conclusion
Taken together, the system utilizes AI, blockchain, and digital twin technologies to support sugarcane breeding, offering a promising solution to overcome the limitations of traditional methods and establish a more sustainable and profitable sugarcane breeding system.}
}
@article{BUDUR2024112243,
title = {Building efficient and effective OpenQA systems for low-resource languages},
journal = {Knowledge-Based Systems},
volume = {302},
pages = {112243},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112243},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124008773},
author = {Emrah Budur and Rıza Özçelik and Dilara Soylu and Omar Khattab and Tunga Güngör and Christopher Potts},
keywords = {Question answering, Open domain question answering, OpenQA, Low-resource languages, Machine translation},
abstract = {Question answering (QA) is the task of answering questions posed in natural language with free-form natural language answers extracted from a given passage. In the OpenQA variant, only a question text is given, and the system must retrieve relevant passages from an unstructured knowledge source and use them to provide answers, which is the case in the mainstream QA systems on the Web. QA systems currently are mostly limited to the English language due to the lack of large-scale labeled QA datasets in non-English languages. In this paper, we show that effective, low-cost OpenQA systems can be developed for low-resource contexts. The key ingredients are (1) weak supervision using machine-translated labeled datasets and (2) a relevant unstructured knowledge source in the target language context. Furthermore, we show that only a few hundred gold assessment examples are needed to reliably evaluate these systems. We apply our method to Turkish as a challenging case study, since English and Turkish are typologically very distinct and Turkish has limited resources for QA. We present SQuAD-TR, a machine translation of SQuAD2.0, and we build our OpenQA system by adapting ColBERT-QA and retraining it over Turkish resources and SQuAD-TR using two versions of Wikipedia dumps spanning two years. We obtain a performance improvement of 24–32% in the Exact Match (EM) score and 22–29% in the F1 score compared to the BM25-based and DPR-based baseline QA reader models. Our results show that SQuAD-TR makes OpenQA feasible for Turkish, which we hope encourages researchers to build OpenQA systems in other low-resource languages. We make all the code, models, and the dataset publicly available at https://github.com/boun-tabi/SQuAD-TR.}
}
@article{WANG2025103029,
title = {EDDINet: Enhancing drug–drug interaction prediction via information flow and consensus constrained multi-graph contrastive learning},
journal = {Artificial Intelligence in Medicine},
volume = {159},
pages = {103029},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103029},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002719},
author = {Hong Wang and Luhe Zhuang and Yijie Ding and Prayag Tiwari and Cheng Liang},
keywords = {DDI prediction, Information flow, Multi-graph, Consensus regularization, Contrastive learning},
abstract = {Predicting drug–drug interactions (DDIs) is crucial for understanding and preventing adverse drug reactions (ADRs). However, most existing methods inadequately explore the interactive information between drugs in a self-supervised manner, limiting our comprehension of drug–drug associations. This paper introduces EDDINet: Enhancing Drug-Drug Interaction Prediction via Information Flow and Consensus-Constrained Multi-Graph Contrastive Learning for precise DDI prediction. We first present a cross-modal information-flow mechanism to integrate diverse drug features, enriching the structural insights conveyed by the drug feature vector. Next, we employ contrastive learning to filter various biological networks, enhancing the model’s robustness. Additionally, we propose a consensus regularization framework that collaboratively trains multi-view models, producing high-quality drug representations. To unify drug representations derived from different biological information, we utilize an attention mechanism for DDI prediction. Extensive experiments demonstrate that EDDINet surpasses state-of-the-art unsupervised models and outperforms some supervised baseline models in DDI prediction tasks. Our approach shows significant advantages and holds promising potential for advancing DDI research and improving drug safety assessments. Our codes are available at: https://github.com/95LY/EDDINet_code.}
}
@incollection{ZUCCO2024,
title = {Deep Learning Methods in NLP},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00249-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002499},
author = {Chiara Zucco},
keywords = {Machine learning, Ensemble learning, Bagging, Random forest, Decision tree, Bias-variance decomposition, Resampling},
abstract = {Deep learning encompasses a series of algorithms that utilize multiple layers of nonlinear transformations to model complex tasks with increasing levels of abstraction. Over the past decades, deep learning has transformed traditional NLP, shifting from manual feature engineering to automated representation learning, allowing for more sophisticated and effective models. This chapter focuses on the impact of deep learning architectures on natural language processing (NLP) and text mining, particularly in the context of bioinformatics. We will explore the evolution of deep learning architectures, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformer-based models, and how these methods have been applied to solve specific challenges in text mining for bioinformatics, such as Named Entity Recognition (NER), Relation Extraction, and Topic Modeling. Ultimately, this chapter aims to provide a comprehensive overview of how deep learning has advanced NLP applications in bioinformatics.}
}
@article{BLAUDINDETHE2023103772,
title = {Transforming drug discovery with a high-throughput AI-powered platform: A 5-year experience with Patrimony},
journal = {Drug Discovery Today},
volume = {28},
number = {11},
pages = {103772},
year = {2023},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2023.103772},
url = {https://www.sciencedirect.com/science/article/pii/S135964462300288X},
author = {François-Xavier {Blaudin de Thé} and Claire Baudier and Renan {Andrade Pereira} and Céline Lefebvre and Philippe Moingeon},
keywords = {artificial intelligence, computational precision medicine, data integration, drug discovery, industrialization, target identification},
abstract = {High-throughput computational platforms are being established to accelerate drug discovery. Servier launched the Patrimony platform to harness computational sciences and artificial intelligence (AI) to integrate massive multimodal data from internal and external sources. Patrimony has enabled researchers to prioritize therapeutic targets based on a deep understanding of the pathophysiology of immuno-inflammatory diseases. Herein, we share our experience regarding main challenges and critical success factors faced when industrializing the platform and broadening its applications to neurological diseases. We emphasize the importance of integrating such platforms in an end-to-end drug discovery process and engaging human experts early on to ensure a transforming impact.}
}
@incollection{2024301,
title = {Index},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {301-306},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.20001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964200011}
}
@article{JIN2025112750,
title = {Multi-LoRA continual learning based instruction tuning framework for universal information extraction},
journal = {Knowledge-Based Systems},
volume = {308},
pages = {112750},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112750},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013844},
author = {Yu Jin and Jie Liu and Shaowei Chen},
keywords = {Multi-LoRA continual learning, Instruction tuning, Universal information extraction},
abstract = {Universal information extraction (Universal IE) aims to develop one model capable of solving multiple IE target tasks. Previous works have enhanced extraction performance of target tasks through auxiliary tasks. However, there are still limitations in terms of learning strategies. From one aspect, joint learning-based universal IE approaches, which simply mix auxiliary tasks with target tasks, fail to enable the model to master basic knowledge from auxiliary tasks before learning target tasks. From another aspect, continual learning-based universal IE approaches, which sequentially update all the model parameters on auxiliary tasks and target tasks, tend to cause catastrophic forgetting. In this study, we design a multi-LoRA continual learning-based instruction fine-tuning framework for universal IE. Specifically, we design unique LoRA modules for learning auxiliary tasks and target tasks. We first freeze pre-trained weights and update additional parameters on auxiliary tasks through one LoRA module. Subsequently, we keep the weights frozen and further adjust parameters through another LoRA module to adapt the model to the target tasks. Finally, we merge the frozen weights with learned weights, thereby enabling the model to better leverage the acquired abilities during the inference phase. Therefore, our model masters basic extraction abilities before learning target tasks and does not forget this basic knowledge during the target learning process. Moreover, we regard extraction, classification, and recognition as basic abilities and further design auxiliary tasks based on these basic abilities. Experimental results on 37 datasets across 3 tasks show that our approach reaches state-of-the-art performance.}
}
@article{OPDAHL2023102182,
title = {Trustworthy journalism through AI},
journal = {Data & Knowledge Engineering},
volume = {146},
pages = {102182},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102182},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000423},
author = {Andreas L Opdahl and Bjørnar Tessem and Duc-Tien Dang-Nguyen and Enrico Motta and Vinay Setty and Eivind Throndsen and Are Tverberg and Christoph Trattner},
keywords = {Artificial Intelligence, Journalism, News Production, Trustworthiness},
abstract = {Quality journalism has become more important than ever due to the need for quality and trustworthy media outlets that can provide accurate information to the public and help to address and counterbalance the wide and rapid spread of disinformation. At the same time, quality journalism is under pressure due to loss of revenue and competition from alternative information providers. This vision paper discusses how recent advances in Artificial Intelligence (AI), and in Machine Learning (ML) in particular, can be harnessed to support efficient production of high-quality journalism. From a news consumer perspective, the key parameter here concerns the degree of trust that is engendered by quality news production. For this reason, the paper will discuss how AI techniques can be applied to all aspects of news, at all stages of its production cycle, to increase trust.}
}
@article{LU2024,
title = {AI: Bridging Ancient Wisdom and Modern Innovation in Traditional Chinese Medicine},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/58491},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000747},
author = {Linken Lu and Tangsheng Lu and Chunyu Tian and Xiujun Zhang},
keywords = {traditional Chinese medicine, TCM, artificial intelligence, AI, diagnosis},
abstract = {The pursuit of groundbreaking health care innovations has led to the convergence of artificial intelligence (AI) and traditional Chinese medicine (TCM), thus marking a new frontier that demonstrates the promise of combining the advantages of ancient healing practices with cutting-edge advancements in modern technology. TCM, which is a holistic medical system with >2000 years of empirical support, uses unique diagnostic methods such as inspection, auscultation and olfaction, inquiry, and palpation. AI is the simulation of human intelligence processes by machines, especially via computer systems. TCM is experience oriented, holistic, and subjective, and its combination with AI has beneficial effects, which presumably arises from the perspectives of diagnostic accuracy, treatment efficacy, and prognostic veracity. The role of AI in TCM is highlighted by its use in diagnostics, with machine learning enhancing the precision of treatment through complex pattern recognition. This is exemplified by the greater accuracy of TCM syndrome differentiation via tongue images that are analyzed by AI. However, integrating AI into TCM also presents multifaceted challenges, such as data quality and ethical issues; thus, a unified strategy, such as the use of standardized data sets, is required to improve AI understanding and application of TCM principles. The evolution of TCM through the integration of AI is a key factor for elucidating new horizons in health care. As research continues to evolve, it is imperative that technologists and TCM practitioners collaborate to drive innovative solutions that push the boundaries of medical science and honor the profound legacy of TCM. We can chart a future course wherein AI-augmented TCM practices contribute to more systematic, effective, and accessible health care systems for all individuals.}
}
@article{LEE2025115039,
title = {Metadata schema for virtual building models in digital twins: VB schema implemented in GPT-based applications},
journal = {Energy and Buildings},
volume = {327},
pages = {115039},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115039},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011551},
author = {Jeyoon Lee and Sungmin Yoon},
keywords = {GPT, Virtual models, Virtual buildings, Ontology, HVAC, Digital twins, Model metadata, Operation and maintenance (O&M), Built environments},
abstract = {A virtual building model (VBM) is a virtual entity that represents the physical behavior of a target building mathematically within a digital twin environment. The creation and synchronization of a VBM are achieved by utilizing various interrelated virtual sub-models, including behavior, correction, and distance models. To achieve continuous digital twinning, it is essential to manage the VBM with virtual sub-models. However, existing metadata schemas have limitations in describing VBMs representing operational building behaviors within the concept of building digital twins (DTs). Therefore, this study proposes a novel metadata schema, termed the virtual building model metadata schema (VB schema), to represent and manage VBMs in DT-built environments. The VB schema is established according to the mathematical and semantic ontology of the in-situ modeling and calibration approach for constructing and correcting virtual models during building operations, and it is linked to physical entities, data, and applications within DTs. Specifically, it involves: (1) determining classes for operational data and virtual models; (2) establishing relationships for interactions between model and data entities, between model classes, between model and physical entities, and between model and applications; (3) defining properties for each class of models; and (4) extending into the exiting metadata schema of Brick. To demonstrate the proposed VB schema, a virtual model describing supply pressure behaviors in a central heating system was developed and represented using the VB schema for DT-enabled building operations. Additionally, the VB schema was utilized for implementing generative pre-trained transformer (GPT)-based DT applications, which highlights its benefits in enhancing ontology comprehension of DTs in the context of VBMs, improving autonomous problem-solving capabilities in real building systems, and providing better interpretation of application results compared to cases where only the Brick schema was used. The VB schema is expected to enable continuous and autonomous in-situ management of VBMs for intelligent building services within the DT.}
}
@article{HU2025332,
title = {An overview of fake news detection: From a new perspective},
journal = {Fundamental Research},
volume = {5},
number = {1},
pages = {332-346},
year = {2025},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2024.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S2667325824000414},
author = {Bo Hu and Zhendong Mao and Yongdong Zhang},
keywords = {Fake news detection, Social media, Intentional creation, Heteromorphic transmission, Controversial reception},
abstract = {With the rapid development and popularization of Internet technology, the propagation and diffusion of information become much easier and faster. While making life more convenient, the Internet also promotes the wide spread of fake news, which will have a great negative impact on countries, societies, and individuals. Therefore, a lot of research efforts have been made to combat fake news. Fake news detection is typically a classification problem aiming at verifying the veracity of news contents, which may include texts, images and videos. This article provides a comprehensive survey of fake news detection. We first summarize three intrinsic characteristics of fake news by analyzing its entire diffusion process, namely intentional creation, heteromorphic transmission, and controversial reception. The first refers to why users publish fake news, the second denotes how fake news propagates and distributes, and the last means what viewpoints different users may hold for fake news. We then discuss existing fake news detection approaches according to these characteristics. Thus, this review will enable readers to better understand this field from a new perspective. We finally discuss the trends of technological advances in this field and also outline some potential directions for future research.}
}
@article{PANESAR202320,
title = {Natural language processing-driven framework for the early detection of language and cognitive decline},
journal = {Language and Health},
volume = {1},
number = {2},
pages = {20-35},
year = {2023},
issn = {2949-9038},
doi = {https://doi.org/10.1016/j.laheal.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2949903823000337},
author = {Kulvinder Panesar and María Beatriz {Pérez Cabello de Alba}},
keywords = {Language production, Memory concerns, Pre-screening model, Role and reference grammar, Speech assessment, Natural language processing},
abstract = {Natural Language Processing (NLP) technology has the potential to provide a non-invasive, cost-effective method using a timely intervention for detecting early-stage language and cognitive decline in individuals concerned about their memory. The proposed pre-screening language and cognition assessment model (PST-LCAM) is based on the functional linguistic model Role and Reference Grammar (RRG) to analyse and represent the structure and meaning of utterances, via a set of language production and cognition parameters. The model is trained on a DementiaBank dataset with markers of cognitive decline aligned to the global deterioration scale (GDS). A hybrid approach of qualitative linguistic analysis and assessment is applied, which includes the mapping of participants´ tasks of speech utterances and words to RRG phenomena. It uses a metric-based scoring with resulting quantitative scores and qualitative indicators as pre-screening results. This model is to be deployed in a user-centred conversational assessment platform.}
}
@article{PENG2025128724,
title = {Easy and effective! Data augmentation for knowledge-aware dialogue generation via multi-perspective sentences interaction},
journal = {Neurocomputing},
volume = {614},
pages = {128724},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128724},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014954},
author = {Sisi Peng and Dan Qu and Wenlin Zhang and Hao Zhang and Shunhang Li and Minchen Xu},
keywords = {Data augmentation, Knowledge-aware, Dialogue generation, Sentence interaction},
abstract = {In recent years, knowledge-based dialogue generation has garnered significant attention due to its capacity to produce informative and coherent responses through the integration of external knowledge into models. However, obtaining high-quality knowledge that aligns with the dialogue content poses a considerable challenge, necessitating substantial time and resources. To tackle the issue of limited dialogue data, a majority of research endeavors concentrate on data augmentation to augment the volume of training data. Regrettably, these methods overlook knowledge augmentation, leading to a restricted diversity in input data and yielding enhancements solely in specific metrics. Real-world conversations exhibit a spectrum of characteristics, including repetitions, reversals, and interruptions, demanding a heightened level of data diversity. In this study, we introduce a straightforward yet effective data augmentation technique known as Multi-perspective Sentence Interaction to bolster the connections among sentences from varied viewpoints. Through an examination of target responses from multiple dialogue perspectives, we enhance our comprehension of the relationships between dialogue sentences, thereby facilitating the expansion of knowledge-based dialogue data. Through experiments conducted on various knowledge-based dialogue datasets and utilizing different models, our findings illustrate a notable enhancement in the quality of model generation facilitated by our method. Specifically, we observed a 3.5% enhancement in reply accuracy and a 0.1506 increase in diversity (DIST-2). Moreover, there was a substantial improvement in knowledge selection accuracy by 19.04% and a reduction in model perplexity by 31.48%.}
}
@incollection{2025201,
title = {Index},
editor = {Faadiel Essop},
booktitle = {Truth Unveiled},
publisher = {Academic Press},
pages = {201-205},
year = {2025},
series = {Fundamentals of Physiology},
isbn = {978-0-443-23655-6},
doi = {https://doi.org/10.1016/B978-0-443-23655-6.09993-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443236556099937}
}
@article{ZHUANG2025113029,
title = {DyCR-Net: A dynamic context-aware routing network for multi-modal sarcasm detection in conversation},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {113029},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113029},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000772},
author = {Xingjie Zhuang and Zhixin Li and Fengling Zhou and Jingliang Gu and Canlong Zhang and Huifang Ma},
keywords = {Multimodal emotion recognition, Sarcasm detection, Multimedia content understanding, Multi-party conversations},
abstract = {Sarcasm is frequently used as a rhetorical device in daily life, where speakers express criticism, mockery, or irony by saying the opposite of what they mean or making statements that contrast with reality. In everyday conversations, humans typically rely on context to detect sarcastic intent, interpreting the background of the dialogue, perceiving emotions conveyed through tone of voice, and decoding non-verbal cues from body language and facial expressions. Most existing studies focus on constructing multimodal representations and identifying inconsistencies between modalities as indicators of sarcasm. However, the dynamic shifts in modality focus are critical for understanding complex real-world sarcastic scenarios. To address this, we propose a Dynamic Context-Aware Routing Network (DyCR-Net), which leverages multi-granularity cues from various modalities and constructs cross-modal routing networks (Text–Video, Text–Audio, and Video–Audio) that prioritize different networks depending on the sarcastic scenario. The weights of the routing networks are dynamically adjusted to better capture the required sarcastic information. Notably, our framework outperforms current state-of-the-art methods across multiple benchmark datasets.}
}
@article{YANG2025110149,
title = {Integrating prompt techniques and multi-similarity matching for named entity recognition in low-resource settings},
journal = {Engineering Applications of Artificial Intelligence},
volume = {144},
pages = {110149},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110149},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625001496},
author = {Jun Yang and Liguo Yao and Taihua Zhang and Chieh-Yuan Tsai and Yao Lu and Mingming Shen},
keywords = {Few-shot learning, Named entity recognition, Metric learning},
abstract = {Few-shot Named Entity Recognition (few-shot NER) is a technique that effectively trains models with limited annotated data, aiming to address the issue of low accuracy in traditional named entity recognition tasks due to sparse data. Existing methods often pay little attention to specific entity information when constructing prompts, which can challenge the model's understanding of semantic relationships between labels and entities. Metric learning methods rely on a single similarity metric, potentially limiting their ability to capture complex relationships between support and query sets. To address these issues, this paper proposes a novel method named Integrating Prompt Techniques and Multi-Similarity Matching for Named Entity Recognition in Low-Resource Settings (ProMSM). This method combines entity-guided masked prompt techniques with a multi-similarity fusion mechanism. The entity-guided masked prompt aims to fully utilize entity information from the support set or source domain training set. It integrates labels and their associated entity instances into input sequences, providing rich contextual information to enhance the model's accurate understanding of semantic relationships between labels and entities. The multi-similarity fusion technique integrates traditional dot product similarity with context-based custom attention scores, enhancing the precision of relationship measurement between tokens in the support and query sets. Finally, ProMSM was evaluated on five widely used public few-shot datasets under cross-label space and domain transfer settings. Extensive experimental results demonstrate that ProMSM outperforms previous few-shot NER methods.}
}
@article{ANDRIES2023100176,
title = {Alexa doesn't have that many feelings: Children's understanding of AI through interactions with smart speakers in their homes},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100176},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100176},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000553},
author = {Valentina Andries and Judy Robertson},
keywords = {And phrases: education, Conversational assistants, Smart speakers, AI education, Child-computer interaction, Trust, Anthropomorphism, LLM},
abstract = {As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google Home, have become commonly embedded in households, many children now routinely interact with Artificial Intelligence (AI) systems. It is important to research children's experiences with consumer devices which use AI techniques because these shape their understanding of AI and its capabilities. We conducted a mixed-methods study (questionnaires and interviews) with primary-school children aged 6–11 in Scotland to establish children's understanding of how voice-based CAs work, how they perceive their cognitive abilities, agency and other human-like qualities, their awareness and trust of privacy aspects when using CAs and what they perceive as appropriate verbal interactions with CAs. Most children overestimated the CAs' intelligence and were uncertain about the systems' feelings or agency. They also lacked accurate understanding of data privacy and security aspects, and believed it was wrong to be rude to conversational assistants. Exploring children's current understanding of AI-supported technology has educational implications; such findings will enable educators to develop appropriate materials to address the pressing need for AI literacy.}
}
@article{PHANUSUPAWIMOL2025101099,
title = {Innovation through intelligent computer-aided formulation design},
journal = {Current Opinion in Chemical Engineering},
volume = {47},
pages = {101099},
year = {2025},
issn = {2211-3398},
doi = {https://doi.org/10.1016/j.coche.2025.101099},
url = {https://www.sciencedirect.com/science/article/pii/S2211339825000103},
author = {Thunyaras Phanusupawimol and Kris Prasopsanti and Naz P Taskiran and Venkat Venkatasubramanian and Rafiqul Gani},
abstract = {This perspective paper presents a focused review of a selected topic of chemical-based products, namely, formulations. As formulations cover a wide range of chemical-based products, we highlight opportunities for innovation in three types of formulations — liquid blends, which are mixtures of chemicals that are in the liquid state at standard conditions; liquid formulations, which are mixtures of chemicals that may exist in different states but the final product is a single-phase liquid; and emulsions, which are also mixtures of chemicals that may exist in different states, but the final product is in the form of an emulsion. In each case, we discuss aspects of design, analysis, and innovation together with issues and challenges that could be tackled to find better and more sustainable products. In particular, the potential of hybrid artificial intelligence augmented computer-aided techniques that can aid in the design, analysis, and innovation of formulations is highlighted.}
}
@article{LIU2024103979,
title = {Multi-level bioinformatics resources support drug target discovery of protein–protein interactions},
journal = {Drug Discovery Today},
volume = {29},
number = {5},
pages = {103979},
year = {2024},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.103979},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624001041},
author = {Jia-Xin Liu and Xiao Zhang and Yuan-Qin Huang and Ge-Fei Hao and Guang-Fu Yang},
keywords = {, protein–protein interactions, drug targets, network, binding site},
abstract = {Drug discovery often begins with a new target. Protein–protein interactions (PPIs) are crucial to multitudinous cellular processes and offer a promising avenue for drug-target discovery. PPIs are characterized by multi-level complexity: at the protein level, interaction networks can be used to identify potential targets, whereas at the residue level, the details of the interactions of individual PPIs can be used to examine a target’s druggability. Much great progress has been made in target discovery through multi-level PPI-related computational approaches, but these resources have not been fully discussed. Here, we systematically survey bioinformatics tools for identifying and assessing potential drug targets, examining their characteristics, limitations and applications. This work will aid the integration of the broader protein-to-network context with the analysis of detailed binding mechanisms to support the discovery of drug targets.}
}
@article{NEUMANN2024101493,
title = {Cross-domain dynamic vocabulary in metrological use cases: Linkage, automatization and implementation},
journal = {Measurement: Sensors},
pages = {101493},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101493},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424004690},
author = {Julia Neumann},
keywords = {Controlled vocabulary, Semantic representation form, Semantics, Thesaurus, Dynamic digital processes},
abstract = {This paper with the title “Cross-Domain Dynamic Vocabulary in Metrological Use Cases: Linkage, Automatization and Implementation” describes concepts of controlled vocabulary in digital metrological workflows. Controlled vocabulary usually either focuses on flexibility at the loss of control or it represents a strict structure at the loss of flexibility. This paper discusses an approach which is called dynamic vocabulary. This concept enables the metrological workflow to maintain a high level of flexibility and to have control over the contents. The five levels of digitalization are used as a reference source to explain the idea of dynamic controlled vocabulary. A concept for domain linkage is described afterwards. To showcase this concept, the representation form of a thesaurus is used. The final topics of the paper consider automatization processes for dynamic vocabulary implementations. It also provides an outlook of the importance for the interaction with artificial intelligence and/or machine learning processes.}
}
@article{MONTEJORAEZ2024100654,
title = {A survey on detecting mental disorders with natural language processing: Literature review, trends and challenges},
journal = {Computer Science Review},
volume = {53},
pages = {100654},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100654},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000388},
author = {Arturo Montejo-Ráez and M. Dolores Molina-González and Salud María Jiménez-Zafra and Miguel Ángel García-Cumbreras and Luis Joaquín García-López},
keywords = {Mental disorders detection, Natural language processing, Machine learning, Survey},
abstract = {For years, the scientific community has researched monitoring approaches for the detection of certain mental disorders and risky behaviors, like depression, eating disorders, gambling, and suicidal ideation among others, in order to activate prevention or mitigation strategies and, in severe cases, clinical treatment. Natural Language Processing is one of the most active disciplines dealing with the automatic detection of mental disorders. This paper offers a comprehensive and extensive review of research works on Natural Language Processing applied to the identification of some mental disorders. To this end, we have identified from a literature review, which are the main types of features used to represent the texts, the machine learning algorithms that are preferred or the most targeted social media platforms, among other aspects. Besides, the paper reports on scientific forums and projects focused on the automatic detection of these problems over the most popular social networks. Thus, this compilation provides a broad view of the matter, summarizing main strategies, and significant findings, but, also, recognizing some of the weaknesses in the research works published so far, serving as clues for future research.}
}
@article{MUYAMA2024104746,
title = {Machine learning approaches for the discovery of clinical pathways from patient data: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {160},
pages = {104746},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104746},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001643},
author = {Lillian Muyama and Antoine Neuraz and Adrien Coulet},
keywords = {Clinical pathway, Machine learning, Data-driven approach, Patient data},
abstract = {Background:
Clinical pathways are sequences of events followed during the clinical care of a group of patients who meet pre-defined criteria. They have many applications ranging from healthcare evaluation and optimization to clinical decision support. These pathways can be discovered from existing healthcare data, in particular with machine learning which is a family of methods used to learn patterns from data. This review provides a comprehensive overview of the literature concerning the use of machine learning methods for clinical pathway discovery from patient data.
Methods:
Guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method , we conducted a systematic review of the existing literature. We searched 6 databases, i.e., ACM Digital Library, ScienceDirect, Web of Science, PubMed, IEEE Xplore, and Scopus spanning from January 2004 to December 2023 using search terms pertinent to clinical pathways and their development. Subsequently, the retrieved papers were analyzed to assess their relevance to the scope of this study.
Results:
In total, 131 papers that met the specified inclusion criteria were identified. These papers expressed diverse motivations behind data-driven clinical pathway discovery ranging from knowledge discovery to conformance checking with established clinical guidelines (derived from existing literature and clinical experts). Notably, the predominant methods employed (67.2%, n=88) involved unsupervised machine learning techniques, such as clustering and process mining.
Conclusions:
Relevant clinical pathways can be discovered from patient data using machine learning methods, with the desirable potential to aid clinical decision-making in healthcare. However, to reach this objective, the methods used to discover pathways should be reproducible, and rigorous performance evaluation by clinical experts needs to be conducted for validation.}
}
@article{AZAD2024103000,
title = {Advances in medical image analysis with vision Transformers: A comprehensive review},
journal = {Medical Image Analysis},
volume = {91},
pages = {103000},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.103000},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002608},
author = {Reza Azad and Amirhossein Kazerouni and Moein Heidari and Ehsan Khodapanah Aghdam and Amirali Molaei and Yiwei Jia and Abin Jose and Rijo Roy and Dorit Merhof},
keywords = {Transformers, Medical image analysis, Vision transformers, Deep neural networks},
abstract = {The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.}
}
@article{ZHENG2025106824,
title = {Personalized multi-head self-attention network for news recommendation},
journal = {Neural Networks},
volume = {181},
pages = {106824},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106824},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024007482},
author = {Cong Zheng and Yixuan Song},
keywords = {News recommendation, Natural language processing, Multi-head self-attention, Neural networks, Embedding},
abstract = {With the rapid explosion of online news and user population, personalized news recommender systems have proved to be efficient ways of alleviating information overload problems by suggesting information which attracts users in line with their tastes. Exploring relationships among words and news is critical to structurally model users’ latent tastes including interested domains, while selecting informative words and news can directly reflect users’ interests. Most of the current studies do not provide an effective framework that combines distilling users’ interested latent spaces and explicit points systematically. Moreover, introducing more advanced techniques to merely chase accuracy has become a universal phenomenon. In this study, we design a Personalized Multi-Head Self-Attention Network (PMSN) for news recommendation, which combines multi-head self-attention network with personalized attention mechanism from both word and news levels. Multi-head self-attention mechanism is used to model interactions among words and news, exploring latent interests. Personalized attention mechanism is applied by embedding users’ IDs to highlight informative words and news, which can enhance the interpretability of personalization. Comprehensive experiments conducted using two real-world datasets demonstrate that PMSN efficiently outperforms state-of-the-art methods in terms of recommendation accuracy, without complicated structure design and exhausted even external resources consumption. Furthermore, visualized case study validates that attention mechanism indeed increases the interpretability.}
}
@article{RAHHAL2024124101,
title = {Data science for job market analysis: A survey on applications and techniques},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124101},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124101},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009679},
author = {Ibrahim Rahhal and Ismail Kassou and Mounir Ghogho},
keywords = {Labor market analytics, Job market needs, Data science, Job title classification, Skill identification, Natural language processing},
abstract = {The job market is evolving continuously due to changes in economic landscapes, technological improvements, and skill requirements. In the era of digitalization, a wealth of data is becoming available, opening up new opportunities for labor market analysis. Many stakeholders can make informed decisions if they benefit from accurate and timely insights about the job market. However, traditional data sources and methods used for labor market analysis often fall short of capturing the diversity and trends of the evolving job market. Recently, researchers started exploring various data sources by leveraging data science techniques, which makes information extraction achievable. This survey reviews recent research published between 2015 and 2022 on labor market analytics through data science techniques and discusses future research directions. 101 primary studies were classified and evaluated to identify the data sources utilized for job market analysis; the skill extraction methods and their type; the occupation and sector identification methods; and the application of the study conducted. Finally, we explore potential avenues for future research in this area.}
}
@article{YUAN2024102616,
title = {Toward dynamic rehabilitation management: A novel smart product-service system development approach based on fine-tuned large vision model and Fuzzy-Dematel},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102616},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102616},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002647},
author = {Wenyu Yuan and Hua Zhao and Xiongjie Yang and Ting Han and Danni Chang},
keywords = {Smart PSS, Fine-tuned large model, Personalized service, Fuzzy DEMATEL, Rehabilitation management},
abstract = {Nowadays, transformative technologies such as artificial intelligence, big data, and cloud computing are significantly influencing and reshaping the daily lives of individuals. Guided by the overarching concept of digital transformation, data-driven Smart Product-Service Systems (SPSS) have emerged, prompting scholars to investigate development approaches tailored to diverse data sources. However, the current approaches employed in the construction of SPSS exhibit limited capability in processing vast amounts of user-generated unstructured data. The relationship between big data intelligence and personalized services remains undisclosed. Moreover, the current focus of SPSS orientation predominantly addresses end consumers or manufacturers, with inadequate attention given to dynamic collaborative models that involve multiple stakeholders. These gaps are particularly conspicuous in complex industries such as rehabilitation management. To tackle these challenges, this study introduces a novel SPSS development approach that integrates a large vision model and the fuzzy-DEMATEL method. Specifically, a data-driven predictive assessment module was proposed, which constructs a medical image dataset and trains a rehabilitation predictive assessment model based on the transformer architecture. Secondly, personalized intervention services were generated, involving the representation of system elements, configuration, and optimization of service parameters. The fuzzy-DEMATEL method is mainly used for the initialization of service parameters. Then, interactive feedback is integrated into rehabilitation exercises for achieving continuous rehabilitation evaluation and service improvement. To validate the proposed approach, a FPRM-SPSS case was implemented, and it shows that the predictive assessment model achieved a high level of accuracy when applied to the clinical dataset constructed in this study, and the system was evaluated with high scores in user satisfaction.}
}
@article{SCHRIJVER2024200340,
title = {Automobile insurance fraud detection using data mining: A systematic literature review},
journal = {Intelligent Systems with Applications},
volume = {21},
pages = {200340},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200340},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000164},
author = {Gilian Schrijver and Dipti K. Sarmah and Mohammed El-hajj},
keywords = {Systematic literature review, Automobile insurance, Insurance fraud, Fraud detection, Data mining, Machine learning},
abstract = {Insurance is a pivotal element in modern society, but insurers face a persistent challenge from fraudulent behaviour performed by policyholders. This behaviour could be detrimental to both insurance companies and their honest customers, but the intricate nature of insurance fraud severely complicates its efficient, automated detection. This study surveys fifty recent publications on automobile insurance fraud detection, published between January 2019 and March 2023, and presents both the most commonly used data sets and methods for resampling and detection, as well as interesting, novel approaches. The study adopts the highly-cited Systematic Literature Review (SLR) methodology for software engineering research proposed by Kitchenham and Charters and collected studies from four online databases. The findings indicate limited public availability of automobile insurance fraud data sets. In terms of detection methods, the prevailing approach involves supervised machine learning methods that utilise structured, intrinsic features of claims or policies and that lack consideration of an example-dependent cost of misclassification. However, alternative techniques are also explored, including the use of graph-based methods, unstructured textual data, and cost-sensitive classifiers. The most common resampling approach was found to be oversampling. This SLR has identified commonly used methods in recent automobile insurance fraud detection research, and interesting directions for future research. It adds value over a related review by also including studies published from 2021 onward, and by detailing the used methodology. Limitations of this SLR include its restriction to a small number of considered publication years and limited validation of choices made during the process.}
}
@article{LI2025126077,
title = {An improved two-stage zero-shot relation triplet extraction model with hybrid cross-entropy loss and discriminative reranking},
journal = {Expert Systems with Applications},
volume = {265},
pages = {126077},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126077},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029440},
author = {Diyou Li and Lijuan Zhang and Juncheng Zhou and Jie Huang and Neal Xiong and Lei Zhang and Jian Wan},
keywords = {Zero-shot relation triplet extraction, Hybrid cross-entropy loss function, Discriminative reranking training},
abstract = {Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation triplets from unstructured text under zero-shot conditions, where the relation sets in the training and testing stages are disjoint. However, most of the existing ZeroRTE models do not fully utilize the generation ability of the model and lack of precise measurement for the sorting of triplets. To this end, we propose a novel ZeroRTE model based on two training stages in this paper, including a generative training stage and a discriminative training stage. In the generative training stage, our model designs the hybrid cross-entropy loss function, which combines the forward cross-entropy loss function with the reverse cross-entropy loss function to improve the generation quality of relation triplets. In the discriminative training stage, our model integrates a reranking task to enhance the sorting accuracy of our model for candidate triplets. We evaluate the proposed model on two ZeroRTE datasets (FewRel and Wiki-ZSL), and relevant experimental results fully demonstrate the effectiveness of our method.}
}
@article{WATKINS2025108638,
title = {Neuradicon: Operational representation learning of neuroimaging reports},
journal = {Computer Methods and Programs in Biomedicine},
pages = {108638},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108638},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725000550},
author = {Henry Watkins and Robert Gray and Adam Julius and Yee-Haur Mah and James Teo and Walter H.L. Pinaya and Paul Wright and Ashwani Jha and Holger Engleitner and Jorge Cardoso and Sebastien Ourselin and Geraint Rees and Rolf Jaeger and Parashkev Nachev},
keywords = {Natural language processing, Neurology, Neuroradiology, Artificial intelligence},
abstract = {Background and Objective:
Radiological reports typically summarize the content and interpretation of imaging studies in unstructured form that precludes quantitative analysis. This limits the monitoring of radiological services to throughput undifferentiated by content, impeding specific, targeted operational optimization. Here we present Neuradicon, a natural language processing (NLP) framework for quantitative analysis of neuroradiological reports.
Methods:
Our framework is a hybrid of rule-based and machine-learning models to represent neurological reports in succinct, quantitative form optimally suited to operational guidance. These include probabilistic models for text classification and tagging tasks, alongside auto-encoders for learning latent representations and statistical mapping of the latent space.
Results:
We demonstrate the application of Neuradicon to operational phenotyping of a corpus of 336,569 reports, and report excellent generalizability across time and two independent healthcare institutions. In particular, we report pathology classification metrics with f1-scores of 0.96 on prospective data, and semantic means of interrogating the phenotypes surfaced via latent space representations.
Conclusion:
Neuradicon allows the segmentation, analysis, classification, representation and interrogation of neuroradiological reports structure and content. It offers a blueprint for the extraction of rich, quantitative, actionable signals from unstructured text data in an operational context.}
}
@article{DAI2024104744,
title = {MultiADE: A Multi-domain benchmark for Adverse Drug Event extraction},
journal = {Journal of Biomedical Informatics},
volume = {160},
pages = {104744},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104744},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400162X},
author = {Xiang Dai and Sarvnaz Karimi and Abeed Sarker and Ben Hachey and Cecile Paris},
keywords = {Adverse drug event, Drug safety, Natural language processing, Information extraction, Named entity recognition},
abstract = {Objective:
Active adverse event surveillance monitors Adverse Drug Events (ADE) from different data sources, such as electronic health records, medical literature, social media and search engine logs. Over the years, many datasets have been created, and shared tasks have been organised to facilitate active adverse event surveillance. However, most – if not all – datasets or shared tasks focus on extracting ADEs from a particular type of text. Domain generalisation – the ability of a machine learning model to perform well on new, unseen domains (text types) – is under-explored. Given the rapid advancements in natural language processing, one unanswered question is how far we are from having a single ADE extraction model that is effective on various types of text, such as scientific literature and social media posts.
Methods:
We contribute to answering this question by building a multi-domain benchmark for adverse drug event extraction, which we named MultiADE. The new benchmark comprises several existing datasets sampled from different text types and our newly created dataset—CADECv2, which is an extension of CADEC (Karimi et al., 2015), covering online posts regarding more diverse drugs than CADEC. Our new dataset is carefully annotated by human annotators following detailed annotation guidelines.
Conclusion:
Our benchmark results show that the generalisation of the trained models is far from perfect, making it infeasible to be deployed to process different types of text. In addition, although intermediate transfer learning is a promising approach to utilising existing resources, further investigation is needed on methods of domain adaptation, particularly cost-effective methods to select useful training instances. The newly created CADECv2 and the scripts for building the benchmark are publicly available at CSIRO’s Data Portal (https://data.csiro.au/collection/csiro:62387). These resources enable the research community to further information extraction, leading to more effective active adverse drug event surveillance.}
}
@article{ZHANG2024104676,
title = {Location-enhanced syntactic knowledge for biomedical relation extraction},
journal = {Journal of Biomedical Informatics},
volume = {156},
pages = {104676},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104676},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000947},
author = {Yan Zhang and Zhihao Yang and Yumeng Yang and Hongfei Lin and Jian Wang},
keywords = {Biomedical relation extraction, Syntactic knowledge, Position information},
abstract = {Biomedical relation extraction has long been considered a challenging task due to the specialization and complexity of biomedical texts. Syntactic knowledge has been widely employed in existing research to enhance relation extraction, providing guidance for the semantic understanding and text representation of models. However, the utilization of syntactic knowledge in most studies is not exhaustive, and there is often a lack of fine-grained noise reduction, leading to confusion in relation classification. In this paper, we propose an attention generator that comprehensively considers both syntactic dependency type information and syntactic position information to distinguish the importance of different dependency connections. Additionally, we integrate positional information, dependency type information, and word representations together to introduce location-enhanced syntactic knowledge for guiding our biomedical relation extraction. Experimental results on three widely used English benchmark datasets in the biomedical domain consistently outperform a range of baseline models, demonstrating that our approach not only makes full use of syntactic knowledge but also effectively reduces the impact of noisy words.}
}
@article{WEN2024100906,
title = {LATTE: Label-efficient incident phenotyping from longitudinal electronic health records},
journal = {Patterns},
volume = {5},
number = {1},
pages = {100906},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100906},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923003136},
author = {Jun Wen and Jue Hou and Clara-Lea Bonzel and Yihan Zhao and Victor M. Castro and Vivian S. Gainer and Dana Weisenfeld and Tianrun Cai and Yuk-Lam Ho and Vidul A. Panickan and Lauren Costa and Chuan Hong and J. Michael Gaziano and Katherine P. Liao and Junwei Lu and Kelly Cho and Tianxi Cai},
abstract = {Summary
Electronic health record (EHR) data are increasingly used to support real-world evidence studies but are limited by the lack of precise timings of clinical events. Here, we propose a label-efficient incident phenotyping (LATTE) algorithm to accurately annotate the timing of clinical events from longitudinal EHR data. By leveraging the pre-trained semantic embeddings, LATTE selects predictive features and compresses their information into longitudinal visit embeddings through visit attention learning. LATTE models the sequential dependency between the target event and visit embeddings to derive the timings. To improve label efficiency, LATTE constructs longitudinal silver-standard labels from unlabeled patients to perform semi-supervised training. LATTE is evaluated on the onset of type 2 diabetes, heart failure, and relapses of multiple sclerosis. LATTE consistently achieves substantial improvements over benchmark methods while providing high prediction interpretability. The event timings are shown to help discover risk factors of heart failure among patients with rheumatoid arthritis.}
}
@article{CHEN2025102969,
title = {RK-VQA: Rational knowledge-aware fusion-in-decoder for knowledge-based visual question answering},
journal = {Information Fusion},
volume = {118},
pages = {102969},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102969},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000429},
author = {Weipeng Chen and Xu Huang and Zifeng Liu and Jin Liu and Lan Yo},
keywords = {Knowledge-based VQA, Knowledge retrieval, Fusion-in-decoder},
abstract = {Knowledge-based Visual Question Answering (KB-VQA) expands traditional VQA by utilizing world knowledge from external sources when the image alone is insufficient to infer a correct answer. Existing methods face challenges due to low recall rates, limiting the ability to gather essential information for accurate answers. While increasing the amount of retrieved knowledge entries can enhance recall, it often introduces irrelevant information, adversely impairing model performance. To overcome these challenges, we propose RK-VQA, which comprises two components: First, a zero-shot weighted hybrid knowledge retrieval method that integrates local and global visual features with textual features from image–question pairs, enhancing the quality of knowledge retrieval and improving recall rates. Second, a rational knowledge-aware Fusion-in-Decoder architecture enhances answer generation by focusing on rational knowledge and reducing the influence of irrelevant information. Specifically, we develop a rational module to extract rational features, subsequently utilized to prioritize pertinent information via a novel rational knowledge-aware attention mechanism. We evaluate our RK-VQA on the OK-VQA, which is the largest knowledge-based VQA dataset. The results demonstrate that RK-VQA achieves significant results, recording an accuracy of 64.11%, surpassing the previous best result by 2.03%.}
}
@article{QIU2024102776,
title = {Artificial intelligence for drug discovery and development in Alzheimer's disease},
journal = {Current Opinion in Structural Biology},
volume = {85},
pages = {102776},
year = {2024},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2024.102776},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X24000034},
author = {Yunguang Qiu and Feixiong Cheng},
abstract = {The complex molecular mechanism and pathophysiology of Alzheimer's disease (AD) limits the development of effective therapeutics or prevention strategies. Artificial Intelligence (AI)-guided drug discovery combined with genetics/multi-omics (genomics, epigenomics, transcriptomics, proteomics, and metabolomics) analysis contributes to the understanding of the pathophysiology and precision medicine of the disease, including AD and AD-related dementia. In this review, we summarize the AI-driven methodologies for AD-agnostic drug discovery and development, including de novo drug design, virtual screening, and prediction of drug-target interactions, all of which have shown potentials. In particular, AI-based drug repurposing emerges as a compelling strategy to identify new indications for existing drugs for AD. We provide several emerging AD targets from human genetics and multi-omics findings and highlight recent AI-based technologies and their applications in drug discovery using AD as a prototypical example. In closing, we discuss future challenges and directions in AI-based drug discovery for AD and other neurodegenerative diseases.}
}
@article{WANG2024101144,
title = {Elucidating the role of artificial intelligence in drug development from the perspective of drug-target interactions},
journal = {Journal of Pharmaceutical Analysis},
pages = {101144},
year = {2024},
issn = {2095-1779},
doi = {https://doi.org/10.1016/j.jpha.2024.101144},
url = {https://www.sciencedirect.com/science/article/pii/S2095177924002417},
author = {Boyang Wang and Tingyu Zhang and Qingyuan Liu and Chayanis Sutcharitchan and Ziyi Zhou and Dingfan Zhang and Shao Li},
keywords = {Artificial intelligence, Drug-target interactions, Deep learning, Machine learning, Drug combination, Network pharmacology},
abstract = {Drug development remains a critical issue in the field of biomedicine. With the rapid advancement of information technologies such as artificial intelligence (AI) and the advent of the big data era, AI-assisted drug development has become a new trend, particularly in predicting drug-target associations. To address the challenge of drug-target prediction, AI-driven models have emerged as powerful tools, offering innovative solutions by effectively extracting features from complex biological data; accurately modeling molecular interactions; and precisely predicting potential drug-target outcomes. Traditional machine learning, network-based, and advanced deep learning architectures such as convolutional neural networks (CNNs), graph convolutional networks (GCNs), and transformers each play a pivotal role. This review systematically compiles and evaluates AI algorithms for drug- and drug combination-target predictions, highlighting their theoretical frameworks, strengths, and limitations. CNNs effectively identify spatial patterns and molecular features critical for drug-target interactions. GCNs provide deep insights into molecular interactions via relational data, whereas transformers increase prediction accuracy by capturing complex dependencies within biological sequences. Network-based models offer a systematic perspective by integrating diverse data sources, and traditional machine learning efficiently handles large datasets to improve overall predictive accuracy. Collectively, these AI-driven methods are transforming drug-target predictions and advancing the development of personalized therapy. This review summarizes the application of AI in drug development, particularly in drug-target prediction, and offers recommendations on models and algorithms for researchers engaged in biomedical research. It also provides typical cases to better illustrate how AI can further accelerate development in the fields of biomedicine and drug discovery.}
}
@article{AGRAWAL2024122470,
title = {Revolutionizing subjective assessments: A three-pronged comprehensive approach with NLP and deep learning},
journal = {Expert Systems with Applications},
volume = {239},
pages = {122470},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122470},
url = {https://www.sciencedirect.com/science/article/pii/S095741742302972X},
author = {Raghav Agrawal and Harshit Mishra and Ilanthenral Kandasamy and Shrishail Ravi Terni and Vasantha W.B.},
keywords = {Deep Neural Networks (DNN), Natural Language Processing (NLP), Question answering, Yet Another Keyword Extractor (YAKE), KeyBERT, Simple Contrastive Sentence Embedding Framework (simCSE), Camembert, Sentence Bidirectional Encoder Representations from Transformers (SBERT)},
abstract = {The enhanced answer evaluation system is a cutting-edge automated tool that evaluates subjective answers in various contexts, such as educational assessments, surveys, and feedback forms. The proposed system leverages Natural Language Processing (NLP) and deep learning techniques to analyse subjective answers and provide evaluation scores with precision. Students’ answers are evaluated based on various criteria, such as keywords, context, relevance, coherence, and similarity. This paper introduces an architecture for a subjective answer evaluator using three main aspects: detection of keywords, similarity matrix, and presence of named entities. It combines the three aspects and provides a final score. It provides a standardized mechanism to score a given user answer compared to the particular model answer without human prejudice. This research aims to transcend traditional methodologies that predominantly utilize keyword or keyphrase scoring (text-based similarity) to determine the final score of an answer without delving into its technical intricacies. The semantic similarity (vector-based) employs vector data representations for score calculation. This approach necessitates partitioning data into multiple vectors for a comprehensive analysis. While text similarity is effective for short answers, its efficacy diminishes as the length of the answer increases. Therefore, this study emphasizes the critical role of similarity scoring and Named Entity Recognition (NER) scoring in evaluating more extended responses based on the stsb-en-main dataset (short answers) and a custom dataset with 190 records. This research reveals its remarkable performance, which excels through a dynamic three-pronged approach: keyword scoring, semantic similarity, and NER scoring with models like Yet Another Keyword Extractor (YAKE), SimCSE and Camembert. These three independent components synergize to produce unmatched results, establishing a new standard in the field. This enhancement led to Root Mean Square Error (RMSE) scores of 0.031 (optimized error rate) and an impressive 71%+ accuracy for our comprehensive system. This achievement surpasses existing works, which typically reached accuracies ranging between 40%–60% for long answers.}
}
@article{WALEK2025125816,
title = {A text-based recommender system for recommending relevant news articles},
journal = {Expert Systems with Applications},
volume = {266},
pages = {125816},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125816},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424026836},
author = {Bogdan Walek and Patrik Müller},
keywords = {Recommender system, Text-based recommender system, News recommender system, Word2Vec, Doc2Vec, TF-IDF},
abstract = {Despite recent advances in the related fields and the growing popularity of AI-based tools, small businesses, and public institutions still face challenges when implementing recommendation systems to increase profits and provide personalised services. This paper provides a comprehensive overview of state-of-the-art systems and a case study of a recommender system for a small, low-budget project, battling with several constraints: the use of uncommon natural language, the use of raw, unlabelled textual data, thus using mainly techniques coming from the field of data mining and text mining rather than relying on supervised learning methods. Another constraint was the reliance on the smaller dataset and the limited resources available for the development. This led us to explore a range of content-based methods, including the utilisation of the similarity measures applied to word embeddings derived from the Word2Vec and Doc2Vec shallow neural network models, as well as the TF-–IDF method. Additionally, a topic modelling approach utilising the Latent Dirichlet Allocation was used, as well as collaborative filtering methods, such as the algorithm using the Singular Value Decomposition method, and hybrid methods integrating the fuzzy inference and fuzzy expert systems. Some of the obstacles identified were subsequently demonstrated to be too challenging for the development of accurate recommendations. However, the item-to-item similarity solutions, which were primarily content-based, yielded satisfactory results when the threshold of 75% of the average precision of recommendations assessed by users was exceeded. One such solution was the one that used the Word2Vec-based model, which had been trained on the parameters obtained from the word similarities and analogies tests. Furthermore, an overview of alternative techniques and methodologies is provided.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{HOU2025128667,
title = {HiNER: Hierarchical feature fusion for Chinese named entity recognition},
journal = {Neurocomputing},
volume = {611},
pages = {128667},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128667},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014383},
author = {Shuxiang Hou and Yurong Qian and Jiaying Chen and Jigui Zhao and Huiyong Lv and Jiyuan Zhang and Hongyong Leng and Mengnan Ma},
keywords = {Chinese named entity recognition, Hierarchical feature fusion, Transformer, Semantic enhancement},
abstract = {Named Entity Recognition (NER) aims to extract structured entity information from unstructured textual data by identifying entity boundaries and categories. Chinese NER is more challenging than that of English due to the complex structure and ambiguous word boundaries, as well as nested and discontinuous occurrences of entities. Previous Chinese NER methods are limited by their character-based approach and dependence on external lexical information, which is often non-contextualized, leading to the introduction of noise and potentially compromising model performance. This paper proposes a novel Chinese NER model, HiNER, which leverages external semantic enhancement and hierarchical attention fusion. Specifically, we initially formulate the Chinese NER as a character–character relation classification task, thoroughly taking into account the cases of nested and discontinuous entities. Then, by incorporating syntactic information, we develop a Triformer module that is used to better integrate Chinese character, lexical, and syntactic embeddings, carefully considering the impact of external semantic enhancement on the original text embeddings and reducing extrinsic information interference to some extent. In addition, through the fusion of local and global attention mechanisms, the representation of character–character relationships is enhanced, allowing for the effective capture of semantic features at various hierarchical levels within the Chinese context. We conduct extensive experiments on seven Chinese NER datasets, and the results indicate that the HiNER model achieves state-of-the-art (SOTA) performance. The outcomes also confirm that external semantic enhancement and hierarchical attention fusion can provide better assistance in accomplishing the Chinese NER task.}
}
@article{LEE2024124786,
title = {ESC-ZSAR: Expanded Semantics from Categories with Cross-Attention for Zero-Shot Action Recognition},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124786},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124786},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424016531},
author = {Jeong-Cheol Lee and Dong-Gyu Lee},
keywords = {Zero-shot action recognition, Cross-attention, Semantics expansion},
abstract = {Zero-shot action recognition endeavors to identify novel action categories not encountered during training by aligning a joint semantic space. However, despite advancements, zero-shot action recognition still needs to grapple with the inadequate semantic representation of seen data, hindering the transfer of diverse action videos. This study introduces a novel framework combining video, optical flow, and expanded label description via a cross-attention mechanism. This integration facilitates the capture of low and high-level motion dynamics, effectively bridging the domain gap between the video and text modalities. The proposed approach of generating expanded label descriptions efficiently enhances semantic information, thus ameliorating zero-shot transferability and providing a comprehensive grasp of semantics and motion. The temporal shuffle and alignment module is designed to enhance the generalization ability of image sequences by capturing discriminative high-level motions through frame sorting. The efficacy of the proposed method is validated through extensive experiments on three benchmark datasets, namely Kinetic-600, UCF-101, and HMDB-51. Notably, our model achieves state-of-the-art results in the zero-shot action recognition task.}
}
@article{YU2024105901,
title = {Future considerations for the Human Affectome: Reply to commentaries},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {167},
pages = {105901},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105901},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424003701},
author = {Alessandra N.C. Yu and Leroy Lowe and Daniela Schiller}
}
@article{EID2024202,
title = {A-MASA: Arabic Multi-Domain Aspect-Based Sentiment Analysis Datasets},
journal = {Procedia Computer Science},
volume = {244},
pages = {202-211},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029946},
author = {Yomna Eid and Hala Zayed and Walaa Medhat},
keywords = {Aspects Extraction, Arabic Aspect-Based Sentiment Analysis, Natural language processing, Datasets},
abstract = {The rapid growth of Natural Language Processing (NLP) applications has gained interest in various languages, including Arabic. One critical NLP task is Aspect-Based Sentiment Analysis (ABSA). ABSA involves identifying the sentiment expressed toward specific aspects or attributes of entities mentioned in a piece of text, rather than determining the overall sentiment. Despite the increased use of Semitic languages like Arabic in NLP over the past decade, there remains a lack of resources and datasets in Arabic, particularly in dialectical Arabic. Additionally, existing ABSA datasets are often limited to a single domain, which does not support all types of ABSA, such as multi-domain ABSA, nor do they allow for testing a model's generalization capabilities. This study addresses these limitations by constructing multi-domain ABSA datasets in Arabic for the tasks of aspect extraction and aspect polarity detection. The datasets include both real and synthetic data, covering dialectical and modern standard Arabic, and comprising 6,500 records across five domains. These datasets can be utilized in single-domain, cross-domain, and multi-domain experiments for ABSA. Furthermore, we evaluated the constructed datasets using various transformer-based models and assessed the impact of generating synthetic data on the diversity and balance of the data, as well as on model performance. The results reported in this research demonstrate that the data is valid for use in ABSA tasks. Moreover, the models used for evaluation achieved competitive or superior results compared to existing models on various datasets. Additionally, the annotation guidelines we created are presented as recommended practices applicable to different tasks. All datasets used in this study are publicly available for research purposes.}
}
@article{BRAGAZZI2023,
title = {The Impact of Generative Conversational Artificial Intelligence on the Lesbian, Gay, Bisexual, Transgender, and Queer Community: Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/52091},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123009330},
author = {Nicola Luigi Bragazzi and Andrea Crapanzano and Manlio Converti and Riccardo Zerbetto and Rola Khamisy-Farah},
keywords = {generative conversational artificial intelligence, chatbot, lesbian, gay, bisexual, transgender, and queer community, LGBTQ, scoping review, mobile phone},
abstract = {Background
Despite recent significant strides toward acceptance, inclusion, and equality, members of the lesbian, gay, bisexual, transgender, and queer (LGBTQ) community still face alarming mental health disparities, being almost 3 times more likely to experience depression, anxiety, and suicidal thoughts than their heterosexual counterparts. These unique psychological challenges are due to discrimination, stigmatization, and identity-related struggles and can potentially benefit from generative conversational artificial intelligence (AI). As the latest advancement in AI, conversational agents and chatbots can imitate human conversation and support mental health, fostering diversity and inclusivity, combating stigma, and countering discrimination. In contrast, if not properly designed, they can perpetuate exclusion and inequities.
Objective
This study aims to examine the impact of generative conversational AI on the LGBTQ community.
Methods
This study was designed as a scoping review. Four electronic scholarly databases (Scopus, Embase, Web of Science, and MEDLINE via PubMed) and gray literature (Google Scholar) were consulted from inception without any language restrictions. Original studies focusing on the LGBTQ community or counselors working with this community exposed to chatbots and AI-enhanced internet-based platforms and exploring the feasibility, acceptance, or effectiveness of AI-enhanced tools were deemed eligible. The findings were reported in accordance with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews).
Results
Seven applications (HIVST-Chatbot, TelePrEP Navigator, Amanda Selfie, Crisis Contact Simulator, REALbot, Tough Talks, and Queer AI) were included and reviewed. The chatbots and internet-based assistants identified served various purposes: (1) to identify LGBTQ individuals at risk of suicide or contracting HIV or other sexually transmitted infections, (2) to provide resources to LGBTQ youth from underserved areas, (3) facilitate HIV status disclosure to sex partners, and (4) develop training role-play personas encompassing the diverse experiences and intersecting identities of LGBTQ youth to educate counselors. The use of generative conversational AI for the LGBTQ community is still in its early stages. Initial studies have found that deploying chatbots is feasible and well received, with high ratings for usability and user satisfaction. However, there is room for improvement in terms of the content provided and making conversations more engaging and interactive. Many of these studies used small sample sizes and short-term interventions measuring limited outcomes.
Conclusions
Generative conversational AI holds promise, but further development and formal evaluation are needed, including studies with larger samples, longer interventions, and randomized trials to compare different content, delivery methods, and dissemination platforms. In addition, a focus on engagement with behavioral objectives is essential to advance this field. The findings have broad practical implications, highlighting that AI’s impact spans various aspects of people’s lives. Assessing AI’s impact on diverse communities and adopting diversity-aware and intersectional approaches can help shape AI’s positive impact on society as a whole.}
}
@article{BILAL2025,
title = {NLP for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review},
journal = {Journal of Pain and Symptom Management},
year = {2025},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2025.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0885392425000375},
author = {Muhammad Bilal and Ameer Hamza and Nadia Malik},
keywords = {Natural language processing, Electronic health records, Clinical notes, Cancer, Information extraction, Text classification},
abstract = {This review examines the application of natural language processing (NLP) techniques in cancer research using electronic health records (EHRs) and clinical notes. It addresses gaps in existing literature by providing a broader perspective than previous studies focused on specific cancer types or applications. A comprehensive literature search in the Scopus database identified 94 relevant studies published between 2019 and 2024. The analysis revealed a growing trend in NLP applications for cancer research, with information extraction (47 studies) and text classification (40 studies) emerging as predominant NLP tasks, followed by named entity recognition (7 studies). Among cancer types, breast, lung, and colorectal cancers were found to be the most studied. A significant shift from rule-based and traditional machine learning approaches to advanced deep learning techniques and transformer-based models was observed. It was found that dataset sizes used in existing studies varied widely, ranging from small, manually annotated datasets to large-scale EHRs. The review highlighted key challenges, including the limited generalizability of proposed solutions and the need for improved integration into clinical workflows. While NLP techniques show significant potential in analyzing EHRs and clinical notes for cancer research, future work should focus on improving model generalizability, enhancing robustness in handling complex clinical language, and expanding applications to understudied cancer types. The integration of NLP tools into palliative medicine and addressing ethical considerations remain crucial for utilizing the full potential of NLP in enhancing cancer diagnosis, treatment, and patient outcomes. This review provides valuable insights into the current state and future directions of NLP applications in cancer research.}
}
@article{BOSL2025101480,
title = {Dynamical measures of developing neuroelectric fields in emerging consciousness},
journal = {Current Opinion in Behavioral Sciences},
volume = {61},
pages = {101480},
year = {2025},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101480},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624001311},
author = {William J Bosl and Jenny R {Capua Shenkar}},
abstract = {Human consciousness emerges over time. From the moment of conception, a process of neurodevelopment and complexification begins, generating and supporting a neuroelectric field that can be quantified by computational methods from dynamical systems theory. In the early embryo, genetically driven cellular processes are mediated by endogenous electromagnetic fields and intrinsic electrical fields produced by migrating neurons. In the ambient cellular environment, these interactions influence each other, impacting neural migration. The emergence of Theory of Mind, often considered a hallmark of conscious awareness, is accompanied by increasing neural connectivity, neuroelectric field complexity, and more integrated information processing. Neurodegeneration in old age and the often-associated decline in conscious awareness correlate closely with changes in the dynamical complexity of the neuroelectric field. Monitoring trajectories of the neuroelectric field and its complexity changes through the lifespan presents a developmental perspective and empirical correlation for studying the emergence and decline of human consciousness.}
}
@article{KAPUSTINA2024100072,
title = {User-friendly and industry-integrated AI for medicinal chemists and pharmaceuticals},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100072},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000307},
author = {Olga Kapustina and Polina Burmakina and Nina Gubina and Nikita Serov and Vladimir Vinogradov},
keywords = {Machine Learning, Medicinal Chemistry, Pharmaceutics, Data-Driven Drug Discovery},
abstract = {Artificial intelligence has brought crucial changes to the whole field of natural sciences. Myriads of machine learning algorithms have been developed to facilitate the work of experimental scientists. Molecular property prediction and drug synthesis planning become routine tasks. Moreover, inverse design of compounds with tunable properties as well as on-the-fly autonomous process optimization and chemical space exploration became possible in silico. Affordable robotic platforms exist able to perform thousands of experiments every day, analyzing the results and tuning the protocols. Despite this, most of these developments get trapped at the stage of code or overlooked, limiting their use by experimental scientists. Meanwhile, visibility and the number of user-friendly tools and technologies available to date is too low to compensate for this fact, rendering the development of novel therapeutic compounds inefficient. In this Review, we set the goal to bridge the gap between modern technologies and experimental scientists to improve drug development efficacy. Here we survey advanced and easy-to-use technologies able to help medical chemists at every stage of their research, including those integrated in technological processes during COVID-19 pandemic motivated by the need for fast yet precise solutions. Moreover, we review how these technologies are integrated by industry and clinics to streamline drug development and production. These technologies already transform the current paradigm of scientific thinking and revolutionize not only medicinal chemistry, but the whole field of natural sciences.}
}
@article{LIU2023104294,
title = {A survey of Semantic Reasoning frameworks for robotic systems},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104294},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104294},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200183X},
author = {Weiyu Liu and Angel Daruna and Maithili Patel and Kartik Ramachandruni and Sonia Chernova},
keywords = {Semantic reasoning, Robotics, Knowledge bases},
abstract = {Robots are increasingly transitioning from specialized, single-task machines to general-purpose systems that operate in diverse and dynamic environments. To address the challenges associated with operation in real-world domains, robots must effectively generalize knowledge, learn, and be transparent in their decision making. This survey examines Semantic Reasoning techniques for robotic systems, which enable robots to encode and use semantic knowledge, including concepts, facts, ideas, and beliefs about the world. Continually perceiving, understanding, and generalizing semantic knowledge allows a robot to identify the meaningful patterns shared between problems and environments, and therefore more effectively perform a wide range of real-world tasks. We identify the three common components that make up a computational Semantic Reasoning Framework: knowledge sources, computational frameworks, and world representations. We analyze the existing implementations and the key characteristics of these components, highlight the many interactions that occur between them, and examine their integration for solving robotic tasks related to five aspects of the world, including objects, spaces, agents, tasks, and actions. By analyzing the computational formulation and underlying mechanisms of existing methods, we provide a unified view of the wide range of semantic reasoning techniques and identify open areas for future research.}
}
@article{HU2025126318,
title = {A traditional Chinese medicine prescription recommendation model based on contrastive pre-training and hierarchical structure network},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126318},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126318},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031853},
author = {Hailong Hu and Yaqian Li and Zeyu Zheng and Wenjun Hu and Riyang Lin and Yanlei Kang},
keywords = {TCM prescription recommendation, Contrastive pre-training, Hierarchical structure network, Network pharmacology},
abstract = {Traditional Chinese Medicine (TCM) prescriptions are personalized treatment plans crafted by Chinese practitioners based on TCM principles and clinical insights, tailored through the examination of patient symptoms, physical constitution, and other relevant data. However, the efficacy of existing TCM prescription recommendation models is often hampered by data scarcity, disparities in node popularity, and challenges in interpreting the recommended prescriptions, leading to outcomes that may need more convincing accuracy and interpretability. This paper introduces a TCM prescription recommendation model utilizing contrastive pre-training and a hierarchical structure network. By fitting node features via multi-view contrastive pre-training, this approach alleviates the issue of data sparsity. It further integrates linked features within homogeneous networks at a granular level. Moreover, a hierarchical structural network focuses on less popular nodes, enriching the representations of symptoms and herbal features. During the analysis of the results, an interpretability analysis of the recommended TCM prescriptions is performed using the network pharmacology method. The performance of our model surpasses the compared methods in the comparison. Compared to the best model, our model shows improvements on both datasets. In Dataset1, Precision@20, Recall@20, and F1-score@20 increase by 2.14%, 5.51%, and 3.07%, respectively. In Dataset2, Precision@20, Recall@20, and F1-score@20 rise by 1.50%, 1.64%, and 1.51%, respectively. The herbal prescription recommendation model in this study enhances the accuracy of herbal recommendations. It not only provides new insights for TCM clinical practice but also promotes the modernization and innovative development of TCM diagnosis and treatment.}
}
@article{PICCOLI2024101835,
title = {Digital transformation requires digital resource primacy: Clarification and future research directions},
journal = {The Journal of Strategic Information Systems},
volume = {33},
number = {2},
pages = {101835},
year = {2024},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2024.101835},
url = {https://www.sciencedirect.com/science/article/pii/S0963868724000179},
author = {Gabriele Piccoli and Varun Grover and Joaquin Rodriguez},
keywords = {Digital transformation, IT-enabled transformation, Digital ontology, Digital organization, Digital resources},
abstract = {Responding to recent calls, this essay offers a commentary on the framing and definition of organizational digital transformation. We focus on the unique ontology of digital transformation and delineate it from neighboring concepts.Our contention is that, despite its volume, current research remains unclear about how the digital transformation of organizations differs from their IT-enabled transformation. We advocate definitional precision to foster knowledge accumulation and to enable scholars to pursue important research questions that are unique to digital transformation. Our perspective, grounded in the notion of digital resources, defines digital transformation as the metamorphosis of an IT-enabled organization into a digital organization – one with a specific digital architecture and design principles.A key departure from previous conceptualization is that we characterize digital transformation as a change in digital technology architecture rather than a change from digital technology use. Our paper achieves the following: describes the constructs underpinning this formulation, digital resources and digital organization; justifies their use; and describes what research directions the new perspective promotes. With sound definitions of key constructs, Information Systems scholars have the unprecedented opportunity to lead the way in digital “x” research, making our discipline the reference point for the burgeoning “digital research” literature in related business fields.}
}
@article{GATTIGLIA2025225,
title = {Managing Artificial Intelligence in Archeology. An overview},
journal = {Journal of Cultural Heritage},
volume = {71},
pages = {225-233},
year = {2025},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2024.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S1296207424002516},
author = {Gabriele Gattiglia},
keywords = {Archaeology, Artificial intelligence, Big Data, Theory, Ethics},
abstract = {The integration of AI in archaeology poses several risks due to the oversimplification of complex archaeological data for computational ease. This reductionist approach fosters a deterministic view, treating provisional classifications as definitive truths and influencing subsequent interpretations. The reliance on legacy data and Big Data for AI training risks perpetuating outdated ideas and frameworks. As AI expands from automating tasks to interpreting and creating reconstructions, archaeologists must adopt a critical approach to avoid biased and harmful outputs. The deterministic view of AI hinders informed debate. Archaeologists should engage in discussions that address the classificatory, and ethical aspects as well as the materiality of AI. The accumulation of data in AI mimics storytelling but lacks the interpretative depth needed to understand historical human perspectives. Developing theories and narrative practices is essential to making archaeological data meaningful. The shift from a representational to a co-creative view of data is necessary to understand its re-use and the power dynamics involved. Finally, to normalise AI in archaeology, a critical and sceptical approach is needed to integrate AI into the real world and understand its implications and ethical considerations.}
}
@article{LIN2023102611,
title = {Medical visual question answering: A survey},
journal = {Artificial Intelligence in Medicine},
volume = {143},
pages = {102611},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102611},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723001252},
author = {Zhihong Lin and Donghao Zhang and Qingyi Tao and Danli Shi and Gholamreza Haffari and Qi Wu and Mingguang He and Zongyuan Ge},
keywords = {Visual question answering, Medical image interpretation, Computer vision, Natural language processing},
abstract = {Medical Visual Question Answering (VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovations, and potential improvements. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive and helpful information for researchers interested in the medical visual question answering field and encourage them to conduct further research in this field.}
}
@article{ZHANG2024123938,
title = {Medical chief complaint classification with hierarchical structure of label descriptions},
journal = {Expert Systems with Applications},
volume = {252},
pages = {123938},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123938},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424008042},
author = {Zibo Zhang and Zheng Lu and Jiandong Liu and Ruibin Bai},
keywords = {Deep learning, Text classification, Chief complaint, Hierarchical label},
abstract = {With rapid growth of online healthcare systems, chief complaint classification plays an important role in areas such as triage or doctor recommendation. Existing medical text classification techniques such as rule-based or learning-based methods fail to effectively utilize the inherent hierarchical structure of label descriptions that contain strong domain knowledge. In this paper, we propose a novel text classification framework for chief complaint by embedding both input text and hierarchical structure of label descriptions based on deep neural networks. The proposed framework makes use of not only three branches (i.e. chief complaint branch, main-category branch, and sub-category branch) with a Sequence Information Encoder to encode semantics from chief complaint and hierarchical structure of label descriptions but also a Hierarchical Relational Network with Attention module to capture complex relationships among them focusing on informative words with attentional scores. We evaluate our framework on two public medical datasets with label descriptions extracted from medical books and websites. Experimental results show that the proposed method outperforms other baseline techniques by a significant margin. The source code of our framework is available at ANONYMISED.}
}
@article{PENG202412,
title = {The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions},
journal = {Engineering},
volume = {34},
pages = {12-22},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2023.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S209580992300293X},
author = {Yujia Peng and Jiaheng Han and Zhenliang Zhang and Lifeng Fan and Tengyu Liu and Siyuan Qi and Xue Feng and Yuxi Ma and Yizhou Wang and Song-Chun Zhu},
keywords = {Artificial general intelligence, Artificial intelligence benchmark, Artificial intelligence evaluation, Embodied artificial intelligence, Value alignment, Turing test, Causality},
abstract = {The release of the generative pre-trained transformer (GPT) series has brought artificial general intelligence (AGI) to the forefront of the artificial intelligence (AI) field once again. However, the questions of how to define and evaluate AGI remain unclear. This perspective article proposes that the evaluation of AGI should be rooted in dynamic embodied physical and social interactions (DEPSI). More specifically, we propose five critical characteristics to be considered as AGI benchmarks and suggest the Tong test as an AGI evaluation system. The Tong test describes a value- and ability-oriented testing system that delineates five levels of AGI milestones through a virtual environment with DEPSI, allowing for infinite task generation. We contrast the Tong test with classical AI testing systems in terms of various aspects and propose a systematic evaluation system to promote standardized, quantitative, and objective benchmarks and evaluation of AGI.}
}
@article{WANG2024105442,
title = {Transformer-based deep learning model for the diagnosis of suspected lung cancer in primary care based on electronic health record data},
journal = {eBioMedicine},
volume = {110},
pages = {105442},
year = {2024},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2024.105442},
url = {https://www.sciencedirect.com/science/article/pii/S235239642400478X},
author = {Lan Wang and Yonghua Yin and Ben Glampson and Robert Peach and Mauricio Barahona and Brendan C. Delaney and Erik K. Mayer},
keywords = {Deep learning, Transformers, Machine learning, Cancer prediction, Primary care, Artificial intelligence},
abstract = {Summary
Background
Due to its late stage of diagnosis lung cancer is the commonest cause of death from cancer in the UK. Existing epidemiological risk models in clinical usage, which have Positive Predictive Values (PPV) of less than 10%, do not consider the temporal relations expressed in sequential electronic health record (EHR) data. We aimed to build a model for lung cancer early detection in primary care using machine learning with deep ‘transformer’ models on EHR data to learn from these complex sequential ‘care pathways’.
Methods
We split the Whole Systems Integrated Care (WSIC) dataset into 70% training and 30% validation. Within the training set we created a case–control study with lung cancer cases and control cases of ‘other’ cancers or respiratory conditions or ‘other’ non cancer conditions. Based on 3,303,992 patients from January 1981 to December 2020 there were 11,847 lung cancer cases. 5789 cases and 7240 controls were used for training and 50,000 randomly selected patients out of the whole validation population of 368,906 for validation. GP EHR data going back three years from the date of diagnosis less the most recent one months were semantically pre-processed by mapping from more than 30,000 terms to 450. Model building was performed using ALBERT with a Logistic Regression Classifier (LRC) head. Clustering was explored using k-means. An additional regression model alone was built on the pre-processed data as a comparator.
Findings
Our model achieved an AUROC of 0.924 (95% CI 0.921–0.927) with a PPV of 3.6% (95% CI 3.5–3.7) and Sensitivity of 86.6% (95% CI 85.3–87.8) based on the three year's data prior to diagnosis less the immediate month before index diagnosis. The comparator regression model achieved a PPV of 3.1% (95% CI 3.0–3.1) and AUROC of 0.887 (95% CI 0.884–0.889). We interpreted our model using cluster analysis and have identified six groups of patients exhibiting similar lung cancer progression patterns and clinical investigation patterns.
Interpretation
Capturing temporal sequencing between cancer and non-cancer pathways to diagnosis enables much more accurate models. Future work will focus on external dataset validation and integration into GP clinical systems for evaluation.
Funding
Cancer Research UK.}
}
@article{FAN2024112718,
title = {A confidence-based knowledge integration framework for cross-domain table question answering},
journal = {Knowledge-Based Systems},
volume = {306},
pages = {112718},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112718},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013522},
author = {Yuankai Fan and Tonghui Ren and Can Huang and Beini Zheng and Yinan Jing and Zhenying He and Jinbao Li and Jianxin Li},
keywords = {Table question answering, Natural language interfaces, Sequence-to-sequence model, Confidence estimation, Similarity ranking},
abstract = {Recent advancements in TableQA leverage sequence-to-sequence (Seq2seq) deep learning models to accurately respond to natural language queries. These models achieve this by converting the queries into SQL queries, using information drawn from one or more tables. However, Seq2seq models often produce uncertain (low-confidence) predictions when distributing probability mass across multiple outputs during a decoding step, frequently yielding translation errors. To tackle this problem, we present Ckif, a confidence-based knowledge integration framework that uses a two-stage deep-learning-based ranking technique to mitigate the low-confidence problem commonly associated with Seq2seq models for TableQA. The core idea of Ckif is to introduce a flexible framework that seamlessly integrates with any existing Seq2seq translation models to enhance their performance. Specifically, by inspecting the probability values in each decoding step, Ckif first masks out each low-confidence prediction from the predicted outcome of an underlying Seq2seq model. Subsequently, Ckif integrates prior knowledge of query language to generalize masked-out queries, enabling the generation of all possible queries and their corresponding NL expressions. Finally, a two-stage deep-learning ranking approach is developed to evaluate the semantic similarity of NL expressions to a given NL question, hence determining the best-matching result. Extensive experiments are conducted to investigate Ckif by applying it to five state-of-the-art Seq2seq models using a widely used public benchmark. The experimental results indicate that Ckif consistently enhances the performance of all the Seq2seq models, demonstrating its effectiveness for better supporting TableQA.}
}
@article{MOLENAAR2025107648,
title = {Concept definition review: A method for studying terminology in software engineering},
journal = {Information and Software Technology},
volume = {180},
pages = {107648},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107648},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002532},
author = {Sabine Molenaar and Nikita {van den Berg} and Fabiano Dalpiaz and Sjaak Brinkkemper},
keywords = {Literature review, Research method, Concept definition, Software engineering, Requirements engineering},
abstract = {Context:
In scientific domains, definitions provide a precise description of fundamental concepts. Although the debate within the philosophy of computer science regarding the scientific nature of software engineering (SE) is inconclusive, SE researchers have laid down important steps toward treating SE as a scientific paradigm.
Objective:
We aim to support precise and effective communication among SE researchers and practitioners by providing a systematic process for the identification and analysis of definitions, in order to support the selection of a suitable definition for a certain use case.
Method:
Inspired by methods for the planning and execution of systematic literature reviews, we construct a method that is specific for concept definition reviews (CDRs). These reviews are performed whenever a research team wishes to obtain a detailed understanding of an SE concept that may have been characterized by dozens, if not hundreds, definitions.
Results:
We built our method via two design science iterations. The first one focused on the concept feature and resulted in the definitive version of the CDR method presented in this paper. We then applied the revised method to two, related concepts: quality requirement and non-functional requirement. Besides showing the applicability of the CDR method, our results include findings regarding the characteristics and evolution of the terms.
Conclusions:
The two applications of the CDR method highlight the existence and citation of hundreds of definitions, many of which are nearly (but not exactly) identical. We put forward our method for other researchers to shed light on the key terminology in other sub-fields of SE.}
}
@article{CHEN2025104085,
title = {Improving cross-document event coreference resolution by discourse coherence and structure},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104085},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104085},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000275},
author = {Xinyu Chen and Peifeng Li and Qiaoming Zhu},
keywords = {Event coreference resolution, Discourse coherence, Discourse structure},
abstract = {Cross-Document Event Coreference Resolution (CD-ECR) is to identify and cluster together event mentions that occur across multiple documents. Existing methods exhibit two limitations: (1) In contrast to within-document event mentions, which are linked by rich, coherent contexts, cross-document event mentions lack such contexts, posing a challenging for the model to understand the relation between two event mentions in different documents. (2) The lack of coherent textual information between cross-document event mentions lead to the inability to capture their global information, which is important to mine long-distance interactions between them. To tackle these issues, we propose a novel discourse coherence enhancement mechanism and introduce discourse structure to improve cross-document event coreference resolution. Specifically, we first introduce a new task: Event-oriented cross-document coherence enhancement (ECD-CoE), which selects coherent sentences that form a coherent text for two cross-document event mentions. Second, we represent the coherent text as a tree structure with rhetorical relation information between textual units. We then obtain the global interaction information of event mentions from the tree structures and finally resolve coreferent events. Experimental results on both the ECB+ and GVC datasets indicate that our proposed method outperforms several state-of-the-art baselines.}
}
@article{CIMMINO2025104282,
title = {Open Digital Rights Enforcement framework (ODRE): From descriptive to enforceable policies},
journal = {Computers & Security},
volume = {150},
pages = {104282},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104282},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005881},
author = {Andrea Cimmino and Juan Cano-Benito and Raúl García-Castro},
keywords = {Open digital rights language, Privacy policies, ODRL enforcement},
abstract = {From centralised platforms to decentralised ecosystems, like Data Spaces, sharing data has become a paramount challenge. For this reason, the definition of data usage policies has become crucial in these domains, highlighting the necessity of effective policy enforcement mechanisms. The Open Digital Rights Language (ODRL) is a W3C standard ontology designed to describe data usage policies, however, it lacks built-in enforcement capabilities, limiting its practical application. This paper introduces the Open Digital Rights Enforcement (ODRE) framework, whose goal is to provide ODRL with enforcement capabilities. The ODRE framework proposes a novel approach to express ODRL policies that integrates the descriptive ontology terms of ODRL with other languages that allow behaviour specification, such as dynamic data handling or function evaluation. The framework includes an enforcement algorithm for ODRL policies and two open-source implementations in Python and Java. The ODRE framework is also designed to support future extensions of ODRL to specific domain scenarios. In addition, current limitations of ODRE, ODRL, and current challenges are reported. Finally, to demonstrate the enforcement capabilities of the implementations, their performance, and their extensibility features, several experiments have been carried out with positive results.}
}
@article{TSIRMPAS2024108231,
title = {Neural natural language processing for long texts: A survey on classification and summarization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108231},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108231},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003890},
author = {Dimitrios Tsirmpas and Ioannis Gkionis and Georgios Th. Papadopoulos and Ioannis Mademlis},
keywords = {Natural language processing, Long document, Document classification, Document summarization, Sentiment analysis, Deep neural networks},
abstract = {The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded online renders automated understanding of lengthy texts a critical issue. Relevant applications include automated Web mining, legal document review, medical records analysis, financial reports analysis, contract management, environmental impact assessment, news aggregation, etc. Despite the relatively recent development of efficient algorithms for analyzing long documents, practical tools in this field are currently flourishing. This article serves as an entry point into this dynamic domain and aims to achieve two objectives. First of all, it provides an introductory overview of the relevant neural building blocks, serving as a concise tutorial for the field. Secondly, it offers a brief examination of the current state-of-the-art in two key long document analysis tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Consequently, this article presents an introductory exploration of document-level analysis, addressing the primary challenges, concerns, and existing solutions. Finally, it offers a concise definition of “long text/document”, presents an original overarching taxonomy of common deep neural methods for long document analysis and lists publicly available annotated datasets that can facilitate further research in this area.}
}
@article{RAVAUT2024,
title = {Targeting COVID-19 and Human Resources for Health News Information Extraction: Algorithm Development and Validation},
journal = {JMIR AI},
volume = {3},
year = {2024},
issn = {2817-1705},
doi = {https://doi.org/10.2196/55059},
url = {https://www.sciencedirect.com/science/article/pii/S2817170524000620},
author = {Mathieu Ravaut and Ruochen Zhao and Duy Phung and Vicky Mengqi Qin and Dusan Milovanovic and Anita Pienkowska and Iva Bojic and Josip Car and Shafiq Joty},
keywords = {COVID-19, SARS-CoV-2, summary, summarize, news articles, deep learning, classification, summarization, machine learning, extract, extraction, news, media, NLP, natural language processing},
abstract = {Background
Global pandemics like COVID-19 put a high amount of strain on health care systems and health workers worldwide. These crises generate a vast amount of news information published online across the globe. This extensive corpus of articles has the potential to provide valuable insights into the nature of ongoing events and guide interventions and policies. However, the sheer volume of information is beyond the capacity of human experts to process and analyze effectively.
Objective
The aim of this study was to explore how natural language processing (NLP) can be leveraged to build a system that allows for quick analysis of a high volume of news articles. Along with this, the objective was to create a workflow comprising human-computer symbiosis to derive valuable insights to support health workforce strategic policy dialogue, advocacy, and decision-making.
Methods
We conducted a review of open-source news coverage from January 2020 to June 2022 on COVID-19 and its impacts on the health workforce from the World Health Organization (WHO) Epidemic Intelligence from Open Sources (EIOS) by synergizing NLP models, including classification and extractive summarization, and human-generated analyses. Our DeepCovid system was trained on 2.8 million news articles in English from more than 3000 internet sources across hundreds of jurisdictions.
Results
Rules-based classification with hand-designed rules narrowed the data set to 8508 articles with high relevancy confirmed in the human-led evaluation. DeepCovid’s automated information targeting component reached a very strong binary classification performance of 98.98 for the area under the receiver operating characteristic curve (ROC-AUC) and 47.21 for the area under the precision recall curve (PR-AUC). Its information extraction component attained good performance in automatic extractive summarization with a mean Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score of 47.76. DeepCovid’s final summaries were used by human experts to write reports on the COVID-19 pandemic.
Conclusions
It is feasible to synergize high-performing NLP models and human-generated analyses to benefit open-source health workforce intelligence. The DeepCovid approach can contribute to an agile and timely global view, providing complementary information to scientific literature.}
}
@article{WANG20241915,
title = {Vocabulary Matters: An Annotation Pipeline and Four Deep Learning Algorithms for Enzyme Named Entity Recognition},
journal = {Journal of Proteome Research},
volume = {23},
number = {6},
pages = {1915-1925},
year = {2024},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.3c00367},
url = {https://www.sciencedirect.com/science/article/pii/S1535390724002981},
author = {Meiqi Wang and Avish Vijayaraghavan and Tim Beck and Joram M. Posma},
keywords = {biomedical natural language processing, deep learning, named entity recognition},
abstract = {Enzymes are indispensable in many biological processes, and with biomedical literature growing exponentially, effective literature review becomes increasingly challenging. Natural language processing methods offer solutions to streamline this process. This study aims to develop an annotated enzyme corpus for training and evaluating enzyme named entity recognition (NER) models. A novel pipeline, combining dictionary matching and rule-based keyword searching, automatically annotated enzyme entities in >4800 full-text publications. Four deep learning NER models were created with different vocabularies (BioBERT/SciBERT) and architectures (BiLSTM/transformer) and evaluated on 526 manually annotated full-text publications. The annotation pipeline achieved an F1-score of 0.86 (precision = 1.00, recall = 0.76), surpassed by fine-tuned transformers for F1-score (BioBERT: 0.89, SciBERT: 0.88) and recall (0.86) with BiLSTM models having higher precision (0.94) than transformers (0.92). The annotation pipeline runs in seconds on standard laptops with almost perfect precision, but was outperformed by fine-tuned transformers in terms of F1-score and recall, demonstrating generalizability beyond the training data. In comparison, SciBERT-based models exhibited higher precision, and BioBERT-based models exhibited higher recall, highlighting the importance of vocabulary and architecture. These models, representing the first enzyme NER algorithms, enable more effective enzyme text mining and information extraction. Codes for automated annotation and model generation are available from https://github.com/omicsNLP/enzymeNER and https://zenodo.org/doi/10.5281/zenodo.10581586.
}
}
@article{KONG2025101749,
title = {TR-Net: Token Relation Inspired Table Filling Network for Joint Entity and Relation Extraction},
journal = {Computer Speech & Language},
volume = {90},
pages = {101749},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101749},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824001323},
author = {Yongle Kong and Zhihao Yang and Zeyuan Ding and Wenfei Liu and Shiqi Zhang and Jianan Xu and Hongfei Lin},
keywords = {Token relation, Table filling, Joint entity and relation extraction},
abstract = {Recently, table filling models have achieved promising performance in jointly extracting relation triplets from complex sentences, leveraging their inherent structural advantage of delineating entities and relations as table cells. Nonetheless, these models predominantly concentrate on the cells corresponding to entity pairs within the predicted tables, neglecting the interrelations among other token pairs. This oversight can potentially lead to the exclusion of essential token information. To address these challenges, we introduce the Token Relation-Inspired Network (TR-Net), a novel framework for the joint extraction of entities and relations. It encompasses a token relation generator that adaptively constructs a token relation table, concentrating on the prominent token cells. Moreover, it also uses a structure-enhanced encoder that integrates the structural and sequential data of sentences via a highway gate mechanism. Our experimental analysis demonstrates that TR-Net delivers considerable enhancements and achieves state-of-the-art performance on four public datasets.}
}
@article{2025100419,
title = {Pathology Visions 2024 Overview},
journal = {Journal of Pathology Informatics},
pages = {100419},
year = {2025},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2025.100419},
url = {https://www.sciencedirect.com/science/article/pii/S215335392500001X}
}
@article{UPADHYAY2024100088,
title = {A comprehensive survey on answer generation methods using NLP},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100088},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000360},
author = {Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi},
keywords = {Question-answering systems, Natural language processing, Question analysis, Answer extraction, Information retrieval},
abstract = {Recent advancements in question-answering systems have significantly enhanced the capability of computers to understand and respond to queries in natural language. This paper presents a comprehensive review of the evolution of question answering systems, with a focus on the developments over the last few years. We examine the foundational aspects of a question answering framework, including question analysis, answer extraction, and passage retrieval. Additionally, we delve into the challenges that question answering systems encounter, such as the intricacies of question processing, the necessity of contextual data sources, and the complexities involved in real-time question answering. Our study categorizes existing question answering systems based on the types of questions they address, the nature of the answers they produce, and the various approaches employed to generate these answers. We also explore the distinctions between opinion-based, extraction-based, retrieval-based, and generative answer generation. The classification provides insight into the strengths and limitations of each method, paving the way for future innovations in the field. This review aims to offer a clear understanding of the current state of question answering systems and to identify the scaling needed to meet the rising expectations and demands of users for coherent and accurate automated responses in natural language.}
}
@article{MANTE20243051,
title = {SeqImprove: Machine-Learning-Assisted Curation of Genetic Circuit Sequence Information},
journal = {ACS Synthetic Biology},
volume = {13},
number = {9},
pages = {3051-3055},
year = {2024},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.4c00392},
url = {https://www.sciencedirect.com/science/article/pii/S2161506324002341},
author = {Jeanet Mante and Zach Sents and Duncan Britt and William Mo and Chunxiao Liao and Ryan Greer and Chris J. Myers},
keywords = {named entity recognition, named entity normalization, SBOL, ontologies, machine learning},
abstract = {The progress and utility of synthetic biology is currently hindered by the lengthy process of studying literature and replicating poorly documented work. Reconstruction of crucial design information through post hoc curation is highly noisy and error-prone. To combat this, author participation during the curation process is crucial. To encourage author participation without overburdening them, an ML-assisted curation tool called SeqImprove has been developed. Using named entity recognition, called entity normalization, and sequence matching, SeqImprove creates machine-accessible sequence data and metadata annotations, which authors can then review and edit before submitting a final sequence file. SeqImprove makes it easier for authors to submit sequence data that is FAIR (findable, accessible, interoperable, and reusable).
}
}
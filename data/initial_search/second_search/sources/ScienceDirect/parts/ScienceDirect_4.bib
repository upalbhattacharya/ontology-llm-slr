@article{SYRIANI2024101287,
title = {Screening articles for systematic reviews with ChatGPT},
journal = {Journal of Computer Languages},
volume = {80},
pages = {101287},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000303},
author = {Eugene Syriani and Istvan David and Gauransh Kumar},
keywords = {Generative AI, GPT, Empirical research, Large language model, Literature review, Mapping study, Screening},
abstract = {Systematic reviews (SRs) provide valuable evidence for guiding new research directions. However, the manual effort involved in selecting articles for inclusion in an SR is error-prone and time-consuming. While screening articles has traditionally been considered challenging to automate, the advent of large language models offers new possibilities. In this paper, we discuss the effect of using ChatGPT on the SR process. In particular, we investigate the effectiveness of different prompt strategies for automating the article screening process using five real SR datasets. Our results show that ChatGPT can reach up to 82% accuracy. The best performing prompts specify exclusion criteria and avoid negative shots. However, prompts should be adapted to different corpus characteristics.}
}
@article{KOVARI2025e42077,
title = {Explainable AI chatbots towards XAI ChatGPT: A review},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e42077},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e42077},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025004578},
author = {Attila Kovari},
keywords = {Explainable AI (XAI), ChatGPT, AI chatbots, Natural language processing (NLP), Transparency, Controllable AI},
abstract = {Advances in artificial intelligence (AI) have had a major impact on natural language processing (NLP), even more so with the emergence of large-scale language models like ChatGPT. This paper aims to provide a critical review of explainable AI (XAI) methodologies for AI chatbots, with a particular focus on ChatGPT. Its main objectives are to investigate the applied methods that improve the explainability of AI chatbots, identify the challenges and limitations within them, and explore future research directions. Such goals emphasize the need for transparency and interpretability of AI systems to build trust with users and allow for accountability. While integrating such interdisciplinary methods, such as hybrid methods combining knowledge graphs with ChatGPT, enhancing explainability, they also highlight industry needs for explainability and user-centred design. This will be followed by a discussion of the balance between explainability and performance, then the role of human judgement, and finally the future of verifiable AI. These are the avenues through which insights can be used to guide the development of transparent, reliable and efficient AI chatbots.}
}
@article{SCHAEFER2024108796,
title = {GPT-4 as a biomedical simulator},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108796},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524008813},
author = {Moritz Schaefer and Stephan Reichl and Rob {ter Horst} and Adele M. Nicolas and Thomas Krausgruber and Francesco Piras and Peter Stepper and Christoph Bock and Matthias Samwald},
keywords = {Biomedical simulation, Large language models, GPT-4, Computational biology, Artificial intelligence},
abstract = {Background
Computational simulation of biological processes can be a valuable tool for accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks. This study provides proof-of-concept for the use of GPT-4 as a versatile simulator of biological systems.
Methods
We introduce SimulateGPT, a proof-of-concept for knowledge-driven simulation across levels of biological organization through structured prompting of GPT-4. We benchmarked our approach against direct GPT-4 inference in blinded qualitative evaluations by domain experts in four scenarios and in two quantitative scenarios with experimental ground truth. The qualitative scenarios included mouse experiments with known outcomes and treatment decision support in sepsis. The quantitative scenarios included prediction of gene essentiality in cancer cells and progression-free survival in cancer patients.
Results
In qualitative experiments, biomedical scientists rated SimulateGPT's predictions favorably over direct GPT-4 inference. In quantitative experiments, SimulateGPT substantially improved classification accuracy for predicting the essentiality of individual genes and increased correlation coefficients and precision in the regression task of predicting progression-free survival.
Conclusion
This proof-of-concept study suggests that LLMs may enable a new class of biomedical simulators. Such text-based simulations appear well suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulations, but for which extensive knowledge is available as written text. Finally, we propose several directions for further development of LLM-based biomedical simulators, including augmentation through web search retrieval, integrated mathematical modeling, and fine-tuning on experimental data.}
}
@incollection{DAVID2024251,
title = {Chapter Thirteen - Automatic programming (source code generator) based on an ontological model},
editor = {Preetha Evangeline David and P. Anandhakumar},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {132},
pages = {251-272},
year = {2024},
booktitle = {Applying Computational Intelligence for Social Good},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245823000748},
author = {Preetha Evangeline David and S. Malathi and P. Anandhakumar},
keywords = {Deep learning, Language model, Source code, Software engineering, Natural language processing, Ontological modeling},
abstract = {Source AI is an AI-powered tool that can generate code in any programming language from any human language description. It can also simplify, find errors and fix them and debug your code. Automatic code generation capabilities continue to evolve within programming languages, IDEs and tools that work at compile time. This coding technique has proliferated because it can reduce mundane programming grunt work, and developers have found that it improves turnaround times and accuracy. Auto generated code usually becomes a hindrance for developers who want to tweak it later on Teams should plan to restrict these tools to only certain parts of the SDLC, such as where they can act as facilitators in smaller, less complex situations.}
}
@article{DWIVEDI2023102642,
title = {Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy},
journal = {International Journal of Information Management},
volume = {71},
pages = {102642},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102642},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000233},
author = {Yogesh K. Dwivedi and Nir Kshetri and Laurie Hughes and Emma Louise Slade and Anand Jeyaraj and Arpan Kumar Kar and Abdullah M. Baabdullah and Alex Koohang and Vishnupriya Raghavan and Manju Ahuja and Hanaa Albanna and Mousa Ahmad Albashrawi and Adil S. Al-Busaidi and Janarthanan Balakrishnan and Yves Barlette and Sriparna Basu and Indranil Bose and Laurence Brooks and Dimitrios Buhalis and Lemuria Carter and Soumyadeb Chowdhury and Tom Crick and Scott W. Cunningham and Gareth H. Davies and Robert M. Davison and Rahul Dé and Denis Dennehy and Yanqing Duan and Rameshwar Dubey and Rohita Dwivedi and John S. Edwards and Carlos Flavián and Robin Gauld and Varun Grover and Mei-Chih Hu and Marijn Janssen and Paul Jones and Iris Junglas and Sangeeta Khorana and Sascha Kraus and Kai R. Larsen and Paul Latreille and Sven Laumer and F. Tegwen Malik and Abbas Mardani and Marcello Mariani and Sunil Mithas and Emmanuel Mogaji and Jeretta Horn Nord and Siobhan O’Connor and Fevzi Okumus and Margherita Pagani and Neeraj Pandey and Savvas Papagiannidis and Ilias O. Pappas and Nishith Pathak and Jan Pries-Heje and Ramakrishnan Raman and Nripendra P. Rana and Sven-Volker Rehm and Samuel Ribeiro-Navarrete and Alexander Richter and Frantz Rowe and Suprateek Sarker and Bernd Carsten Stahl and Manoj Kumar Tiwari and Wil {van der Aalst} and Viswanath Venkatesh and Giampaolo Viglia and Michael Wade and Paul Walton and Jochen Wirtz and Ryan Wright},
keywords = {Conversational agent, Generative artificial intelligence, Generative AI, ChatGPT, Large language models},
abstract = {Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.}
}
@article{WU2025103158,
title = {Retrieval augmented generation-driven information retrieval and question answering in construction management},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103158},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103158},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000515},
author = {Chengke Wu and Wenjun Ding and Qisen Jin and Junjie Jiang and Rui Jiang and Qinge Xiao and Longhui Liao and Xiao Li},
keywords = {Large language model, Retrieval augmented generation, Construction management},
abstract = {Construction management is a communication-intensive field, requiring prompt responses to queries from various stakeholders to ensure project continuity. However, retrieving accurate information from project documents is hampered by the mismatch in granularity between queries and vast contents and by inherent ambiguities in information. Large language models (LLMs) and retrieval-augmented generation (RAG) offer new opportunities to address the challenges. However, their effectiveness is limited by the segmentation of documents and insufficient consideration of engineers’ preferences. Therefore, we propose a novel paradigm: RAG for Construction Management (RAG4CM). It includes three components: 1) a pipeline that parses project documents into hierarchical structures to establish a knowledge pool; 2) novel RAG search algorithms; and 3) a user preference learning mechanism. The first two components enhance granularity alignment and RAG results by integrating document-level hierarchical features with raw contents. The preference learning realizes continuously improved responses along with user-system interactions. We developed a prototype system and conducted extensive experiments, demonstrating that the knowledge pool efficiently accommodates texts, tables, and images. RAG4CM realized a 0.924 Top-3 and 0.898 answer accuracy, surpassing both open-source frameworks and commercial products. In addition, preference learning further increases answer accuracy by 1.3 % to 9.5 %. Consequently, RAG4CM enables multi-source information retrieval in a user-friendly manner, improving communication efficiency and facilitating construction management activities.}
}
@article{ZHANG2025102816,
title = {Active in-context learning for cross-domain entity resolution},
journal = {Information Fusion},
volume = {117},
pages = {102816},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102816},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005943},
author = {Ziheng Zhang and Weixin Zeng and Jiuyang Tang and Hongbin Huang and Xiang Zhao},
keywords = {Entity resolution, Cross-domain entity resolution, In-context learning},
abstract = {Entity resolution (ER) is the task of determining the equivalence between two entity descriptions. In traditional settings, the testing data and training data come from the same domain, e.g., sharing the same attribute structure. Nevertheless, in practical situations, the testing and training data often span different domains, hence calling for the study of the cross-domain ER problem. To tackle the domain shift in cross-domain ER, state-of-the-art solutions devise neural models to utilize the information from the entity pairs in the target domain to guide the feature modeling in the source domain and also the model training. Nevertheless, these approaches require excessive computational resources and fine-tuning efforts to achieve effective matching. To mitigate these issues, in this work, we for the first time investigate the in-context learning (ICL) capabilities of large language models (LLMs) for cross-domain ER and introduce a new framework, CiDER. CiDER consists of three main modules, i.e., active candidate source data generation, in-context demonstration selection, and prompt generation, which can select optimal demonstrations from the source data to enhance LLM inference performance on ER in the target domain. Comprehensive experiments on multiple benchmarks demonstrate that CiDER offers significant improvements over existing methods on cross-domain ER.}
}
@article{LUO2025126579,
title = {DeBERTA-Att-LMCQA: A hybrid model of DeBERTA and attention for legal multi-choice question answering},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126579},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126579},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002015},
author = {Ying Luo and Xudong Luo and Guibin Chen},
keywords = {Question answering, Legal AI, Natural language processing, Large language model, Neural network},
abstract = {This paper presents a Multi-Choice Question Answering (MCQA) model designed specifically for the legal domain, utilising a DeBERTa-based framework combined with a bilinear attention mechanism. MCQA plays a critical role in the legal field by enabling the automated interpretation and analysis of complex legal articles, which is essential for tasks such as legal research, judicial examination preparation, and decision-making. The proposed model addresses the intricacies of legal language by employing vector similarity searches to align questions with relevant statutes, while a neural network processes these connections for accurate answer selection. Additionally, a binary classifier is integrated to enhance decision-making accuracy. Experimental evaluations on the JEC-QA dataset, a large-scale Legal MCQA (LMCQA) dataset for the Chinese judicial examination, demonstrate the model’s superior performance in accuracy, precision, recall, and F1 score when compared to existing methods. These findings underscore the model’s potential to advance legal Artificial Intelligence (AI) applications significantly, offering robust tools for legal professionals and improving efficiency in high-stakes legal tasks.}
}
@article{QI2024106417,
title = {KEMoS: A knowledge-enhanced multi-modal summarizing framework for Chinese online meetings},
journal = {Neural Networks},
volume = {178},
pages = {106417},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106417},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024003411},
author = {Peng Qi and Yan Sun and Muyan Yao and Dan Tao},
keywords = {Multi-modal meeting knowledge graph, Topic-based hierarchical clustering approach, Multi-modal enhanced encoding strategy, Topic-enhanced decoding strategy},
abstract = {The demand for “online meetings” and “collaborative office work” keeps surging recently, producing an abundant amount of relevant data. How to provide participants with accurate and fast summarizing service has attracted extensive attention. Existing meeting summarizing models overlook the utilization of multi-modal information and the information offsetting during summarizing. In this paper, we develop a knowledge-enhanced multi-modal summarizing framework. Firstly, we construct a three-layer multi-modal meeting knowledge graph, including basic, knowledge, and multi-modal layer, to integrate meeting information thoroughly. Then, we raise a topic-based hierarchical clustering approach, which considers information entropy and difference simultaneously, to capture the semantic evolution of meetings. Next, we devise a multi-modal enhanced encoding strategy, including a sentence-level cross-modal encoder, a joint loss function, and a knowledge graph embedding module, to learn the meeting and topic-level presentations. Finally, when generating summaries, we design a topic-enhanced decoding strategy for the Transformer decoder which mitigates semantic offsetting with the aid of topic information. Extensive experiments show that our proposed work consistently outperforms state-of-the-art solutions on the Chinese meeting dataset, where the ROUGE-1, ROUGE-2, and ROUGE-L are 49.98%, 21.03%, and 32.03% respectively.}
}
@article{LIU2025102486,
title = {Automatic medical report generation based on deep learning: A state of the art survey},
journal = {Computerized Medical Imaging and Graphics},
volume = {120},
pages = {102486},
year = {2025},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2024.102486},
url = {https://www.sciencedirect.com/science/article/pii/S0895611124001630},
author = {Xinyao Liu and Junchang Xin and Qi Shen and Zhihong Huang and Zhiqiong Wang},
keywords = {Automatic medical report generation, Deep learning, Encoder–decoder framework, Medical image, Natural language process},
abstract = {With the increasing popularity of medical imaging and its expanding applications, posing significant challenges for radiologists. Radiologists need to spend substantial time and effort to review images and manually writing reports every day. To address these challenges and speed up the process of patient care, researchers have employed deep learning methods to automatically generate medical reports. In recent years, researchers have been increasingly focusing on this task and a large amount of related work has emerged. Although there have been some review articles summarizing the state of the art in this field, their discussions remain relatively limited. Therefore, this paper provides a comprehensive review of the latest advancements in automatic medical report generation, focusing on four key aspects: (1) describing the problem of automatic medical report generation, (2) introducing datasets of different modalities, (3) thoroughly analyzing existing evaluation metrics, (4) classifying existing studies into five categories: retrieval-based, domain knowledge-based, attention-based, reinforcement learning-based, large language models-based, and merged model. In addition, we point out the problems in this field and discuss the directions of future challenges. We hope that this review provides a thorough understanding of automatic medical report generation and encourages the continued development in this area.}
}
@article{MUMUNI2024101188,
title = {Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101188},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101188},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001225},
author = {Fuseini Mumuni and Alhassan Mumuni},
keywords = {Domain knowledge, Cognitive architecture, Brain-inspired neural network, Explainable AI, Adversarial attack, Zero-shot generalization},
abstract = {We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-shot learning. Data-driven machine learning models have achieved remarkable performance and demonstrated capabilities surpassing humans in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural networks and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms like mathematical relations, logic rules, knowledge graphs, and large language models (LLMs). and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human brain to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience—that is, to deepen human understanding on how the brain works in general, and how it handles these problems.}
}
@article{FENG2025109587,
title = {Cognitive Digital Twins of the natural environment: Framework and application},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109587},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109587},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017457},
author = {Jun Feng and Hailin Tang and Siyuan Zhou and Yang Cai and Jianxin Zhang},
keywords = {Cognitive digital twin, Natural environment, Framework, Knowledge graph, Ontology},
abstract = {Digital Twin (DT) technology offers a method of creating digital models of natural systems to enhance their ability to withstand natural disasters. Currently, DT of the natural environment is in its initial phases, lacking adaptive capabilities and relying on human-assisted modeling. The key to endowing DT of the natural environment with greater autonomy lies in the integration of expert knowledge. Knowledge graphs can efficiently arrange and structurally store expert knowledge, thereby supporting the autonomous functionality of DT. This paper introduces the concept of Cognitive Digital Twin(CDT) derived from the industrial domain and presents a framework for CDT of the natural environment. This framework is centered around knowledge graph technology, aiming to provide more insights and guidance for system development. This framework integrates human cognition by constructing knowledge graphs of objects, models, events, and scene modes. Moreover, these knowledge graphs support agents for the dynamic adjustment of processes, as well as the adaptation and parameter optimization of related models. As a use case, we utilize this framework to implement digital twin watersheds. We develop appropriate ontologies and agents to facilitate the construction of cognitive digital watersheds for various regions. Cognitive digital watersheds effectively fulfill the application needs of integrated flood forecasting and control scheduling. This application validates the framework’s effectiveness and provides a reference for constructing CDTs of other natural systems.}
}
@article{TIBAU2024101331,
title = {ChatGPT for chatting and searching: Repurposing search behavior},
journal = {Library & Information Science Research},
volume = {46},
number = {4},
pages = {101331},
year = {2024},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2024.101331},
url = {https://www.sciencedirect.com/science/article/pii/S0740818824000525},
author = {Marcelo Tibau and Sean Wolfgand Matsui Siqueira and Bernardo Pereira Nunes},
keywords = {ChatGPT, Large language models (LLMs), Searching as learning (SaL), Search tactics, Search strategy adaptation, Conversational information systems},
abstract = {Generative AI tools, exemplified by ChatGPT, are transforming the way users interact with information by enabling dialogue-based querying instead of traditional keyword searches. While this conversational approach can simplify user interactions, it also presents challenges in structuring effective searches, refining prompts, and verifying AI-generated content. This study addresses these complexities by repurposing traditional search tactics for use in conversational AI environments, specifically to support the Searching as Learning (SaL) paradigm. Forty-five adapted tactics are introduced to aid users in defining information needs, refining queries, and evaluating ChatGPT's responses for relevance, utility, and reliability. Using the Efficient Search Tactic Identification (ESTI) method and constant comparison analysis, these tactics were mapped into a stratified model with seven categories. The framework provides a structured approach for users to leverage conversational agents more effectively, promoting critical thinking and iterative learning. This research underscores the importance of developing robust search strategies tailored to conversational AI environments, facilitating deeper learning and reflective information engagement. Additionally, it highlights the need for ongoing research into the design and evaluation of future chat-and-search systems.}
}
@article{TRIGUERO2024102135,
title = {General Purpose Artificial Intelligence Systems (GPAIS): Properties, definition, taxonomy, societal implications and responsible governance},
journal = {Information Fusion},
volume = {103},
pages = {102135},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102135},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004517},
author = {Isaac Triguero and Daniel Molina and Javier Poyatos and Javier {Del Ser} and Francisco Herrera},
keywords = {General-purpose AI, Meta-learning, Reinforcement learning, Neuroevolution, Few-shot learning, AutoML, Transfer learning, Generative AI, Large language models},
abstract = {Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research. This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgement of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI (commonly referred to as AI-powered AI) or (single) foundation models. As a prime example, we delve into generative AI (GenAI), aligning them with the terms and concepts presented in the taxonomy. Similarly, we explore the challenges and prospects of multi-modality, which involves fusing various types of data sources to expand the capabilities of GPAIS. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general purpose tasks, as they share many common aspects. Finally, with the goal of providing a holistic view of GPAIS, we discuss the current state of GPAIS, its prospects, implications for our society, and the need for regulation and governance of GPAIS to ensure their responsible and trustworthy development.}
}
@article{HU2024103279,
title = {Interpretable medical image Visual Question Answering via multi-modal relationship graph learning},
journal = {Medical Image Analysis},
volume = {97},
pages = {103279},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103279},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002044},
author = {Xinyue Hu and Lin Gu and Kazuma Kobayashi and Liangchen Liu and Mengliang Zhang and Tatsuya Harada and Ronald M. Summers and Yingying Zhu},
keywords = {Visual Question Answering, Medical dataset, Graph neural network, Multi-modal large vision language model, Large Language Model, Chain of thought},
abstract = {Medical Visual Question Answering (VQA) is an important task in medical multi-modal Large Language Models (LLMs), aiming to answer clinically relevant questions regarding input medical images. This technique has the potential to improve the efficiency of medical professionals while relieving the burden on the public health system, particularly in resource-poor countries. However, existing medical VQA datasets are small and only contain simple questions (equivalent to classification tasks), which lack semantic reasoning and clinical knowledge. Our previous work proposed a clinical knowledge-driven image difference VQA benchmark using a rule-based approach (Hu et al., 2023). However, given the same breadth of information coverage, the rule-based approach shows an 85% error rate on extracted labels. We trained an LLM method to extract labels with 62% increased accuracy. We also comprehensively evaluated our labels with 2 clinical experts on 100 samples to help us fine-tune the LLM. Based on the trained LLM model, we proposed a large-scale medical VQA dataset, Medical-CXR-VQA, using LLMs focused on chest X-ray images. The questions involved detailed information, such as abnormalities, locations, levels, and types. Based on this dataset, we proposed a novel VQA method by constructing three different relationship graphs: spatial relationships, semantic relationships, and implicit relationship graphs on the image regions, questions, and semantic labels. We leveraged graph attention to learn the logical reasoning paths for different questions. These learned graph VQA reasoning paths can be further used for LLM prompt engineering and chain-of-thought, which are crucial for further fine-tuning and training multi-modal large language models. Moreover, we demonstrate that our approach has the qualities of evidence and faithfulness, which are crucial in the clinical field. The code and the dataset is available at https://github.com/Holipori/Medical-CXR-VQA.}
}
@article{FENG2025106833,
title = {Retrieval In Decoder benefits generative models for explainable complex question answering},
journal = {Neural Networks},
volume = {181},
pages = {106833},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106833},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024007573},
author = {Jianzhou Feng and Qin Wang and Huaxiao Qiu and Lirong Liu},
keywords = {Explainable AI, Information retrieval, Generative decoding, Knowledge distillation, Question answering},
abstract = {Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical applications. Prevailing retrieval-augmented methods treat the retriever and generator as separate components, which inadvertently restricts the generator’s capabilities to those of the retriever through intensive supervised training. In this work, we propose an unsupervised Retrieval In Decoder framework for multi-granularity decoding called RID, which integrates retrieval directly into the decoding process of generative models. It dynamically adjusts decoding granularity based on retrieval outcomes, and duly corrects the decoding direction through its direct impact on the next token. Moreover, we introduce a reinforcement learning-driven knowledge distillation method for adaptive explanation generation to better apply to Small-scale Language Models (SLMs). The experimental results across six public benchmarks surpass popular LLMs and existing retrieval-augmented methods, which demonstrates the effectiveness of RID in models of different scales and verifies its applicability and scalability.}
}
@article{XU2024102636,
title = {A representation learning-based approach to enhancing manufacturing quality for low-voltage electrical products},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102636},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102636},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002842},
author = {Yuming Xu and Tao Peng and Jiaqi Tao and Ao Bai and Ningyu Zhang and Kendrik Lim},
keywords = {Low-voltage electrical product, Manufacturing quality, Word embedding, Graph embedding, Defect case recommendation},
abstract = {In low-voltage electrical product manufacturing, resolving quality issues is heavily reliant on engineering experience, and can be time-consuming and error-prone. Through quality management systems, a large number of historical defect cases can be consolidated for analysis along with relevant causes. However, these defect descriptions are often casually described with a mix of Chinese and English language, containing domain-specific terms. Additionally, defect product features have varieties and complex relationships. Therefore, historical defect cases have not been effectively utilized to support manufacturing quality issues. To address this challenge, this study proposes a representation learning-based approach to enhance manufacturing quality. Key research contributions include: (1) A two-stage word embedding technique based on the pre-trained language model. First, TSDAE is utilized for unsupervised pre-training on a large amount of unlabeled data. Then, Sentence-BERT is utilized for fine-tuning on a small set of labeled similar sentence pairs. This process yields a pre-trained language model specific to low-voltage electrical product defects. (2) NSHPSAGE graph embedding model based on the constructed product feature knowledge graph. We select more valuable neighboring nodes during sampling and explore different aggregation functions to enhance graph embedding performance. This model effectively aggregates product feature information into “Defect_Case” nodes, yielding graph embedding vectors. The model exhibits good Weighted-Precision and Weighted-Recall with a short training duration, and it can handle new nodes, addressing the issue of heterogeneous graph embedding. (3) A defect case recommendation technique that fuses word embedding and graph embedding. We use Multi-Head Attention Fusion in the late-fusion to obtain defect case vectors. This approach comprehensively considers defect description semantic knowledge and complex product feature relationships, enabling accurate defect case recommendation with the prototype system.}
}
@article{MCSHANE2025101335,
title = {A neurosymbolic approach to authorship anonymization},
journal = {Cognitive Systems Research},
pages = {101335},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2025.101335},
url = {https://www.sciencedirect.com/science/article/pii/S1389041725000154},
author = {Marjorie McShane and Sergei Nirenburg and Christian Arndt and Sanjay Oruganti and Jesse English},
abstract = {We report a neurosymbolic approach to authorship anonymization that combines knowledge-based paraphrasing, grounded in cognitive modeling, with support functions provided by a large language model (LLM). The cognitive model accounts for four things: what it means to faithfully retain meaning and discourse coherence in a paraphrase, how do deal with polysemy given that full semantic analysis of open text is beyond the state of the art, how to define and characterize an author’s style, and how to leverage human linguistic capabilities when preparing systems to automatically anonymize texts. LLMs augment the knowledge-based paraphrases in three ways: by filtering out atypical formulations, by selecting the best from multiple candidate paraphrases, and by offering additional paraphrases in case the knowledge-based paraphrasing fails to adequately anonymize the text. This neurosymbolic architecture favors knowledge-based processing for being reliable and explainable, while exploiting LLMs for what they do best: manipulate regularities in the surface form of language.}
}
@article{TIAN2024101875,
title = {Foundation model of ECG diagnosis: Diagnostics and explanations of any form and rhythm on ECG},
journal = {Cell Reports Medicine},
volume = {5},
number = {12},
pages = {101875},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101875},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124006463},
author = {Yuanyuan Tian and Zhiyuan Li and Yanrui Jin and Mengxiao Wang and Xiaoyang Wei and Liqun Zhao and Yunqing Liu and Jinlei Liu and Chengliang Liu},
keywords = {electrocardiograms, foundation model, zero-shot diagnosis, signal-language model, contrast learning, knowledge enhancement, cardiovascular diseases, multimodal},
abstract = {Summary
We propose a knowledge-enhanced electrocardiogram (ECG) diagnosis foundation model (KED) that utilizes large language models to incorporate domain-specific knowledge of ECG signals. This model is trained on 800,000 ECGs from nearly 160,000 unique patients. Despite being trained on single-center data, KED demonstrates exceptional zero-shot diagnosis performance across various regions, including different locales in China, the United States, and other regions. This performance spans across all age groups for various conditions such as morphological abnormalities, rhythm abnormalities, conduction blocks, hypertrophy, myocardial ischemia, and infarction. Moreover, KED exhibits robust performance on diseases it has not encountered during its training. When compared to three experienced cardiologists on real clinical datasets, the model achieves comparable performance in zero-shot diagnosis of seven common clinical ECG types. We concentrate on the zero-shot diagnostic capability and the generalization performance of the proposed ECG foundation model, particularly in the context of external multi-center data and previously unseen disease.}
}
@article{CORDEIRO2024105714,
title = {Petro NLP: Resources for natural language processing and information extraction for the oil and gas industry},
journal = {Computers & Geosciences},
volume = {193},
pages = {105714},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105714},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424001973},
author = {Fábio Corrêa Cordeiro and Patrícia Ferreira {da Silva} and Alexandre Tessarollo and Cláudia Freitas and Elvis {de Souza} and Diogo {da Silva Magalhaes Gomes} and Renato Rocha Souza and Flávio Codeço Coelho},
keywords = {Natural language processing, Information extraction, Ontology, Knowledge graphs, Linguistic corpora},
abstract = {Most companies struggle to find and extract relevant information from their technical documents. In particular, the Oil and Gas (O&G) industry faces the challenge of dealing with large amounts of data hidden within old and new geoscientific reports collected over decades of operation. Making this information available in a structured format can unlock valuable information among these mountains of data, which is crucial to support a wide range of industrial and academic applications. However, most natural language processing resources were built from general domain corpora extracted from the Internet and primarily written in English. This paper presents Petro NLP, a comprehensive set of natural language processing and information extraction resources for the oil and gas industry in Portuguese. We connected an interdisciplinary team of geoscientists, linguists, computer scientists, petroleum engineers, librarians, and ontologists to build a knowledge graph and several annotated corpora. The Petro NLP resources comprise: (i) Petro KGraph– a knowledge graph populated with entities and relations commonly found on technical reports; and (ii) Petrolês, PetroGold, PetroNER, and PetroRE– sets of corpora containing raw text and documents annotated with morphosyntactic labels, named entities, and relations. These resources are fundamental infrastructure for future research in natural language processing and information extraction in the oil industry. Our ongoing research uses these datasets to train and enhance pre-trained machine learning models that automatically extract information from geoscientific technical documents.}
}
@article{LIAO2024112507,
title = {Zero-shot relation triplet extraction as Next-Sentence Prediction},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112507},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112507},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124011419},
author = {Wenxiong Liao and Zhengliang Liu and Yiyang Zhang and Xiaoke Huang and Ninghao Liu and Tianming Liu and Quanzheng Li and Xiang Li and Hongmin Cai},
keywords = {Zero-shot learning, Relation triplet extraction, Information extraction, Knowledge graph},
abstract = {Zero-shot relation triplet extraction (ZeroRTE) endeavors to extract relation triplets from a test set using a model trained on a training set with disjoint relations from the test set. Current ZeroRTE approaches primarily rely on two strategies: 1) Combining pre-trained language models to generate additional training samples; 2) Adding a large number of parameters that require training from scratch on top of a pre-trained language model. However, the former approach does not ensure the quality of generated samples, and the latter often struggles to generalize to unseen relations in the test set, particularly when the training set is small. In this paper, we introduce a novel method, Next Sentence Prediction for Relation Triplet Extraction (NSP-RTE), abstracting ZeroRTE as a higher-level next sentence prediction (NSP) task to enhance its generalization ability to unseen relation categories. NSP-RTE integrates modules for relation recognition, entity detection, and triplet classification, leveraging pre-trained BERT models with fewer parameters requiring training from scratch, while eliminating the need for additional sample generation. Our experiments on the FewRel and Wiki-ZSL datasets demonstrate that NSP-RTE, with its simple and efficient design, significantly outperforms previous methods.}
}
@article{PARK2024578,
title = {Can ChatGPT be used to generate scientific hypotheses?},
journal = {Journal of Materiomics},
volume = {10},
number = {3},
pages = {578-584},
year = {2024},
issn = {2352-8478},
doi = {https://doi.org/10.1016/j.jmat.2023.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352847823001557},
author = {Yang Jeong Park and Daniel Kaplan and Zhichu Ren and Chia-Wei Hsu and Changhao Li and Haowei Xu and Sipei Li and Ju Li},
keywords = {large language models, scientific hypothesis generation, generative AI, GPT-4},
abstract = {We investigate whether large language models can perform the creative hypothesis generation that human researchers regularly do. While the error rate is high, generative AI seems to be able to effectively structure vast amounts of scientific knowledge and provide interesting and testable hypotheses. The future scientific enterprise may include synergistic efforts with a swarm of “hypothesis machines”, challenged by automated experimentation and adversarial peer reviews.}
}
@article{ZONG2024104716,
title = {Advancing Chinese biomedical text mining with community challenges},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104716},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104716},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001345},
author = {Hui Zong and Rongrong Wu and Jiaxue Cha and Weizhe Feng and Erman Wu and Jiakun Li and Aibin Shao and Liang Tao and Zuofeng Li and Buzhou Tang and Bairong Shen},
keywords = {Biomedical text mining, Health information processing, Natural language processing, Artificial intelligence, Large language model},
abstract = {Objective
This study aims to review the recent advances in community challenges for biomedical text mining in China.
Methods
We collected information of evaluation tasks released in community challenges of biomedical text mining, including task description, dataset description, data source, task type and related links. A systematic summary and comparative analysis were conducted on various biomedical natural language processing tasks, such as named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation.
Results
We identified 39 evaluation tasks from 6 community challenges that spanned from 2017 to 2023. Our analysis revealed the diverse range of evaluation task types and data sources in biomedical text mining. We explored the potential clinical applications of these community challenge tasks from a translational biomedical informatics perspective. We compared with their English counterparts, and discussed the contributions, limitations, lessons and guidelines of these community challenges, while highlighting future directions in the era of large language models.
Conclusion
Community challenge evaluation competitions have played a crucial role in promoting technology innovation and fostering interdisciplinary collaboration in the field of biomedical text mining. These challenges provide valuable platforms for researchers to develop state-of-the-art solutions.}
}
@article{FELIX2025100296,
title = {Why are you traveling? Inferring trip profiles from online reviews and domain-knowledge},
journal = {Online Social Networks and Media},
volume = {45},
pages = {100296},
year = {2025},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2024.100296},
url = {https://www.sciencedirect.com/science/article/pii/S2468696424000211},
author = {Lucas G.S. Félix and Washington Cunha and Claudio M.V. {de Andrade} and Marcos André Gonçalves and Jussara M. Almeida},
keywords = {Trip purpose, Tourism, Machine learning, Large language models, Text classification},
abstract = {This paper addresses the task of inferring trip profiles (TPs), which consists of determining the profile of travelers engaged in a particular trip given a set of possible categories. TPs may include working trips, leisure journeys with friends, or family vacations. Travelers with different TPs typically have varied plans regarding destinations and timing. TP inference may provide significant insights for numerous tourism-related services, such as geo-recommender systems and tour planning. We focus on TP inference using TripAdvisor, a prominent tourism-centric social media platform, as our data source. Our goal is to evaluate how effectively we can automatically discern the TP from a user review on this platform. A user review encompasses both textual feedback and domain-specific data (such as a user’s previous visits to the location), which are crucial for accurately characterizing the trip. To achieve this, we assess various feature sets (including text and domain-specific) and implement advanced machine learning models, such as neural Transformers and open-source Large Language Models (Llama 2, Bloom). We examine two variants of the TP inference task—binary and multi-class. Surprisingly, our findings reveal that combining domain-specific features with TF-IDF-based representation in an LGBM model performs as well as more complex Transformer and LLM models, while being much more efficient and interpretable.}
}
@article{TIAN2024112625,
title = {Agent-DA: Enhancing low-resource event extraction with collaborative multi-agent data augmentation},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112625},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112625},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124012590},
author = {Xuemeng Tian and Yikai Guo and Bin Ge and Xiaoguang Yuan and Hang Zhang and Yuting Yang and Wenjun Ke and Guozheng Li},
keywords = {Event extraction, Data augmentation, Multi-agent},
abstract = {Low-resource event extraction presents a significant challenge in real-world applications, particularly in domains like pharmaceuticals, military and law, where data is frequently insufficient. Data augmentation, as a direct method for expanding samples, is considered an effective solution. However, existing data augmentation methods often suffer from text fluency issues and label hallucination. To address these challenges, we propose a framework called Agent-DA, which leverages multi-agent collaboration for event extraction data augmentation. Specifically, Agent-DA follows a three-step process: data generation by the large language model, collaborative filtering by both the large language model and small language model to discriminate easy samples, and the use of an adjudicator to identify hard samples. Through iterative and selective augmentation, our method significantly enhances both the quantity and quality of event samples, improving text fluency and label consistency. Extensive experiments on the ACE2005-EN and ACE2005-EN+ datasets demonstrate the effectiveness of Agent-DA, with F1-score improvements ranging from 0.15% to 16.18% in trigger classification and from 2.2% to 15.67% in argument classification.}
}
@article{HEREDIAALVARO2025103007,
title = {An advanced retrieval-augmented generation system for manufacturing quality control},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103007},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103007},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400658X},
author = {José Antonio {Heredia Álvaro} and Javier González Barreda},
keywords = {Knowledge-based systems, Quality control, Ceramic tile, Retrieval-augmented generation, Large language models},
abstract = {The rise of Large Language Models (LLMs) with generative artificial intelligence has revolutionized the development of knowledge-based systems, enabling intuitive interactions through natural language. This paper explores the implementation of an advanced Retrieval-Augmented Generation (RAG) system, designed to improve manufacturing quality control by utilizing the capabilities of LLMs, particularly OpenAI’s GPT models. We focus on the ceramic tile manufacturing process, where the system retrieves and analyzes specialized bibliographic sources to diagnose defects and propose solutions. In addition to core RAG functionalities, the system incorporates tailored pre-processing and post-processing mechanisms to optimize document retrieval and response generation. The system’s effectiveness in solving quality issues is demonstrated through its application in identifying defect causes and generating actionable solutions, significantly improving non-conformities management. This approach not only streamlines troubleshooting but also enhances the quality control system, providing a comprehensive, scalable tool for manufacturers.}
}
@article{NIU2024102069,
title = {EHR-KnowGen: Knowledge-enhanced multimodal learning for disease diagnosis generation},
journal = {Information Fusion},
volume = {102},
pages = {102069},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102069},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003858},
author = {Shuai Niu and Jing Ma and Liang Bai and Zhihua Wang and Li Guo and Xian Yang},
keywords = {Multimodal learning, Multimodal electronic health records, Knowledge enhancement, Generative large language model, Disease diagnosis},
abstract = {Electronic health records (EHRs) contain diverse patient information, including medical notes, clinical events, and laboratory test results. Integrating this multimodal data can improve disease diagnoses using deep learning models. However, effectively combining different modalities for diagnosis remains challenging. Previous approaches, such as attention mechanisms and contrastive learning, have attempted to address this but do not fully integrate the modalities into a unified feature space. This paper presents EHR-KnowGen, a multimodal learning model enhanced with external domain knowledge, for improved disease diagnosis generation from diverse patient information in EHRs. Unlike previous approaches, our model integrates different modalities into a unified feature space with soft prompts learning and leverages large language models (LLMs) to generate disease diagnoses. By incorporating external domain knowledge from different levels of granularity, we enhance the extraction and fusion of multimodal information, resulting in more accurate diagnosis generation. Experimental results on real-world EHR datasets demonstrate the superiority of our generative model over comparative methods, providing explainable evidence to enhance the understanding of diagnosis results.}
}
@article{SAHADEVAN2025103141,
title = {Knowledge augmented generalizer specializer: A framework for early stage design exploration},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103141},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000345},
author = {Vijayalaxmi Sahadevan and Rohin Joshi and Kane Borg and Vishal Singh and Abhishek Raj Singh and Bilal Muhammed and Soban Babu Beemaraj and Amol Joshi},
abstract = {In non-routine engineering design projects, the design outcome is determined by how the problem is formulated and represented in the early conceptual stage. The problem representation comprises schemas, ontologies, variables, and parameters relevant to the given problem class. Despite the critical role of early conceptual decisions in shaping the eventual design outcome, most of the computational support and automation are focused on the latter stages of parametric modelling, problem-solving, and optimization. There is inadequate support for aiding and automating problem formulation, variable and parameter identification and representation, and early-stage conceptual decisions. Therefore, this paper presents an innovative, transparent, and explainable method employing semantic reasoning to automate the step-by-step conceptual design generation process, including problem formulation, identification and representation of the variables and parameters and their dependencies. The method is realized through a novel framework called Knowledge Augmented Generalizer Specializer (KAGS). KAGS employs the Function-Behavior-Structure (FBS) ontology and the Graph-of-Thought (GoT) mechanism to enable automated reasoning with a Large Language Model (LLM). The workflow comprises various stages: problem breakdown, design prototype creation, assessment, and prototype merging. The framework is implemented and tested on a Subsea Layout (SSL) planning problem, a special class of infrastructure planning projects in deep-sea oil and gas production systems. The experimentations with KAGS demonstrate its capacity to support problem formulation, hierarchical decomposition, and solution generation. The research also provides new insights into the FBS framework and meta-level reasoning in early design stages.}
}
@article{VIRVOU2024120759,
title = {VIRTSI: A novel trust dynamics model enhancing Artificial Intelligence collaboration with human users – Insights from a ChatGPT evaluation study},
journal = {Information Sciences},
volume = {675},
pages = {120759},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120759},
url = {https://www.sciencedirect.com/science/article/pii/S002002552400673X},
author = {Maria Virvou and George A. Tsihrintzis and Evangelia-Aikaterini Tsichrintzi},
keywords = {Artificial Intelligence, AI-Empowered Software, Autonomous Systems, AI Trust, Human-AI Interaction, Human-Centered Artificial Intelligence, User Modelling, Finite State Modelling, Confusion Matrix, AI in Education},
abstract = {The rapid integration of intelligent processes and methods into information systems in the Artificial Intelligence (AI) era has led to a substantial shift towards autonomous software decision-making. This evolution necessitates robust human oversight, especially in critical domains like Healthcare, Education, and Energy. Human trust in AI plays a vital role in influencing decision-making processes of users interacting with AI. This paper presents VIRTSI (Variability and Impact of Reciprocal Trust States towards Intelligent systems), a novel rigorous computational model for human-AI Interaction. VIRTSI simulates human trust states, spanning from overtrust to distrust, through user modelling. It comprises: 1. A trust dynamics representational model based on Deterministic Finite State Automata (DFAs), illustrating transitions among cognitive trust states in response to AI-generated replies. 2. A trust evaluation model based on Confusion Matrices, originating from machine learning and Accuracy Metrics, providing a quantitative framework for analysing human trust dynamics. As a result, this is the first time that trust dynamics have been thoroughly traced in a representational model and a method has been developed to assess the impact of possibly harmful states like overtrust and distrust. An empirical study on the recently launched Large Language Model of generative AI, ChatGPT (version 3.5), provides a radical underexplored AI-generated platform for evaluating the human-AI interaction through VIRTSI. The study involved 1200 interactions of real users as well as AI experts together with experts in two very different domains of evaluation, namely software engineering and poetry. This study traces trust dynamics and the emerging human-AI interaction, in concrete examples of real user synergies with generative AI. The research reveals the vital role of maintaining normal trust states for optimal human-AI interaction and that both AI and human users need further steps towards this goal. The real-world implications of this research can guide the creation and evaluation of user interfaces with AI and the incorporation of functionalities in the development of generative AI chatbots in terms of trust by providing a new rigorous DFA representational method of trust dynamics and a corresponding new perspective of confusion matrix evaluation method of the dynamics’ impact in the efficiency of human-AI dialogues.}
}
@article{DHANDA2025102937,
title = {Reviewing human-robot collaboration in manufacturing: Opportunities and challenges in the context of industry 5.0},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {93},
pages = {102937},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102937},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524002242},
author = {Mandeep Dhanda and Benedict Alexander Rogers and Stephanie Hall and Elies Dekoninck and Vimal Dhokia},
keywords = {Human Robot Collaboration, Digital Manufacturing, Industry 5.0, Artificial Intelligence, Ontology},
abstract = {Industry 4.0 (I4.0) has been characterized by the increasing use of automation, artificial intelligence, and big data in manufacturing. It has brought different machines, tools, robots and devices together through integration with cyber-physical systems as well as Internet of Things and computer systems. This has dramatically improved efficiency, productivity, and flexibility of automated systems, but it has also raised concerns about the impact of automation on jobs, the ethical considerations and the future of work in general. Industry 5.0 (I5.0) is the next manufacturing paradigm evolution and builds on I4.0 with the addition of ‘people’, in which robots will be designed to work alongside humans in a safe and efficient manner. Human-robot collaboration (HRC) is its key enabler. In manufacturing, HRC has the potential to improve safety, efficiency, and productivity by allowing humans to focus on tasks that require creativity, judgment, and flexibility, while robots perform more repetitive and dangerous tasks. This paper explores the concept of HRC and its advancement within 21st century industry. It identifies the opportunities and challenges arising from the interactions between robots and humans in manufacturing applications, assembly, and inspection. It also highlights the significance of HRC in I4.0 and its potential in I5.0. In addition, the role of artificial intelligence, machine learning, large language models, information modelling (ontologies) and new emerging digital technologies (augmented reality, virtual reality, digital twins, cyber-physical system) in the development of HRC and I5.0 is documented and discussed adding new perspectives to the growing literature in this area. This investigation sheds light on the emerging paradigms that have come about as parts of I5.0 and the transformative role of human-robot interaction in shaping the future of manufacturing. This critical review provides a realistic picture of manufacturing automation and the benefits and weaknesses of current HRC systems. It presents a researched view on the concept, needs, enabling technologies and system frameworks of human-robot interaction in manufacturing, providing a practical vision and research agenda for future work in this area and its associated systems.}
}
@article{LIU2024103768,
title = {Enhancing Chinese abbreviation prediction with LLM generation and contrastive evaluation},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103768},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103768},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001286},
author = {Jingping Liu and Xianyang Tian and Hanwen Tong and Chenhao Xie and Tong Ruan and Lin Cong and Baohua Wu and Haofen Wang},
keywords = {Chinese abbreviation prediction, LLM generation, Contrastive evaluation},
abstract = {Chinese abbreviation prediction plays an important role in natural language processing. The prevalent approach often utilizes generation models to predict abbreviations for full forms, but relying solely on a single generation model may not yield high-quality abbreviations. We emphasize the importance of introducing an evaluation model after the generation model to assess the rationality of generated abbreviations. Hence, in this paper, we propose a novel two-stage method with LLM generation and contrastive evaluation for Chinese abbreviation prediction. In the first stage, we design a type discriminator to determine the abbreviation type and then introduce a pre-trained and fine-tuned LLM to generate multiple candidate abbreviations. In the second stage, we propose a contrastive evaluation model to assess the rationality of the candidates based on the abbreviation scorer and phrase scorer with a joint learning strategy. Experiments on two public datasets indicate that our method outperforms the current state-of-the-art method, achieving improvements of 3.32% and 1.73%, respectively. More importantly, we deploy it on the Fliggy application and the 20-day online A/B testing shows a 0.65% increase in Point of Interest Recognition Rate and a 1.37% increase in Page View Click-Through Rate when using abbreviations predicted by our method in the search system.}
}
@article{DWIVEDI2024102725,
title = {Artificial intelligence (AI) futures: India-UK collaborations emerging from the 4th Royal Society Yusuf Hamied workshop},
journal = {International Journal of Information Management},
volume = {76},
pages = {102725},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102725},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223001068},
author = {Yogesh K. Dwivedi and Laurie Hughes and Harshad K.D.H. Bhadeshia and Sophia Ananiadou and Anthony G. Cohn and Jacqueline M. Cole and Gareth J. Conduit and Maunendra Sankar Desarkar and Xinwei Wang},
keywords = {Artificial intelligence, ChatGPT, Generative AI, Gen AI, Large language models, Technological disruption, uncertainties, Natural Language Processing},
abstract = {“Artificial Intelligence” in all its forms has emerged as a transformative technology that is in the process of reshaping many aspects of industry and wider society at a global level. It has evolved from a concept to a technology that is driving innovation, transforming productivity and disrupting existing business models across numerous sectors. The industrial and societal impact of AI is profound and multifaceted, offering opportunities for growth, efficiency, and improved healthcare, but also raising ethical and societal challenges as the method is integrated into many aspects of human life and work. This editorial is developed by contributors of the 4th Royal Society Yusef Hamied Workshop ( in 2023 devoted to Artificial Intelligence), designed to enhance collaboration between Indian and the UK scientists and to explore future research opportunities. The insights shared at the workshop are shared here.}
}
@article{ELCHAFEI2024229,
title = {Arabic NER Evaluation: Pre-Trained Models via Contrastive Learning vs. LLM Few-Shot Prompting},
journal = {Procedia Computer Science},
volume = {244},
pages = {229-237},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.196},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029971},
author = {Passant Elchafei and Amany Fashwan},
keywords = {Arabic NER, Named Entity Recognition, Contrastive Learning, BERT, LLM, Few-Shot, LLaMA, GPT3.5, Prompt Engineering},
abstract = {Developing Natural Language Processing (NLP) tools for the Arabic language and its dialects is very challenging. Named Entity Recognition (NER) is one of these challenges, which serves as the core component in many NLP systems such as information extraction, question answering, machine translation and knowledge graph building. This paper sheds light on applying diferent approaches for Arabic NER (Flat and Nested) using a large and rich Arabic NER corpus, Wojood dataset, which consists of about 550K tokens annotated with 21 entity types. First, we apply the Wojood base model, AraBERTv2, along with various other Arabic BERT models such as MARBERTv2, CaMelBert, mBert, ..etc. Next, we utilize the Bi-Encoder Contrastive Learning (CL) approach, a framework developed by Microsoft, which maps candidate text spans and entity types into the same vector representation space. The primary challenge in this approach is distinguishing non-entity spans from entity mentions. This approach could achieve F1 score 91.25% for Flat and 91.40% for Nested NER. Additionally, for evaluating the predicted NER, we employ Few-Shot prompting on LLaMA, and GPT-3.5 using refined prompt-based strategy. Our findings reveal that LLaMA outperforms GPT3.5.}
}
@article{LIVNE20248380,
title = {nach0: multimodal natural and chemical languages foundation model††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4sc00966e},
journal = {Chemical Science},
volume = {15},
number = {22},
pages = {8380-8389},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc00966e},
url = {https://www.sciencedirect.com/science/article/pii/S204165202400717X},
author = {Micha Livne and Zulfat Miftahutdinov and Elena Tutubalina and Maksim Kuznetsov and Daniil Polykovskiy and Annika Brundyn and Aastha Jhunjhunwala and Anthony Costa and Alex Aliper and Alán Aspuru-Guzik and Alex Zhavoronkov},
abstract = {Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder–decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.}
}
@article{LONGO2024102301,
title = {Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions},
journal = {Information Fusion},
volume = {106},
pages = {102301},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102301},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000794},
author = {Luca Longo and Mario Brcic and Federico Cabitza and Jaesik Choi and Roberto Confalonieri and Javier Del Ser and Riccardo Guidotti and Yoichi Hayashi and Francisco Herrera and Andreas Holzinger and Richard Jiang and Hassan Khosravi and Freddy Lecue and Gianclaudio Malgieri and Andrés Páez and Wojciech Samek and Johannes Schneider and Timo Speith and Simone Stumpf},
keywords = {Explainable artificial intelligence, XAI, Interpretability, Manifesto, Open challenges, Interdisciplinarity, Ethical AI, Large language models, Trustworthy AI, Responsible AI, Generative AI, Multi-faceted explanations, Concept-based explanations, Causality, Actionable XAI, Falsifiability},
abstract = {Understanding black box models has become paramount as systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper highlights the advancements in XAI and its application in real-world scenarios and addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. We aim to develop a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 28 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.}
}
@article{LU2024109451,
title = {Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy},
journal = {iScience},
volume = {27},
number = {4},
pages = {109451},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109451},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224006722},
author = {Muyu Lu and Fengyu Gao and Xiaolong Tang and Linjiang Chen},
keywords = {Natural sciences, Chemistry, Computer science},
abstract = {Summary
This study explores the use of large language models (LLMs) in interpreting and predicting experimental outcomes based on given experimental variables, leveraging the human-like reasoning and inference capabilities of LLMs, using selective catalytic reduction of NOx with NH3 as a case study. We implement the chain of thought (CoT) concept to formulate logical steps for uncovering connections within the data, introducing an “Ordered-and-Structured” CoT (OSCoT) prompting strategy. We compare the OSCoT strategy with the more conventional “One-Pot” CoT (OPCoT) approach and with human experts. We demonstrate that GPT-4, equipped with this new OSCoT prompting strategy, outperforms the other two settings and accurately predicts experimental outcomes and provides intuitive reasoning for its predictions.}
}
@article{BAO2025102616,
title = {Data-driven stock forecasting models based on neural networks: A review},
journal = {Information Fusion},
volume = {113},
pages = {102616},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102616},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003944},
author = {Wuzhida Bao and Yuting Cao and Yin Yang and Hangjun Che and Junjian Huang and Shiping Wen},
keywords = {Stock forecast, Finance, Financial market, Neural network, Deep learning},
abstract = {As a core branch of financial forecasting, stock forecasting plays a crucial role for financial analysts, investors, and policymakers in managing risks and optimizing investment strategies, significantly enhancing the efficiency and effectiveness of economic decision-making. With the rapid development of information technology and computer science, data-driven neural network technologies have increasingly become the mainstream method for stock forecasting. Although recent review studies have provided a basic introduction to deep learning methods, they still lack detailed discussion on network architecture design and innovative details. Additionally, the latest research on emerging large language models and neural network structures has yet to be included in existing review literature. In light of this, this paper comprehensively reviews the literature on data-driven neural networks in the field of stock forecasting from 2015 to 2023, discussing various classic and innovative neural network structures, including Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Transformers, Graph Neural Networks (GNNs), Generative Adversarial Networks (GANs), and Large Language Models (LLMs). It analyzes the application and achievements of these models in stock market forecasting. Moreover, the article also outlines the commonly used datasets and various evaluation metrics in the field of stock forecasting, further exploring unresolved issues and potential future research directions, aiming to provide clear guidance and reference for researchers in stock forecasting.}
}
@article{GONG2024108399,
title = {Advancing microbial production through artificial intelligence-aided biology},
journal = {Biotechnology Advances},
volume = {74},
pages = {108399},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108399},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024000934},
author = {Xinyu Gong and Jianli Zhang and Qi Gan and Yuxi Teng and Jixin Hou and Yanjun Lyu and Zhengliang Liu and Zihao Wu and Runpeng Dai and Yusong Zou and Xianqiao Wang and Dajiang Zhu and Hongtu Zhu and Tianming Liu and Yajun Yan},
keywords = {Microbial production, Synthetic biology, Genome annotation, Enzyme function prediction, Artificial protein design, Pathway prediction, Artificial intelligence (AI), Large language models (LLMs)},
abstract = {Microbial cell factories (MCFs) have been leveraged to construct sustainable platforms for value-added compound production. To optimize metabolism and reach optimal productivity, synthetic biology has developed various genetic devices to engineer microbial systems by gene editing, high-throughput protein engineering, and dynamic regulation. However, current synthetic biology methodologies still rely heavily on manual design, laborious testing, and exhaustive analysis. The emerging interdisciplinary field of artificial intelligence (AI) and biology has become pivotal in addressing the remaining challenges. AI-aided microbial production harnesses the power of processing, learning, and predicting vast amounts of biological data within seconds, providing outputs with high probability. With well-trained AI models, the conventional Design-Build-Test (DBT) cycle has been transformed into a multidimensional Design-Build-Test-Learn-Predict (DBTLP) workflow, leading to significantly improved operational efficiency and reduced labor consumption. Here, we comprehensively review the main components and recent advances in AI-aided microbial production, focusing on genome annotation, AI-aided protein engineering, artificial functional protein design, and AI-enabled pathway prediction. Finally, we discuss the challenges of integrating novel AI techniques into biology and propose the potential of large language models (LLMs) in advancing microbial production.}
}
@article{MUZANENHAMO2024102735,
title = {ChatGPT and accounting in African contexts: Amplifying epistemic injustice},
journal = {Critical Perspectives on Accounting},
volume = {99},
pages = {102735},
year = {2024},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2024.102735},
url = {https://www.sciencedirect.com/science/article/pii/S1045235424000340},
author = {Penelope Muzanenhamo and Sean Bradley Power},
keywords = {ChatGPT, Epistemic injustice, Large language models, Pluriversality, Accounting, Africa},
abstract = {Large Language Models (LLMs) such as ChatGPT are likely to amplify epistemic injustice through the lack of transparency and traceability of data sources. The unethical alienation of original knowledge producers from their intellectual products, which are repackaged by LLMs as artificial intelligence, conceals power asymmetries in the global knowledge production and dissemination system. As elaborated by Miranda Fricker (2010), Western White male actors traditionally dominate knowledge production; therefore, ChatGPT and other LLMs are inclined to reproduce patriarchal perspectives as universal understandings of the World. Our commentary applies this logic to accounting practice and research in Africa, and asserts that epistemic injustice, resulting from colonization and racism, means that ontological and epistemological approaches situated in the accounting needs and experiences of African communities are missing from or poorly articulated by ChatGPT and other LLMs. If LLMs are to attain legitimacy as (ethical) sources of knowledge, regulation must be enforced to ensure transparency—as a foundation for promoting pluriversality and eliminating epistemic injustice.}
}
@article{DENG2024103001,
title = {OphGLM: An ophthalmology large language-and-vision assistant},
journal = {Artificial Intelligence in Medicine},
volume = {157},
pages = {103001},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103001},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002434},
author = {Zhuo Deng and Weihao Gao and Chucheng Chen and Zhiyuan Niu and Zheng Gong and Ruiheng Zhang and Zhenjie Cao and Fang Li and Zhaoyi Ma and Wenbin Wei and Lan Ma},
keywords = {Ophthalmology, Visual dialogue interaction, Large language models},
abstract = {Vision computer-aided diagnostic methods have been used in early ophthalmic disease screening and diagnosis. However, the limited output formats of these methods lead to poor human–computer interaction and low clinical applicability value. Thus, ophthalmic visual question answering is worth studying. Unfortunately, no practical solutions exist before Large Language Models(LLMs). In this paper, we investigate the ophthalmic visual diagnostic interaction problem. We construct an ophthalmology large language-and-vision assistant, OphGLM, consisting of an image encoder, a text encoder, a fusion module, and an LLM module. We establish a new Chinese ophthalmic fine-tuning dataset, FundusTuning-CN, including the fundus instruction and conversation sets. Based on FundusTuning-CN, we establish a novel LLM-tuning strategy to introduce visual model understanding and ophthalmic knowledge into LLMs at a low cost and high efficiency. Leveraging the pre-training of the image encoder, OphGLM demonstrates strong visual understanding and surpasses open-source visual language models in common fundus disease classification tasks. The FundusTuning-CN enables OphGLM to surpass open-source medical LLMs in both ophthalmic knowledge and interactive capabilities. Our proposed OphGLM has the potential to revolutionize clinical applications in ophthalmology. The dataset, code, and models will be publicly available at https://github.com/ML-AILab/OphGLM.}
}
@article{KWON2024488,
title = {On knowing a gene: A distributional hypothesis of gene function},
journal = {Cell Systems},
volume = {15},
number = {6},
pages = {488-496},
year = {2024},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2024.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405471224001236},
author = {Jason J. Kwon and Joshua Pan and Guadalupe Gonzalez and William C. Hahn and Marinka Zitnik},
keywords = {lexical semantics, gene function, machine learning, artificial intelligence, distributed representations, word embeddings, large language models, transformers},
abstract = {Summary
As words can have multiple meanings that depend on sentence context, genes can have various functions that depend on the surrounding biological system. This pleiotropic nature of gene function is limited by ontologies, which annotate gene functions without considering biological contexts. We contend that the gene function problem in genetics may be informed by recent technological leaps in natural language processing, in which representations of word semantics can be automatically learned from diverse language contexts. In contrast to efforts to model semantics as “is-a” relationships in the 1990s, modern distributional semantics represents words as vectors in a learned semantic space and fuels current advances in transformer-based models such as large language models and generative pre-trained transformers. A similar shift in thinking of gene functions as distributions over cellular contexts may enable a similar breakthrough in data-driven learning from large biological datasets to inform gene function.}
}
@article{ZHANG20241050,
title = {Accelerating drug discovery, development, and clinical trials by artificial intelligence},
journal = {Med},
volume = {5},
number = {9},
pages = {1050-1070},
year = {2024},
issn = {2666-6340},
doi = {https://doi.org/10.1016/j.medj.2024.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S2666634024003088},
author = {Yilun Zhang and Mohamed Mastouri and Yang Zhang},
keywords = {artificial intelligence, deep learning, drug development, clinical trial, small molecule, antibody, RNA},
abstract = {Summary
Artificial intelligence (AI) has profoundly advanced the field of biomedical research, which also demonstrates transformative capacity for innovation in drug development. This paper aims to deliver a comprehensive analysis of the progress in AI-assisted drug development, particularly focusing on small molecules, RNA, and antibodies. Moreover, this paper elucidates the current integration of AI methodologies within the industrial drug development framework. This encompasses a detailed examination of the industry-standard drug development process, supplemented by a review of medications presently undergoing clinical trials. Conclusively, the paper tackles a predominant obstacle within the AI pharmaceutical sector: the absence of AI-conceived drugs receiving approval. This paper also advocates for the adoption of large language models and diffusion models as a viable strategy to surmount this challenge. This review not only underscores the significant potential of AI in drug discovery but also deliberates on the challenges and prospects within this dynamically progressing field.}
}
@article{DING2025128849,
title = {Adversarial contrastive representation training with external knowledge injection for zero-shot stance detection},
journal = {Neurocomputing},
volume = {614},
pages = {128849},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128849},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224016205},
author = {Yifan Ding and Ying Lei and Anqi Wang and Xiangrun Liu and Tuanfei Zhu and Yizhou Li},
keywords = {Natural language process, Stance detection, Contrastive learning, Zero-shot, Adversarial learning, Large language model},
abstract = {Zero-shot stance detection (ZSSD) is a task that involves identifying the author’s perspective on specific issues in text, particularly when the target topic has not been encountered during the model training process, to address rapidly evolving topics on social media. This paper introduces a ZSSD framework named KEL-CA. To enable the model to more effectively utilize transferable stance features for representing unseen targets, the framework incorporates a multi-layer contrastive learning and adversarial domain transfer module. Unlike traditional contrastive or adversarial learning, our framework captures both correlations and distinctions between invariant and specific features, as well as between different stance labels, and enhances the generalization ability and robustness of the features. Subsequently, to address the problem of insufficient information about the target context, we designed a dual external knowledge injection module that uses a large language model (LLM) to extract external knowledge from a Wikipedia-based local knowledge base and a Chain-of-Thought (COT) process to ensure the timeliness and relevance of the knowledge to infer the stances of unseen targets. Experimental results demonstrate that our approach outperforms existing models on two benchmark datasets, thereby validating its efficacy in ZSSD tasks.}
}
@article{CHATTERJEE2024570,
title = {Checking Counterfeit Critiques on Commodities using Ensemble Classifiers Enhancing Information Credibility},
journal = {Procedia Computer Science},
volume = {233},
pages = {570-579},
year = {2024},
note = {5th International Conference on Innovative Data Communication Technologies and Application (ICIDCA 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.246},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924006057},
author = {Ram Chatterjee and Mrinal Pandey and Hardeo Kumar Thakur and Anand Gupta},
keywords = {Information credibility, Large Language Models, Amazon Mechanical Turks, Elastic-net Classifier, LightGBM Trees Classifier},
abstract = {The conundrum of the ubiquitous deceptive reviews has overruled the online ontology with the obsession of obscure but obligatory posting of product reviews for the customers to believe, behold and beget the online product marketing. This mandates contemporary research in the direction to delve deeper on the application and analysis of deceiving online reviews with matured and advanced AI models functional on large scale datasets to effectively and efficiently demarcate between the genuine and the sham. The research counteracts the counterfeiting product reviews via the applications, assessment and analysis of the befitting AI models - Elastic-net Classifier model based on block coordinate descent with Wordcloud and its further performance enhancement through LightGBM Trees Classifier with Grid Search and Early Stopping support, with Log-Loss as performance metric for experimentation to gain insight into the intricacies of detection, diagnosis and diminution of fake product reviews. The paper also delineates discriminative and affirmative aspects of the dataset quality, statistics, stability and standards inherent and coherent to the creation of the dataset using Large Language Models (LLMs) intrinsic to the zeitgeist juncture of recent times promoting machines to produce large scale, cost effective bogus reviews in lieu of the Amazon Mechanical Turks. The results obtained with the Log-Loss holdout score of 0.1462 conforming the LightGBM classifier proves its performance better than the Elastic-Net classifier, conforming it as better than the ROC-AUC in terms of its proximity to the prediction probability for the matching actual/true value.}
}
@article{LI2025112913,
title = {Context-enhanced framework for medical image report generation using multimodal contexts},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112913},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112913},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124015478},
author = {Hongzhao Li and Hongyu Wang and Xia Sun and Hua He and Jun Feng},
keywords = {Medical image, Report generation, Multimodal, Context, Large language model},
abstract = {As deep learning technology continues to advance, including large language models and multimodal models, its application in the medical field has become a widely recognized research topic. In this context, a series of automated systems based on deep learning have been developed, aiming to generate corresponding text reports from medical images. However, these current methods often generate text reports solely based on patients’ images, overlooking the multimodal medical context, which encompasses various factors such as clinical information, diagnostic results, and medical knowledge. This limitation restricts the clinical application of automatically generated reports. To address this issue, we propose a novel Context-Enhanced Framework for medical image report generation. Our approach integrates various multimodal contextual elements, including but not limited to clinical text, medical knowledge, diagnostic results, and image data, to enrich the report generation process. We evaluated this framework on two public chest X-ray datasets, IU-Xray and MIMIC-CXR, using standard natural language generation and clinical effectiveness metrics. The results showed state-of-the-art performance, indicating improved quality in language and clinical accuracy. Our source code is available here.}
}
@article{YANG2024156116,
title = {PresRecRF: Herbal prescription recommendation via the representation fusion of large TCM semantics and molecular knowledge},
journal = {Phytomedicine},
volume = {135},
pages = {156116},
year = {2024},
issn = {0944-7113},
doi = {https://doi.org/10.1016/j.phymed.2024.156116},
url = {https://www.sciencedirect.com/science/article/pii/S0944711324007736},
author = {Kuo Yang and Xin Dong and Shuhan Zhang and Haibin Yu and Liqun Zhong and Lei Zhang and He Zhao and Yutong Hou and Xinpeng Song and Xuezhong Zhou},
keywords = {Herbal prescription recommendation, TCM semantics, Molecular knowledge, Herb dosage prediction, Feature fusion, Large language model},
abstract = {Background: Herbal prescription recommendation (HPR) is a hotspot in the research of clinical intelligent decision support. Recently plentiful HPR models based on deep neural networks have been proposed. Owing to insufficient data, e.g., lack of knowledge of molecular, TCM theory, and herbal dosage in HPR modeling, the existing models suffer from challenges, e.g., plain prediction precision, and are far from real-world clinics. Purpose: To address these problems, we proposed a novel herbal prescription recommendation model with the representation fusion of large TCM semantics and molecular knowledge (termed PresRecRF). Study Design and Methods: PresRecRF comprises three key modules. The representation learning module consists of two key components: a molecular knowledge representation component, integrating molecular knowledge into the herb-symptom-protein knowledge graph to enhance representations for herbs and symptoms; and a TCM knowledge representation component, leveraging BERT and ChatGPT to acquire TCM knowledge-enriched semantic representations. We introduced a representation fusion module to effectively merge molecular and TCM semantic representations. In the herb recommendation module, a multi-task objective loss is implemented to predict both herbs and dosages simultaneously. Results: The experimental results on two clinical datasets show that PresRecRF can achieve the optimal performance. Further analysis of ablation, hyper-parameters, and case studies indicate the effectiveness and reliability of the proposed model, suggesting that it can help precision medicine and treatment recommendations. Conclusion: The entire process of the proposed PresRecRF model closely mirrors the actual diagnosis and treatment procedures carried out by doctors, which are better applied in real clinical scenarios. The source codes of PresRecRF is available at https://github.com/2020MEAI/PresRecRF.}
}
@article{LI2024124760,
title = {mt4CrossOIE: Multi-stage tuning for cross-lingual open information extraction},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124760},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124760},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424016270},
author = {Tongliang Li and Zixiang Wang and Linzheng Chai and Jian Yang and Jiaqi Bai and Yuwei Yin and Jiaheng Liu and Hongcheng Guo and Liqun Yang and Hebboul {Zine el-abidine} and Zhoujun Li},
keywords = {Information extraction, Cross-lingual transfer, Disentangled training, Multi-lingual training, Mixture of LoRA},
abstract = {Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multi-stage tuning framework called mt4CrossOIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture of LoRAs for model-based cross-lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual raw data for data-based cross-lingual transfer. The model is trained with multi-lingual objectives on our proposed dataset OpenIE4++ by combining the model-based and data-based transfer techniques. Experimental results on various benchmarks emphasize the importance of aggregating multiple plug-in-and-play language-specific modules and demonstrate the effectiveness of mt4CrossOIE in cross-lingual OIE.22https://github.com/CSJianYang/Multilingual-Multimodal-NLP.}
}
@article{HOSEINI2024100819,
title = {A survey on semantic data management as intersection of ontology-based data access, semantic modeling and data lakes},
journal = {Journal of Web Semantics},
volume = {81},
pages = {100819},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100819},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000052},
author = {Sayed Hoseini and Johannes Theissen-Lipp and Christoph Quix},
keywords = {Semantic data management, Semantic web, Big data, Data lakes, Ontology-based data-access},
abstract = {In recent years, data lakes emerged as a way to manage large amounts of heterogeneous data for modern data analytics. One way to prevent data lakes from turning into inoperable data swamps is semantic data management. Such approaches propose the linkage of metadata to knowledge graphs based on the Linked Data principles to provide more meaning and semantics to the data in the lake. Such a semantic layer may be utilized not only for data management but also to tackle the problem of data integration from heterogeneous sources, in order to make data access more expressive and interoperable. In this survey, we review recent approaches with a specific focus on the application within data lake systems and scalability to Big Data. We classify the approaches into (i) basic semantic data management, (ii) semantic modeling approaches for enriching metadata in data lakes, and (iii) methods for ontology-based data access. In each category, we cover the main techniques and their background, and compare latest research. Finally, we point out challenges for future work in this research area, which needs a closer integration of Big Data and Semantic Web technologies.}
}
@article{QIAO2025104877,
title = {Food recommendation towards personalized wellbeing},
journal = {Trends in Food Science & Technology},
volume = {156},
pages = {104877},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.104877},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425000135},
author = {Guanhua Qiao and Dachuan Zhang and Nana Zhang and Xiaotao Shen and Xidong Jiao and Wenwei Lu and Daming Fan and Jianxin Zhao and Hao Zhang and Wei Chen and Jinlin Zhu},
keywords = {Food recommendation system, Machine learning, Personalized diet, Recommendation algorithm, Precision nutrition},
abstract = {Background
The intersection of nutrition and technology gave birth to the research of food recommendation system (FRS), which marked the transformation of traditional diet to a more personalized and healthy direction. The FRS uses advanced data analysis and machine learning technology to provide customized dietary advice according to users' personal preferences, and nutritional needs, which plays a vital role in promoting public health and reducing disease risks.
Scope and approach
This review presents the architecture of FRS and deeply discusses various recommendation algorithms, including the content-based method, collaborative filtering method, knowledge graph-based method, and hybrid methods. The review further introduces existing data resources and evaluation metrics, and highlights key technologies in user profiling and food analysis. In addition, the wide application of personalized FRS is summarized, and the importance of these systems in satisfying users' dietary preferences and maintaining balanced nutrition is emphasized. Finally, the key challenges and development trends of FRS are deeply analyzed from data level, model level and user experience level.
Key findings and conclusions
Personalized FRS shows great potential in helping users make healthier dietary decisions. Although there are still many challenges, such as dealing with heterogeneous data and interpretability. But with the progress of technology, there will be broader development in the future. For example, the powerful data processing ability of deep learning will effectively improve the accuracy of the system. In addition, the application of interactive recommendation system and large language model will also provide strong support for satisfying user experience and improving acceptance.}
}
@article{GALUSZA2024124990,
title = {Graph-based document-level relationship extraction for risk analysis: A transitive and dialog coherence approach},
journal = {Expert Systems with Applications},
volume = {257},
pages = {124990},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124990},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424018578},
author = {Michał Gałusza and Andrzej Walczak},
keywords = {Risk analysis, Relationship extraction, Graph representation, Language models, Knowledge acquisition, Semantic networks, Knowledge graphs, Entity recognition},
abstract = {This paper proposes a solution to extracting relationships at the document level in the context of risk analysis. It addresses the problem of identifying the flow of hazard’s impact in the system’s description or the description of the consequences of its failure. The problem is challenging, as information on impact may form complex, interwoven relations distributed across many sentences within a description and across many sources. The proposed approach involves a stepwise decomposition of the descriptions: first into a Semantic Frames Graph (SFG) to detect risk-relevant relationships, then into the Intermediate Relationship Graph (IRG), which is built upon detected relations, and finally, the aggregation of risk interaction represented in Asset-Vulnerability-Hazard (A-V-H) graph. This approach allows for the modeling of risk interactions without needing a dedicated training set, as the authors present a method for relationship detection based on the verbalization of transitive relationships and dialog consistency validated through prompt engineering over the generative language model. Overall, this research provides insights into a novel approach to acquiring risk interactions using document-level relationship extraction. It demonstrates its potential in graph-based representations and transitive relationships to understand complex risk interactions.}
}
@article{SHANG2025113017,
title = {From local to global: Leveraging document graph for named entity recognition},
journal = {Knowledge-Based Systems},
pages = {113017},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113017},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000656},
author = {Yu-Ming Shang and Hongli Mao and Tian Tian and Heyan Huang and Xian-Ling Mao},
keywords = {Named entity recognition, Document-level, Span graph, LLM},
abstract = {Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that aims to identify the span and category of entities within text. Recent advancements have demonstrated significant improvements in NER performance by incorporating document-level context. However, due to input length limitations, these models only consider the context of nearby sentences, failing to capture global long-range dependencies within the entire document. To address this issue, we propose a novel span-based two-stage method that formulates the document as a span graph, enabling the capture of global long-range dependencies at both token and span levels. Specifically, (1) we first train a binary classifier without considering entity types to extract candidate spans from each sentence. (2) Then, we leverage the robust contextual understanding and structural reasoning capabilities of Large Language Models (LLMs) like GPT to incrementally integrate these spans into the document-level span graph. By utilizing this span graph as a guide, we retrieve relevant contextual sentences for each target sentence and jointly encode them using BERT to capture token-level dependencies. Furthermore, by employing a Graph Transformer with well-designed position encoding to incorporate graph structure, our model effectively exploits span-level dependencies throughout the document. Extensive experiments on resource-rich nested and flat NER datasets, as well as low-resource distantly supervised NER datasets, demonstrate that our proposed model outperforms previous state-of-the-art models, showcasing its effectiveness in capturing long-range dependencies and enhancing NER accuracy.}
}
@article{ZHAO2025103951,
title = {ME3A: A Multimodal Entity Entailment framework for multimodal Entity Alignment},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103951},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103951},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003108},
author = {Yu Zhao and Ying Zhang and Xuhui Sui and Xiangrui Cai},
keywords = {Entity Alignment, Multimodal learning, Knowledge graph, Prompt learning},
abstract = {Current methods for multimodal entity alignment (MEA) primarily rely on entity representation learning, which undermines entity alignment performance because of cross-KG interaction deficiency and multimodal heterogeneity. In this paper, we propose a Multimodal Entity Entailment framework of multimodal Entity Alignment task, ME3A, and recast the MEA task as an entailment problem about entities in the two KGs. This way, the cross-KG modality information directly interacts with each other in the unified textual space. Specifically, we construct the multimodal information in the unified textual space as textual sequences: for relational and attribute modalities, we combine the neighbors and attribute values of entities as sentences; for visual modality, we map the entity image as trainable prefixes and insert them into sequences. Then, we input the concatenated sequences of two entities into the pre-trained language model (PLM) as an entailment reasoner to capture the unified fine-grained correlation pattern of the multimodal tokens between entities. Two types of entity aligners are proposed to model the bi-directional entailment probability as the entity similarity. Extensive experiments conducted on nine MEA datasets with various modality combination settings demonstrate that our ME3A effectively incorporates multimodal information and surpasses the performance of the state-of-the-art MEA methods by 16.5% at most.}
}
@article{QIU2024200308,
title = {ChatGPT and finetuned BERT: A comparative study for developing intelligent design support systems},
journal = {Intelligent Systems with Applications},
volume = {21},
pages = {200308},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200308},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323001333},
author = {Yunjian Qiu and Yan Jin},
keywords = {Language model, Knowledge transferring, Knowledge elicitation, Text classification, Text generation},
abstract = {Large Language Models (LLMs), like ChatGPT, have sparked considerable interest among researchers across diverse disciplines owing to their remarkable text processing and generation capabilities. While ChatGPT is typically employed for tasks involving general knowledge, researchers increasingly explore the potential of this LLM-based tool in specific domains to enhance productivity. This study aims to compare the performance of a finetuned BERT model with that of ChatGPT on a domain-specific dataset in the context of developing an intelligent design support system. Through experiments conducted on classification and generation tasks, the knowledge transfer and elicitation abilities of ChatGPT are examined and contrasted with those of the finetuned BERT model. The findings indicate that ChatGPT exhibits comparable performance to the finetuned BERT model in sentence-level classification tasks but struggles with short sequences. However, ChatGPT's classification performance significantly improves when a few-shot setting is applied. Moreover, it can filter out unrelated data and enhance dataset quality by assimilating the underlying domain knowledge. Regarding content generation, ChatGPT with a zero-shot setting produces informative and readable output for domain-specific questions, albeit with an excessive amount of unrelated information, which can burden readers. In conclusion, ChatGPT demonstrates a promising potential for application in facilitating data labeling, knowledge transfer, and knowledge elicitation tasks. With minimal guidance, ChatGPT can substantially enhance the efficiency of domain experts in accomplishing their objectives. The findings suggest a nuanced integration of artificial intelligence (AI) with human expertise, bridging the gap from mere classification models to sophisticated human-analogous text generation systems. This signals a future in AI-augmented engineering design where the robust capabilities of AI technologies integrate with human creativity and innovation, creating a dynamic interactions to redefine how we tackle design challenges.}
}
@article{ARSLAN20244534,
title = {Exploring Business Events using Multi-source RAG},
journal = {Procedia Computer Science},
volume = {246},
pages = {4534-4540},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023226},
author = {Muhammad Arslan and Saba Munawar and Christophe Cruz},
keywords = {Business events, Extraction methods, Large Language Models, Retrieval-Augmented Generation, Dynamic business environments},
abstract = {Business events signify crucial activities within a company, indicating growth opportunities and investment prospects. They encompass various developments such as recruitment drives, market expansions, mergers, and product launches. Understanding these events is vital for businesses seeking to stay updated with market dynamics, as they provide real-time insights into a company’s trajectory. Moreover, comprehending the business events of one company can offer strategic advantages to others, facilitating informed decision-making and fostering collaboration within the business ecosystem. Extracting information about these events involves diverse structured, semi-structured, and unstructured data sources, posing challenges for traditional extraction methods. Despite the promise shown by existing openly available LLMs driven by Generative Artificial Intelligence (GenAI), they face challenges when dealing with domain-specific queries. Retrieval-Augmented Generation (RAG) addresses this challenge by seamlessly integrating multiple external data sources of varying structures. In our study, we demonstrate how RAG with LLM facilitates precise extraction of business events, ensuring adaptability in dynamic business environments where datasets are constantly evolving.}
}
@article{DOO2023877,
title = {Exploring the Clinical Translation of Generative Models Like ChatGPT: Promise and Pitfalls in Radiology, From Patients to Population Health},
journal = {Journal of the American College of Radiology},
volume = {20},
number = {9},
pages = {877-885},
year = {2023},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2023.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1546144023005161},
author = {Florence X. Doo and Tessa S. Cook and Eliot L. Siegel and Anupam Joshi and Vishwa Parekh and Ameena Elahi and Paul H. Yi},
keywords = {generative artificial intelligence, radiology, limitations, large language models, ChatGPT},
abstract = {Generative artificial intelligence (AI) tools such as GPT-4, and the chatbot interface ChatGPT, show promise for a variety of applications in radiology and health care. However, like other AI tools, ChatGPT has limitations and potential pitfalls that must be considered before adopting it for teaching, clinical practice, and beyond. We summarize five major emerging use cases for ChatGPT and generative AI in radiology across the levels of increasing data complexity, along with pitfalls associated with each. As the use of AI in health care continues to grow, it is crucial for radiologists (and all physicians) to stay informed and ensure the safe translation of these new technologies.}
}
@article{CHIANG2024110918,
title = {Customized GPT model largely increases surgery decision accuracy for pharmaco-resistant epilepsy},
journal = {Journal of Clinical Neuroscience},
volume = {130},
pages = {110918},
year = {2024},
issn = {0967-5868},
doi = {https://doi.org/10.1016/j.jocn.2024.110918},
url = {https://www.sciencedirect.com/science/article/pii/S0967586824004570},
author = {Kuo-Liang Chiang and Yu-Cheng Chou and Hsin Tung and Chin-Yin Huang and Liang-Po Hsieh and Kai-Ping Chang and Shang-Yeong Kwan and Wan-Yu Huang},
keywords = {Semiology, Seizure descriptors, Localization, Generative pre-trained transformer, Large-scale language model},
abstract = {Background
To develop an enhanced epilepsy diagnosis system by integrating an expert-informed ontology with a custom generative pre-trained transformer (GPT), validated by inferring possible seizure lateralization and localization using retrospective textual data from the pre-surgical assessments of patients with pharmaco-resistant epilepsy (PRE).
Methods
We developed an AI system for epilepsy diagnosis using Protégé with OWL/SWRL, integrating a knowledge base with seizure semiology, seizure types EEG descriptors, expert insights, and literature to pinpoint seizure locations. A customized GPT model was then tailored for specific diagnostic needs. Validated through 16 surgical cases, the system’s accuracy in seizure localization and the JSON (JavaScript Object Notation) Epilepsy Matcher’s term matching capabilities were confirmed against a Protégé-based knowledge base.
Results
A total of 117 patients with PRE underwent video-EEG monitoring at a single institution. However, only 16 of these patients received epilepsy surgery. The Protégé system achieved 75 % accuracy in diagnosing epilepsy from 16 cases using semiology, which increased to 87.5 % with EEG data. The Json Epilepsy Matcher further improved accuracy to 87.5 % with symptoms alone and 93.8 % when including EEG data, highlighting the benefits of applying GPT techniques.
Conclusions
This study highlights the efficacy of the JSON Epilepsy Matcher in improving seizure diagnosis accuracy. When combined with EEG data, it achieves a 93.8 % accuracy rate, suggesting a potential improvement in the practicality and generalizability of the original ontology expert system, boosting physicians’ confidence in confirming surgery and potentially sparing many children from prolonged suffering. This innovative approach not only improves diagnostic accuracy but also sets a precedent for future applications of AI in neurology.}
}
@article{PAN2024111224,
title = {A semantic augmented approach to FEMA P-58 based dynamic regional seismic loss estimation application},
journal = {Journal of Building Engineering},
volume = {98},
pages = {111224},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.111224},
url = {https://www.sciencedirect.com/science/article/pii/S235271022402792X},
author = {Zeyu Pan and Jianyong Shi and Liu Jiang},
keywords = {Dynamic regional seismic loss estimation, Mainshock-aftershock sequence, FEMA P-58, Semantic web, Large language model},
abstract = {Regional seismic loss estimation (RSLE) is a crucial process in both immediate post-earthquake emergency response and long-term reconstruction endeavors. Over the years, significant progress has been made in RSLE: sensing approaches such as field investigation and remote sensing offers a comprehensive overview necessary for real-time disaster response, while simulation techniques such as the methodology proposed in Federal Emergency Management Agency (FEMA) P-58 series provide insights into the mechanism of disaster development and its potential long-term impacts on urban assets. Nonetheless, challenges persist in the realm of practical RSLE applications. Firstly, a dynamic understanding of a disaster event and its influence is deemed important for effective emergency responses while challenging to achieve in current approaches. Secondly, stakeholders with varying roles, including administrators, rescue teams and ordinary citizens have distinct information requirements for RSLE. Thirdly, the complexity of seismic loss estimation, involving diverse data sources such as building information models, performance models, fragility data and sensor observations, poses interoperability issue. To tackle these issues, this article introduces a dynamic, multi-granularity, ontological representation scheme tailored for RSLE decision-supporting systems. This scheme operates across various scales, from individual building components to broader regional scales by synergistically employing FEMA P-58 guidelines and Semantic Web technologies. Upon the corresponding semantics, a question-and-answer agent powered by large language model is further developed to facilitate interaction requirements within the RSLE process via FEMA P-58 pipeline. The practical efficacy of this approach is validated through a prototype deployed under a real earthquake event, demonstrating its value in real-world scenarios.}
}
@article{LI2024e04051,
title = {Classification and application of deep learning in construction engineering and management – A systematic literature review and future innovations},
journal = {Case Studies in Construction Materials},
volume = {21},
pages = {e04051},
year = {2024},
issn = {2214-5095},
doi = {https://doi.org/10.1016/j.cscm.2024.e04051},
url = {https://www.sciencedirect.com/science/article/pii/S2214509524012038},
author = {Qingze Li and Yang Yang and Gang Yao and Fujia Wei and Rui Li and Mingtao Zhu and Huiwen Hou},
keywords = {Deep learning(DL), Construction engineering management(CEM), Condition monitoring, Damage detection, Large language models (LLM)},
abstract = {In the ever-evolving landscape of construction engineering and management (CEM), the dynamic and unique characteristics of construction project environments constantly present multifaceted challenges. These challenges are characterized by the extensive volume of project-specific information and intricate engineering data. Deep learning (DL), with its advanced analytical capabilities, has been emerging as a robust solution to these complexities. While the application of DL in CEM is on an upward trajectory, a systematic review of its implementation is conspicuously lacking. This paper, therefore, embarks on a scientometric and qualitative analysis of 296 DL-based studies related to CEM from 2014 to 2024 in the renowned data science repositories Scopus, Science Direct and Web of Science to explore the characteristics of journals, keywords and clusters. It is found that six research topics have fully utilized the advantages of DL in CEM in the last decade, including construction equipment management, structural health monitoring, construction site safety management, construction schedule management, worker health management and workforce assessment and intelligent design. Then, the studies under each research topic are summarized separately and a searchable taxonomy is proposed that secondarily categorizes each study according to the specific CEM task and DL method used to facilitate understanding and access. Finally, the primary obstacles encountered in DL itself and in its practical application in CEM are discussed. It further articulates five critical future research directions that are evolving in tandem with advances in CEM, multimodal construction site management, real-time structural health monitoring and prediction, project progress visualization and management, intelligent design with data sharing and the incorporating large language models (LLM) for text data analysis. The three goals of this study are providing CEM researchers and practitioners with an in-depth and nuanced understanding of DL, elucidating the diverse nature of CEM activities and the resulting benefits of applying DL, and identifying future opportunities for applying DL in CEM to inform subsequent ongoing academic inquiry and pragmatic applications.}
}
@article{MANN2023108446,
title = {SUSIE: Pharmaceutical CMC ontology-based information extraction for drug development using machine learning},
journal = {Computers & Chemical Engineering},
volume = {179},
pages = {108446},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108446},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003162},
author = {Vipul Mann and Shekhar Viswanath and Shankar Vaidyaraman and Jeya Balakrishnan and Venkat Venkatasubramanian},
keywords = {Ontology, Pharmaceutical drug development, Information extraction, Hybrid machine learning, Chemistry manufacturing and control},
abstract = {Automatically extracting information from unstructured text in pharmaceutical documents is important for drug discovery and development. This information can be integrated with structured datasets to ultimately accelerate pharmaceutical product development. To this end, we report an end-to-end information extraction framework based on a custom-built pharmaceutical drug development ontology, a weak supervision framework, contextualization algorithms, and a fine-tuned BioBERT model (adaptation of BERT or Bidirectional Encoder Representations from Transformers for biomedical text). The proposed framework, SUSIE (Schema-based Unsupervised Semantic Information Extraction), was trained on ICH (International Conference on Harmonization) documents to identify important entities and relations from unstructured text and auto-generate knowledge graphs representing crucial information in a structured format. On the entity identification task, the framework achieves a test accuracy and F1-score of 96% and 88%, respectively, on out-of-sample documents. A major contribution of this work is to build an automated, unsupervised information extraction framework around a domain-specific, custom-built pharmaceutical drug development ontology without the need for manual curation of training datasets for specific tasks. The efficacy of the approach was tested on out-of-sample documents including an internal Eli Lilly technical document.}
}
@article{DASILVEIRA2024102286,
title = {A knowledge-sharing platform for space resources},
journal = {Data & Knowledge Engineering},
volume = {151},
pages = {102286},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102286},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000107},
author = {Marcos {Da Silveira} and Louis Deladiennee and Emmanuel Scolan and Cedric Pruski},
keywords = {Knowledge engineering, Knowledge graph, Ontology, Space resources},
abstract = {The ever-increasing interest of academia, industry, and government institutions in space resource information highlights the difficulty of finding, accessing, integrating, and reusing this information. Although information is regularly published on the internet, it is disseminated on many different websites and in different formats, including scientific publications, patents, news, and reports. We are currently developing a knowledge management and sharing platform for space resources. This tool, which relies on the combined use of knowledge graphs and ontologies, formalises the domain knowledge contained in the above-mentioned documents and makes it more readily available to the community. In this article, we describe the concepts and techniques of knowledge extraction and management adopted during the design and implementation of the platform.}
}
@article{GUILLAUMET2024131,
title = {The power of generative AI for CRIS systems: a new paradigm for scientific information management},
journal = {Procedia Computer Science},
volume = {249},
pages = {131-149},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.057},
url = {https://www.sciencedirect.com/science/article/pii/S187705092403268X},
author = {Anna Guillaumet},
keywords = {CRIS, AI, GenerativeAI, euroCRIS, FECYT, DRIS, OpenAccess, Research, CERIF, FAIR, AI-Act, ENIA, Sandbox, Law, Regulations, Standards, ethics},
abstract = {The paper analyses the implications of the emergence of artificial intelligence (AI), especially generative AI, on current research information systems (CRIS). It reviews the recent European regulations for high-risk AI systems, the Spanish AI strategy, and the IntelComp project as use cases. The study found that the maturity of CRIS systems, coupled with the increasing complexity due to data aggregation, sets the stage for innovative AI applications. The paper proposes key domains where AI can impact and be applied in CRIS, including data management, research assessment, and advanced analytics. It also provides examples of how generative AI can be leveraged to enhance scientific information management within CRIS. The findings highlight the need to ensure the responsible and ethical development of AI technologies in the research domain.}
}
@article{ZOU2025102606,
title = {Deep learning for cross-domain data fusion in urban computing: Taxonomy, advances, and outlook},
journal = {Information Fusion},
volume = {113},
pages = {102606},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102606},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003841},
author = {Xingchen Zou and Yibo Yan and Xixuan Hao and Yuehong Hu and Haomin Wen and Erdong Liu and Junbo Zhang and Yong Li and Tianrui Li and Yu Zheng and Yuxuan Liang},
keywords = {Urban computing, Data fusion, Deep learning, Multi-modal data, Large language models, Sustainable development},
abstract = {As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.}
}
@article{TING2025100893,
title = {AstroMLab 1: Who wins astronomy jeopardy!?},
journal = {Astronomy and Computing},
volume = {51},
pages = {100893},
year = {2025},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2024.100893},
url = {https://www.sciencedirect.com/science/article/pii/S2213133724001082},
author = {Y.-S. Ting and T.D. Nguyen and T. Ghosal and R. Pan and H. Arora and Z. Sun and T. {de Haan} and N. Ramachandra and A. Wells and S. Madireddy and A. Accomazzi},
keywords = {Large Language Models, Astronomy, Benchmarking, Question Answering, Scientific Knowledge Assessment},
abstract = {We present a comprehensive evaluation of proprietary and open-weights large language models using the first astronomy-specific benchmarking dataset. This dataset comprises 4,425 multiple-choice questions curated from the Annual Review of Astronomy and Astrophysics, covering a broad range of astrophysical topics.11Jeopardy is a popular American quiz show where contestants are tested on their knowledge across various subjects. Other similar shows include Who’s Still Standing () in China and University Challenge in the UK, among others. Our analysis examines model performance across various astronomical subfields and assesses response calibration, crucial for potential deployment in research environments. Claude-3.5-Sonnet outperforms competitors by up to 4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we observed a universal reduction in cost every 3-to-12 months to achieve similar score in this particular astronomy benchmark. open-weights models have rapidly improved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with some of the best proprietary models. We identify performance variations across topics, with non-English-focused models generally struggling more in exoplanet-related fields, stellar astrophysics, and instrumentation related questions. These challenges likely stem from less abundant training data, limited historical context, and rapid recent developments in these areas. This pattern is observed across both open-weights and proprietary models, with regional dependencies evident, highlighting the impact of training data diversity on model performance in specialized scientific domains. Top-performing models demonstrate well-calibrated confidence, with correlations above 0.9 between confidence and correctness, though they tend to be slightly underconfident. The development for fast, low-cost inference of open-weights models presents new opportunities for affordable deployment in astronomy. The rapid progress observed suggests that LLM-driven research in astronomy may become feasible in the near future.}
}
@article{BERTHON2024461,
title = {Trajectories of AI technologies: Insights for managers},
journal = {Business Horizons},
volume = {67},
number = {5},
pages = {461-470},
year = {2024},
note = {SPECIAL ISSUE: WRITTEN BY CHATGPT},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2024.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007681324000284},
author = {Pierre Berthon and Taylan Yalcin and Ekin Pehlivan and Tamara Rabinovich},
keywords = {Trajectories of technology, Generative AI, Chatbots, ChatGPT, Large language models, Social media},
abstract = {Generative artificial intelligence (GenAI) has long been considered a technology for the future. With the release of the chatbot ChatGPT 4, many now feel the future has arrived. Long in gestation, this new technology promises many benefits to humankind, but worries persist that as AI technology scales and comes to rival or exceed human intelligence, the servant may become the master. Amid such hyperbole, the more nuanced trajectories of this technology have been neglected. In this article, we use the Trajectories of Technology (ToT) framework developed by Berthon and colleagues to explore the disparate paths that AI has taken and will take in the coming years, especially in the form of chatbots. This framework provides managers with a conceptual tool to strategically plan for the enormous promises and perils of AI in general and of chatbots specifically.}
}
@article{KPODO2024109349,
title = {AgXQA: A benchmark for advanced Agricultural Extension question answering},
journal = {Computers and Electronics in Agriculture},
volume = {225},
pages = {109349},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109349},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924007403},
author = {Josué Kpodo and Parisa Kordjamshidi and A. Pouyan Nejadhashemi},
keywords = {Agricultural Extension, Question-Answering, Annotated Dataset, Large Language Models, Zero-Shot Learning},
abstract = {Large language models (LLMs) have revolutionized various scientific fields in the past few years, thanks to their generative and extractive abilities. However, their applications in the Agricultural Extension (AE) domain remain sparse and limited due to the unique challenges of unstructured agricultural data. Furthermore, mainstream LLMs excel at general and open-ended tasks but struggle with domain-specific tasks. We proposed a novel QA benchmark dataset, AgXQA, for the AE domain to address these issues. We trained and evaluated our domain-specific LM, AgRoBERTa, which outperformed other mainstream encoder- and decoder- LMs, on the extractive QA downstream task by achieving an EM score of 55.15% and an F1 score of 78.89%. Besides automated metrics, we also introduced a custom human evaluation metric, AgEES, which confirmed AgRoBERTa’s performance, as demonstrated by a 94.37% agreement rate with expert assessments, compared to 92.62% for GPT 3.5. Notably, we conducted a comprehensive qualitative analysis, whose results provide further insights into the weaknesses and strengths of both domain-specific and general LMs when evaluated on in-domain NLP tasks. Thanks to this novel dataset and specialized LM, our research enhanced further development of specialized LMs for the agriculture domain as a whole and AE in particular, thus fostering sustainable agricultural practices through improved extractive question answering.}
}
@article{SIVARAJKUMAR2024,
title = {Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/52289},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000322},
author = {Sonish Sivarajkumar and Fengyi Gao and Parker Denny and Bayan Aldhahwani and Shyam Visweswaran and Allyn Bove and Yanshan Wang},
keywords = {natural language processing, electronic health records, rehabilitation, physical exercise, ChatGPT, artificial intelligence, stroke, physical rehabilitation, rehabilitation therapy, exercise, machine learning},
abstract = {Background
The rehabilitation of a patient who had a stroke requires precise, personalized treatment plans. Natural language processing (NLP) offers the potential to extract valuable exercise information from clinical notes, aiding in the development of more effective rehabilitation strategies.
Objective
This study aims to develop and evaluate a variety of NLP algorithms to extract and categorize physical rehabilitation exercise information from the clinical notes of patients who had a stroke treated at the University of Pittsburgh Medical Center.
Methods
A cohort of 13,605 patients diagnosed with stroke was identified, and their clinical notes containing rehabilitation therapy notes were retrieved. A comprehensive clinical ontology was created to represent various aspects of physical rehabilitation exercises. State-of-the-art NLP algorithms were then developed and compared, including rule-based, machine learning–based algorithms (support vector machine, logistic regression, gradient boosting, and AdaBoost) and large language model (LLM)–based algorithms (ChatGPT [OpenAI]). The study focused on key performance metrics, particularly F1-scores, to evaluate algorithm effectiveness.
Results
The analysis was conducted on a data set comprising 23,724 notes with detailed demographic and clinical characteristics. The rule-based NLP algorithm demonstrated superior performance in most areas, particularly in detecting the “Right Side” location with an F1-score of 0.975, outperforming gradient boosting by 0.063. Gradient boosting excelled in “Lower Extremity” location detection (F1-score: 0.978), surpassing rule-based NLP by 0.023. It also showed notable performance in the “Passive Range of Motion” detection with an F1-score of 0.970, a 0.032 improvement over rule-based NLP. The rule-based algorithm efficiently handled “Duration,” “Sets,” and “Reps” with F1-scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot prompts, achieved high recall but generally lower precision and F1-scores. However, it notably excelled in “Backward Plane” motion detection, achieving an F1-score of 0.846, surpassing the rule-based algorithm’s 0.720.
Conclusions
The study successfully developed and evaluated multiple NLP algorithms, revealing the strengths and weaknesses of each in extracting physical rehabilitation exercise information from clinical notes. The detailed ontology and the robust performance of the rule-based and gradient boosting algorithms demonstrate significant potential for enhancing precision rehabilitation. These findings contribute to the ongoing efforts to integrate advanced NLP techniques into health care, moving toward predictive models that can recommend personalized rehabilitation treatments for optimal patient outcomes.}
}
@article{HUANG2024362,
title = {From explainable to interpretable deep learning for natural language processing in healthcare: How far from reality?},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {362-373},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001508},
author = {Guangming Huang and Yingya Li and Shoaib Jameel and Yunfei Long and Giorgos Papanastasiou},
keywords = {Explainable, Interpretable, Deep learning, NLP, Healthcare},
abstract = {Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term “eXplainable and Interpretable Artificial Intelligence” (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore “global” modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare.}
}
@article{WU2025113057,
title = {CoT-driven framework for short text classification: Enhancing and transferring capabilities from large to smaller model},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113057},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113057},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001042},
author = {Hui Wu and Yuanben Zhang and Zhonghe Han and Yingyan Hou and Lei Wang and Siye Liu and Qihang Gong and Yunping Ge},
keywords = {Short text classification, Large language models, Chain-of-thought},
abstract = {Short Text Classification (STC) is crucial for processing and understanding the brief but substantial content prevalent on contemporary digital platforms. The STC encounters difficulties in grasping the semantic and syntactic intricacies, an issue that is apparent in traditional pre-trained language models. Although Graph Convolutional Networks enhance performance by integrating external knowledge bases, these methods are limited by the quality and extent of the knowledge applied. Recently, the emergence of Large Language Models (LLMs) and Chain-of-Thought (CoT) has significantly improved the performance of complex reasoning tasks. However, some studies have highlighted the limitations of their application in fundamental NLP tasks. Consequently, this study first employs CoT to investigate and enhance the capabilities of LLMs in STC tasks. We propose the Syntactic and Semantic Enrichment CoT (SSE-CoT) method, effectively decomposing the STC tasks into four distinct steps: (i) essential concept identification, (ii) common-sense knowledge retrieval, (iii) text rewriting, and (iv) classification. Furthermore, recognizing resource constraints in sectors like finance and healthcare, we then introduce the CoT-Driven Multi-Task Learning (CDMT) framework to extend these capabilities to smaller models. This framework begins by extracting rationales from LLMs and subsequently fine-tunes smaller models to optimize their performance. Extensive experimentation across six short-text benchmarks validated the efficacy of the proposed methods. In particular, SSE-CoT achieved state-of-the-art performance with substantial improvements on all datasets, particularly on the Ohsumed and TagMyNews datasets.}
}
@article{KERRE2025111255,
title = {An instruction dataset for extracting quantum cascade laser properties from scientific text},
journal = {Data in Brief},
volume = {58},
pages = {111255},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.111255},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924012174},
author = {Deperias Kerre and Anne Laurent and Kenneth Maussang and Dickson Owuor},
keywords = {Information extraction, Large language models, Machine learning, Quantum cascade lasers},
abstract = {Quantum Cascade Lasers (QCL) are promising semiconductor lasers, compact and powerful, but of complex design. Availability of structured data of the QCL properties can support data mining activities that seek to understand the relationship between these properties, for instance between the design and performance features. The main open source of QCL data is in scientific text which in most cases is usually unstructured. One of the ways to extract and organize this data is by utilizing Information Extraction techniques. These techniques can accelerate the process of curating QCL properties data from scientific articles for further analysis. One of the main challenges in developing machine learning algorithms for extraction of QCL properties from text is lack of quality training data for these algorithms. Large Language Models (LLMs) have demonstrated great capabilities in materials property extraction from text. They however experience challenges with domain specific properties, for instance the heterostructure and design types in the QCL domain hence for adaptation. In this paper, we present an original instruction dataset for training and evaluation of LLMs for QCL properties extraction from text. The data is generated by augmenting sample sentences from scientific articles with GPT-3.5 instruct with a few shot strategy. The dataset then is manually annotated with the help of QCL experts and is composed of 1300 rows of training examples consisting of an Instruction, Input Text and the Output.}
}
@article{SIVAKUMAR2024124653,
title = {Prompting GPT–4 to support automatic safety case generation},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124653},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124653},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424015203},
author = {Mithila Sivakumar and Alvine B. Belle and Jinjun Shan and Kimya {Khakzad Shahandashti}},
keywords = {Safety cases, Safety assurance, Machine learning, Large language models, Generative AI, Requirements engineering},
abstract = {In the ever-evolving field of software engineering, the advent of large language models and conversational interfaces, exemplified by ChatGPT, represents a significant revolution. While their potential is evident in various domains, this paper expands upon our previous research, where we experimented with GPT–4, on its ability to create safety cases. A safety case is a structured argument supported by a body of evidence to demonstrate that a given system is safe to operate in a given environment. In this paper, we first determine GPT–4’s comprehension of the Goal Structuring Notation (GSN), a well-established notation for visually representing safety cases. Additionally, we conduct four distinct experiments using GPT–4 to evaluate its ability to generate safety cases within a specified system and application domain. To assess GPT–4’s performance in this context, we compare the results it produces with the ground-truth safety cases developed for an X-ray system, a machine learning-enabled component for tire noise recognition in a vehicle, and a lane management system from the automotive domain. This comparison enables us to gain valuable insights into the model’s generative capabilities. Our findings indicate that GPT–4 is able to generate moderately accurate and reasonable safety cases.}
}
@article{MORALESSANCHEZ2024108830,
title = {Early diagnosis of HIV cases by means of text mining and machine learning models on clinical notes},
journal = {Computers in Biology and Medicine},
volume = {179},
pages = {108830},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108830},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524009156},
author = {Rodrigo Morales-Sánchez and Soto Montalvo and Adrián Riaño and Raquel Martínez and María Velasco},
keywords = {HIV, Text mining, Automated screening, Electronic Health Records (EHRs), Large Language Models (LLMs), Machine Learning (ML)},
abstract = {Undiagnosed and untreated human immunodeficiency virus (HIV) infection increases morbidity in the HIV-positive person and allows onward transmission of the virus. Minimizing missed opportunities for HIV diagnosis when a patient visits a healthcare facility is essential in restraining the epidemic and working toward its eventual elimination. Most state-of-the-art proposals employ machine learning (ML) methods and structured data to enhance HIV diagnoses, however, there is a dearth of recent proposals utilizing unstructured textual data from Electronic Health Records (EHRs). In this work, we propose to use only the unstructured text of the clinical notes as evidence for the classification of patients as suspected or not suspected. For this purpose, we first compile a dataset of real clinical notes from a hospital with patients classified as suspects and non-suspects of having HIV. Then, we evaluate the effectiveness of two types of classification models to identify patients suspected of being infected with the virus: classical ML algorithms and two Large Language Models (LLMs) from the biomedical domain in Spanish. The results show that both LLMs outperform classical ML algorithms in the two settings we explore: one dataset version is balanced, containing an equal number of suspicious and non-suspicious patients, while the other reflects the real distribution of patients in the hospital, being unbalanced. We obtain F1 score figures of 94.7 with both LLMs in the unbalanced setting, while in the balance one, RoBERTaBio model outperforms the other one with a F1 score of 95.7. The findings indicate that leveraging unstructured text with LLMs in the biomedical domain yields promising outcomes in diminishing missed opportunities for HIV diagnosis. A tool based on our system could assist a doctor in deciding whether a patient in consultation should undergo a serological test.}
}
@article{HUANG2025104033,
title = {A knowledge-enhanced network for joint multimodal entity-relation extraction},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104033},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104033},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003923},
author = {Shubin Huang and Yi Cai and Li Yuan and Jiexin Wang},
keywords = {Joint multimodal entity-relation extraction, Knowledge graphs, Large language models},
abstract = {In the domain of multimodal analysis, Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) are two pivotal tasks aiming at identifying named entities and their relations by leveraging integrated information from text–image pairs. Recently, Joint Multimodal Entity-Relation Extraction (JMERE) has emerged as a unified task to combine MNER and MRE, exploiting the bidirectional interactions between the two tasks for enhanced performance. However, existing JMERE studies primarily focus on improving visual information utilization through feature alignment, often falling short when social media texts and accompanying images lack sufficient information, leading to inaccuracies in entity type and relation recognition. To tackle this challenge, we propose KEJME, a knowledge-enhanced network tailored for the JMERE task. KEJME utilizes GPT-3.5 and ConceptNet as knowledge sources to supplement the missing context information of text–image pairs by infusing external knowledge. Moreover, to ensure the relevance of imported knowledge, we introduce a knowledge feature selection module, which performs a fine-grained selection of knowledge features according to different visual objects and textual entities. Numerous experiments have demonstrated that KEJME significantly surpasses state-of-the-art methods, achieving substantial improvements in both recall and F1 score. These findings highlight the critical role of external knowledge integration and fine-grained knowledge selection in advancing multimodal entity-relation extraction.}
}
@article{SHOHAM2024109089,
title = {MedConceptsQA: Open source medical concepts QA benchmark},
journal = {Computers in Biology and Medicine},
volume = {182},
pages = {109089},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109089},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524011740},
author = {Ofir Ben Shoham and Nadav Rappoport},
keywords = {Benchmark, Large Language Models, LLM, Machine learning, Clinical knowledge, Health care},
abstract = {Background:
Clinical data often includes both standardized medical codes and natural language texts. This highlights the need for Clinical Large Language Models to understand these codes and their differences. We introduce a benchmark for evaluating the understanding of medical codes by various Large Language Models.
Methods:
We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs. The questions are categorized into three levels of difficulty: easy, medium, and hard. We conduct evaluations of the benchmark using various Large Language Models.
Results:
Our findings show that most of the pre-trained clinical Large Language Models achieved accuracy levels close to random guessing on this benchmark, despite being pre-trained on medical data. However, GPT-4 achieves an absolute average improvement of 9-11% (9% for few-shot learning and 11% for zero-shot learning) compared to Llama3-OpenBioLLM-70B, the clinical Large Language Model that achieved the best results.
Conclusion:
Our benchmark serves as a valuable resource for evaluating the abilities of Large Language Models to interpret medical codes and distinguish between medical concepts. We demonstrate that most of the current state-of-the-art clinical Large Language Models achieve random guess performance, whereas GPT-3.5, GPT-4, and Llama3-70B outperform these clinical models, despite their primary focus during pre-training not being on the medical domain. Our benchmark is available at https://huggingface.co/datasets/ofir408/MedConceptsQA.}
}
@incollection{NGUYEN2025169,
title = {Chapter 11 - Towards human digital twins for healthcare agent-based modeling in the Metaverse☆☆Disclaimer: This book chapter and my related published materials reflect my personal views only, and do not necessarily reflect the views of the US HHS nor the FDA. The chapter contains a part of my earlier published paper at https://xmed.jmir.org/2022/2/e33502.},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {169-194},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962000139},
author = {Tam N. Nguyen},
keywords = {Human digital twins, Metaverse, Agent-based modeling, Cognitive computing, Smart agents},
abstract = {Agent-based modeling (ABM) has been increasingly used to model complex real-life issues, such as informing prompt COVID-19 response policies. ABM represents subsystems and their entities as agents while employing flexible rules to describe complex relationships and interactions among the agents. The Metaverse, with its sophisticated agents like digital twins (DTs) and human digital twins (HDTs), can significantly boost ABM performance. However, current cognitive architectures are not ready for HDTs use in the Metaverse. Here we show that extending current digital cognitive architectures is a crucial first step towards building more robust HDTs. We introduce Cybonto, a novel ontology that packages 108 psychology constructs and thousands of related paths based on 20 time-tested psychology theories. Using 20 network science centrality algorithms, we rank the Cybonto psychology constructs by their influences, identifying the top 10 constructs: behavior, arousal, goals, perception, self-efficacy, circumstances, evaluating, behavior-controllability, knowledge, and intentional modality. These findings confirm the need for specific extensions of current digital cognitive architectures in preparation for future HDTs in the Metaverse. Additionally, Cybonto can be used to develop cognitive evaluation metrics for large language models, moving the field forward in terms of practical applications and theoretical advancements.}
}
@article{PAPATHEODOSIOU20242539,
title = {Leveraging Artificial Intelligence for personalised insomnia-sleep calibration via the Big Five Personality Traits},
journal = {Procedia Computer Science},
volume = {246},
pages = {2539-2548},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.723},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027923},
author = {Persephone Papatheodosiou and Dimitrios P. Panagoulias and Maria Virvou and George A. Tsihrintzis and Anastasios Bonakis and Dimitrios Dikeos},
keywords = {AI-empowered software engineering, Large Language Models, GPT, BERT, Insomnia, Sleep-therapy, Personalisation},
abstract = {This paper introduces Morpheas, an AI-empowered sleep evaluation and calibration system that leverages state-of-the-art technologies, like Large Language Models (LLMs) and Named Entity Recognition (NER). Morpheas integrates Sequential Language Model Integration (SLMI) workflows to simulate the initial steps of sleep disorder diagnosis, utilizing the GPT-4 engine enhanced with Rules of Conduct. Using SLMI and medical and psychological diagnostic tools, we propose a novel multi-step personalisation methodology for creating a gradation system for the improvement of patient-AI interactions. To test and showcase this personalisation approach, we simulate keeping a sleep diary for diagnosing insomnia using a trait-based personalised LLM aimed at addressing sleep concerns. For this purpose, we apply the Big Five Personality Traits (BFPT) where LLMs are again used to extract the responses and facilitate the patient throughout the process, providing guidance and explanation. We then extract the entities from these interactions with NER, in order to identify patterns, provide an explainability basis for patients and effectively customize Cognitive Behavioral Therapy for Insomnia (CBTi), whether through one-on-one sessions or via digital platforms.}
}
@article{GAO2024878,
title = {Integrating IoT and visual question answering in smart cities: Enhancing educational outcomes},
journal = {Alexandria Engineering Journal},
volume = {108},
pages = {878-888},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.09.059},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824010834},
author = {Tian Gao and Guanqi Wang},
keywords = {Smart cities, IoT framework, Visual question answering, Large language models, Smart education technology},
abstract = {Emerging as a paradigmatic shift in urban development, smart cities harness the potential of advanced information and communication technologies to seamlessly integrate urban functions, optimize resource allocation, and improve the effectiveness of city management. Within the domain of smart education, the imperative application of Visual Question Answering (VQA) technology encounters significant limitations at the prevailing stage, particularly the absence of a robust Internet of Things (IoT) framework and the inadequate incorporation of large pre-trained language models (LLMs) within contemporary smart education paradigms, especially in addressing zero-shot VQA scenarios, which pose considerable challenges. In response to these constraints, this paper introduces an IoT-based smart city framework that is designed to refine the functionality and efficacy of educational systems. This framework is delineated into four cardinal layers: the data collection layer, data transmission layer, data management layer, and application layer. Furthermore, we introduce the innovative TeachVQA methodology at the application layer, synergizing VQA technology with extensive pre-trained language models, thereby considerably enhancing the dissemination and assimilation of educational content. Evaluative metrics in the VQAv2 and OKVQA datasets substantiate that the TeachVQA methodology not only outperforms existing VQA approaches, but also underscores its profound potential and practical relevance in the educational sector.}
}
@article{LIU2025104062,
title = {Beyond expression: Comprehensive visualization of knowledge triplet facts},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104062},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104062},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000044},
author = {Wei Liu and Yixue He and Chao Wang and Shaorong Xie and Weimin Li},
keywords = {Generating model, Multi-modal Knowledge Graph, Non-visual relation, Visual element, Visual relation},
abstract = {Multi-modal Knowledge Graphs (KGs) enhance traditional KGs by incorporating multi-modal data to bridge the information gap in natural language processing (NLP) tasks. One direct method to incorporate multi-modal data is to associate structured KG with corresponding image modalities, thereby visualizing entities and triplet facts. However, existing visualization methods for triplet facts often exclude triplet facts containing abstract entities and non-visual relations, resulting in their disassociation from corresponding image modalities. This exclusion compromises the completeness and utility of multi-modal KGs. In this paper, we aim to construct a comprehensive multi-modal KG that includes abstract entities and non-visual relations, ensuring complete visualization of every triplet fact. To achieve this purpose, we propose a method for the integration of image Retrieval-Generation-Editing (RGE) to completely and accurately visualize each triplet fact. Initially, we correct the triplet facts by integrating a Large Language Model (LLM) with a retrieved knowledge database about triplet facts. Subsequently, by providing appropriate contextual examples to the LLM, we generate visual elements of relations, enriching the semantics of the triplet facts. We then employ image retrieval to obtain images that reflect the semantics of each triplet fact. For those triplet facts for which images cannot be directly retrieved, we utilize image generation and editing to create and modify images that can express the semantics of the triplet facts. Through the RGE method, we construct a multi-modal KG named DB15kFact, which includes 86,722 triplet facts, 274 relations, 12,842 entities, and 387,096 images. The construction of DB15kFact has resulted in a fourfold increase in the number of relations compared to the previous multi-modal KG, ImgFact. In experiments, both automatic and manual evaluations confirm the quality of DB15kFact. The results demonstrate that the DB15kFact significantly enhances model performance in link prediction and relation classification. Notably, in link prediction, the model optimized with DB15kFact achieves a 7.12% improvement in the H@10 metric compared to existing solutions.}
}
@article{WALDOCK2024,
title = {The Accuracy and Capability of Artificial Intelligence Solutions in Health Care Examinations and Certificates: Systematic Review and Meta-Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/56532},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007520},
author = {William J Waldock and Joe Zhang and Ahmad Guni and Ahmad Nabeel and Ara Darzi and Hutan Ashrafian},
keywords = {large language model, LLM, artificial intelligence, AI, health care exam, narrative medical response, health care examination, clinical commissioning, health services, safety},
abstract = {Background
Large language models (LLMs) have dominated public interest due to their apparent capability to accurately replicate learned knowledge in narrative text. However, there is a lack of clarity about the accuracy and capability standards of LLMs in health care examinations.
Objective
We conducted a systematic review of LLM accuracy, as tested under health care examination conditions, as compared to known human performance standards.
Methods
We quantified the accuracy of LLMs in responding to health care examination questions and evaluated the consistency and quality of study reporting. The search included all papers up until September 10, 2023, with all LLMs published in English journals that report clear LLM accuracy standards. The exclusion criteria were as follows: the assessment was not a health care exam, there was no LLM, there was no evaluation of comparable success accuracy, and the literature was not original research.The literature search included the following Medical Subject Headings (MeSH) terms used in all possible combinations: “artificial intelligence,” “ChatGPT,” “GPT,” “LLM,” “large language model,” “machine learning,” “neural network,” “Generative Pre-trained Transformer,” “Generative Transformer,” “Generative Language Model,” “Generative Model,” “medical exam,” “healthcare exam,” and “clinical exam.” Sensitivity, accuracy, and precision data were extracted, including relevant CIs.
Results
The search identified 1673 relevant citations. After removing duplicate results, 1268 (75.8%) papers were screened for titles and abstracts, and 32 (2.5%) studies were included for full-text review. Our meta-analysis suggested that LLMs are able to perform with an overall medical examination accuracy of 0.61 (CI 0.58-0.64) and a United States Medical Licensing Examination (USMLE) accuracy of 0.51 (CI 0.46-0.56), while Chat Generative Pretrained Transformer (ChatGPT) can perform with an overall medical examination accuracy of 0.64 (CI 0.6-0.67).
Conclusions
LLMs offer promise to remediate health care demand and staffing challenges by providing accurate and efficient context-specific information to critical decision makers. For policy and deployment decisions about LLMs to advance health care, we proposed a new framework called RUBRICC (Regulatory, Usability, Bias, Reliability [Evidence and Safety], Interoperability, Cost, and Codesign–Patient and Public Involvement and Engagement [PPIE]). This presents a valuable opportunity to direct the clinical commissioning of new LLM capabilities into health services, while respecting patient safety considerations.
Trial Registration
OSF Registries osf.io/xqzkw; https://osf.io/xqzkw}
}
@article{CHEN2024104651,
title = {Chat-ePRO: Development and pilot study of an electronic patient-reported outcomes system based on ChatGPT},
journal = {Journal of Biomedical Informatics},
volume = {154},
pages = {104651},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104651},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000698},
author = {Zikang Chen and Qinchuan Wang and Yaoqian Sun and Hailing Cai and Xudong Lu},
keywords = {Patient Reported Outcome Measures, Large Language Model, Knowledge Distillation, Breast Cancer, Mobile Health},
abstract = {Objective
Chatbots have the potential to improve user compliance in electronic Patient-Reported Outcome (ePRO) system. Compared to rule-based chatbots, Large Language Model (LLM) offers advantages such as simplifying the development process and increasing conversational flexibility. However, there is currently a lack of practical applications of LLMs in ePRO systems. Therefore, this study utilized ChatGPT to develop the Chat-ePRO system and designed a pilot study to explore the feasibility of building an ePRO system based on LLM.
Materials and Methods
This study employed prompt engineering and offline knowledge distillation to design a dialogue algorithm and built the Chat-ePRO system on the WeChat Mini Program platform. In order to compare Chat-ePRO with the form-based ePRO and rule-based chatbot ePRO used in previous studies, we conducted a pilot study applying the three ePRO systems sequentially at the Sir Run Run Shaw Hospital to collect patients’ PRO data.
Result
Chat-ePRO is capable of correctly generating conversation based on PRO forms (success rate: 95.7 %) and accurately extracting the PRO data instantaneously from conversation (Macro-F1: 0.95). The majority of subjective evaluations from doctors (>70 %) suggest that Chat-ePRO is able to comprehend questions and consistently generate responses. Pilot study shows that Chat-ePRO demonstrates higher response rate (9/10, 90 %) and longer interaction time (10.86 s/turn) compared to the other two methods.
Conclusion
Our study demonstrated the feasibility of utilizing algorithms such as prompt engineering to drive LLM in completing ePRO data collection tasks, and validated that the Chat-ePRO system can effectively enhance patient compliance.}
}
@article{FILTER2024100309,
title = {Food Safety Knowledge Exchange (FSKX) format: Current status and strategic development plans based on a SWOT analysis},
journal = {Microbial Risk Analysis},
volume = {27-28},
pages = {100309},
year = {2024},
issn = {2352-3522},
doi = {https://doi.org/10.1016/j.mran.2024.100309},
url = {https://www.sciencedirect.com/science/article/pii/S2352352224000203},
author = {Matthias Filter and Thomas Schüler and Racem {Ben Romdhane}},
keywords = {FAIR data, Knowledge exchange, Data standards, Linked models},
abstract = {The Food Safety Knowledge Exchange (FSKX) format is a community-driven effort initially created to promote the efficient exchange of data and models in the food safety domain. Over the past years this effort was driven by the Risk Assessment Knowledge Integration Platform (RAKIP) Initiative that also provided a number of software tools and FSKX-compliant model files via their website https://foodrisklabs.bfr.bund.de/rakip-initiative/. This paper describes the results of a SWOT analysis that was conducted to identify strategic avenues for enhancing FSKX's usability and adoption. The SWOT analysis identified a number of recommendations for the future evolution of FSKX. First, it is recommended to reduce the complexity of the annotation schema to ease the adoption of the format. Second, a clear distinction between the descriptive part of FSKX and the executable part is proposed. To promote the broad usage of FSKX-compliant models, it is also recommended to develop and provide FSKX-compliant APIs and resources that facilitate cloud-based execution. As part of the research to prioritize future FSKX development options, we also considered the implications of the emerging generative AI technologies, particularly which impact large language models (LLMs) might have in supporting the adoption of FSKX by the research community. Recognizing the format's application potential beyond the food safety domain, we then proposed to re-brand the FSKX acronym as "FAIR Scientific Knowledge Exchange Format" which better reflects its broad applicability in various scientific domains. Our research findings suggest that with the implementation of the improvements identified by the SWOT analysis and the broader availability of generative AI technologies the broad adoption of FSKX as a method to share data and models in a FAIR way comes into reach.}
}
@article{KOH20243454,
title = {Confronting the data deluge: How artificial intelligence can be used in the study of plant stress},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3454-3466},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024002988},
author = {Eugene Koh and Rohan Shawn Sunil and Hilbert Yuen In Lam and Marek Mutwil},
keywords = {Plant stress resilience, Large-scale data, Artificial intelligence, Large language models},
abstract = {The advent of the genomics era enabled the generation of high-throughput data and computational methods that serve as powerful hypothesis-generating tools to understand the genomic and gene functional basis of plant stress resilience. The proliferation of experimental and analytical methods used in biology has resulted in a situation where plentiful data exists, but the volume and heterogeneity of this data has made analysis a significant challenge. Current advanced deep-learning models have displayed an unprecedented level of comprehension and problem-solving ability, and have been used to predict gene structure, function and expression based on DNA or protein sequence, and prominently also their use in high-throughput phenomics in agriculture. However, the application of deep-learning models to understand gene regulatory and signalling behaviour is still in its infancy. We discuss in this review the availability of data resources and bioinformatic tools, and several applications of these advanced ML/AI models in the context of plant stress response, and demonstrate the use of a publicly available LLM (ChatGPT) to derive a knowledge graph of various experimental and computational methods used in the study of plant stress. We hope this will stimulate further interest in collaboration between computer scientists, computational biologists and plant scientists to distil the deluge of genomic, transcriptomic, proteomic, metabolomic and phenomic data into meaningful knowledge that can be used for the benefit of humanity.}
}
@article{LIU2024269,
title = {LLM technologies and information search},
journal = {Journal of Economy and Technology},
volume = {2},
pages = {269-277},
year = {2024},
issn = {2949-9488},
doi = {https://doi.org/10.1016/j.ject.2024.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2949948824000398},
author = {Lin Liu and Jiajun Meng and Yongliang Yang},
keywords = {LLM technologies, Information search, Google, Online advertising},
abstract = {With the booming of LLM technologies (e.g., ChatGPT), people’s goals and behaviors in information search have been reshaped significantly. This paper attempts to conceptually discuss how LLM technologies might revolutionize these important aspects in information search and provides a comprehensive analysis of the technological advancements and capabilities of ChatGPT, highlighting its potential to disrupt traditional search engines like Google. In addition, this paper contrasts ChatGPT’s conversational approach with Google’s link-based search model, offering a detailed examination of the implications for online search advertising and user behavior and explaining why Google is concerned about ChatGPT as well as its potential reactions.}
}
@article{RODRIGUES2024100248,
title = {Assessing the quality of automatic-generated short answers using GPT-4},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100248},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100248},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000511},
author = {Luiz Rodrigues and Filipe {Dwan Pereira} and Luciano Cabral and Dragan Gašević and Geber Ramalho and Rafael {Ferreira Mello}},
keywords = {Automatic answer generation, Question-answering, Large language models, GPT-4, Natural language processing},
abstract = {Open-ended assessments play a pivotal role in enabling instructors to evaluate student knowledge acquisition and provide constructive feedback. Integrating large language models (LLMs) such as GPT-4 in educational settings presents a transformative opportunity for assessment methodologies. However, existing literature on LLMs addressing open-ended questions lacks breadth, relying on limited data or overlooking question difficulty levels. This study evaluates GPT-4's proficiency in responding to open-ended questions spanning diverse topics and cognitive complexities in comparison to human responses. To facilitate this assessment, we generated a dataset of 738 open-ended questions across Biology, Earth Sciences, and Physics and systematically categorized it based on Bloom's Taxonomy. Each question included eight human-generated responses and two from GPT-4. The outcomes indicate GPT-4's superior performance over humans, encompassing both native and non-native speakers, irrespective of gender. Nevertheless, this advantage was not sustained in ’remembering’ or ’creating’ questions aligned with Bloom's Taxonomy. These results highlight GPT-4's potential for underpinning advanced question-answering systems, its promising role in supporting non-native speakers, and its capacity to augment teacher assistance in assessments. However, limitations in nuanced argumentation and creativity underscore areas necessitating refinement in these models, guiding future research toward bolstering pedagogical support.}
}
@article{YANG2024105817,
title = {Prompt-based automation of building code information transformation for compliance checking},
journal = {Automation in Construction},
volume = {168},
pages = {105817},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105817},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005533},
author = {Fan Yang and Jiansong Zhang},
keywords = {Building code, Information transformation, Automated compliance checking, Prompt engineering, Natural language processing, Large language models (LLMs)},
abstract = {Transforming building code information into a machine-processable format is essential for automated compliance checking, yet it presents significant challenges. A prompt-based framework was developed to automate the conversion into a logic programming language. Its effectiveness was assessed by testing the framework on 51 requirements from the International Building Code (IBC) 2015, achieving 97.37 % precision and 95.88 % recall at the logic clause level, with only 2 % of the data used for training. Further testing on crash report transformation enhanced efficiency, reducing the average code generation time to approximately 60.8 s, thereby achieving a 27.8 % time savings compared to existing rule-based methods. This paper contributes to the body of knowledge by introducing an effective, versatile, and user-friendly approach to automated building code information transformation, markedly decreasing the reliance on training data, time, and manual efforts.}
}
@article{SONG2025102378,
title = {A goal-oriented document-grounded dialogue based on evidence generation},
journal = {Data & Knowledge Engineering},
volume = {155},
pages = {102378},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102378},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001022},
author = {Yong Song and Hongjie Fan and Junfei Liu and Yunxin Liu and Xiaozhou Ye and Ye Ouyang},
keywords = {Document-grounded dialogue, Evidence generation, Large language model, Vector representation, Retrieval augmented generation},
abstract = {Goal-oriented Document-grounded Dialogue (DGD) is used for retrieving specific domain documents, assisting users in document content retrieval, question answering, and document management. Existing methods typically employ keyword extraction and vector space models to understand the content of documents, identify the intent of questions, and generate answers based on the capabilities of generation models. However, challenges remain in semantic understanding, long text processing, and context understanding. The emergence of Large Language Models (LLMs) has brought new capabilities in context learning and step-by-step reasoning. These models, combined with Retrieval Augmented Generation(RAG) methods, have made significant breakthroughs in text comprehension, intent detection, language organization, offering exciting prospects for DGD research. However, the “hallucination” issue arising from LLMs requires complementary methods to ensure the credibility of their outputs. In this paper we propose a goal-oriented document-grounded dialogue approach based on evidence generation using LLMs. It designs and implements methods for document content retrieval & reranking, fine-tuning and inference, and evidence generation. Through experiments, the method of combining LLMs with vector space model, or with key information matching technique is used as a comparison, the accuracy of the proposed method is improved by 21.91% and 12.81%, while the comprehensiveness is increased by 10.89% and 69.83%, coherence is enhanced by 38.98% and 53.27%, and completeness is boosted by 16.13% and 36.97%, respectively, on average. Additional, ablation analysis conducted reveals that the evidence generation method also contributes significantly to the comprehensiveness and completeness.}
}
@article{GUTIERREZMAQUILON2024,
title = {Integrating GPT-Based AI into Virtual Patients to Facilitate Communication Training Among Medical First Responders: Usability Study of Mixed Reality Simulation},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/58623},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24007133},
author = {Rodrigo {Gutiérrez Maquilón} and Jakob Uhl and Helmut Schrom-Feiertag and Manfred Tscheligi},
keywords = {medical first responders, verbal communication skills, training, virtual patient, generative artificial intelligence, GPT, large language models, prompt engineering, mixed reality},
abstract = {Background
Training in social-verbal interactions is crucial for medical first responders (MFRs) to assess a patient’s condition and perform urgent treatment during emergency medical service administration. Integrating conversational agents (CAs) in virtual patients (VPs), that is, digital simulations, is a cost-effective alternative to resource-intensive human role-playing. There is moderate evidence that CAs improve communication skills more effectively when used with instructional interventions. However, more recent GPT-based artificial intelligence (AI) produces richer, more diverse, and more natural responses than previous CAs and has control of prosodic voice qualities like pitch and duration. These functionalities have the potential to better match the interaction expectations of MFRs regarding habitability.
Objective
We aimed to study how the integration of GPT-based AI in a mixed reality (MR)–VP could support communication training of MFRs.
Methods
We developed an MR simulation of a traffic accident with a VP. ChatGPT (OpenAI) was integrated into the VP and prompted with verified characteristics of accident victims. MFRs (N=24) were instructed on how to interact with the MR scenario. After assessing and treating the VP, the MFRs were administered the Mean Opinion Scale-Expanded, version 2, and the Subjective Assessment of Speech System Interfaces questionnaires to study their perception of the voice quality and the usability of the voice interactions, respectively. Open-ended questions were asked after completing the questionnaires. The observed and logged interactions with the VP, descriptive statistics of the questionnaires, and the output of the open-ended questions are reported.
Results
The usability assessment of the VP resulted in moderate positive ratings, especially in habitability (median 4.25, IQR 4-4.81) and likeability (median 4.50, IQR 3.97-5.91). Interactions were negatively affected by the approximately 3-second latency of the responses. MFRs acknowledged the naturalness of determining the physiological states of the VP through verbal communication, for example, with questions such as “Where does it hurt?” However, the question-answer dynamic in the verbal exchange with the VP and the lack of the VP’s ability to start the verbal exchange were noticed. Noteworthy insights highlighted the potential of domain-knowledge prompt engineering to steer the actions of MFRs for effective training.
Conclusions
Generative AI in VPs facilitates MFRs’ training but continues to rely on instructions for effective verbal interactions. Therefore, the capabilities of the GPT-VP and a training protocol need to be communicated to trainees. Future interactions should implement triggers based on keyword recognition, the VP pointing to the hurting area, conversational turn-taking techniques, and add the ability for the VP to start a verbal exchange. Furthermore, a local AI server, chunk processing, and lowering the audio resolution of the VP’s voice could ameliorate the delay in response and allay privacy concerns. Prompting could be used in future studies to create a virtual MFR capable of assisting trainees.}
}
@article{SHAKIL2024128255,
title = {Abstractive text summarization: State of the art, challenges, and improvements},
journal = {Neurocomputing},
volume = {603},
pages = {128255},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128255},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224010269},
author = {Hassan Shakil and Ahmad Farooq and Jugal Kalita},
keywords = {Automatic summarization, Abstractive summarization, Extractive summarization, Knowledge representation, Text generation},
abstract = {Specifically focusing on the landscape of abstractive text summarization, as opposed to extractive techniques, this survey presents a comprehensive overview, delving into state-of-the-art techniques, prevailing challenges, and prospective research directions. We categorize the techniques into traditional sequence-to-sequence models, pre-trained large language models, reinforcement learning, hierarchical methods, and multi-modal summarization. Unlike prior works that did not examine complexities, scalability and comparisons of techniques in detail, this review takes a comprehensive approach encompassing state-of-the-art methods, challenges, solutions, comparisons, limitations and charts out future improvements — providing researchers an extensive overview to advance abstractive summarization research. We provide vital comparison tables across techniques categorized — offering insights into model complexity, scalability and appropriate applications. The paper highlights challenges such as inadequate meaning representation, factual consistency, controllable text summarization, cross-lingual summarization, and evaluation metrics, among others. Solutions leveraging knowledge incorporation and other innovative strategies are proposed to address these challenges. The paper concludes by highlighting emerging research areas like factual inconsistency, domain-specific, cross-lingual, multilingual, and long-document summarization, as well as handling noisy data. Our objective is to provide researchers and practitioners with a structured overview of the domain, enabling them to better understand the current landscape and identify potential areas for further research and improvement.}
}
@article{ZENG2025112784,
title = {Explainable next POI recommendation based on spatial–temporal disentanglement representation and pseudo profile generation},
journal = {Knowledge-Based Systems},
volume = {309},
pages = {112784},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112784},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124014187},
author = {Jun Zeng and Hongjin Tao and Junhao Wen and Min Gao},
keywords = {POI recommendation, Large language model, Disentangled representation, Graph neural network},
abstract = {The current research in Point-of-Interest (POI) recommendation primarily aims to decipher users’ transitional patterns to predict their future location visits. Traditional approaches often intertwine various features to model these check-in transitions, which inadvertently compromises the quality of the resulting representations. This issue is compounded in both industrial and academic settings, where user-generated textual data is frequently inaccessible or restricted due to privacy concerns. Such limitations in user profiles pose significant challenges to the effectiveness of subsequent applications. In response to these challenges, the recent rise of Large Language Models (LLMs) offers a novel perspective. Diverging from the conventional approach of leveraging LLMs for semantic-based next check-in predictions, our research investigates the potential of integrating LLMs with sequential recommendation systems. This integration aims to augment feature dimensions and facilitate the generation of explicit explanations. To this end, we introduce CrossDR-Gen, a Cross-sequence Location Disentanglement Representation methodology. CrossDR-Gen is specifically designed for next POI recommendation and explanation generation. It uniquely considers spatial and temporal factors in shaping check-in behaviors, offering a comprehensive global view of location transitions. Crucially, CrossDR-Gen utilizes LLMs for pseudo profile generation in scenarios with limited semantic context, thereby enriching user features without relying on additional textual profiles or conversational data. Our experiments on real-world datasets demonstrate that CrossDR-Gen not only excels in addressing cold-start scenarios but also showcases robust recommendation capabilities. These findings validate the effectiveness of our proposed cooperative paradigm between LLMs and sequential recommendation models, highlighting a promising avenue for future research in POI recommendation systems.}
}
@article{YANG2024,
title = {Ascle—A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/60601},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006289},
author = {Rui Yang and Qingcheng Zeng and Keen You and Yujie Qiao and Lucas Huang and Chia-Chun Hsieh and Benjamin Rosand and Jeremy Goldwasser and Amisha Dave and Tiarnan Keenan and Yuhe Ke and Chuan Hong and Nan Liu and Emily Chew and Dragomir Radev and Zhiyong Lu and Hua Xu and Qingyu Chen and Irene Li},
keywords = {natural language processing, machine learning, deep learning, generative artificial intelligence, large language models, retrieval-augmented generation, healthcare},
abstract = {Background
Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to automate text processing. In the biomedical field, various toolkits for text processing exist, which have greatly improved the efficiency of handling unstructured text. However, these existing toolkits tend to emphasize different perspectives, and none of them offer generation capabilities, leaving a significant gap in the current offerings.
Objective
This study aims to describe the development and preliminary evaluation of Ascle. Ascle is tailored for biomedical researchers and clinical staff with an easy-to-use, all-in-one solution that requires minimal programming expertise. For the first time, Ascle provides 4 advanced and challenging generative functions: question-answering, text summarization, text simplification, and machine translation. In addition, Ascle integrates 12 essential NLP functions, along with query and search capabilities for clinical databases.
Methods
We fine-tuned 32 domain-specific language models and evaluated them thoroughly on 27 established benchmarks. In addition, for the question-answering task, we developed a retrieval-augmented generation (RAG) framework for large language models that incorporated a medical knowledge graph with ranking techniques to enhance the reliability of generated answers. Additionally, we conducted a physician validation to assess the quality of generated content beyond automated metrics.
Results
The fine-tuned models and RAG framework consistently enhanced text generation tasks. For example, the fine-tuned models improved the machine translation task by 20.27 in terms of BLEU score. In the question-answering task, the RAG framework raised the ROUGE-L score by 18% over the vanilla models. Physician validation of generated answers showed high scores for readability (4.95/5) and relevancy (4.43/5), with a lower score for accuracy (3.90/5) and completeness (3.31/5).
Conclusions
This study introduces the development and evaluation of Ascle, a user-friendly NLP toolkit designed for medical text generation. All code is publicly available through the Ascle GitHub repository. All fine-tuned language models can be accessed through Hugging Face.}
}
@article{LIU20241049,
title = {Research status and application of artificial intelligence large models in the oil and gas industry},
journal = {Petroleum Exploration and Development},
volume = {51},
number = {4},
pages = {1049-1065},
year = {2024},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(24)60524-0},
url = {https://www.sciencedirect.com/science/article/pii/S1876380424605240},
author = {He LIU and Yili REN and Xin LI and Yue DENG and Yongtao WANG and Qianwen CAO and Jinyang DU and Zhiwei LIN and Wenjie WANG},
keywords = {foundation model, large language mode, visual large model, multimodal large model, large model of oil and gas industry, pre-training, fine-tuning},
abstract = {This article elucidates the concept of large model technology, summarizes the research status of large model technology both domestically and internationally, provides an overview of the application status of large models in vertical industries, outlines the challenges and issues confronted in applying large models in the oil and gas sector, and offers prospects for the application of large models in the oil and gas industry. The existing large models can be briefly divided into three categories: large language models, visual large models, and multimodal large models. The application of large models in the oil and gas industry is still in its infancy. Based on open-source large language models, some oil and gas enterprises have released large language model products using methods like fine-tuning and retrieval augmented generation. Scholars have attempted to develop scenario-specific models for oil and gas operations by using visual/multimodal foundation models. A few researchers have constructed pre-trained foundation models for seismic data processing and interpretation, as well as core analysis. The application of large models in the oil and gas industry faces challenges such as current data quantity and quality being difficult to support the training of large models, high research and development costs, and poor algorithm autonomy and control. The application of large models should be guided by the needs of oil and gas business, taking the application of large models as an opportunity to improve data lifecycle management, enhance data governance capabilities, promote the construction of computing power, strengthen the construction of “artificial intelligence + energy” composite teams, and boost the autonomy and control of large model technology.}
}
@article{SONG2025126558,
title = {HeteroHTC: Enhancing Hierarchical Text Classification via Heterogeneity Encoding of Label Hierarchy},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126558},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126558},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001800},
author = {Junru Song and Tianlei Chen and Yang Yang and Feifei Wang},
keywords = {Hierarchical Text Classification, Heterogeneous Graph Transformer, Large Language Models},
abstract = {Hierarchical Text Classification (HTC) is a challenging subtask of multi-label text classification, where labels are organized into a pre-defined hierarchy. Recent works primarily encode documents and labels separately before cross attention-based feature extraction, and in the process, they collectively overlook a crucial characteristic of label hierarchies: “heterogeneity”. Specifically, labels on different levels hold different granularities, and they should be projected onto distinct feature spaces; The relationships among labels are various, dictating that the message transmission among them should occur in unique feature spaces. We term these properties “granularity heterogeneity” and “relationship heterogeneity”, respectively. To fully exploit these ubiquitous yet overlooked properties, we propose HeteroHTC, which features a heterogeneous label hierarchy encoder. Additionally, we leverage pre-trained Large Language Models (LLMs) to generate high-quality label descriptions with strategically designed prompts. HeteroHTC outperforms almost all baselines in our extensive experiments on three datasets, proving its effectiveness and the necessity to take “granularity and relationship heterogeneity” into consideration.}
}
@article{HE2025103875,
title = {The more quality information the better: Hierarchical generation of multi-evidence alignment and fusion model for multimodal entity and relation extraction},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103875},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103875},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002346},
author = {Xinyu He and Shixin Li and Yuning Zhang and Binhe Li and Sifan Xu and Yuqing Zhou},
keywords = {Multimodal entity and relation extraction, Hierarchical generation, Multi-evidence fusion, LLM},
abstract = {Multimodal Entity and Relation Extraction (MERE) encompasses tasks, including Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE), aiming to extract valuable information from environments rich in multimodal data. Currently, many research endeavors face various challenges, including the insufficient utilization of emotional information in multimodal data, mismatches between textual and visual content, ambiguous meanings, and difficulties achieving precise alignment across different semantic levels. To address these issues, we propose the Hierarchical Generation of Multi Evidence Alignment Fusion Model for Multimodal Entity and Relation Extraction (HGMAF). This model comprises a hierarchical diffusion semantic generation stage and a multi-evidence alignment fusion module. Initially, we designed different prompt templates for the original text, using the Large Language Model (LLM) to generate corresponding hierarchical textual content. Subsequently, the generated hierarchical content is diffused to obtain images with rich hierarchical semantic information. This stage contributes to enhancing the model's understanding of hierarchical information in the original content. Following this, we design the multi-evidence alignment fusion module, which combines the generated textual and image evidence, fully leveraging information from different sources to improve extraction accuracy. Experimental results demonstrate that our model achieves F1 scores of 76.29 %, 87.66 %, and 87.34 % on the Twitter2015, Twitter2017, and MNRE datasets, respectively. These results surpass the previous state-of-the-art models by 0.29 %, 0.1 %, and 2.77 %. Furthermore, our model demonstrates superior performance in low-resource scenarios, confirming its effectiveness. The related code can be found at https://github.com/lsx314/HGMAF.}
}
@article{JANG2025103100,
title = {Semantic elaboration of low-LOD BIMs: Inferring functional requirements using graph neural networks},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103100},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103100},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007511},
author = {Suhyung Jang and Ghang Lee and Minkyeong Park and Jaekun Lee and Seungah Suh and Bonsang Koo},
keywords = {Semantic elaboration, Building information model (BIM), Threshold-enhanced triangle intersection (TETI), Graph neural network (GNN), Large language model (LLM) embedding},
abstract = {This study proposes a method to automatically subcategorize early object types in low levels of development (LODs) into detailed types (i.e., subtypes) with distinct functional requirements, such as insulation, waterproofing, and load-bearing. While rough cost estimation is possible in the early design phase without detailed object classifications, its accuracy is often limited. Subcategorizing generic objects like walls and columns into more detailed types enhances the precision of early-stage engineering analyses, including cost estimation, load assessments, and material takeoffs. Existing automated object subclassification methods rely on information extracted from highly detailed models, which are unavailable in early-stage building information models (BIMs) due to a lack of geometric and attributive distinctions. This study addresses these limitations by leveraging functional requirements inferred from object connections and placement in early BIMs, achieved using a graph neural network (GNN). To convert BIMs into graphs, a novel threshold-enhanced triangle intersection (TETI) algorithm is introduced, overcoming inaccuracies and exception-handling issues in existing methods. The study explores two GNN-based approaches: node property prediction and node prediction. The former distinguished generic object types into 14 detailed categories, but cost estimation required greater specificity. The latter successfully classified objects into 42 subtypes, with the best results achieved using semantically rich embeddings from a large language model (LLM) and GraphSAGE with three SAGE convolution layers, three hops, and 1,024 dimensions, yielding a weighted F1-score of 0.8766. This approach significantly reduces input data requirements compared to existing methods, enabling more accurate early identification of functional requirements in low-LOD BIMs and supporting both early engineering analyses and detailing processes.}
}
@article{LIU2025101818,
title = {Harnessing AI for understanding scientific literature: Innovations and applications of chat-agent system in battery recycling research},
journal = {Materials Today Energy},
volume = {49},
pages = {101818},
year = {2025},
issn = {2468-6069},
doi = {https://doi.org/10.1016/j.mtener.2025.101818},
url = {https://www.sciencedirect.com/science/article/pii/S2468606925000267},
author = {Rongfan Liu and Zhi Zou and Sihui Chen and Yang Liu and Jiayu Wan},
abstract = {Scientific research heavily relies on literature and patent reviews to build upon existing knowledge and innovation, particularly in rapidly advancing fields like battery recycling. Traditional methods for conducting these reviews, which involve manual collection and analysis, are becoming increasingly inefficient due to the overwhelming volume of information. Recent advancements in Artificial Intelligence (AI), particularly in the form of large language models (LLMs) and retrieval-augmented generation (RAG) architectures, offer promising solutions to these challenges. LLMs, enhanced by RAG, can automate the tedious tasks of literature review, providing more accurate, timely, and comprehensive insights. Despite their great potential, current RAG systems still face limitations, such as the reliance on manual database updates and challenges in providing precise answers to specific professional queries. To address these issues, we developed an innovative Agent-Based literature research system that integrates actor-critic architecture to enhance the accuracy and relevance of responses. This system is validated in the context of battery recycling, demonstrating superior performance in literature review tasks. Our findings indicate that AI-empowered literature research agents can significantly enhance the efficiency and depth of scientific inquiry, although further advancements are necessary to fully release their potential across various scientific domains.}
}
@article{PEREZPEREZ2024,
title = {Tracking the Spread of Pollen on Social Media Using Pollen-Related Messages From Twitter: Retrospective Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/58309},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006848},
author = {Martín Pérez-Pérez and María {Fernandez Gonzalez} and Francisco Javier Rodriguez-Rajo and Florentino Fdez-Riverola},
keywords = {pollen, respiratory allergies, Twitter, large language model, LLM, knowledge reconstruction, text mining},
abstract = {Background
Allergy disorders caused by biological particles, such as the proteins in some airborne pollen grains, are currently considered one of the most common chronic diseases, and European Academy of Allergy and Clinical Immunology forecasts indicate that within 15 years 50% of Europeans will have some kind of allergy as a consequence of urbanization, industrialization, pollution, and climate change.
Objective
The aim of this study was to monitor and analyze the dissemination of information about pollen symptoms from December 2006 to January 2022. By conducting a comprehensive evaluation of public comments and trends on Twitter, the research sought to provide valuable insights into the impact of pollen on sensitive individuals, ultimately enhancing our understanding of how pollen-related information spreads and its implications for public health awareness.
Methods
Using a blend of large language models, dimensionality reduction, unsupervised clustering, and term frequency–inverse document frequency, alongside visual representations such as word clouds and semantic interaction graphs, our study analyzed Twitter data to uncover insights on respiratory allergies. This concise methodology enabled the extraction of significant themes and patterns, offering a deep dive into public knowledge and discussions surrounding respiratory allergies on Twitter.
Results
The months between March and August had the highest volume of messages. The percentage of patient tweets appeared to increase notably during the later years, and there was also a potential increase in the prevalence of symptoms, mainly in the morning hours, indicating a potential rise in pollen allergies and related discussions on social media. While pollen allergy is a global issue, specific sociocultural, political, and economic contexts mean that patients experience symptomatology at a localized level, needing appropriate localized responses.
Conclusions
The interpretation of tweet information represents a valuable tool to take preventive measures to mitigate the impact of pollen allergy on sensitive patients to achieve equity in living conditions and enhance access to health information and services.}
}
@article{ARDEKANI2024103291,
title = {FinSentGPT: A universal financial sentiment engine?},
journal = {International Review of Financial Analysis},
volume = {94},
pages = {103291},
year = {2024},
issn = {1057-5219},
doi = {https://doi.org/10.1016/j.irfa.2024.103291},
url = {https://www.sciencedirect.com/science/article/pii/S1057521924002230},
author = {Aref Mahdavi Ardekani and Julie Bertz and Cormac Bryce and Michael Dowling and Suwan(Cheng) Long},
keywords = {ChatGPT, Large language models, Financial sentiment, Monetary policy, Fine-tuning},
abstract = {We present FinSentGPT, a financial sentiment prediction model based on a fine-tuned version of the artificial intelligence language model, ChatGPT. To assess the model’s effectiveness, we analyse a sample of US media news and a multi-language dataset of European Central Bank Monetary Policy Decisions. Our findings demonstrate that FinSentGPT’s sentiment classification ability aligns well with a prominent English-language finance sentiment model, surpasses an established alternative machine learning model, and is capable of predicting sentiment across various languages. Consequently, we offer preliminary evidence that advanced large-language AI models can facilitate flexible and contextual financial sentiment determination, transcending language barriers.}
}
@article{BREITUNG2025104007,
title = {Global Business Networks},
journal = {Journal of Financial Economics},
volume = {166},
pages = {104007},
year = {2025},
issn = {0304-405X},
doi = {https://doi.org/10.1016/j.jfineco.2025.104007},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X25000157},
author = {Christian Breitung and Sebastian Müller},
keywords = {Business network, Textual analysis, Natural language processing, GPT-3, Large language models},
abstract = {We leverage the capabilities of GPT-3 to generate historical business descriptions for over 63,000 global firms. Utilizing these descriptions and advanced embedding models from OpenAI, we construct time-varying business networks that represent business links across the globe. We showcase the performance of these networks by studying the lead–lag effect for global stocks and predicting target firms in M&A deals. We demonstrate how masking firm-specific details can mitigate look-ahead bias concerns that may arise from the use of embedding models with a recent knowledge cutoff, and how to differentiate between competitor, supplier, and customer links by fine-tuning an open-source language model.}
}
@article{COX2024100130,
title = {An AI assistant to help review and improve causal reasoning in epidemiological documents},
journal = {Global Epidemiology},
volume = {7},
pages = {100130},
year = {2024},
issn = {2590-1133},
doi = {https://doi.org/10.1016/j.gloepi.2023.100130},
url = {https://www.sciencedirect.com/science/article/pii/S2590113323000330},
author = {Louis Anthony Cox},
keywords = {Artificial intelligence, Causality, Review methodology, Causal AI boosting, Large language models (LLMs)},
abstract = {Drawing sound causal inferences from observational data is often challenging for both authors and reviewers. This paper discusses the design and application of an Artificial Intelligence Causal Research Assistant (AIA) that seeks to help authors improve causal inferences and conclusions drawn from epidemiological data in health risk assessments. The AIA-assisted review process provides structured reviews and recommendations for improving the causal reasoning, analyses and interpretations made in scientific papers based on epidemiological data. Causal analysis methodologies range from earlier Bradford-Hill considerations to current causal directed acyclic graph (DAG) and related models. AIA seeks to make these methods more accessible and useful to researchers. AIA uses an external script (a “Causal AI Booster” (CAB) program based on classical AI concepts of slot-filling in frames organized into task hierarchies to complete goals) to guide Large Language Models (LLMs), such as OpenAI's ChatGPT or Google's LaMDA (Bard), to systematically review manuscripts and create both (a) recommendations for what to do to improve analyses and reporting; and (b) explanations and support for the recommendations. Review tables and summaries are completed systematically by the LLM in order. For example, recommendations for how to state and caveat causal conclusions in the Abstract and Discussion sections reflect previous analyses of the Study Design and Data Analysis sections. This work illustrates how current AI can contribute to reviewing and providing constructive feedback on research documents. We believe that such AI-assisted review shows promise for enhancing the quality of causal reasoning and exposition in epidemiological studies. It suggests the potential for effective human-AI collaboration in scientific authoring and review processes.}
}
@article{GUO2024108709,
title = {A survey on advancements in image–text multimodal models: From general techniques to biomedical implementations},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108709},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007947},
author = {Ruifeng Guo and Jingxuan Wei and Linzhuang Sun and Bihui Yu and Guiyong Chang and Dawei Liu and Sibo Zhang and Zhengbing Yao and Mingjun Xu and Liping Bu},
keywords = {Image–text multimodal models, Artificial intelligence, Technological evolution, Biomedical applications, Challenges and strategies},
abstract = {With the significant advancements of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), the development of image–text multimodal models has garnered widespread attention. Current surveys on image–text multimodal models mainly focus on representative models or application domains, but lack a review on how general technical models influence the development of domain-specific models, which is crucial for domain researchers. Based on this, this paper first reviews the technological evolution of image–text multimodal models, from early explorations of feature space to visual language encoding structures, and then to the latest large model architectures. Next, from the perspective of technological evolution, we explain how the development of general image–text multimodal technologies promotes the progress of multimodal technologies in the biomedical field, as well as the importance and complexity of specific datasets in the biomedical domain. Then, centered on the tasks of image–text multimodal models, we analyze their common components and challenges. After that, we summarize the architecture, components, and data of general image–text multimodal models, and introduce the applications and improvements of image–text multimodal models in the biomedical field. Finally, we categorize the challenges faced in the development and application of general models into external factors and intrinsic factors, further refining them into 2 external factors and 5 intrinsic factors, and propose targeted solutions, providing guidance for future research directions. For more details and data, please visit our GitHub page: https://github.com/i2vec/A-survey-on-image-text-multimodal-models.}
}
@article{LIGA2023105864,
title = {Fine-tuning GPT-3 for legal rule classification},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105864},
year = {2023},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2023.105864},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923000742},
author = {Davide Liga and Livio Robaldo},
keywords = {Rule classification, GPT-3, AI&Law},
abstract = {In this paper, we propose a Legal Rule Classification (LRC) task using one of the most discussed language model in the field of Artificial Intelligence, namely GPT-3, a generative pretrained language model. We train and test the proposed LRC task on the GDPR encoded in LegalDocML (Palmirani and Vitali, 2011) and LegalRuleML (Athan et al., 2013), two widely used XML standards for the legal domain. We use the LegalDocML and LegalRuleML annotations provided in Robaldo et al. (2020) to fine-tuned GPT-3. While showing the ability of large language models (LLMs) to easily learn to classify legal and deontic rules even on small amount of data, we show that GPT-3 can significantly outperform previous experiments on the same task. Our work focused on a multiclass task, showing that GPT-3 is capable to recognize the difference between obligation rules, permission rules and constitutive rules with performances that overcome previous scores in LRC.}
}
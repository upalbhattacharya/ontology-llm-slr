@article{ZHANG2025100767,
title = {The impact of generative AI on management innovation},
journal = {Journal of Industrial Information Integration},
volume = {44},
pages = {100767},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100767},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24002103},
author = {Caiming Zhang and Hui Zhang},
keywords = {Generative artificial intelligence, Management decision-making, Management Algorithms, Information Integration},
abstract = {Generative Artificial Intelligence (GAI) demonstrates significant potential in the application of management and organizational innovation. This paper systematically investigates the multifaceted impacts of GAI on management decision-making, management algorithms, information integration, and various specific domains. GAI significantly enhances the accuracy of management decisions through its robust data analysis and predictive capabilities. By effectively integrating internal and external information, it reduces information asymmetry and improves both information transparency and the quality of decisions. In terms of specific application areas, GAI shows broad prospects in multiple fields, including business, education, healthcare, content creation, and game development. As GAI technology continues to advance, it will become more intelligent and adaptive. However, further research and the establishment of relevant ethical guidelines and legal frameworks are necessary to ensure its safety and reliability.}
}
@article{EKE2023100060,
title = {ChatGPT and the rise of generative AI: Threat to academic integrity?},
journal = {Journal of Responsible Technology},
volume = {13},
pages = {100060},
year = {2023},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2023.100060},
url = {https://www.sciencedirect.com/science/article/pii/S2666659623000033},
author = {Damian Okaibedi Eke},
keywords = {ChatGPT, Large language models, OpenAI, Academic integrity, Generative AI},
abstract = {The emergence of OpenAI's ChatGPT has put intense spotlight on Generative AI (Gen-AI) systems and their possible impacts on Academic integrity. This paper provides an overview of the current arguments around ChatGPT and Academic integrity and concludes that although these technologies are capable of revolutionising academia, the way ChatGPT and other generative AI systems are used could surely undermine academic integrity. However, to ensure that the risks to academic integrity are mitigated for greater maximisation, institutional and multi-stakeholder efforts are required.}
}
@article{CONG2025103131,
title = {Enhancing novel product iteration: An integrated framework for heuristic ideation via interpretable conceptual design knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103131},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103131},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000242},
author = {Yangfan Cong and Suihuai Yu and Jianjie Chu and Yuexin Huang and Ning Ding and Cong Fang and Stephen Jia Wang},
keywords = {Novel product iteration, Conceptual product design, Design knowledge, Interpretable knowledge graph, Heuristic product ideation},
abstract = {Novel products emerge over time to survive the competitive landscape as no existing product can perpetually satisfy all evolving customer expectations. These products are often characterized by groundbreaking solutions previously unavailable on the market. However, the swift imitation of successful novel products by competitors underscores the need for sustained iteration and continuous improvement. Designers increasingly face challenges in keeping up to date with the growing volume and fragmented nature of design information from diverse sources. While knowledge graphs show promise in structuring and organizing complex design information, their effective application in the ideation process remains limited due to difficulties in automatic knowledge extraction and the lack of interpretability aligned well with designers’ cognitive processes. This study proposes an integrated method to construct an interpretable conceptual design knowledge graph (I-CDKG) that features both inherent and acquired interpretability for heuristic product ideation. First, the schema layer models product design knowledge and governs the semantic connection of design information reinforced by design cognition principles to create a reasonable organizational framework to foster intuitive knowledge exploration. Second, the data layer mainly fulfills automatic and smooth design knowledge extraction for I-CDKG construction through the deep learning ERNIE-BiGRU-CRF model combined with BIESO labeling mode and triple-extracting algorithm. Third, the application layer empowers designers to visually delve into interpretable design knowledge to locate inspiration from cluster, relation, and nest levels and enable constant I-CDKG expansion as design schemes proliferate. A case study on the smart cat litter box demonstrates the feasibility of the proposed methodology. The evaluation results confirm the I-CDKG’s advantages as a productive design tool for inspiring creative, practical, and cost-effective product ideations, thereby empowering the iterative development of competitive novel products.}
}
@article{LI20243825,
title = {Survey and Prospect for Applying Knowledge Graph in Enterprise Risk Management},
journal = {Computers, Materials and Continua},
volume = {78},
number = {3},
pages = {3825-3865},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.046851},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824003618},
author = {Pengjun Li and Qixin Zhao and Yingmin Liu and Chao Zhong and Jinlong Wang and Zhihan Lyu},
keywords = {Knowledge graph, enterprise risk, risk identification, risk management, review},
abstract = {Enterprise risk management holds significant importance in fostering sustainable growth of businesses and in serving as a critical element for regulatory bodies to uphold market order. Amidst the challenges posed by intricate and unpredictable risk factors, knowledge graph technology is effectively driving risk management, leveraging its ability to associate and infer knowledge from diverse sources. This review aims to comprehensively summarize the construction techniques of enterprise risk knowledge graphs and their prominent applications across various business scenarios. Firstly, employing bibliometric methods, the aim is to uncover the developmental trends and current research hotspots within the domain of enterprise risk knowledge graphs. In the succeeding section, systematically delineate the technical methods for knowledge extraction and fusion in the standardized construction process of enterprise risk knowledge graphs. Objectively comparing and summarizing the strengths and weaknesses of each method, we provide recommendations for addressing the existing challenges in the construction process. Subsequently, categorizing the applied research of enterprise risk knowledge graphs based on research hotspots and risk category standards, and furnishing a detailed exposition on the applicability of technical routes and methods. Finally, the future research directions that still need to be explored in enterprise risk knowledge graphs were discussed, and relevant improvement suggestions were proposed. Practitioners and researchers can gain insights into the construction of technical theories and practical guidance of enterprise risk knowledge graphs based on this foundation.}
}
@article{LIANG2024103805,
title = {Candidate-Heuristic In-Context Learning: A new framework for enhancing medical visual question answering with LLMs},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103805},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103805},
url = {https://www.sciencedirect.com/science/article/pii/S030645732400164X},
author = {Xiao Liang and Di Wang and Haodi Zhong and Quan Wang and Ronghan Li and Rui Jia and Bo Wan},
keywords = {Medical Visual Question Answering, In-Context Learning, Large language models, Knowledge-based visual question answering, Multi-modal learning},
abstract = {Medical Visual Question Answering (MedVQA) is designed to answer natural language questions related to medical images. Existing methods largely adopting the cross-modal pre-training and fine-tuning paradigm, face limitations in accuracy due to data scarcity and insufficient incorporation of extensive medical knowledge. Drawing inspiration from the Knowledge-Based Visual Question Answering (KB-VQA) domain, which leverages Large Language Models (LLMs) and external knowledge bases, we introduce the Candidate-Heuristic In-Context Learning (CH-ICL) framework, a novel approach that leverages LLMs augmented with external knowledge to directly enhance existing MedVQA models. Specifically, we collect a pathology terminology dictionary from a public digital pathology library as an external knowledge base and use it to train a knowledge scope discriminator, which helps identify the knowledge scope required to answer a question. Then, we employ existing MedVQA models to provide reliable answer candidates along with their confidence scores. Finally, the knowledge scope and candidates, combined with retrieved in-context exemplars, are aggregated into prompts for heuristically guiding LLMs in answer generation. Experimental results on the PathVQA, VQA-RAD, and SLAKE public benchmarks show state-of-the-art performance, with improvements of 1.91%, 1.88%, and 2.17% respectively over the baseline. Code and dataset are available at https://github.com/ecoxial2007/CH-ICL.}
}
@article{LIMA2025103263,
title = {ULKB Logic: A HOL-based framework for reasoning over knowledge graphs},
journal = {Science of Computer Programming},
volume = {242},
pages = {103263},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103263},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000024},
author = {Guilherme Lima and Alexandre Rademaker and Rosario Uceda-Sosa},
keywords = {HOL, Python, Wikidata, SPARQL, MRS, NLP},
abstract = {ULKB Logic is an open-source framework written in Python for reasoning over knowledge graphs. It provides an interactive theorem prover-like environment equipped with a higher-order language similar to the one used by HOL Light. The main goal of ULKB Logic is to ease the construction of applications that combine state-of-the-art computational logic tools with the knowledge available in knowledge graphs, such as Wikidata. To this end, the framework provides APIs for fetching statements from SPARQL endpoints and operating over the constructed theories using automated theorem provers and SMT solvers (such as the E prover and Z3). In this paper, we describe the design and implementation of ULKB Logic, present its interfaces for querying knowledge graphs and for calling external provers, and discuss a use case of commonsense reasoning in which ULKB Logic is used as the target logic for representing the semantics of English sentences.}
}
@article{PHAM2024109517,
title = {How rationals boost textual entailment modeling: Insights from large language models},
journal = {Computers and Electrical Engineering},
volume = {119},
pages = {109517},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109517},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624004440},
author = {Duc-Huy Pham and Tung Le and Huy Tien Nguyen},
keywords = {Rationale, Distillation, Large language model, Textual entailment},
abstract = {This study introduces an innovative methodology for rationale-based distillation in textual entailment. Central to our methodology is the use of diverse and deep rationale types generated by large language models, eliminating the need for explicit feature engineering between text-hypothesis pairs. Through extensive experimentation, we demonstrate the effectiveness of our rationale-enhanced distillation process, underpinned by a comprehensive study of rationales. Remarkably, our model, which utilizes the most impactful rationales and operates with 795 times fewer parameters, exhibits competitive performance, especially in contexts limited by resource availability. Specifically, our model significantly surpasses RoBERTa, FLAN-T5 Large, and GPT-3.5 in performance. Our findings underscore the potential of rationale-based approaches, which improve textual entailment modeling and pave the way for future research. These techniques could be applied to other areas of NLP and beyond. This study’s contributions are poised to benefit researchers and practitioners seeking to leverage the power of large language models in augmenting reasoning capabilities with rationales. The novelty of our approach lies in its ability to deliver high performance with significantly fewer computational resources, making it a valuable advancement in the field.}
}
@article{VIDAL2025100856,
title = {Integrating Knowledge Graphs with Symbolic AI: The Path to Interpretable Hybrid AI Systems in Medicine},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100856},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100856},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000428},
author = {Maria-Esther Vidal and Yashrajsinh Chudasama and Hao Huang and Disha Purohit and Maria Torrente},
keywords = {Knowledge Graphs, Neuro-symbolic systems, Semantic Data Management, Valid link prediction, Counterfactual prediction, KG-based applications},
abstract = {Knowledge Graphs (KGs) are graph-based structures that integrate heterogeneous data, capture domain knowledge, and enable explainable AI through symbolic reasoning. This position paper examines the challenges and research opportunities in integrating KGs with neuro-symbolic AI, highlighting their potential to enhance explainability, scalability, and context-aware reasoning in hybrid AI systems. Using a lung cancer use case, we illustrate how hybrid approaches address tasks such as link prediction—uncovering hidden relationships in medical data—and counterfactual reasoning—analyzing alternative scenarios to understand causal factors. The discussion is framed around TrustKG, which demonstrates how constraint validation, causal reasoning, and user-centric communication can support transparent and reliable decision-making. Additionally, we identify current limitations of KGs, including gaps in knowledge coverage, evolving data integration challenges, and the need for improved usability and impact assessment. These insights are not limited to healthcare but extend to other domains like energy, manufacturing, and mobility, showcasing the broad applicability of KGs. Finally, we propose research directions to unlock their full potential in building robust, transparent, and widely adopted real-world applications.}
}
@article{FENWICK2023105892,
title = {Originality and the future of copyright in an age of generative AI},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105892},
year = {2023},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2023.105892},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923001024},
author = {Mark Fenwick and Paulius Jurcys},
keywords = {AI, Generative AI, Copyright, Creativity, Originality, Artificial intelligence, Data, Feist, David Guetta, ChatGPT, Input, Output, Machine Learning, Authorship, Intellectual Property},
abstract = {This paper takes the occasion of French DJ David Guetta's use of generative AI tools to create lyrics and a voice in the style of Eminem, which he then used in one of his concerts, as the basis for an exploration of the shifting meaning of creativity and originality in the age of generative AI. Our main contention is that the Guetta form of creativity with generative AI tools differs in certain important respects from what has come before. The paper describes an iterative, dynamic process of conception, prompting, generation, refining, and deployment to characterise creativity in this context. Nevertheless, we contend that copyright – specifically the concept of originality as articulated in US federal law – is a sufficiently durable legal mechanism that can manage these new cultural forms, and that the two basic requirements of modern copyright law (a tangible medium of expression and a modest degree of creativity) remain relevant in identifying the scope of legal protection. The paper argues that the David Guetta story reveals something more general about creativity in a digital age, namely that while hybrid-networked (i.e., human – corporate – machine) creators have always created hybrid-networked cultural forms (i.e., creations that blend human and technology-constituted elements), such hybridity becomes increasingly visible and complex in the context of a new world of generative AI. At the very least, earlier – and influential – models of creativity as human-driven involving creation ex nihilo become harder to sustain in a new age of generative AI. But this does not mean copyright or notions of originality are redundant or that copyright law cannot accommodate Guetta and other cases. Such an account seems important as it challenges the hegemonic and reductive view that AI “generates” artistic works autonomously and avoids reducing the copyright issues raised by such creative works to the related but distinct question of whether learning models rely on copyrighted data. As such, copyright law should remain an important mechanism to facilitate genuine creators who are using AI systems in innovative and unique ways to push the boundaries of their creativity.}
}
@article{MA2025125366,
title = {Historical Trends and Normalizing Flow for One-shot Temporal Knowledge Graph Reasoning},
journal = {Expert Systems with Applications},
volume = {260},
pages = {125366},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125366},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424022334},
author = {Ruixin Ma and Longfei Wang and Huinan Wu and Buyun Gao and Xiaoru Wang and Liang Zhao},
keywords = {One-shot, Temporal knowledge graphs, Normalizing flow, Historical trends},
abstract = {Temporal Knowledge Graphs (TKGs) have garnered significant scholarly interest for their ability to dynamically represent the evolution of real-world events. However, the presence of the long tail effect in knowledge graphs necessitates the exploration of one-shot temporal knowledge graph reasoning. Concurrently, the issues of out-of-distribution and overfitting heavily impact the performance of models. Empirical evidence from real-world cases has demonstrated the significance of historical trend information in accurately predicting future events, and normalizing flows effectively address out-of-distribution and overfitting challenges. Thus, this paper proposes a novel approach named Historical Trends and Normalizing Flow for One-shot Temporal Knowledge Graph Reasoning (HNOT). Specifically, HNOT combine normalizing flows with a historical information aggregator to obtain more complex distributions, thereby resolving out-of-distribution and overfitting problems and capturing global representations of TKGs. In contrast to existing models that focus solely on entity and relationship modeling, we have introduced a novel historical trend information aggregator. This aggregator is designed to extract trend information from temporal knowledge subgraphs. By employing trend information as the primary modeling object, it facilitates the exploration of trend variations at different timestamps. This approach serves as an auxiliary source of global information. Subsequently, we integrate these two distinct types of information to achieve predictions of future events. Experimental results demonstrate the superiority of the proposed method over state-of-the-art baselines on three benchmark datasets.}
}
@article{LIU2025102900,
title = {Knowledge extraction for additive manufacturing process via named entity recognition with LLMs},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {93},
pages = {102900},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102900},
url = {https://www.sciencedirect.com/science/article/pii/S073658452400187X},
author = {Xuan Liu and John Ahmet Erkoyuncu and Jerry Ying Hsi Fuh and Wen Feng Lu and Bingbing Li},
keywords = {Domain knowledge management, Named entity recognition, Retrieval augmented generation, Large Language Models, Additive manufacturing, Fused deposition modeling},
abstract = {This paper proposes a novel NER framework, leveraging the advanced capabilities of Large Language Models (LLMs), to address the limitations of manually defined taxonomy. Our framework integrates the expert knowledge internalized in both academic materials and LLMs through retrieval-augmented generation (RAG) to automatically customize taxonomies for specific manufacturing processes and adopts two distinct strategies of using LLMs — In-Context Learning (ICL) and fine-tuning to complete manufacturing NER tasks with minimal training data. We demonstrate the framework efficiency through its superior ability to define precise taxonomies, identify and classify process-level entities related to the most popular additive manufacturing process fused deposition modeling (FDM) as case study, achieving a high F1 score of 0.9192.}
}
@article{ZHENG202464,
title = {Construction of microgravity biological knowledge graph and its applications in anti-osteoporosis drug prediction},
journal = {Life Sciences in Space Research},
volume = {41},
pages = {64-73},
year = {2024},
issn = {2214-5524},
doi = {https://doi.org/10.1016/j.lssr.2024.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214552424000142},
author = {Yu-Han Zheng and Guan-Jing Pan and Yuan Quan and Hong-Yu Zhang},
keywords = {Bone loss, Drug repurposing, Knowledge graph embedding, Chinese herbal medicine, Western medicine},
abstract = {Microgravity in the space environment can potentially have various negative effects on the human body, one of which is bone loss. Given the increasing frequency of human space activities, there is an urgent need to identify effective anti-osteoporosis drugs for the microgravity environment. Traditional microgravity experiments conducted in space suffer from limitations such as time-consuming procedures, high costs, and small sample sizes. In recent years, the in-silico drug discovery method has emerged as a promising strategy due to the advancements in bioinformatics and computer technology. In this study, we first collected a total of 184,915 literature articles related to microgravity and bone loss. We employed a combination of dependency path extraction and clustering techniques to extract data from the text. Afterwards, we conducted data cleaning and standardization to integrate data from several sources, including The Global Network of Biomedical Relationships (GNBR), Curated Drug–Drug Interactions Database (DDInter), Search Tool for Interacting Chemicals (STITCH), DrugBank, and Traditional Chinese Medicines Integrated Database (TCMID). Through this integration process, we constructed the Microgravity Biology Knowledge Graph (MBKG) consisting of 134,796 biological entities and 3,395,273 triplets. Subsequently, the TransE model was utilized to perform knowledge graph embedding. By calculating the distances between entities in the model space, the model successfully predicted potential drugs for treating osteoporosis and microgravity-induced bone loss. The results indicate that out of the top 10 ranked western medicines, 7 have been approved for the treatment of osteoporosis. Additionally, among the top 10 ranked traditional Chinese medicines, 5 have scientific literature supporting their effectiveness in treating bone loss. Among the top 20 predicted medicines for microgravity-induced bone loss, 15 have been studied in microgravity or simulated microgravity environments, while the remaining 5 are also applicable for treating osteoporosis. This research highlights the potential application of MBKG in the field of space drug discovery.}
}
@article{QI2025105873,
title = {Linking geo-models for geomorphological classification using knowledge graphs},
journal = {Computers & Geosciences},
volume = {196},
pages = {105873},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2025.105873},
url = {https://www.sciencedirect.com/science/article/pii/S0098300425000238},
author = {Yanmin Qi and Yunqiang Zhu and Shu Wang and Yutao Zhong and Stuart Marsh and Amin Farjudian and Heshan Du},
keywords = {Knowledge graph, Geographic computation, Geographic model, Geomorphological classification},
abstract = {Geographic computation is an important process in geographic information systems to detect, predict, and simulate geographic entities, events, and phenomena, which is performed through a series of geographic models over geographic data. However, selecting and sequencing appropriate models is challenging for users with limited knowledge. To automate the process of linking models into workflows, a knowledge graph-based approach is proposed. In this approach, the first part is to construct a knowledge graph that integrates knowledge from geographic models and domain experts. Then, an algorithm is designed to assist the constructed knowledge graph in automating model linking. This paper takes the geomorphological classification of the Hengduan Mountains in China as a case study, which geomorphological classification maps are generated by performing querying and computing through the geomorphological classification knowledge graph. Experimental results demonstrate that the proposed knowledge graph-based approach links the models into workflows automatically and generates reliable classification results.}
}
@article{IJEBU2025112551,
title = {Soft cosine and extended cosine adaptation for pre-trained language model semantic vector analysis},
journal = {Applied Soft Computing},
volume = {169},
pages = {112551},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112551},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013255},
author = {Funebi Francis Ijebu and Yuanchao Liu and Chengjie Sun and Patience Usoro Usip},
keywords = {Transformer-based model, Large language model, Vector space model, Transfer learning, Semantic textual similarity},
abstract = {Semantic textual analysis is a natural language processing task that has enjoyed several research contributions towards solving diverse real-life problems. Vector comparison is a core subtask in semantic textual similarity analysis. A plethora of solutions including recent state-of-the-art transformer-based pre-trained language models for transfer learning have focused on using only cosine similarity for embedding evaluation in downstream tasks and ignored other vector comparison methods. To investigate the relative performance of some such ignored measures, this work proposes novel adaptations for soft cosine and extended cosine vector measures. We investigate their performance against the conventional cosine measure, distance-weighted cosine, vector similarity measure, negative Manhattan, and Euclidean distances on downstream semantic textual similarity tasks, under same conditions, for the first time in literature. Adopting transformer-based Universal sentence encoder, SBERT, SRoBERTa, SimCSE, and ST5 for text encoding; the performances of the adapted measures are evaluated on diverse real world datasets using Pearson, Spearman, accuracy and F1 evaluation metrics. Results obtained show that the adapted measures significantly surpass previously reported state-of-the-art cosine similarity-based correlations in several test cases considered.}
}
@article{CHEN2024e29048,
title = {A hyper-knowledge graph system for research on AI ethics cases},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e29048},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29048},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024050795},
author = {Chuan Chen and Yu Feng and Mengyi Wei and Zihan Liu and Peng Luo and Shengkai Wang and Liqiu Meng},
keywords = {AI ethics, Hyper-knowledge graph system, Case-oriented ontological model, Knowledge-based system, Visual analytics},
abstract = {Current studies on the artificial intelligence (AI) ethics focus either on very broad guidelines or on a very special domain. Therefore, the research outcome can hardly be converted into actionable measures or transferred to other domains. Potential correlations between various cases of AI ethics at different granularity levels are unexplored. To overcome these deficiencies, the authors designed a case-oriented ontological model (COOM) and a hyper-knowledge graph system (HKGS) for the research of collected AI ethics cases. COOM describes criteria for modelling cases by attributes from three perspectives: event attributes, relational attributes, and positional attributes on the value chain. Based on it, HKGS stores the correlation between cases as knowledge and allows advanced visual analysis. The correlations between cases and their dynamic changes on value chain can be observed and explored. In HKGS's implementation part, one of the collected ethics cases is used as an example to demonstrate how to generate a hyper-knowledge graph and to visually analyze it. The authors also anticipated how different practitioners of AI ethics, can achieve the desired outputs from HKGS in their diverse scenarios.}
}
@article{GHOLAMIDASTGERDI2024103040,
title = {SSKG: Subject stream knowledge graph, a new approach for event detection from text},
journal = {Ain Shams Engineering Journal},
volume = {15},
number = {12},
pages = {103040},
year = {2024},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2024.103040},
url = {https://www.sciencedirect.com/science/article/pii/S2090447924004155},
author = {Pejman Gholami-Dastgerdi and Mohammad-Reza Feizi-Derakhshi and Pedram Salehpour},
keywords = {Text stream, Event detection, Stream graph, Subject stream, Knowledge graph},
abstract = {Events play a crucial role in shaping societal meaning and discourse. They are recorded in text data streams from social networks or online sources and analyzing them is vital for predicting and managing future occurrences. Methods for event detection from text data streams have been developed to enhance accuracy and efficiency in analyzing large datasets. This study focuses on events published as text data streams in Telegram. Two main perspectives are considered for better results: firstly, major events can overshadow the detection of smaller ones with fewer narrative instances. Secondly, due to diverse narratives surrounding each event, establishing meaningful connections between narrative sets is essential. In this research, two new concepts, “subject stream” and “stream graph,” are introduced. The subject stream aims to process topics to mitigate the impact of bursty events on detecting others. The stream graph models data stream and identifies various narratives associated with each event for better identification and categorization. Combining these approaches accurately represents events and their characteristics in the real world. Implementing the system in three versions demonstrates the effectiveness of using the “stream graph” alongside the “subject stream,” resulting in improved execution speed and accuracy. Evaluation results show a 6% enhancement in topic recall.}
}
@article{WANG2023100033,
title = {R2GenGPT: Radiology Report Generation with frozen LLMs},
journal = {Meta-Radiology},
volume = {1},
number = {3},
pages = {100033},
year = {2023},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2023.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2950162823000334},
author = {Zhanyu Wang and Lingqiao Liu and Lei Wang and Luping Zhou},
keywords = {Radiology report generation, Large language models, LLAMA},
abstract = {Large Language Models (LLMs) have consistently showcased remarkable generalization capa-bilities when applied to various language tasks. Nonetheless, harnessing the full potential of LLMs for Radiology Report Generation (R2Gen) still presents a challenge, stemming from the inherent disparity in modality between LLMs and the R2Gen task. To bridge this gap effectively, we propose R2GenGPT, which is a novel solution that aligns visual features with the word embedding space of LLMs using an efficient visual alignment module. This innovative approach empowers the previously static LLM to seamlessly integrate and process image information, marking a step forward in optimizing R2Gen performance. R2GenGPT offers the following benefits. First, it attains state-of-the-art (SOTA) performance by training only the lightweight visual alignment module while freezing all the parameters of LLM. Second, it exhibits high training efficiency, as it requires the training of an exceptionally minimal number of parameters while achieving rapid convergence. By employing delta tuning, our model only trains 5 ​M parameters (which constitute just 0.07 ​% of the total parameter count) to achieve performance close to the SOTA levels. Our code is available at https://github.com/wang-zhanyu/R2GenGPT.}
}
@article{RONG2024111297,
title = {KGCDP-T: Interpreting knowledge graphs into text by content ordering and dynamic planning with three-level reconstruction},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111297},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111297},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123010456},
author = {Huan Rong and Shengjie Sun and Tinghuai Ma and Di Jin and Victor S. Sheng},
keywords = {Knowledge graph interpretation, KG-to-text, Natural language generation, Neural text generation},
abstract = {Knowledge graph-to-text (KG-to-text) interpretation is employed to interpret given KG into semantically coherent and logically reasonable text to enhance the applicability of KG in more natural language generation (NLG) scenarios, such as search engines and text-dialog systems. Unfortunately, existing relevant research suffers from the semantic gap between structural knowledge graphs and unstructured text. To overcome this challenge, in this paper, we propose a novel pipeline-based Knowledge Graph interpreting model constructed by Content Ordering and Dynamic Planning with Three-Level Reconstruction (KGCDP-T). Specifically, the first “pipe” Content Ordering converts the given KG into several triple-groups and then into an ordered triple-sequence to plan the ordering by which the “content” in the given knowledge graph should be interpreted. Next, an entity-sequence is derived from the above ordered triple-sequence, where the second “pipe” Dynamic Planning captures the context shifting in the entity-sequence via Memory Network. In this way, the entity-sequence-level and memory-level contexts are learned and fused to generate the interpreted-text with more context-adaptive tokens. Moreover, the Three-Level Reconstruction mechanism is incorporated to capture the critical features transferred among the triple-groups, the ordered triple-sequence, the generated text-sequence and the given KG. The experimental results indicate that our proposed KGCDP-T can achieve the overall best KG-interpretation performance on content integrity (reflected by a 7.69% average improvement in Coverage), sentence fluency (reflected by 5.98% and 6.00% average improvements in BLEU and ROUGE-L, respectively) and logic coherency (reflected by 6.10% and 9.18% average improvements in METEOR and Chrf++, respectively) when compared with state-of-art KG-to-text models.}
}
@article{CHEN2024123320,
title = {Optimizing automated compliance checking with ontology-enhanced natural language processing: Case in the fire safety domain},
journal = {Journal of Environmental Management},
volume = {371},
pages = {123320},
year = {2024},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2024.123320},
url = {https://www.sciencedirect.com/science/article/pii/S0301479724033061},
author = {Yian Chen and Huixian Jiang},
keywords = {Ontology automatic construction, Automated compliance checking (ACC), Named entity recognition, Relationship extraction, Pre-trained language model, Rule interpretation},
abstract = {The fire safety compliance checking (FSCC) plays a crucial role in ensuring the quality of fire engineering design and eliminating inherent fire hazards. It requires an objective and rational interpretation of fire regulations. However, the texts of fire regulations are filled with numerous rules related to spatial limitations, which pose a significant challenge in interpreting them. The current method of interpreting these rules mostly relies on manual translation, which is not efficient. To address this issue, this study proposes an innovative automated framework for interpreting rules by combining ontology technology with natural language processing (NLP). Through the utilization of pre-trained language models (PLMs), concepts and relationships are extracted from sentences, a domain-specific ontology is established, spatial knowledge is transformed into language-agnostic tree structures based on the ontology, and the semantic components of spatial relationships are extracted. The tree structure is then mapped to logical clauses based on semantic consistency, thereby improving the efficiency of interpretation. Experimental results demonstrate that the architecture achieves an F1 score of 86.27 for entity extraction and 81.81 for spatial relationship joint extraction tasks, with an accuracy of 96.26% in the formalization of logical rules, highlighting its proficiency in automatically interpreting fire spatial rules. This study offers technical support to enhance public understanding of fire safety management and fire prevention predictions, thereby promoting the intelligent management of the building safety environment.}
}
@article{ARSLAN20243781,
title = {A Survey on RAG with LLMs},
journal = {Procedia Computer Science},
volume = {246},
pages = {3781-3790},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924021860},
author = {Muhammad Arslan and Hussam Ghanem and Saba Munawar and Christophe Cruz},
keywords = {Large Language Models (LLMs), Natural Language Processing (NLP), Retrieval-Augmented Generation (RAG), Text generation, Digital transformation},
abstract = {In the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain competitive and foster growth. Large Language Models (LLMs) have emerged as game-changers across industries, revolutionizing various sectors by harnessing extensive text data to analyze and generate human-like text. Despite their impressive capabilities, LLMs often encounter challenges when dealing with domain-specific queries, potentially leading to inaccuracies in their outputs. In response, Retrieval-Augmented Generation (RAG) has emerged as a viable solution. By seamlessly integrating external data retrieval into text generation processes, RAG aims to enhance the accuracy and relevance of the generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG, overlooking a comprehensive exploration of its applications. This paper seeks to address this gap by providing a thorough review of RAG applications, encompassing both task-specific and discipline-specific studies, while also outlining potential avenues for future research. By shedding light on current RAG research and outlining future directions, this review aims to catalyze further exploration and development in this dynamic field, thereby contributing to ongoing digital transformation efforts.}
}
@article{CONSTANTINOU2025126120,
title = {Using GPT-4 to guide causal machine learning},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126120},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126120},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029877},
author = {Anthony C. Constantinou and Neville K. Kitson and Alessio Zanga},
keywords = {Bayesian networks, Causal discovery, ChatGPT, Directed acyclic graphs, Knowledge graphs, LLMs, Structure learning},
abstract = {Since its introduction to the public, ChatGPT has had an unprecedented impact. While some experts praised AI advancements and highlighted their potential risks, others have been critical about the accuracy and usefulness of Large Language Models (LLMs). In this paper, we are interested in the ability of LLMs to identify causal relationships. We focus on the well-established GPT-4 (Turbo) and evaluate its performance under the most restrictive conditions, by isolating its ability to infer causal relationships based solely on the variable labels without being given any other context by humans, demonstrating the minimum level of effectiveness one can expect when it is provided with label-only information. We show that questionnaire participants judge the GPT-4 graphs as the most accurate in the evaluated categories, closely followed by knowledge graphs constructed by domain experts, with causal Machine Learning (ML) far behind. We use these results to highlight the important limitation of causal ML, which often produces causal graphs that violate common sense, affecting trust in them. However, we show that pairing GPT-4 with causal ML overcomes this limitation, resulting in graphical structures learnt from real data that align more closely with those identified by domain experts, compared to structures learnt by causal ML alone. Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.}
}
@article{SONG2024114983,
title = {Ontology-assisted GPT-based building performance simulation and assessment: Implementation of multizone airflow simulation},
journal = {Energy and Buildings},
volume = {325},
pages = {114983},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114983},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824010995},
author = {Jihwan Song and Sungmin Yoon},
keywords = {GPT, ChatGPT, Large language model (LLM), Building performance simulation (BPS), Building performance, Digital twins, Artificial intelligence, CONTAM},
abstract = {Building performance simulation (BPS) is crucial for building performance assessments across its lifecycle. However, the complexity of buildings and the iterative nature of simulation poses challenges, leading to high costs and low values. Previous studies focused on simplification, but did not fully utilize advanced simulation engines. Despite recent advancements, there is a lack of research on leveraging artificial intelligence (AI), specifically generative pre-trained transformer (GPT), for BPS. Therefore, this study proposes a GPT-based BPS system, enhancing simulation efficiency and value by integrating simulation engines and advanced data analytics in the GPT environment. The ontology for GPT-based BPS is also developed to enable comprehensive, reliable, informative BPS environments. Based on this framework, case studies were conducted for GPT-based multizone airflow network simulation in a high-rise residential building using CONTAM software. They demonstrate GPT’s capabilities in retrieving simulation data, visualizing results with data mining, answering questions based on building knowledge, checking compliance with design guidelines, and proposing design alternatives. Finally, this study emphasizes expert interventions with ontological engineering informatics to utilize strictly structured BPS engines.}
}
@article{ABUSALIH2024e25383,
title = {A systematic literature review of knowledge graph construction and application in education},
journal = {Heliyon},
volume = {10},
number = {3},
pages = {e25383},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e25383},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024014142},
author = {Bilal Abu-Salih and Salihah Alotaibi},
keywords = {Knowledge graphs, Knowledge graph construction, Education, Learning, Systematic literature review, Survey},
abstract = {In the dynamic landscape of modern education, the search for improved pedagogical methods, enriched learning experiences, and empowered educators remains a perpetual pursuit. In recent years, a remarkable technological innovation has asserted its dominance in education: Knowledge Graphs (KGs). These structured representations of knowledge are increasingly proving to be indispensable tools, fostering advancements driven by the growing recognition of their essential role in enriching personalised learning, curriculum design, concept mapping, and educational content recommendation systems. In this paper, a systematic literature review (SLR) has been conducted to comprehensively examine KG construction methodologies and their applications across five key domains in education. In each examined study, we highlight the specific KG functionalities, knowledge extraction techniques, knowledge base characteristics, resource requirements, evaluation criteria, and limitations. This paper distinguishes itself by offering a broad overview of KGs in education, analyzing state-of-the-art methodologies, and identifying research gaps and limitations, paving the way for future advancements.}
}
@article{ROMANO2024,
title = {The Alzheimer’s Knowledge Base: A Knowledge Graph for Alzheimer Disease Research},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/46777},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124001857},
author = {Joseph D Romano and Van Truong and Rachit Kumar and Mythreye Venkatesan and Britney E Graham and Yun Hao and Nick Matsumoto and Xi Li and Zhiping Wang and Marylyn D Ritchie and Li Shen and Jason H Moore},
keywords = {Alzheimer disease, knowledge graph, knowledge base, artificial intelligence, drug repurposing, drug discovery, open source, Alzheimer, etiology, heterogeneous graph, therapeutic targets, machine learning, therapeutic discovery},
abstract = {Background
As global populations age and become susceptible to neurodegenerative illnesses, new therapies for Alzheimer disease (AD) are urgently needed. Existing data resources for drug discovery and repurposing fail to capture relationships central to the disease’s etiology and response to drugs.
Objective
We designed the Alzheimer’s Knowledge Base (AlzKB) to alleviate this need by providing a comprehensive knowledge representation of AD etiology and candidate therapeutics.
Methods
We designed the AlzKB as a large, heterogeneous graph knowledge base assembled using 22 diverse external data sources describing biological and pharmaceutical entities at different levels of organization (eg, chemicals, genes, anatomy, and diseases). AlzKB uses a Web Ontology Language 2 ontology to enforce semantic consistency and allow for ontological inference. We provide a public version of AlzKB and allow users to run and modify local versions of the knowledge base.
Results
AlzKB is freely available on the web and currently contains 118,902 entities with 1,309,527 relationships between those entities. To demonstrate its value, we used graph data science and machine learning to (1) propose new therapeutic targets based on similarities of AD to Parkinson disease and (2) repurpose existing drugs that may treat AD. For each use case, AlzKB recovers known therapeutic associations while proposing biologically plausible new ones.
Conclusions
AlzKB is a new, publicly available knowledge resource that enables researchers to discover complex translational associations for AD drug discovery. Through 2 use cases, we show that it is a valuable tool for proposing novel therapeutic hypotheses based on public biomedical knowledge.}
}
@article{QU20243583,
title = {A Review of Knowledge Graph in Traditional Chinese Medicine: Analysis, Construction, Application and Prospects},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {3583-3616},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.055671},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008294},
author = {Xiaolong Qu and Ziwei Tian and Jinman Cui and Ruowei Li and Dongmei Li and Xiaoping Zhang},
keywords = {Systematic review, traditional Chinese medicine, knowledge graph, deep learning, medical applications},
abstract = {As an advanced data science technology, the knowledge graph systematically integrates and displays the knowledge framework within the field of traditional Chinese medicine (TCM). This not only contributes to a deeper comprehension of traditional Chinese medical theories but also provides robust support for the intelligent decision systems and medical applications of TCM. Against this backdrop, this paper aims to systematically review the current status and development trends of TCM knowledge graphs, offering theoretical and technical foundations to facilitate the inheritance, innovation, and integrated development of TCM. Firstly, we introduce the relevant concepts and research status of TCM knowledge graphs. Secondly, we conduct an in-depth analysis of the challenges and trends faced by key technologies in TCM knowledge graph construction, such as knowledge representation, extraction, fusion, and reasoning, and classifies typical knowledge graphs in various subfields of TCM. Next, we comprehensively outline the current medical applications of TCM knowledge graphs in areas such as information retrieval, diagnosis, question answering, recommendation, and knowledge mining. Finally, the current research status and future directions of TCM knowledge graphs are concluded and discussed. We believe this paper contributes to a deeper understanding of the research dynamics in TCM knowledge graphs and provides essential references for scholars in related fields.}
}
@article{FABRE2024264,
title = {Knowledge Graphs – The Future of Integration in CRIS Systems for Uses of Assistance to Scientific Reasoning},
journal = {Procedia Computer Science},
volume = {249},
pages = {264-279},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032836},
author = {Renaud Fabre and Otmane Azeroual},
keywords = {Knowledge graphs, data integration, knowledge representation, CRIS systems, semantic data model, scientific uses, comprehensive information},
abstract = {Knowledge graphs (KGs) are gaining prominence for their efficacy in data integration and knowledge representation within Current Research Information Systems (CRIS) Systems. By employing a semantic data model to represent entities, attributes, and their relationships, they prove versatile across scientific applications. In the era of Science platformization, particularly within CRIS systems, KGs serve to amalgamate diverse data sources and formats, facilitating the creation of interconnected data models. This enables stakeholders to access comprehensive, consistent information pertinent to their endeavors. This paper examines the pivotal roles of KGs in current and future data integration within CRIS systems, emphasizing their contributions to scientific reasoning. While their benefits include flexible knowledge modeling, support for semantic queries, and interoperability with various data sources, they face systemic limitations, particularly in methodological and technological aspects, hindering classical scientific investigations. The paper underscores the necessity for novel approaches to address these limitations, offering insights, use cases, and best practices for implementing KGs in CRIS systems. This paves the way for research institutions and scientific organizations to enhance their data analytics capabilities and support scientific reasoning effectively.}
}
@article{JALDI2025100857,
title = {Education in the era of Neurosymbolic AI},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100857},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100857},
url = {https://www.sciencedirect.com/science/article/pii/S157082682400043X},
author = {Chris Davis Jaldi and Eleni Ilkou and Noah Schroeder and Cogan Shimizu},
keywords = {Education, Knowledge graphs, Large language models, Neurosymbolic AI, Agents},
abstract = {Education is poised for a transformative shift with the advent of neurosymbolic artificial intelligence (NAI), which will redefine how we support deeply adaptive and personalized learning experiences. The integration of Knowledge Graphs (KGs) with Large Language Models (LLMs), a significant and popular form of NAI, presents a promising avenue for advancing personalized instruction via neurosymbolic educational agents. By leveraging structured knowledge, these agents can provide individualized learning experiences that align with specific learner preferences and desired learning paths, while also mitigating biases inherent in traditional AI systems. NAI-powered education systems will be capable of interpreting complex human concepts and contexts while employing advanced problem-solving strategies, all grounded in established pedagogical frameworks. In this paper, we propose a system that leverages the unique affordances of KGs, LLMs, and pedagogical agents – embodied characters designed to enhance learning – as critical components of a hybrid NAI architecture. We discuss the rationale for our system design and the preliminary findings of our work. We conclude that education in the era of NAI will make learning more accessible, equitable, and aligned with real-world skills. This is an era that will explore a new depth of understanding in educational tools.}
}
@article{LU2024109395,
title = {Heterogeneous propagation graph convolution network for a recommendation system based on a knowledge graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109395},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109395},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624015537},
author = {Jiawei Lu and Jiapeng Li and Wenhui Li and Junfeng Song and Gang Xiao},
keywords = {Recommendation system, Knowledge graph, Heterogeneous propagation, Attention mechanism, Graph convolution network, Subgraph},
abstract = {Recently, there has been a surge of interest in recommendation systems that leverage knowledge graphs, primarily because of their effectiveness in addressing sparsity and cold-start challenges inherent in collaborative filtering approaches. In most previous studies, researchers have focused on the way knowledge associations are encoded in knowledge graphs, but have not sufficiently highlighted the signals of collaboration that are implicit in the interaction between users and items. As a result, the learned embeddings do not provide a complete representation of the semantic information. In this paper, we describe a new model called a heterogeneous propagation graph convolution network for a recommendation system combined with a knowledge graph (HP-GCN). It adopts a heterogeneous propagation to generate user embedding representations, thereby combining encoded collaborative signals and auxiliary knowledge in knowledge graphs. Furthermore, we incorporate an attention mechanism to differentiate the contributions made by diverse neighbors as opposed to those made by users. Since most graph convolutions tend to suffer from over-smoothing when the number of convolutional layers increases, leading to insufficient utilization of high-order information, this paper uses an improved graph convolution strategy to generate item embeddings. This strategy has two different aggregation mechanisms embedding into different subgraphs, which can more fully utilize high-order information and mitigate the over-smoothing problem. Thus, we are able to efficiently prevent negative information originating from higher-order neighbors into the process of embedding learning. In extensive experiments, we applied HP-GCN to four large-scale real datasets for music, books, movies, and restaurants. The experimental outcomes revealed that HP-GCN generally surpassed the baseline methods in both recommendation accuracy and diversity, showing superior recommendation performance overall.}
}
@article{SIDDHARTH2024112410,
title = {Retrieval augmented generation using engineering design knowledge},
journal = {Knowledge-Based Systems},
volume = {303},
pages = {112410},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112410},
url = {https://www.sciencedirect.com/science/article/pii/S095070512401044X},
author = {L. Siddharth and Jianxi Luo},
keywords = {Knowledge graphs, Retrieval-augmented generation, Large-language models, Engineering design knowledge, Patent documents, Graph neural networks},
abstract = {Aiming to support Retrieval Augmented Generation (RAG) in the design process, we present a method to identify explicit, engineering design facts – {head entity:: relationship:: tail entity} from patented artefact descriptions. Given a sentence with a pair of entities (selected from noun phrases) marked in a unique manner, our method extracts their relationship that is explicitly communicated in the sentence. For this task, we create a dataset of 375,084 examples and fine-tune language models for relation identification (token classification task) and relation elicitation (sequence-to-sequence task). The token classification approach achieves up to 99.7% accuracy. Upon applying the method to a domain of 4,870 fan system patents, we populate a knowledge base of over 2.93 million facts. Using this knowledge base, we demonstrate how Large Language Models (LLMs) are guided by explicit facts to synthesise knowledge and generate technical and cohesive responses when sought out for knowledge retrieval tasks in the design process.}
}
@article{DU2023110996,
title = {A contrastive framework for enhancing Knowledge Graph Question Answering: Alleviating exposure bias},
journal = {Knowledge-Based Systems},
volume = {280},
pages = {110996},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110996},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123007463},
author = {Huifang Du and Xixie Zhang and Meng Wang and Yunwen Chen and Daqi Ji and Jun Ma and Haofen Wang},
keywords = {Question answering, Knowledge graph, Semantic parsing, Contrastive learning, Sampling augmentation},
abstract = {Current encoder–decoders for Knowledge Graph Question Answering (KGQA) commonly utilize teacher-forcing training to accelerate convergence. However, this training approach limits the model’s exposure to ground truths, resulting in exposure bias that hampers generalization performance during autoregressive inference. To alleviate the issue, we propose a contrastive framework that enables the model to access a variety of positive and negative examples, thereby enhancing generalization. Firstly, we introduce a sampling augmentation strategy to construct contrastive samples, which can ensure explicit semantic consistency of positive pairs and inconsistency of negative pairs. Secondly, we augment the training process by incorporating “hard” negatives to enhance the contrastive objective, along with augmented positives to improve the generation objective. Finally, we also sample multiple logical forms for each question during the inference to reduce the bias potential and train a contrastive ranking model to obtain the target logical form. We achieve improvements of 1.95% and 1% over the previous state-of-the-art methods on the KQA Pro and OVERNIGHT benchmarks, respectively. Furthermore, our approach obtains competitive results on the WebQSP dataset. These findings validate the efficacy of our contrastive framework for advancing KGQA performance.}
}
@article{ZHAO2024114879,
title = {Generating Java code pairing with ChatGPT},
journal = {Theoretical Computer Science},
volume = {1021},
pages = {114879},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114879},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524004961},
author = {Zelong Zhao and Nan Zhang and Bin Yu and Zhenhua Duan},
keywords = {Code generation, Large language models, ChatGPT, Prompt engineering, Iterative prompting},
abstract = {The Large Language Models (LLMs) like ChatGPT 3.5 have created a new era of automatic code generation. However, the existing research primarily focuses on generating simple code based on datasets (such as HumanEval, etc.). Most of approaches pay less attention to complex and practical code generation. Therefore, in this paper, we propose a new approach called “Xd-CodeGen” which can be used to generate large scale Java code. This approach is composed of four phases: requirement analysis, modeling, code generation, and code verification. In the requirement analysis phase, ChatGPT 3.5 is utilized to decompose and restate user requirements. To do so, a knowledge graph is developed to describe entities and their relationship in detail. Further, Propositional Projection Temporal Logic (PPTL) formulas are employed to define the properties of requirements. In the modeling phase, we use knowledge graphs to enhance prompts and generate UML class and activity diagrams for each sub-requirement using ChatGPT 3.5. In the code generation phase, based on established UML models, we make use of prompt engineering and knowledge graph to generate Java code. In the code verification phase, a runtime verification at code level approach is employed to verify generated Java code. Finally, we apply the proposed approach to develop a practical Java web project.}
}
@article{BIAN2024104703,
title = {Call for papers: Special issue on biomedical multimodal large language models − novel approaches and applications},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104703},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104703},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001217},
author = {Jiang Bian and Yifan Peng and Eneida Mendonca and Imon Banerjee and Hua Xu and Hong Sun and Ye Ye and Casey {Overby Taylor} and Anália {Maria Garcia Lourenço} and Alejandro {Rodríguez González} and Elena Tutubalina}
}
@article{PAEK2024S18,
title = {CO14 Profiling Adverse Events in Multiple Myeloma: Insights from Clinical Trials Via Large Language Models},
journal = {Value in Health},
volume = {27},
number = {6, Supplement },
pages = {S18},
year = {2024},
note = {ISPOR Abstracts 2024},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2024.03.103},
url = {https://www.sciencedirect.com/science/article/pii/S1098301524002183},
author = {H. Paek and K. Lee and S. Datta and LC. Huang and J. Higashi and N. Ofoegbu and L. He and B. Lin and J. Wang and X. Wang}
}
@article{YANG2024100465,
title = {ChatDiet: Empowering personalized nutrition-oriented food recommender chatbots through an LLM-augmented framework},
journal = {Smart Health},
volume = {32},
pages = {100465},
year = {2024},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2024.100465},
url = {https://www.sciencedirect.com/science/article/pii/S2352648324000217},
author = {Zhongqi Yang and Elahe Khatibi and Nitish Nagesh and Mahyar Abbasian and Iman Azimi and Ramesh Jain and Amir M. Rahmani},
keywords = {Large Language Model, Personalization, Explainability, Interactivity, Chatbots, Recommender systems, Causal reasoning, Nutrition, Food},
abstract = {The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The personal model leverages causal discovery and inference techniques to assess personalized nutritional effects for a specific user, whereas the population model provides generalized information on food nutritional content. The orchestrator retrieves, synergizes and delivers the output of both models to the LLM, providing tailored food recommendations designed to support targeted health outcomes. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessments, including a food recommendation test showcasing a 92% effectiveness rate, coupled with illustrative dialogue examples, underscore ChatDiet’s strengths in explainability, personalization, and interactivity.}
}
@article{BABAIHA2023100078,
title = {A natural language processing system for the efficient updating of highly curated pathophysiology mechanism knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {4},
pages = {100078},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2667318523000223},
author = {Negin Sadat Babaiha and Hassan Elsayed and Bide Zhang and Abish Kaladharan and Priya Sethumadhavan and Bruce Schultz and Jürgen Klein and Bruno Freudensprung and Vanessa Lage-Rupprecht and Alpha Tom Kodamullil and Marc Jacobs and Stefan Geissler and Sumit Madan and Martin Hofmann-Apitius},
keywords = {Knowledge graphs, Relation extraction, Natural language processing, Biomedical text mining, Biological expression language (BEL), Human brain pharmacome (HBP)},
abstract = {Background
Biomedical knowledge graphs (KG) have become crucial for describing biological findings in a structured manner. To keep up with the constantly changing flow of knowledge, their embedded information must be regularly updated with the latest findings. Natural language processing (NLP) has created new possibilities for automating this upkeep by facilitating information extraction from free text. However, due to annotated and labeled biomedical data limitations, the development of completely autonomous information extraction systems remains a substantial scientific and technological hurdle. This study aims to explore methodologies best suited to support the automatic extraction of causal relationships from biomedical literature with the aim of regular and rapid updating of disease-specific pathophysiology mechanism KGs.
Methods
Our proposed approach first searches and retrieves PubMed abstracts using the desired terms and keywords. The extension corpora are then passed through the NLP pipeline for automatic information extraction. We then identify triples representing cause-and-effect relationships and encode this content using the Biological Expression Language (BEL). Finally, domain experts perform an analysis of the completeness, relevance, accuracy, and novelty of the extracted triples.
Results
In our test scenario, which is focused on the KG regarding the phosphorylation of the Tau protein, our pipeline successfully contributed novel data, which was then subsequently used to update the KG leading to the identification of six additional upstream regulators of Tau phosphorylation.
Conclusion
Here, it is demonstrated that the NLP-based workflow we created is capable of rapidly updating pathophysiology mechanism graphs. As a result, production-scale, semi-automated updating of pre-existing, curated mechanism graphs is enabled.}
}
@article{LIMA2024108464,
title = {The new chassis in the flask: Advances in Vibrio natriegens biotechnology research},
journal = {Biotechnology Advances},
volume = {77},
pages = {108464},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108464},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024001587},
author = {Matthew Lima and Charandatta Muddana and Zhengyang Xiao and Anindita Bandyopadhyay and Pramod P. Wangikar and Himadri B. Pakrasi and Yinjie J. Tang},
keywords = {Biomanufacturing, Synthetic biology, Systems biology, Genome scale model, Knowledge graph, Large language model},
abstract = {Biotechnology has been built on the foundation of a small handful of well characterized and well-engineered organisms. Recent years have seen a breakout performer gain attention as a new entrant into the bioengineering toolbox: Vibrio natriegens. This review covers recent research efforts into making V. natriegens a biotechnology platform, using a large language model (LLM) and knowledge graph to expedite the literature survey process. Scientists have made advancements in research pertaining to the fundamental metabolic characteristics of V. natriegens, development and characterization of synthetic biology tools, systems biology analysis and metabolic modeling, bioproduction and metabolic engineering, and microbial ecology. Each of these subcategories has relevance to the future of V. natriegens for bioengineering applications. In this review, we cover these recent advancements and offer context for the impact they may have on the field, highlighting benefits and drawbacks of using this organism. From examining the recent bioengineering research, it appears that V. natriegens is on the precipice of becoming a platform bacterium for the future of biotechnology.}
}
@article{HERNANDEZRAMIREZ2024414,
title = {The Future End of Design Work: A Critical Overview of Managerialism, Generative AI, and the Nature of Knowledge Work, and Why Craft Remains Relevant},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {10},
number = {4},
pages = {414-440},
year = {2024},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000960},
author = {Rodrigo Hernández-Ramírez and João Batalheiro Ferreira},
keywords = {creativity, design work, generative artificial intelligence (GenAI), knowledge work, managerialism},
abstract = {This article examines the transformation of design work under the influence of managerialism and the rise of Generative Artificial Intelligence (GenAI). Drawing on John Maynard Keynes’s projections of technological unemployment and the evolving nature of work, it argues that despite advancements in automation, work has not diminished but rather devalued. Design, understood as a type of knowledge work, faces an apparent existential crisis. GenAI grows adept at mimicking the output of creative processes. The article explores how the fear of the end of design work fueled by the rise of GenAI is rooted in a misunderstanding of design work. This misunderstanding is driven by managerialism—an ideology that prioritizes efficiency and quantifiable outcomes over the intrinsic value of work. Managerialism seeks to instrumentalize and automate design, turning it into a controllable procedure to generate quantifiable creative outputs. The article argues why design work cannot be turned into a procedure and automated using GenAI. Advocates of these systems claim they enhance productivity and open new opportunities. However, evidence so far shows that flawed GenAI models produce disappointing outcomes while operating at a significant environmental cost. The article concludes by arguing for a robust theory of design—one that acknowledges the unique ontological and epistemic boundaries of design work and underscores why design cannot be reduced to a procedural output.}
}
@article{YANG2023101200,
title = {API comparison knowledge extraction via prompt-tuned language model},
journal = {Journal of Computer Languages},
volume = {75},
pages = {101200},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101200},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000102},
author = {Yangrui Yang and Yaping Zhu and Sisi Chen and Pengpeng Jian},
keywords = {Knowledge extraction, API entity, Semantic relation, Joint extraction},
abstract = {Application Programming Interfaces (APIs) are frequent in software engineering domain texts, such as API references and Stack Overflow. These APIs and the comparison knowledge between them are not only important for solving programming issues (e.g., question answering), but they are also organized into structured knowledge to support many software engineering tasks (e.g., API misuse detection). As a result, extracting API comparison knowledge (API entities and semantic relations) from texts is essential. Existing rule-based and sequence labeling-based approaches must manually enumerate all linguistic patterns or label a large amount of data. Therefore, they involve a significant labor overhead and are exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, we formulates heterogeneous API extraction and API relation extraction tasks as a sequence-to-sequence generation task. It proposes APICKnow, an API entity-relation joint extraction model based on the large language model. To improve our model’s performance and quick learning ability, we adopt the prompt learning method to stimulate APICKnow to recognize API entities and relations. We systematically evaluate APICKnow on a set of sentences from Stack Overflow. The experimental results show that APICKnow can outperform the state-of-the-art baselines, and APICKnow has a quick learning ability and strong generalization ability.}
}
@article{ZHENG2025126313,
title = {A topic model-based knowledge graph to detect product defects from social media data},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126313},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126313},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031804},
author = {Lu Zheng and Zhen He and Shuguang He},
keywords = {Defect detection, Knowledge graph, Social media data, Topic model},
abstract = {With the help of topic models, social media data offers valuable insights for manufacturers to detect product defects in the after-sales stage. However, topic models struggle with texts mentioning multiple defects or discussing them at a coarse granularity level. Low topic discrimination further limits the application of topic models in defect discovery. To address these problems, we introduce a topic model-based Defect Knowledge Graph (DKG) for accurate defect detection. Firstly, to address the topic-indiscriminative problem, we utilize a topic model named Defect Latent Dirichlet Allocation and an improved Gibbs sampling to extract defect information from multi-source data and construct DKG. Secondly, we establish the Product Component Knowledge Graph (PCKG) to identify multiple defects discussed at coarse granularity levels. Thirdly, with DKG and PCKG, we unveil product defects and related defect information from social media data. Case studies of automobiles and laptops are used for validation. Experimental results show that our method outperforms the state-of-the-art in product defect discovery and provides more comprehensive defect information, which facilitates manufacturers to take prompt remedial actions.}
}
@article{ZHU2025106995,
title = {Knowledge graph based question-answering model with subgraph retrieval optimization},
journal = {Computers & Operations Research},
volume = {177},
pages = {106995},
year = {2025},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2025.106995},
url = {https://www.sciencedirect.com/science/article/pii/S0305054825000231},
author = {Rui Zhu and Bo Liu and Qiuyu Tian and Ruwen Zhang and Shengxiang Zhang and Yanna Hu and Jiuxin Cao},
keywords = {Subgraph retrieval, Entity disambiguation, Intelligent question-answering},
abstract = {Knowledge graph-based question answering (QA) is a critical domain within natural language processing, aimed at delivering precise and efficient responses to user queries. Current research predominantly focuses on minimizing subgraph sizes to enhance the efficiency and compactness of the search space. However, natural language queries often exhibit ambiguities, and merely reducing subgraph sizes may overlook relevant answer entities. Additionally, redundant relationships among entities in the knowledge graph can adversely affect QA model performance. To address these limitations, this paper introduces a novel QA model that optimizes subgraph retrieval. The proposed model enhances entity linking and subgraph retrieval by leveraging contextual features from both questions and entities. It disambiguates entities using relevant contextual features and refines the search process through entity relation merging and entity ranking strategies. This methodology improves entity recognition and linking, reduces subgraph dimensions, and broadens answer coverage, resulting in substantial improvements in QA performance. Experimental results on the CCKS2019-CKBQA dataset demonstrate the modelś effectiveness, showing an average F1 score improvement of 2.99% over the leading baseline model. Furthermore, the model’s application in the field of ocean engineering underscores its practical utility and significance.}
}
@article{ALBAYRAK2025100409,
title = {Enhancing human phenotype ontology term extraction through synthetic case reports and embedding-based retrieval: A novel approach for improved biomedical data annotation},
journal = {Journal of Pathology Informatics},
volume = {16},
pages = {100409},
year = {2025},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2024.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2153353924000488},
author = {Abdulkadir Albayrak and Yao Xiao and Piyush Mukherjee and Sarah S. Barnett and Cherisse A. Marcou and Steven N. Hart},
keywords = {Human phenotype ontology, PhenoTagger, Vector embeddings},
abstract = {With the increasing utilization of exome and genome sequencing in clinical and research genetics, accurate and automated extraction of human phenotype ontology (HPO) terms from clinical texts has become imperative. Traditional methods for HPO term extraction, such as PhenoTagger, often face limitations in coverage and precision. In this study, we propose a novel approach that leverages large language models (LLMs) to generate synthetic sentences with clinical context, which were semantically encoded into vector embeddings. These embeddings are linked to HPO terms, creating a robust knowledgebase that facilitates precise information retrieval. Our method circumvents the known issue of LLM hallucinations by storing and querying these embeddings within a true database, ensuring accurate context matching without the need for a predictive model. We evaluated the performance of three different embedding models, all of which demonstrated substantial improvements over PhenoTagger. Top recall (sensitivity), precision (positive-predictive value, PPV), and F1 are 0.64, 0.64, and 0.64, respectively, which were 31%, 10%, and 21% better than PhenoTagger. Furthermore, optimal performance was achieved when we combined the best performing embedding model with PhenoTagger (a.k.a. Fused model), resulting in recall (sensitivity), precision (PPV), and F1 values of 0.7, 0.7, and 0.7, respectively, which are 10%, 10%, and 10% better than the best embedding models. Our findings underscore the potential of this integrated approach to enhance the precision and reliability of HPO term extraction, offering a scalable and effective solution for biomedical data annotation.}
}
@article{LIU2025126562,
title = {A label knowledge graph powered multi-task framework for crowdsourcing and mobile crowd sensing tasks},
journal = {Expert Systems with Applications},
volume = {270},
pages = {126562},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126562},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001848},
author = {Yimeng Liu and Zhiwen Yu and Nuo Li and Bin Guo and Sumi Helal},
keywords = {Heterogeneous label knowledge graph, Multi-task Hybrid Reasoning, Crowdsourcing, Mobile crowd sensing tasks},
abstract = {Crowdsourcing and Mobile Crowd Sensing (MCS) platforms have revolutionized data collection, harnessing the collective intelligence of crowdsourced sensing. Accurately classifying and extracting core information such as heterogeneous sensors in MCS tasks plays a key role in the platform execution efficiency. However, existing methods struggle with extracting pivotal information from task descriptions that are open-domain, implicitly expressed, and linguistically diverse, ultimately hindering the efficiency of task assignment and execution. To overcome these challenges, we propose LKG-MF, a Label Knowledge Graph-powered Multi-task Framework, to achieve better core information mining performance in crowdsourcing and mobile crowd sensing tasks. Specifically, we first construct an MCS task dataset comprising over 10,000 real tasks from 7 platforms. Then we devise a label knowledge graph to capture heterogeneous semantics and relationships among labels and enhance label representation. Further, we present a multi-granularity feature extraction network to capture precise task-specific features. To optimize performance across disparate tasks, we incorporate a task-adaptive loss function that adeptly balances their optimization rates. Experimental results show that LKG-MF outperforms baselines average by 2.3%, significantly improving multi-task classification accuracy. Notably, when we integrate the LKG-MF model into MCS platforms, the task assignment efficiency is improved by 38.6% and the task completion time is reduced by 45.1%, which demonstrates the practical impact and effectiveness of our model in improving the performance of MCS platforms.}
}
@article{XU2025103976,
title = {Advancing rule learning in knowledge graphs with structure-aware graph transformer},
journal = {Information Processing & Management},
volume = {62},
number = {2},
pages = {103976},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103976},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003352},
author = {Kang Xu and Miqi Chen and Yifan Feng and Zhenjiang Dong},
keywords = {Rule learning, Knowledge graph reasoning, Graph neural networks},
abstract = {In knowledge graphs (KGs), logic rules offer interpretable explanations for predictions and are essential for reasoning on downstream tasks, such as question answering. However, a key challenge remains unresolved: how to effectively encode and utilize the structural features around the head entity to generate the most applicable rules. This paper proposes a structure-aware graph transformer for rule learning, namely Structure-Aware Rule Learning (SARL), which leverages both local and global structural information of the subgraph around the head entity to generate the most suitable rule path. SARL employs a generalized attention mechanism combined with replaceable feature extractors to aggregate local structural information of entities. It then incorporates global structural and relational information to further model the subgraph structure. Finally, a rule decoder utilizes the comprehensive subgraph representation to generate the most appropriate rules. Comprehensive experiments on four real-world knowledge graph datasets reveal that SARL significantly enhances performance and surpasses existing methods in the link prediction task on large-scale KGs, with Hits@1 improvements of 6.5% on UMLS and 4.5% on FB15K-237.}
}
@article{MANTLE2024125115,
title = {Querying large-scale knowledge graphs using Qualitative Spatial Reasoning},
journal = {Expert Systems with Applications},
volume = {258},
pages = {125115},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125115},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424019821},
author = {Matthew Mantle and Sotirios Batsakis and Grigoris Antoniou},
keywords = {Spatial reasoning, RCC, Knowledge graphs, Spatial queries, Distributed computing, GeoSPARQL},
abstract = {In this paper we consider how Qualitative Spatial Reasoning (QSR) can be used to answer queries over large-scale knowledge graphs such as YAGO and DBPedia. We describe the challenges associated with spatially querying knowledge graphs such as point based representations, sparsity of qualitative relations, and scale. We address these challenges and present a query engine, Parallel Qualitative Reasoner-Query Engine (ParQR-QE), that uses a novel distributed qualitative spatial reasoning algorithm to provide answers to GeoSPARQL queries. An experimental evaluation using a range of different query types and the YAGO knowledge graph shows the advantages of QSR techniques in comparison to purely quantitative approaches.}
}
@article{GANGEMI2025100859,
title = {Logic Augmented Generation},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100859},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100859},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000453},
author = {Aldo Gangemi and Andrea Giovanni Nuzzolese},
keywords = {Knowledge graphs, Large language models, Logic augmented generation},
abstract = {Semantic Knowledge Graphs (SKG) face challenges with scalability, flexibility, contextual understanding, and handling unstructured or ambiguous information. However, they offer formal and structured knowledge enabling highly interpretable and reliable results by means of reasoning and querying. Large Language Models (LLMs) may overcome those limitations, making them suitable in open-ended tasks and unstructured environments. Nevertheless, LLMs are hardly interpretable and often unreliable. To take the best out of LLMs and SKGs, we envision Logic Augmented Generation (LAG) to combine the benefits of the two worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate potentially infinite relations and tacit knowledge on-demand. LAG uses SKGs to inject a discrete heuristic dimension with clear logical and factual boundaries. We exemplify LAG in two tasks of collective intelligence, i.e., medical diagnostics and climate projections. Understanding the properties and limitations of LAG, which are still mostly unknown, is of utmost importance for enabling a variety of tasks involving tacit knowledge in order to provide interpretable and effective results.}
}
@article{ROUMELIOTIS2024100056,
title = {LLMs in e-commerce: A comparative analysis of GPT and LLaMA models in product review evaluation},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100056},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000049},
author = {Konstantinos I. Roumeliotis and Nikolaos D. Tselikas and Dimitrios K. Nasiopoulos},
keywords = {Sentiment analysis, LLMs, Instruction tuning, GPT model, LLaMA model, LLM fine-tuning},
abstract = {E-commerce has witnessed remarkable growth, especially following the easing of COVID-19 restrictions. Many people, who were initially hesitant about online shopping, have now embraced it, while existing online shoppers increasingly prefer the convenience of e-commerce. This surge in e-commerce has prompted the implementation of automated customer service processes, incorporating innovations such as chatbots and AI-driven sales. Despite this growth, customer satisfaction remains vital for E-commerce sustainability. Data scientists have made progress in utilizing machine learning to assess satisfaction levels but struggled to understand emotions within product reviews’ context. The recent AI revolution, marked by the release of powerful Large Language Models (LLMs) to the public, has brought us closer than ever before to understanding customer sentiment. This study aims to illustrate the effectiveness of LLMs by conducting a comparative analysis of two cutting-edge LLMs, GPT-3.5 and LLaMA-2, along with two additional Natural Language Process (NLP) models, BERT and RoBERTa. We evaluate the performance of these models before and after fine-tuning them specifically for product review sentiment analysis. The primary objective of this research is to determine if these specific LLMs, could contribute to understanding customer satisfaction within the context of an e-commerce environment. By comparing the effectiveness of these models, we aim to uncover insights into the potential impact of LLMs on customer satisfaction analysis and enhance our understanding of their capabilities in this particular context.}
}
@article{FUELLEN2025102617,
title = {Validation requirements for AI-based intervention-evaluation in aging and longevity research and practice},
journal = {Ageing Research Reviews},
volume = {104},
pages = {102617},
year = {2025},
issn = {1568-1637},
doi = {https://doi.org/10.1016/j.arr.2024.102617},
url = {https://www.sciencedirect.com/science/article/pii/S1568163724004355},
author = {Georg Fuellen and Anton Kulaga and Sebastian Lobentanzer and Maximilian Unfried and Roberto A. Avelar and Daniel Palmer and Brian K. Kennedy},
keywords = {Longevity, Preventive Medicine, Large Language Models},
abstract = {The field of aging and longevity research is overwhelmed by vast amounts of data, calling for the use of Artificial Intelligence (AI), including Large Language Models (LLMs), for the evaluation of geroprotective interventions. Such evaluations should be correct, useful, comprehensive, explainable, and they should consider causality, interdisciplinarity, adherence to standards, longitudinal data and known aging biology. In particular, comprehensive analyses should go beyond comparing data based on canonical biomedical databases, suggesting the use of AI to interpret changes in biomarkers and outcomes. Our requirements motivate the use of LLMs with Knowledge Graphs and dedicated workflows employing, e.g., Retrieval-Augmented Generation. While naive trust in the responses of AI tools can cause harm, adding our requirements to LLM queries can improve response quality, calling for benchmarking efforts and justifying the informed use of LLMs for advice on longevity interventions.}
}
@article{NAZI2025100124,
title = {Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100124},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000724},
author = {Zabir Al Nazi and Md. Rajib Hossain and Faisal Al Mamun},
keywords = {Large language models, Zero-shot, Few-shot, Chain-of-thought, GPT-4, Llama 3, Ablation studies, Prompting, LLM reasoning, Low-resource, Bangla},
abstract = {As the global deployment of Large Language Models (LLMs) increases, the demand for multilingual capabilities becomes more crucial. While many LLMs excel in real-time applications for high-resource languages, few are tailored specifically for low-resource languages. The limited availability of text corpora for low-resource languages, coupled with their minimal utilization during LLM training, hampers the models’ ability to perform effectively in real-time applications. Additionally, evaluations of LLMs are significantly less extensive for low-resource languages. This study offers a comprehensive evaluation of both open-source and closed-source multilingual LLMs focused on low-resource language like Bengali, a language that remains notably underrepresented in computational linguistics. Despite the limited number of pre-trained models exclusively on Bengali, we assess the performance of six prominent LLMs, i.e., three closed-source (GPT-3.5, GPT-4o, Gemini) and three open-source (Aya 101, BLOOM, LLaMA) across key natural language processing (NLP) tasks, including text classification, sentiment analysis, summarization, and question answering. These tasks were evaluated using three prompting techniques: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT). This study found that the default hyperparameters of these pre-trained models, such as temperature, maximum token limit, and the number of few-shot examples, did not yield optimal outcomes and led to hallucination issues in many instances. To address these challenges, ablation studies were conducted on key hyperparameters, particularly temperature and the number of shots, to optimize Few-Shot learning and enhance model performance. The focus of this research is on understanding how these LLMs adapt to low-resource downstream tasks, emphasizing their linguistic flexibility and contextual understanding. Experimental results demonstrated that the closed-source GPT-4o model, utilizing Few-Shot learning and Chain-of-Thought prompting, achieved the highest performance across multiple tasks: an F1 score of 84.54% for text classification, 99.00% for sentiment analysis, a F1bert score of 72.87% for summarization, and 58.22% for question answering. For transparency and reproducibility, all methodologies and code from this study are available on our GitHub repository: https://github.com/zabir-nabil/bangla-multilingual-llm-eval.}
}
@article{WU2024,
title = {Knowledge-Empowered, Collaborative, and Co-Evolving AI Models: The Post-LLM Roadmap},
journal = {Engineering},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924007239},
author = {Fei Wu and Tao Shen and Thomas Bäck and Jingyuan Chen and Gang Huang and Yaochu Jin and Kun Kuang and Mengze Li and Cewu Lu and Jiaxu Miao and Yongwei Wang and Ying Wei and Fan Wu and Junchi Yan and Hongxia Yang and Yi Yang and Shengyu Zhang and Zhou Zhao and Yueting Zhuang and Yunhe Pan},
keywords = {Artificial intelligence, Large language models, Knowledge empowerment, Model collaboration, Model co-evolution},
abstract = {Large language models (LLMs) have significantly advanced artificial intelligence (AI) by excelling in tasks such as understanding, generation, and reasoning across multiple modalities. Despite these achievements, LLMs have inherent limitations including outdated information, hallucinations, inefficiency, lack of interpretability, and challenges in domain-specific accuracy. To address these issues, this survey explores three promising directions in the post-LLM era: knowledge empowerment, model collaboration, and model co-evolution. First, we examine methods of integrating external knowledge into LLMs to enhance factual accuracy, reasoning capabilities, and interpretability, including incorporating knowledge into training objectives, instruction tuning, retrieval-augmented inference, and knowledge prompting. Second, we discuss model collaboration strategies that leverage the complementary strengths of LLMs and smaller models to improve efficiency and domain-specific performance through techniques such as model merging, functional model collaboration, and knowledge injection. Third, we delve into model co-evolution, in which multiple models collaboratively evolve by sharing knowledge, parameters, and learning strategies to adapt to dynamic environments and tasks, thereby enhancing their adaptability and continual learning. We illustrate how the integration of these techniques advances AI capabilities in science, engineering, and society—particularly in hypothesis development, problem formulation, problem-solving, and interpretability across various domains. We conclude by outlining future pathways for further advancement and applications.}
}
@article{HE2024,
title = {Physician Versus Large Language Model Chatbot Responses to Web-Based Questions From Autistic Patients in Chinese: Cross-Sectional Comparative Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54706},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124002164},
author = {Wenjie He and Wenyan Zhang and Ya Jin and Qiang Zhou and Huadan Zhang and Qing Xia},
keywords = {artificial intelligence, chatbot, ChatGPT, ERNIE Bot, autism},
abstract = {Background
There is a dearth of feasibility assessments regarding using large language models (LLMs) for responding to inquiries from autistic patients within a Chinese-language context. Despite Chinese being one of the most widely spoken languages globally, the predominant research focus on applying these models in the medical field has been on English-speaking populations.
Objective
This study aims to assess the effectiveness of LLM chatbots, specifically ChatGPT-4 (OpenAI) and ERNIE Bot (version 2.2.3; Baidu, Inc), one of the most advanced LLMs in China, in addressing inquiries from autistic individuals in a Chinese setting.
Methods
For this study, we gathered data from DXY—a widely acknowledged, web-based, medical consultation platform in China with a user base of over 100 million individuals. A total of 100 patient consultation samples were rigorously selected from January 2018 to August 2023, amounting to 239 questions extracted from publicly available autism-related documents on the platform. To maintain objectivity, both the original questions and responses were anonymized and randomized. An evaluation team of 3 chief physicians assessed the responses across 4 dimensions: relevance, accuracy, usefulness, and empathy. The team completed 717 evaluations. The team initially identified the best response and then used a Likert scale with 5 response categories to gauge the responses, each representing a distinct level of quality. Finally, we compared the responses collected from different sources.
Results
Among the 717 evaluations conducted, 46.86% (95% CI 43.21%-50.51%) of assessors displayed varying preferences for responses from physicians, with 34.87% (95% CI 31.38%-38.36%) of assessors favoring ChatGPT and 18.27% (95% CI 15.44%-21.10%) of assessors favoring ERNIE Bot. The average relevance scores for physicians, ChatGPT, and ERNIE Bot were 3.75 (95% CI 3.69-3.82), 3.69 (95% CI 3.63-3.74), and 3.41 (95% CI 3.35-3.46), respectively. Physicians (3.66, 95% CI 3.60-3.73) and ChatGPT (3.73, 95% CI 3.69-3.77) demonstrated higher accuracy ratings compared to ERNIE Bot (3.52, 95% CI 3.47-3.57). In terms of usefulness scores, physicians (3.54, 95% CI 3.47-3.62) received higher ratings than ChatGPT (3.40, 95% CI 3.34-3.47) and ERNIE Bot (3.05, 95% CI 2.99-3.12). Finally, concerning the empathy dimension, ChatGPT (3.64, 95% CI 3.57-3.71) outperformed physicians (3.13, 95% CI 3.04-3.21) and ERNIE Bot (3.11, 95% CI 3.04-3.18).
Conclusions
In this cross-sectional study, physicians’ responses exhibited superiority in the present Chinese-language context. Nonetheless, LLMs can provide valuable medical guidance to autistic patients and may even surpass physicians in demonstrating empathy. However, it is crucial to acknowledge that further optimization and research are imperative prerequisites before the effective integration of LLMs in clinical settings across diverse linguistic environments can be realized.
Trial Registration
Chinese Clinical Trial Registry ChiCTR2300074655; https://www.chictr.org.cn/bin/project/edit?pid=199432}
}
@article{HAQUE2025100356,
title = {Leveraging LLMs for optimised feature selection and embedding in structured data: A case study on graduate employment classification},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100356},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100356},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001590},
author = {Radiah Haque and Hui-Ngo Goh and Choo-Yee Ting and Albert Quek and M.D. Rakibul Hasan},
keywords = {Machine learning, Student employability prediction, Feature selection, Large language models, BERT, Tabular data},
abstract = {The application of Machine Learning (ML) for predicting graduate student employability is a growing area of research, driven by the need to align educational outcomes with job market requirements. In this context, this paper investigates the application of Large Language Models (LLMs) for tabular data transformation and embedding, specifically using Bidirectional Encoder Representations from Transformers (BERT), to enhance the performance of ML models in binary classification tasks for student employability prediction. The primary objective is to determine whether converting structured data into text format improves model accuracy. The study involves several ML models including Artificial Neural Networks (ANN), CatBoost, and BERT classifier. The focus is on predicting the employment status of graduate students based on demographic, academic, and graduate tracer study data, collected from over 4000 university graduates. Feature selection methods, including Boruta and Extra Tree Classifier (ETC) are employed to identify the optimal feature set, guided by a sliding window algorithm for automatic feature selection. The models are trained in four stages: 1) original dataset without feature selection or word embedding, 2) dataset with selected optimal features, 3) transformed data with word embedding, and 4) transformed data with feature selection applied both before and after word embedding. The baseline model (without feature selection and embedding) achieved the highest accuracy with the ANN model (79%). Subsequently, applying ETC for feature selection improved accuracy, with CatBoost achieving 83%. Further transformation with BERT-based embeddings raised the highest accuracy to 85% using the BERT classifier. Finally, the optimal accuracy of 88% was obtained by applying feature selection before and after embedding, with the BERT-Boruta model. The findings from this study demonstrate that using the dual-stage feature selection approach in combination with BERT embedding significantly increases the classification accuracy. This highlights the potential of LLMs in transforming tabular data for enhanced graduate employment prediction.}
}
@article{MA2025109117,
title = {f-KGQA: A fuzzy question answering system for knowledge graphs},
journal = {Fuzzy Sets and Systems},
volume = {498},
pages = {109117},
year = {2025},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109117},
url = {https://www.sciencedirect.com/science/article/pii/S016501142400263X},
author = {Ruizhe Ma and Yunxing Liu and Zongmin Ma},
keywords = {Knowledge graphs, Question answering, Fuzzy term, Fuzzy question answering system},
abstract = {The wide usage of large-scale knowledge graphs (KGs) motivates the development of user-friendly interfaces so that knowledge graphs become more readily accessible to a larger population. Natural language-based question answering (QA) systems are widely investigated and developed in the context of KGs, which can provide users with a natural means to retrieve the information they need from KGs without expecting them to know the query language. It is very common that natural language contains linguistic terms (fuzzy terms), and fuzzy (flexible) query has been widely investigated in the context of databases. This paper contributes a QA system with fuzzy terms over KGs called f-KGQA. f-KGQA can deal with different types of questions, including simple questions, complex questions, and questions with fuzzy terms. More importantly, users are provided with a channel to flexibly define their fuzzy terms based on their understanding. Our experimental results demonstrate the effectiveness and applicability of f-KGQA in handling questions with fuzzy terms.}
}
@article{PENG2025102729,
title = {Open knowledge graph completion with negative-aware representation learning and multi-source reliability inference},
journal = {Information Fusion},
volume = {115},
pages = {102729},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102729},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005074},
author = {Huang Peng and Weixin Zeng and Jiuyang Tang and Mao Wang and Hongbin Huang and Xiang Zhao},
keywords = {Knowledge graph completion, Representation learning, Truth inference, Source estimation},
abstract = {Multi-source data fusion is essential for building smart cities by providing a comprehensive and holistic understanding of urban environments. Specifically, smart city-oriented knowledge graphs (KGs) require supplementary information from other open sources to increase their completeness, thus better supporting downstream tasks for smart cities. Nevertheless, existing open knowledge graph completion (KGC) approaches often overlook source quality assessment and fail to fully utilize prior knowledge, which tend to yield less satisfying results. To fill in these gaps, in this work, we propose a new open KGC method with negative-aware representation learning and multi-source reliability inference, i.e., Nari, which can effectively integrate the multi-source data concerning sustainable cities, providing reliable knowledge for downstream tasks. Specifically, we first train a graph neural network based encoder with a novel negative sampling strategy to better characterize prior knowledge in KG, and then identify new facts based on the learned prior knowledge and source reliability. The experiments on both general benchmark and waterlogging benchmark pertaining to sustainable cities demonstrate the effectiveness and wide applicability of Nari.}
}
@article{SUN2024113506,
title = {Development of an intelligent design and simulation aid system for heat treatment processes based on LLM},
journal = {Materials & Design},
volume = {248},
pages = {113506},
year = {2024},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2024.113506},
url = {https://www.sciencedirect.com/science/article/pii/S0264127524008815},
author = {Yixiao Sun and Xusheng Li and Chao Liu and Xiaohu Deng and Wenyu Zhang and Jiangang Wang and Zeyu Zhang and Tengyang Wen and Tianyu Song and Dongying Ju},
keywords = {Metal material heat treatment, Expert knowledge system, Large language model, Knowledge embedding, Intelligent simulation system},
abstract = {Heat treatment of steel is a multi-physics coupled process. Designing programs that meet the desired results is challenging. The current design of processes relies on experience and experimentation, leading to high costs in developing processes and challenges in training practitioners. To reduce research and development costs in the industry and enable novices to reach expert levels, we propose an intelligent heat treatment process design and simulation assistant system based on large language models (LLMs), named Chat-IMSHT. Chat-IMSHT can impart knowledge and recommend processes. Additionally, Chat-IMSHT optimizes the interaction between humans and Computer Aided Engineering (CAE) software. To achieve knowledge impartation and process recommendation, a dialogue model based on Retrieval Augmented Generation (RAG) and LLMs was designed. It characterizes and compresses a massive amount of heat treatment knowledge and process data. The system designs a new CAE software interaction paradigm, using LLMs to map parameters from natural language into formatted text for the CAE software COSMAP. A Steel Heat Treatment Knowledge Understanding (SHTKU) evaluation method was designed. The improved model significantly increased the accuracy of knowledge responses, with a maximum accuracy of 94.54 %. Experimental results show that Chat-IMSHT effectively imparts knowledge and generates formatted text, completing the task of process recommendation.}
}
@article{YAN2024112684,
title = {Collaborate SLM and LLM with latent answers for event detection},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112684},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112684},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013182},
author = {Youcheng Yan and Jinshuo Liu and Donghong Ji and Jinguang Gu and Ahmed Abubakar Aliyu and Xinyan Wang and Jeff Z. Pan},
keywords = {Large language model, Small language model, Event detection, Latent answers},
abstract = {Event detection (ED) intends to identify events from text and classify them into predefined event types. One of the major issues for ED is the low-resource problem due to inadequate samples. Some studies address the low-resource issue with retrieving knowledge entries directly from knowledge bases while introducing a lot of irrelevant knowledge or failing the lookup. Moreover, recent work has attempted to employ large language models (LLMs, e.g., ChatGPT) that directly access event types in unstructured text under low-resource scenarios. Although LLM-based approaches have obtained promising results, we consider that the full potential of LLMs has not been activated due to insufficient prompt information. Our research proposes a two-stage event detection method that collaborates small language models (SLMs) and LLMs, namely LSLAED. Specifically, we first fine-tune the SLM to generate three types of latent answers: answer-aware examples, structure-aware examples, and corresponding answer candidates. Subsequently, all latent answers will form the prompt and enable the LLM to improve performance through in-context learning. We evaluate the proposed method using precision, recall, and F1-score as evaluation metrics. Experiments on the ACE2005 and ERE-EN datasets have demonstrated that LSLAED achieves significant improvement in both full-shot and few-shot scenarios.}
}
@article{WAN2024103,
title = {Making knowledge graphs work for smart manufacturing: Research topics, applications and prospects},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {103-132},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001572},
author = {Yuwei Wan and Ying Liu and Zheyuan Chen and Chong Chen and Xinyu Li and Fu Hu and Michael Packianather},
keywords = {Smart manufacturing, Industry 4.0, Knowledge graph, Semantic modelling under industry 4.0, Knowledge reasoning},
abstract = {Smart manufacturing (SM) confronts several challenges inherently suited to knowledge graphs (KGs) capabilities. The first key challenge lies in the synthesis of complex and varied data surrounding the manufacturing context, which demands advanced semantic analysis and inference capabilities. The second main limitation is the contextualization of manufacturing systems and the exploitation of manufacturing domain knowledge, which requires a dynamic and holistic representation of knowledge. The last major obstacle arises from the facilitation of intricate decision-making processes towards correlated manufacturing ecosystems, which benefit from interconnected data structures that KGs excel at organizing. However, the existing survey studies concentrated on distinct facets of SM and offered isolated insights into KG applications while overlooking the interconnections between various KG technologies and their application across multiple domains. What specific role KGs should play in SM towards the aforementioned challenges, how to effectively harness KGs for these challenges, and the essential topics and methodologies required to make KGs functional remain underexplored. To explore the potential of KGs in SM, this study adopts a systematic approach to investigate, evaluate, and analyse current research on KGs, identifying core advancements and their implications for future manufacturing practices. Firstly, cutting-edge developments in the challenge-driven roles of KGs and KG techniques are identified, from knowledge extraction and mining to techniques for KG construction and updates, further extending to KG embedding, fusion, and reasoning—central to driving SM ecosystems. Specifically, the KG technologies for SM are depicted holistically, emphasizing the interplay of diverse KG techniques with a comprehensive framework. Subsequently, this foundation outlines and discusses key application scenarios of KGs from engineering design to predictive maintenance, covering the main representative stages of the manufacturing life cycle. Lastly, this study explores the intricate interplay of the practical challenges and advantages of KGs in manufacturing systems, pointing to emerging research avenues.}
}
@article{ZENG2024121144,
title = {RuMER-RL: A hybrid framework for sparse knowledge graph explainable reasoning},
journal = {Information Sciences},
volume = {680},
pages = {121144},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121144},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524010582},
author = {Zefan Zeng and Qing Cheng and Yuehang Si and Zhong Liu},
keywords = {Knowledge graph reasoning, Rule mining, Embedding, Reinforcement learning, Interpretability},
abstract = {Knowledge Graph (KG) reasoning is a crucial technology for ensuring the accuracy and utility of KGs. However, robust and explainable reasoning on sparse KGs is challenging due to the lack of information and truncated paths. To address this issue, we introduce RuMER-RL, a hybrid reasoning framework comprising three modules: Rule Mining (RM), Embedding Representation (ER), and Reinforcement Learning (RL). The ER and RM modules collaborate to enhance the embedding models and rule quality, generating additional triples to mitigate the sparsity of the KG. The RL module models multi-hop KG reasoning as a Markov Decision Process (MDP), employing dynamic anticipation, action space expansion, and curiosity-driven strategies to enrich the reasoning process and mitigate sparsity. Additionally, we reshape the reward function by incorporating embedding representation, rule matching, and curiosity rewards to guide the training and optimization of the policy network. Extensive experiments on six sparse KG datasets demonstrate that RuMER-RL outperforms state-of-the-art models in terms of link prediction accuracy and interpretability.}
}
@article{NUNES2024,
title = {Health Care Language Models and Their Fine-Tuning for Information Extraction: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/60164},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001480},
author = {Miguel Nunes and Joao Bone and Joao C Ferreira and Luis B Elvas},
keywords = {language model, information extraction, healthcare, PRISMA-ScR, scoping literature review, transformers, natural language processing, European Portuguese},
abstract = {Background
In response to the intricate language, specialized terminology outside everyday life, and the frequent presence of abbreviations and acronyms inherent in health care text data, domain adaptation techniques have emerged as crucial to transformer-based models. This refinement in the knowledge of the language models (LMs) allows for a better understanding of the medical textual data, which results in an improvement in medical downstream tasks, such as information extraction (IE). We have identified a gap in the literature regarding health care LMs. Therefore, this study presents a scoping literature review investigating domain adaptation methods for transformers in health care, differentiating between English and non-English languages, focusing on Portuguese. Most specifically, we investigated the development of health care LMs, with the aim of comparing Portuguese with other more developed languages to guide the path of a non–English-language with fewer resources.
Objective
This study aimed to research health care IE models, regardless of language, to understand the efficacy of transformers and what are the medical entities most commonly extracted.
Methods
This scoping review was conducted using the PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) methodology on Scopus and Web of Science Core Collection databases. Only studies that mentioned the creation of health care LMs or health care IE models were included, while large language models (LLMs) were excluded. The latest were not included since we wanted to research LMs and not LLMs, which are architecturally different and have distinct purposes.
Results
Our search query retrieved 137 studies, 60 of which met the inclusion criteria, and none of them were systematic literature reviews. English and Chinese are the languages with the most health care LMs developed. These languages already have disease-specific LMs, while others only have general–health care LMs. European Portuguese does not have any public health care LM and should take examples from other languages to develop, first, general-health care LMs and then, in an advanced phase, disease-specific LMs. Regarding IE models, transformers were the most commonly used method, and named entity recognition was the most popular topic, with only a few studies mentioning Assertion Status or addressing medical lexical problems. The most extracted entities were diagnosis, posology, and symptoms.
Conclusions
The findings indicate that domain adaptation is beneficial, achieving better results in downstream tasks. Our analysis allowed us to understand that the use of transformers is more developed for the English and Chinese languages. European Portuguese lacks relevant studies and should draw examples from other non-English languages to develop these models and drive progress in AI. Health care professionals could benefit from highlighting medically relevant information and optimizing the reading of the textual data, or this information could be used to create patient medical timelines, allowing for profiling.}
}
@article{LI2025107108,
title = {Temporal multi-modal knowledge graph generation for link prediction},
journal = {Neural Networks},
volume = {185},
pages = {107108},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107108},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024010372},
author = {Yuandi Li and Hui Ji and Fei Yu and Lechao Cheng and Nan Che},
keywords = {Multimodal knowledge graph, Temporal knowledge graphs, Knowledge graph generation, Link prediction},
abstract = {Temporal Multi-Modal Knowledge Graphs (TMMKGs) can be regarded as a synthesis of Temporal Knowledge Graphs (TKGs) and Multi-Modal Knowledge Graphs (MMKGs), combining the characteristics of both. TMMKGs can effectively model dynamic real-world phenomena, particularly in scenarios involving multiple heterogeneous information sources and time series characteristics, such as e-commerce websites, scene recording data, and intelligent transportation systems. We propose a Temporal Multi-Modal Knowledge Graph Generation (TMMKGG) method that can automatically construct TMMKGs, aiming to reduce construction costs. To support this, we construct a dynamic Visual-Audio-Language Multimodal (VALM) dataset, which is particularly suitable for extracting structured knowledge in response to temporal multimodal perception data. TMMKGG explores temporal dynamics and cross-modal integration, enabling multimodal data processing for dynamic knowledge graph generation and utilizing alignment strategies to enhance scene perception. To validate the effectiveness of TMMKGG, we compare it with state-of-the-art dynamic graph generation methods using the VALM dataset. Furthermore, TMMKG exhibits a significant disparity in the ratio of newly introduced entities to their associated newly introduced edges compared to TKGs. Based on this phenomenon, we introduce a Temporal Multi-Modal Link Prediction (TMMLP) method, which outperforms existing state-of-the-art techniques.}
}
@article{SINGH2025100848,
title = {Knowledge graph based entity selection framework for ad-hoc retrieval},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100848},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100848},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000349},
author = {Pankaj Singh and Plaban Kumar Bhowmick},
keywords = {Information retrieval, Entity-based retrieval, Query expansion, Knowledge graph, Pseudo-relevance feedback},
abstract = {Recent entity-based retrieval models utilizing knowledge bases have shown significant improvement in ad-hoc retrieval. However, a lack of coherence between candidate entities can lead to query intent drift at retrieval time. To address this issue, we present an entity selection algorithm that utilizes a graph clustering framework to discover the semantics between entities and encompass the query with highly coherent entities accumulated from different resources, including knowledge bases, and pseudo-relevance feedback documents. Through this work, we propose: (1) An entity acquisition strategy to systematically acquire coherent entities for query expansion. (2) We propose a graph representation of entities to capture the coherence between entities where nodes correspond to the entities and edges represent semantic relatedness between entities. (3) We propose two different entity ranking approaches to select candidate entities based on the coherence with query entities and other coherent entities. A set of experiments on five TREC collections: ClueWeb09B, ClueWeb12B, Robust04, GOV2, and MS-Marco dataset under document retrieval task were conducted to verify the proposed algorithm’s performance. The reported results indicated that the proposed methodology outperforms existing state-of-the-art retrieval approaches in terms of MAP, NDCG, and P@20. The code and relevant data are available in https://github.com/pankajkashyap65/KnowledgeGraph.}
}
@article{XIAO202560,
title = {Network for knowledge Organization (NEKO): An AI knowledge mining workflow for synthetic biology research},
journal = {Metabolic Engineering},
volume = {87},
pages = {60-67},
year = {2025},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2024.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1096717624001484},
author = {Zhengyang Xiao and Himadri B. Pakrasi and Yixin Chen and Yinjie J. Tang},
keywords = {Foundation model, Large language model, Qwen, Retrieval augmented generation, Knowledge graph},
abstract = {Large language models (LLMs) can complete general scientific question-and-answer, yet they are constrained by their pretraining cut-off dates and lack the ability to provide specific, cited scientific knowledge. Here, we introduce Network for Knowledge Organization (NEKO), a workflow that uses LLM Qwen to extract knowledge through scientific literature text mining. When user inputs a keyword of interest, NEKO can generate knowledge graphs to link bioinformation entities and produce comprehensive summaries from PubMed search. NEKO significantly enhance LLM ability and has immediate applications in daily academic tasks such as education of young scientists, literature review, paper writing, experiment planning/troubleshooting, and new ideas/hypothesis generation. We exemplified this workflow's applicability through several case studies on yeast fermentation and cyanobacterial biorefinery. NEKO's output is more informative, specific, and actionable than GPT-4's zero-shot Q&A. NEKO offers flexible, lightweight local deployment options. NEKO democratizes artificial intelligence (AI) tools, making scientific foundation model more accessible to researchers without excessive computational power.}
}
@article{LI2024205469,
title = {Artificial general intelligence for the upstream geoenergy industry: A review},
journal = {Gas Science and Engineering},
volume = {131},
pages = {205469},
year = {2024},
issn = {2949-9089},
doi = {https://doi.org/10.1016/j.jgsce.2024.205469},
url = {https://www.sciencedirect.com/science/article/pii/S2949908924002656},
author = {Jimmy Xuekai Li and Tiancheng Zhang and Yiran Zhu and Zhongwei Chen},
keywords = {Artificial general intelligence (AGI), ChatGPT, Large language models (LLMs), Generative AI, Multimodal, Upstream geoenergy industry},
abstract = {Artificial General Intelligence (AGI) is set to profoundly impact the traditional upstream geoenergy industry (i.e., geothermal energy, oil and gas industry) by introducing unprecedented efficiencies and innovations. This paper explores AGI's foundational principles and its transformative applications, particularly focusing on the advancements brought about by large language models (LLMs) and extensive computer vision systems in the upstream sectors of the industry. The integration of Artificial Intelligence (AI) has already begun reshaping the upstream geoenergy landscape, offering enhancements in production optimization, downtime reduction, safety improvements, and advancements in exploration and drilling techniques. These technologies streamline logistics, minimize maintenance costs, automate monotonous tasks, refine decision-making processes, foster team collaboration, and amplify profitability through error reduction and actionable insights extraction. Despite these advancements, the deployment of AI technologies faces challenges, including the necessity for skilled professionals for implementation and the limitations of model training on constrained datasets, which affects the models' adaptability across different contexts. The advent of generative AI, exemplified by innovations like ChatGPT and the Segment Anything Model (SAM), heralds a new era of high-density innovation. These developments highlight a shift towards natural language interfaces and domain-knowledge-driven AI, promising more accessible and tailored solutions for the upstream geoenergy industry. This review articulates the vast potential AGI holds for tackling complex operational challenges within the upstream geoenergy industry, requiring near-human levels of intelligence. We discussed the promising applications, the hurdles of large-scale AGI model deployment, and the necessity for domain-specific knowledge in maximizing the benefits of these technologies.}
}
@article{KLAASSEN2025100122,
title = {Advancing transdiagnostic data analytics using knowledge graphs},
journal = {Biomarkers in Neuropsychiatry},
volume = {12},
pages = {100122},
year = {2025},
issn = {2666-1446},
doi = {https://doi.org/10.1016/j.bionps.2025.100122},
url = {https://www.sciencedirect.com/science/article/pii/S2666144625000048},
author = {Fiona Klaassen and Emanuel Schwarz},
keywords = {Biomarker, Construct, Knowledge graph, RDoC, Transdiagnostic, Validity},
abstract = {Artificial intelligence approaches have tremendous potential to advance our understanding of biological and other processes contributing to mental illness risk. An important question is how such approaches can be tailored to support transdiagnostic investigations that are considered central for gaining deeper insight into etiological processes and psychopathology that may not align well with categorical illness delineations. Here, we present the so-called “knowledge graphs” that could be leveraged in analytic approaches to synthesize multimodal data of transdiagnostic relevance, identify important latent structures and biomarkers, and support the evaluation of existing transdiagnostic frameworks.}
}
@article{LU2025104091,
title = {LLM-infused bi-level semantic enhancement for corporate credit risk prediction},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104091},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104091},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000330},
author = {Sichong Lu and Yi Su and Xiaoming Zhang and Jiahui Chai and Lean Yu},
keywords = {Corporate credit risk prediction, Semantic enhancement, Large language model, Contrastive learning, Multitask learning},
abstract = {Corporate credit risk (CCR) prediction enables investors, governments, and companies to make informed financial decisions. Existing research primarily focuses solely on the tabular feature values, yet it often overlooks the rich inherent semantic information. In this paper, a novel bi-level semantic enhancement framework for CCR prediction is proposed. Firstly, at the data-level, a large language model (LLM) generates detailed textual descriptions of companies’ financial conditions, infusing raw tabular training data with semantic information and domain knowledge. Secondly, to enable semantic perception during inference when only tabular data is available, a contrastive multimodal multitask learning model (CMML) is proposed at the model level. CMML leverages the semantically enhanced data from the previous level to acquire semantic perception capabilities during the training phase, requiring only tabular data during prediction. It aligns the representations of tabular data with textual data, enabling extracting semantically rich features from tabular data. Furthermore, a semantic alignment classifier and an MLP classifier are integrated into a weighted ensemble learner within a multitask learning architecture to enhance robustness. Empirical verification on two datasets demonstrates that CMML surpasses benchmark models in key metrics, particularly in scenarios with limited samples and high proportions of unseen corporations, implying its effectiveness in CCR prediction through bi-level semantic enhancement.}
}
@article{ZHENG2025112547,
title = {A LLM-driven and motif-informed linearizing graph transformer for Web API recommendation},
journal = {Applied Soft Computing},
volume = {169},
pages = {112547},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112547},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013218},
author = {Xin Zheng and Guiling Wang and Guiyue Xu and Jianye Yang and Boyang Han and Jian Yu},
keywords = {Graph transformer, Web API recommendation, Motif, Higher-order connectivity, Semantic information},
abstract = {The rapid growth in the number of Web APIs makes it difficult for developers to find the appropriate ones. To tackle this issue, researchers have created various powerful automatic approaches for recommendations. Recently, a range of graph neural networks, drawing inspiration from Transformers, have incorporated global attention to enhance recommendations. However, these approaches still have limitations in terms of processing information between nodes and utilizing other valid information, which reduces their effectiveness in large-scale Web API recommendations. To tackle these problems, this paper introduces a novel positional and semantic encoding method and motif-based linearizing graph Transformer for automatic Web API recommendation. We integrate the semantic information of the nodes into the positional information by using a fine-tuned large language model and graph attention mechanism. Furthermore, we leverage motif information to alter the computational sequence of the Vanilla Transformer, achieving linear time complexity. Experimental results on the two real-world datasets demonstrate the suitability of our model for Web API recommendation, surpassing existing state-of-the-art methods. In summary, our proposed technique exhibits promising results for Web API recommendation and underscores the potential of global attention in this field.}
}
@article{CHRISTINO2024104092,
title = {ChatKG: Visualizing time-series patterns aided by intelligent agents and a knowledge graph},
journal = {Computers & Graphics},
volume = {124},
pages = {104092},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.104092},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324002279},
author = {Leonardo Christino and Fernando V. Paulovich},
keywords = {Knowledge graphs, Intelligent agents, Visual analytics},
abstract = {Line-chart visualizations of temporal data enable users to identify interesting patterns for the user to inquire about. Using Intelligent Agents (IA), Visual Analytic tools can automatically uncover explicit knowledge related information to said patterns. Yet, visualizing the association of data, patterns, and knowledge is not straightforward. In this paper, we present ChatKG, a novel visual analytics strategy that allows exploratory data analysis of a Knowledge Graph that associates temporal sequences, the patterns found in each sequence, the temporal overlap between patterns, the related knowledge of each given pattern gathered from a multi-agent IA, and the IA’s suggestions of related datasets for further analysis visualized as annotations. We exemplify and informally evaluate ChatKG by analyzing the world’s life expectancy. For this, we implement an oracle that automatically extracts relevant or interesting patterns, populates the Knowledge Graph to be visualized, and, during user interaction, inquires the multi-agent IA for related information and suggests related datasets to be displayed as visual annotations. Our tests and an interview conducted showed that ChatKG is well suited for temporal analysis of temporal patterns and their related knowledge when applied to history studies.}
}
@article{YANG2024107497,
title = {Application of question answering systems for intelligent agriculture production and sustainable management: A review},
journal = {Resources, Conservation and Recycling},
volume = {204},
pages = {107497},
year = {2024},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2024.107497},
url = {https://www.sciencedirect.com/science/article/pii/S0921344924000910},
author = {Tian Yang and Yupeng Mei and Ling Xu and Huihui Yu and Yingyi Chen},
keywords = {Question answering system, Intelligent agriculture, Knowledge graphs, Large language models},
abstract = {The increasing application of artificial intelligence in agriculture production and management has generated a large amount of data, leading to a demand for processing this data. This review focuses on the knowledge storage approaches in agricultural question answering systems, namely corpora, knowledge graphs, and large language models. These systems are built on massive amounts of data and aim to process and retrieve information effectively in the context of sustainable agriculture. Corpora refer to large collections of diverse documents that serve as foundational resources for training and fine-tuning question answering systems. Knowledge graphs capture structured and interconnected knowledge by representing entities, relationships, and attributes, enabling efficient organization and querying of information. Large language models, such as GPT-4, enhance the capacity of question answering systems to provide accurate and relevant responses. By exploring these three prominent knowledge storage approaches, this review analyses the methodology and impact of agricultural question answering systems, highlighting their applications in the production process. The findings provide important implications for future research in agriculture, and potential directions for further exploration.}
}
@article{CHEN2025126226,
title = {Temporal knowledge graph extrapolation with subgraph information bottleneck},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126226},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126226},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030938},
author = {Kai Chen and Han Yu and Ye Wang and Xin Song and Xiaojuan Zhao and Yalong Xie and Liqun Gao and Aiping Li},
keywords = {Temporal knowledge graph extrapolation, Knowledge representation and reasoning, Subgraph information bottleneck},
abstract = {In the realm of temporal knowledge graph (TKG) extrapolation, subgraph-based reasoning methods offer clearer insights than those based on individual paths, effectively capturing local evidence. However, these subgraph-based methods face particular challenges, such as obtaining subgraph-level annotations and maintaining a balance between keeping enough information for accurate predictions and discarding unnecessary details. To overcome these obstacles, we introduce a novel reasoning method known as Subgraph Information Bottleneck based Reasoning (SIBR) for TKG extrapolation. Based on information bottleneck theory, SIBR aims to find subgraphs that are full of predictive value yet concise, leading to an efficient learning and inference process. SIBR is designed to capture the key temporal dynamics and evolution within TKGs by identifying subgraphs that are just large enough to represent the TKG’s structure and temporal changes. It skillfully handles the computational complexities related to mutual information using variational techniques, offering a practical optimization strategy for TKG analysis. Our method’s effectiveness is substantiated by comprehensive experiments on six public datasets, which demonstrate its superiority from multiple perspectives: it showcases improvement in predictive accuracy, robustness against data sparsity and data noise, and sensitivity to parameters, highlighting its strength in the sophisticated domain of TKG extrapolation.}
}
@article{XUE2025105525,
title = {How to realize the knowledge reuse and sharing from accident reports? A knowledge-driven modeling method combining ontology and deep learning},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {94},
pages = {105525},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2024.105525},
url = {https://www.sciencedirect.com/science/article/pii/S0950423024002833},
author = {Nannan Xue and Wei Zhang and Huayu Zhong and Wenbin Liao and Tingsheng Zhao},
keywords = {Process safety, Knowledge graph, Ontology design, Joint extraction model, Knowledge application},
abstract = {The exploration and understanding of past accidents are of great significance in enhancing the process safety. However, manually reading and analyzing a large number of accident reports is a time-consuming and inefficient task. In this study, a novel modeling method is developed to build the knowledge graph of process safety accidents, aiming to overcome the problem of knowledge reuse and sharing. Firstly, the dataset consists of 409 process safety accident reports selected from the official website of the Ministry of Emergency Management of China. Secondly, the ontology design schema is defined based on the seven-step method, including 34 ontology classes and 11 relations. Then, a new joint extraction model for the process domain is proposed based on the CasRel framework, which achieves 95.85% in precision, 61.54% in recall, and 74.95% in F1-score. Finally, the knowledge graph containing 9192 nodes and 11,257 edges is constructed in the Neo4j graph database, followed by the discussion of various related applications such as query, statistics, and analysis. The results indicate that the proposed method is a useful tool for obtaining valuable knowledge from accident reports, contributing to analysis and prevention of accidents.}
}
@article{ZHAO2023225,
title = {A Survey of Knowledge Graph Construction Using Machine Learning},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {1},
pages = {225-257},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.031513},
url = {https://www.sciencedirect.com/science/article/pii/S152614922300098X},
author = {Zhigang Zhao and Xiong Luo and Maojian Chen and Ling Ma},
keywords = {Knowledge graph (KG), semantic network, relation extraction, entity linking, knowledge reasoning},
abstract = {Knowledge graph (KG) serves as a specialized semantic network that encapsulates intricate relationships among real-world entities within a structured framework. This framework facilitates a transformation in information retrieval, transitioning it from mere string matching to far more sophisticated entity matching. In this transformative process, the advancement of artificial intelligence and intelligent information services is invigorated. Meanwhile, the role of machine learning method in the construction of KG is important, and these techniques have already achieved initial success. This article embarks on a comprehensive journey through the last strides in the field of KG via machine learning. With a profound amalgamation of cutting-edge research in machine learning, this article undertakes a systematical exploration of KG construction methods in three distinct phases: entity learning, ontology learning, and knowledge reasoning. Especially, a meticulous dissection of machine learning-driven algorithms is conducted, spotlighting their contributions to critical facets such as entity extraction, relation extraction, entity linking, and link prediction. Moreover, this article also provides an analysis of the unresolved challenges and emerging trajectories that beckon within the expansive application of machine learning-fueled, large-scale KG construction.}
}
@article{PRAMANIK2024100833,
title = {Uniqorn: Unified question answering over RDF knowledge graphs and natural language text},
journal = {Journal of Web Semantics},
volume = {83},
pages = {100833},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100833},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000192},
author = {Soumajit Pramanik and Jesujoba Alabi and Rishiraj Saha Roy and Gerhard Weikum},
keywords = {Complex question answering, Heterogeneous sources, Group Steiner Trees},
abstract = {Question answering over RDF data like knowledge graphs has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents a method for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called Uniqorn, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph typically contains all question-relevant evidences but also a lot of noise. Uniqorn copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that Uniqorn significantly outperforms state-of-the-art methods for heterogeneous QA – in a full training mode, as well as in zero-shot settings. The graph-based methodology provides user-interpretable evidence for the complete answering process.}
}
@article{DORON2025104272,
title = {Generative AI: driving productivity and scientific breakthroughs in pharmaceutical R&D},
journal = {Drug Discovery Today},
volume = {30},
number = {1},
pages = {104272},
year = {2025},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.104272},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624003970},
author = {Guy Doron and Sam Genway and Mark Roberts and Sai Jasti},
abstract = {The rapid advancement of generative artificial intelligence (AI) is reshaping pharmaceutical research and development (R&D), offering opportunities across drug discovery and development. Generative AI (GenAI) enhances productivity by enabling virtual assistants, which help automate routine tasks. It advances novel small-molecule drug design and drives new machine learning (ML) applications through synthetic data generation. Further impact is anticipated in drug development from improving operational efficiencies to novel digital innovations. Converging technologies enable rich data set capture, and next-generation AI will enable rapid, automated hypothesis generation and testing. Here, we assess the current and future applications, and the mid-term and long-term transformative potential, of GenAI in pharmaceutical R&D.}
}
@article{WALTERSDORFER2025100849,
title = {Leveraging Knowledge Graphs for AI System Auditing and Transparency},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100849},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100849},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000350},
author = {Laura Waltersdorfer and Marta Sabou},
keywords = {AI auditing, Knowledge Graphs, AI transparency},
abstract = {Auditing complex Artificial Intelligence (AI) systems is gaining importance in light of new regulations and is particularly challenging in terms of system complexity, knowledge integration, and differing transparency needs. Current AI auditing tools however, lack semantic context, resulting in difficulties for auditors in effectively collecting and integrating, but also for analysing and querying audit data. In this position paper, we explore how Knowledge Graphs (KGs) can address these challenges by offering a structured and integrative approach to collecting and transforming audit traces. This work discusses the current limitations in both AI auditing processes and tools. Furthermore, we examine how KGs can play a transformative role in overcoming these obstacles to achieve improved auditability and transparency of AI systems.}
}
@article{CHEN2024111968,
title = {A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models},
journal = {Knowledge-Based Systems},
volume = {299},
pages = {111968},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111968},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124006026},
author = {Xiaojun Chen and Ting Liu and Philippe Fournier-Viger and Bowen Zhang and Guodong Long and Qin Zhang},
keywords = {Natural language processing, Few-shot learning, Prompt learning},
abstract = {Pre-trained language models have demonstrated remarkable performance in few-shot learning through the emergence of “prompt-based learning” methods, where the performance of these tasks highly rely on the quality of prompts. Existing prompt learning methods typically customize a single prompt to each few-shot learning task and all the examples in the task share the universal prompt. However, a fine-grained prompt design can enhance the performance of few-shot learning task by leveraging more diverse information hidden in the set of examples. In light of this motivation, this paper introduce an example-specific prompt learning method to embody fine-grained self-adapting prompts for few-shot learning with pre-trained models. Specifically, we introduce the concept of the “weak consistency assumption”, to trade-off the task-specific consistent and example-specific diversity. Based on this assumption, a novel method called Self-adapting Continuous Prompt Learning (SP-learning) to learn example-specific prompts is proposed. It employs a cross-attention prompt generator that considers the characteristics of input samples and utilizes a diversity calibration technique to adjust the prompt generator accordingly. By personalizing prompts for each example, SP-learning aims to improve few-shot learning performance. We perform a systematic evaluation on 10 public benchmark tasks and our method outperforms 8 of those tasks. Our research sheds light on the importance of personalized prompts and opens up new possibilities for improving few-shot learning tasks.}
}
@article{WANG2025126308,
title = {Multimodal fusion framework based on knowledge graph for personalized recommendation},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126308},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126308},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031750},
author = {Jingjing Wang and Haoran Xie and Siyu Zhang and S. Joe Qin and Xiaohui Tao and Fu Lee Wang and Xiaoliang Xu},
keywords = {Knowledge graphs, Multimodal fusion framework, Recommender system},
abstract = {Knowledge Graphs (KGs), which contain a wealth of knowledge, have been commonly employed in recommendation systems as a valuable knowledge-driven tool for supporting high-quality representations. To further enhance the model’s ability to understand the real world, Multimodal Knowledge Graphs (MKGs) are proposed to extract rich knowledge and facts among objects from text and visual content. However, existing MKG-based methods primarily focus on the reasoning relationships between entities by utilizing multimodal information as auxiliary data in the KG while overlooking the interactions between modalities. In this paper, we propose a Multimodal fusion framework based on Knowledge Graph for personalized Recommendation (Multi-KG4Rec) to address these limitations. Specifically, we systematically analyze the shortcomings of existing multimodal graph construction methods. To this end, we propose a modal fusion module to extract the user modal preference at a fine-grained level. Furthermore, we conduct extensive experiments on two real-world datasets from different domains to evaluate the performance of our model, and the results demonstrate the efficiency of the Multi-KG4Rec.}
}
@article{LIU2025110889,
title = {A blockchain-based LLM-driven energy-efficient scheduling system towards distributed multi-agent manufacturing scenario of new energy vehicles within the circular economy},
journal = {Computers & Industrial Engineering},
volume = {201},
pages = {110889},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.110889},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225000348},
author = {Changchun Liu and Qingwei Nie},
keywords = {Large language model, Energy-efficient scheduling, Blockchain, Distributed manufacturing, Multi-agent, Circular economy},
abstract = {As processing technology is becoming increasingly complex, a single enterprise is no longer able to satisfy all the customization needs, which also requires extra energy consumption and vast time to seek cooperation from other enterprises for processing, especially in the field of new energy vehicle manufacturing. To coincide with the circular economy principle, a blockchain-based Large Language Model (LLM)-driven energy-efficient scheduling solution is proposed towards distributed multi-agent manufacturing scenario of new energy vehicles in this work. Firstly, distributed manufacturing system has the potential to efficiently organize distributed manufacturing resources by abstracting various machines from different factory nodes into agents with corresponding processing capabilities. Additionally, energy-efficient scheduling in line with the circular economy principles helps to optimize production cycle and reduce energy consumption and delay time, thereby lowering production costs and enhancing competitiveness. Compared with the traditional methods that suffer from long training time and local optimization, LLMs offer innovative solutions by learning a wealth of experiential knowledge in advance from vast amounts of data to further support self-adaptive and real-time energy-efficient scheduling. It is also worth noting that untrusted production data in factories may mislead the learning process of LLM, which may generate incorrect decision results. Therefore, a credit evaluation-based consensus mechanism is proposed to provide a trustworthy data access in distributed manufacturing, which can improve the transparency and traceability of the whole production process. Finally, the proposed approach is validated in the distributed manufacturing scenario for new energy vehicles. Compared with common methods, experimental results demonstrate the superiority of the proposed method on the distributed multi-agent manufacturing scenario for new energy vehicles, highlighting its potential to enhance production efficiency and circular economy.}
}
@article{CHO2024,
title = {Task-Specific Transformer-Based Language Models in Health Care: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/49724},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001650},
author = {Ha Na Cho and Tae Joon Jun and Young-Hak Kim and Heejun Kang and Imjin Ahn and Hansle Gwon and Yunha Kim and Jiahn Seo and Heejung Choi and Minkyoung Kim and Jiye Han and Gaeun Kee and Seohyun Park and Soyoung Ko},
keywords = {transformer-based language models, medicine, health care, medical language model},
abstract = {Background
Transformer-based language models have shown great potential to revolutionize health care by advancing clinical decision support, patient interaction, and disease prediction. However, despite their rapid development, the implementation of transformer-based language models in health care settings remains limited. This is partly due to the lack of a comprehensive review, which hinders a systematic understanding of their applications and limitations. Without clear guidelines and consolidated information, both researchers and physicians face difficulties in using these models effectively, resulting in inefficient research efforts and slow integration into clinical workflows.
Objective
This scoping review addresses this gap by examining studies on medical transformer-based language models and categorizing them into 6 tasks: dialogue generation, question answering, summarization, text classification, sentiment analysis, and named entity recognition.
Methods
We conducted a scoping review following the Cochrane scoping review protocol. A comprehensive literature search was performed across databases, including Google Scholar and PubMed, covering publications from January 2017 to September 2024. Studies involving transformer-derived models in medical tasks were included. Data were categorized into 6 key tasks.
Results
Our key findings revealed both advancements and critical challenges in applying transformer-based models to health care tasks. For example, models like MedPIR involving dialogue generation show promise but face privacy and ethical concerns, while question-answering models like BioBERT improve accuracy but struggle with the complexity of medical terminology. The BioBERTSum summarization model aids clinicians by condensing medical texts but needs better handling of long sequences.
Conclusions
This review attempted to provide a consolidated understanding of the role of transformer-based language models in health care and to guide future research directions. By addressing current challenges and exploring the potential for real-world applications, we envision significant improvements in health care informatics. Addressing the identified challenges and implementing proposed solutions can enable transformer-based language models to significantly improve health care delivery and patient outcomes. Our review provides valuable insights for future research and practical applications, setting the stage for transformative advancements in medical informatics.}
}
@article{ZHANG2024112454,
title = {A survey on temporal knowledge graph embedding: Models and applications},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112454},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112454},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010888},
author = {Yuchao Zhang and Xiangjie Kong and Zhehui Shen and Jianxin Li and Qiuhua Yi and Guojiang Shen and Bo Dong},
keywords = {Temporal knowledge graph embedding, Time information, Extension of static knowledge graph embedding model, Evolutionary model, Downstream task},
abstract = {Knowledge graph embedding (KGE), as a pivotal technology in artificial intelligence, plays a significant role in enhancing the logical reasoning and management efficiency of downstream tasks in knowledge graphs (KGs). It maps the intricate structure of a KG to a continuous vector space. Conventional KGE techniques primarily focus on representing static data within a KG. However, in the real world, facts frequently change over time, as exemplified by evolving social relationships and news events. The effective utilization of embedding technologies to represent KGs that integrate temporal data has gained significant scholarly interest. This paper comprehensively reviews the existing methods for learning KG representations that incorporate temporal data. It offers a highly intuitive perspective by categorizing temporal KGE (TKGE) methods into seven main classes based on dynamic evolution models and extensions of static KGE. The review covers various aspects of TKGE, including the background, problem definition, symbolic representation, training process, commonly used datasets, evaluation schemes, and relevant research. Furthermore, detailed descriptions of related embedding models are provided, followed by an introduction to typical downstream tasks in temporal KG scenarios. Finally, the paper concludes by summarizing the challenges faced in TKGE and outlining future research directions.}
}
@incollection{GALITSKY2025175,
title = {8 - Truth-O-Meter: Collaborating with LLM in fighting its hallucinations},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {175-210},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000043},
author = {Boris Galitsky},
keywords = {Collaborative iterative mode, Fact-checking, Handling inconsistent verification sources, Large language model hallucination, Syntax-semantic alignment},
abstract = {Large language models (LLMs), like GPT-4, often generate texts plagued with inaccuracies and fabricated information. We develop a fact-checking system known as “Truth-O-Meter” which detects erroneous facts by cross-referencing generated content with information from the web and reputable sources, and then offers appropriate corrections. We employ text mining and web mining techniques to pinpoint accurate corresponding sentences and to employ a syntactic and semantic generalization process to enhance content quality. To effectively handle the challenges posed by inconsistent information sources during fact-checking, we employ an argumentation-analysis framework based on defeasible logic programming. In a comparative evaluation with competitive approaches that rely on reinforcement learning integrated with LLM or token-based hallucination detection, our fact-checking engine demonstrates significant enhancements in the factual accuracy and meaningfulness of LLM-generated content.11https://github.com/bgalitsky/Truth-O-Meter-Making-ChatGPT-Truthful.}
}
@article{LONGHAO20231493,
title = {The method of constructing basic-element base using large language model- Take the issue of rice waste},
journal = {Procedia Computer Science},
volume = {221},
pages = {1493-1500},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923007706},
author = {WANG Long-hao and LI Ding-jie and LI Xing-sen},
keywords = {Large language model;Extenics;Basic-element base;Rice waste;ChatGPT},
abstract = {The rapid development of artificial intelligence technology has led to the emergence of large language models such as ChatGPT represented by natural language processing technology, but currently there is no effective way to input all the information to be exchanged. In this paper, a method of constructing local basic-element base of input information by combining the large language model with extenics is proposed. Taking rice waste problem as an example, the method is successfully applied to a practical project to verify the feasibility of the method.}
}
@article{MACILENTI20241289,
title = {Prompting is not all you need Evaluating GPT-4 performance on a real-world ontology alignment use case},
journal = {Procedia Computer Science},
volume = {246},
pages = {1289-1298},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.557},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026036},
author = {Giulio Macilenti and Armando Stellato and Manuel Fiorelli},
keywords = {Semantic technologies, Ontology alignment, Large language models},
abstract = {Ontology Alignment (OA) is a complex, demanding and error-prone task, requiring the intervention of domain and Semantic Web experts. Automating the alignment process thus becomes a must-do, especially when involving large datasets, to at least produce a first input for human experts. Automated ontology alignment could benefit from the outstanding language ability of Large Language Models (LLMs), which could implicitly provide the background knowledge that has been the Achilles’ heel of traditional alignment systems. However, this requires a correct evaluation of the performance of LLMs and understanding the best way to incorporate them into more specific tools. In this paper, we show that a naive prompting approach on the popular GPT-4 model could face several problems when transferred to real-world use cases. To this end, we replicated the methods of Norouzi et al. (2023), applied to the OAEI 2022 conference track, on a reference alignment between a pair of datasets (reduced versions of two popular thesauri: European Commission’s EuroVoc and TESEO, from the Italian Senate of the Republic), which has never been tested in OAEI evaluation campaigns. This reference alignment has several features common to real-world use cases: it is has a larger size than those considered in the study we replicated, it is not published online and is therefore not subject to data contamination and it involves relations between concepts that are more complex than simple equivalence. The replicated methods achieved a significantly lower performance on our reference alignment than on the OAEI 2022 conference track, suggesting that size, data contamination, and semantic complexity need to be considered when using LLMs for the alignment task.}
}
@article{JIANG2025109796,
title = {A two-stage framework for pig disease knowledge graph fusing},
journal = {Computers and Electronics in Agriculture},
volume = {229},
pages = {109796},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924011876},
author = {Tingting Jiang and Zhiyi Zhang and Shunxin Hu and Shuai Yang and Jin He and Chao Wang and Lichuan Gu},
keywords = {Pig disease knowledge graph, Knowledge graph embedding, Data augmentation, Entity alignment},
abstract = {Pig disease knowledge graphs (KGs) are crucial for the prevention and treatment of pig diseases. Due to the difficulty of knowledge mining in the field of traditional animal husbandry, there is a lack of high-quality KGs of pig diseases. To tackle this issue, a novel two-stage framework for pig disease KG fusing is proposed in this manuscript. In the first stage, a multi-view augmentation method for pig disease KGs is designed. The domain characteristics in the field of pig disease are considered and four valid strategies are utilized for augmenting triples, which not only enriches the pig disease KGs and provides abundant training data for KG embedding. In the second stage, an unsupervised entity alignment method is introduced to match entities. Importantly, the similarities of entity name, relation, attribute, and structure information are learned alternatively to avoid annotating data manually. Extensive experiments on the pig disease datasets and the public dataset MED_BBK_9K demonstrate that the proposed method can achieve state-of-the-art performance, i.e., the multi-view augmentation method improves hits@1 by 0.387 compared with the suboptimal model on the Pig1 dataset, and the entity alignment model outperforms the second-best model by 0.168 in terms of hits@1 on the Pig1_Pig2 dataset.}
}
@article{FORTH2024110312,
title = {Semantic enrichment for BIM-based building energy performance simulations using semantic textual similarity and fine-tuning multilingual LLM},
journal = {Journal of Building Engineering},
volume = {95},
pages = {110312},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110312},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224018801},
author = {Kasimir Forth and André Borrmann},
keywords = {BIM to BEM, BEPS, Semantic enrichment, Semantic textual similarity, Fine tuning LLM},
abstract = {To achieve the global targets of the Paris Agreement of limiting global warming, it is necessary to reduce the operational energy of buildings, which are responsible for around 30% of the global greenhouse gas emissions. Building Energy Performance Simulation (BEPS) is an established method to estimate the building’s energy demand in early design stages. Building Information Models (BIM) provides geometric and semantic information to create precise Building Energy Models (BEM) in early design stages. However, manual enrichment of missing semantic information is still a time-consuming and laborious process. Therefore, we propose a novel methodology to automatically enrich missing information to BIM using Semantic Textual Similarity (STS) and fine-tuned Large Language Models (LLM). For every IfcSpace, we match room-specific space types and constructions with missing thermal properties using the semantic most similar pairs of the BIM model and the according databases. We use three real-world case studies to fine-tune LLMs, and two case studies evaluate the whole methodology. Different fine-tuning strategies, such as using different loss functions, adding opposing word pairs or domain-specific abbreviations, significantly improve the accuracy of the matching. At the same time, however, findings show that semantic matching based on multilingual fine-tuned LLM performs worse than translated, monolingually fine-tuned LLM. Finally, BEPS results from automatically enriched BEM only slightly deviate from manually enriched BEM.}
}
@article{DEVARAKONDA2024104627,
title = {Clinical trial recommendations using Semantics-Based inductive inference and knowledge graph embeddings},
journal = {Journal of Biomedical Informatics},
volume = {154},
pages = {104627},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104627},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000455},
author = {Murthy V. Devarakonda and Smita Mohanty and Raja Rao Sunkishala and Nag Mallampalli and Xiong Liu},
keywords = {Clinical trials, Knowledge graphs, Graph embeddings, Transductive inference, Inductive inference, Recommendation systems, Graph Neural Networks (GNNs), Graph Attention Networks (GATs)},
abstract = {Objective
Designing a new clinical trial entails many decisions, such as defining a cohort and setting the study objectives to name a few, and therefore can benefit from recommendations based on exhaustive mining of past clinical trial records. This study proposes an approach based on knowledge graph embeddings and semantics-driven inductive inference for generating such recommendations.
Method
The proposed recommendation methodology is based on neural embeddings trained on first-of-its-kind knowledge graph constructed from clinical trials data. The methodology includes design of a knowledge graph for clinical trial data, evaluation of various knowledge graph embedding techniques for it, application of a novel inductive inference method using these embeddings, and generation of recommendations for clinical trial design. The study uses freely available data from clinicaltrials.gov and related sources.
Results
The proposed approach for recommendations obtained relevance scores ranging from 70% to 83%. These scores were determined by evaluating the text similarity of recommended elements to actual elements used in clinical trials that are in progress. Furthermore, the most pertinent recommendations were consistently located towards the top of the list, indicating the effectiveness of our method.
Conclusion
Our study suggests that inductive inference using node semantics is a viable approach for generating recommendations using graphs neural embeddings, and that there is a potential for improvement in training graph embeddings using node semantics.}
}
@article{PAN2025103971,
title = {Concept-aware embedding for logical query reasoning over knowledge graphs},
journal = {Information Processing & Management},
volume = {62},
number = {2},
pages = {103971},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103971},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003303},
author = {Pengwei Pan and Jingpei Lei and Jiaan Wang and Dantong Ouyang and Jianfeng Qu and Zhixu Li},
keywords = {Knowledge graph, Complex query answering, Information enhancement},
abstract = {Logical query reasoning over knowledge graphs (KGs) is an important task for querying some information upon specified conditions. Despite recent advancements, existing methods typically focus on the inherent structure of logical queries and fail to capture the commonality among entities and relations, resulting in cascading errors during multi-hop inference. To mitigate this issue, we resort to inferring relations’ domain constraints based on the commonality of their connected entities implicitly. Specifically, to capture the domain constraints of relations, we treat the set of relations emitted by an entity as its implicit concept information and derive a relation’s domain constraint by aggregating the implicit concept information of its head entities. Employing a geometric-based embedding strategy, we enrich the representations of entities in the query with their implicit concept information. Additionally, we design a straightforward yet effective curriculum learning strategy to refine its reasoning skills. Notably, our model can be integrated into any existing query embedding-based logical query reasoning methods in a plug-and-play manner, enhancing their understanding of the entities as well as relations in queries. Experiments on three widely used datasets show that our model can achieve comparable outcomes and improve the performance of existing logical query reasoning models. Particularly, as a plug-in, it achieves an absolute improvement of the maximum 8.4% Hits@3 compared to the original model on the FB15k dataset, and it surpasses the former state-of-the-art plug-and-play logical query reasoning model in most scenes, exceeding it by up to 2.1% average Hits@3 results.}
}
@article{YAN2024108259,
title = {Enhancing large language model capabilities for rumor detection with Knowledge-Powered Prompting},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108259},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108259},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624004172},
author = {Yeqing Yan and Peng Zheng and Yongjun Wang},
keywords = {Social networks, Rumor detection, Knowledge augmentation, Prompt tuning, Large language model},
abstract = {Amid the proliferation of misinformation on social networks, automated rumor detection has emerged as a pivotal and pressing research domain. Nonetheless, current methodologies are hindered by constrained feature representations and limited adaptability in effectively addressing diverse and unconventional rumors. The incorporation of large-scale language models holds the promise of delivering heightened semantic comprehension and broader adaptability. Regrettably, prevailing general-purpose prompting approaches frequently fall short in furnishing adequate domain-specific context and guidance, thereby restricting their utility in the context of rumor detection. To ameliorate these concerns, we introduce the Knowledge-Powered Prompting strategy, which imparts task-relevant prompts and context to the model by amalgamating domain expertise with large-scale language models. This fusion equips the model to better align with the exigencies of rumor detection, mitigating the challenges posed by sensitivity to semantic subtleties and a paucity of training samples. In particular, we devise exploration prompts and bolster the prompt representation with a dynamic knowledge injection module, thereby facilitating profound reasoning about pivotal entities. Subsequently, we extract valuable external knowledge through the filtration of interactions between knowledge and claim, thereby diminishing the impact of noise. Concurrently, we undertake joint optimization, encompassing multi-task prompt population and categorical judgment objectives, fostering synergistic semantic modeling and discriminative assessments. Empirical evaluations reveal that our methodology substantially outperforms existing models.}
}
@article{ZHONG2024105316,
title = {Domain-specific language models pre-trained on construction management systems corpora},
journal = {Automation in Construction},
volume = {160},
pages = {105316},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105316},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524000529},
author = {Yunshun Zhong and Sebastian D. Goodfellow},
keywords = {Construction management, Domain-specific large language models, Pre-training, Natural language processing (NLP), Transfer learning, Text classification (TC), Named entity recognition (NER), Corpus development},
abstract = {The rising demand for automated methods in the Construction Management Systems (CMS) sector highlights opportunities for the Transformer architecture, which enables pre-training Deep Learning models on large, unlabeled datasets for Natural Language Processing (NLP) tasks, outperforming traditional Recurrent Neural Network models. However, their potential in the CMS domain remains underexplored. Therefore, this research produced the first CMS domain corpora from academic papers and introduced an end-to-end pipeline for pre-training and fine-tuning domain-specific Pre-trained Language Models. Four corpora were constructed and transfer learning was employed to pre-train BERT and RoBERTa using the corpora. The best-performing models were then fine-tuned and outperformed models pre-trained on general corpora. In two key NLP tasks, text classification using an infrastructure condition prediction dataset and named entity recognition using an automatic construction control dataset, domain-specific pre-training improved F1 scores by 5.9% and 8.5%, respectively. These promising results demonstrate extended applicability beyond CMS to the Architecture, Engineering, and Construction sectors.}
}
@article{ZHANG2024308,
title = {Empathetic Language in LLMs under Prompt Engineering: A Comparative Study in the Legal Field},
journal = {Procedia Computer Science},
volume = {244},
pages = {308-317},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030059},
author = {Yifan Zhang and Christopher Radishian and Sabine Brunswicker and Dan Whitenack and Daniel W. Linna},
keywords = {LLM, Human-AI Interaction, Empathetic Response},
abstract = {The demand for empathetic conversations increases with conversational AIs’ rise and exponentially spreading applications. In areas like law and healthcare, where professional and empathetic conversations are essential, conversational AIs must strive to retain the correctness of information and logic while improving on empathetic language use. When addressing such an issue, we focus on linguistic empathy, relating only to syntactic and rhetoric choices in language while disregarding the emotional aspect of influence. By performing this study, we are interested in finding whether current open-sourced Large Language Models (LLMs) can match human experts in the legal field by using empathetic language while not compromising facts and logic in responses. We compare responses from three open-sourced LLMs under four prompting strategies with the expert responses. In the comparison, we use metrics from three aspects: text and semantic similarity, factual consistency, and ten rules of linguistic empathy from previous research literature. After statistical tests, the comparison results show that language models can use empathetic language without compromising the default knowledge base of LLMs when properly prompt-engineered. To accomplish this, additional domain knowledge is still needed to match factually. The data supporting this study is publicly available at huggingface.co/datasets/RCODI/empathy-prompt and code is available at github.com/RCODI-ConversationalAI/Empathy-Prompt.}
}
@article{ZHANG2025105938,
title = {Dynamic hazard analysis on construction sites using knowledge graphs integrated with real-time information},
journal = {Automation in Construction},
volume = {170},
pages = {105938},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105938},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006745},
author = {Juntong Zhang and Xin Ruan and Han Si and Xiangyu Wang},
keywords = {Hazard analysis, Knowledge graph, Real-time information, Bridge construction},
abstract = {Construction, as a significant production activity, is inherently prone to accidents. These accidents often result from a chain of multiple hazards. However, existing methods of hazard analysis are limited to single-dimensional network modeling and static analysis, which makes them inadequate for addressing the complexity and variability of construction sites. This paper presents a dynamic construction hazard analysis method that integrates real-time information into knowledge graphs. In this approach, label entities are added to general knowledge graphs, linking hazard entities to their labels. Labels identified through vision-based methods are then incorporated into the graphs, allowing for the effective extraction and updating of subgraphs in response to spatiotemporal changes in the scenario. Additionally, graph analysis metrics have been proposed to evaluate the system from multiple levels. Finally, the method was applied to a bridge foundation construction case, demonstrating its practicality and significance in preventing accidents by enabling dynamic hazard analysis.}
}
@article{TRILLO2024452,
title = {A Group Decision-Making Approach Leveraging Preference Relations Derived from Large Language Model},
journal = {Procedia Computer Science},
volume = {242},
pages = {452-459},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.161},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018805},
author = {José Ramón Trillo and María Ángeles Martínez and Sławomir Zadrożny and Janusz Kacprzyk and Enrique Herrera-Viedma and Francisco Javier Cabrerizo},
keywords = {Sentiment Analysis, Large Language Model, Group Decision-Making, Detection System, Consensus Method},
abstract = {Group decision-making involves selecting among limited options. Experts share their perspectives in a comparative debate but then evaluate alternatives using preference relations, which can result in inconsistencies between their expressions and assessments. To address this, a method is proposed that automates the generation of these relationships from the debate comments, classifying them into positive and negative using sentiment analysis, namely with the Large Language Model. A new operator is introduced that weights these comments to calculate preference relations. Furthermore, modification of the relationships is allowed if the experts so wish. Moreover, another operator is incorporated that adjusts the weight of each expert according to his or her active participation in the discussion, assigning more weight to those who contribute more comments. Finally, this innovative method promotes coherence and equal participation in group decision-making by employing an innovative sentiment analysis detection system.}
}
@article{SELLAMI2025100716,
title = {Knowledge graph representation learning: A comprehensive and experimental overview},
journal = {Computer Science Review},
volume = {56},
pages = {100716},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100716},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000996},
author = {Dorsaf Sellami and Wissem Inoubli and Imed Riadh Farah and Sabeur Aridhi},
keywords = {Knowledge graphs, Knowledge graph embedding, Representation space, Link prediction, Scalability},
abstract = {Knowledge graph embedding (KGE) is a hot topic in the field of Knowledge graphs (KG). It aims to transform KG entities and relations into vector representations, facilitating their manipulation in various application tasks and real-world scenarios. So far, numerous models have been developed in KGE to perform KG embedding. However, several challenges must be addressed when designing effective KGE models. The most discussed challenges in the literature include scalability (KGs contain millions of entities and relations), incompleteness (missing links), the complexity of relations (symmetries, inversion, composition, etc.), and the sparsity of some entities and relations. The purpose of this paper is to provide a comprehensive overview of KGE models. We begin with a theoretical analysis and comparison of the existing methods proposed so far for generating KGE, which we have classified into four categories. We then conducted experiments using four benchmark datasets to compare the efficacy, efficiency, inductiveness, the electricity and the CO2 emission of five state-of-the-art methods in the link prediction task, providing a comprehensive analysis of the most commonly used benchmarks in the literature.}
}
@article{BUEHLER2023105454,
title = {MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {181},
pages = {105454},
year = {2023},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2023.105454},
url = {https://www.sciencedirect.com/science/article/pii/S0022509623002582},
author = {Markus J. Buehler},
keywords = {Mechanics, Attention, Transformer, Language model, Forward, Inverse, Design, Modeling, Multiscale, Atomistic, Encoding, Representation, Causal, Emergent, Collective, Graph neural network, GPT, Human-machine interaction},
abstract = {We report a flexible multi-modal mechanics language model, MeLM, applied to solve various nonlinear forward and inverse problems, that can deal with a set of instructions, numbers and microstructure data. The framework is applied to various examples including bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding. In spite of the adaptable nature of the model-which allows us to easily incorporate diverse materials, scales, and mechanical features-it performs well across disparate forward and inverse tasks. Based on an autoregressive attention-model, MeLM effectively represents a large multi-particle system consisting of hundreds of millions of neurons, where the interaction potentials are discovered through graph-forming self-attention mechanisms that are then used to identify relationships from emergent structures, while taking advantage of synergies discovered in the training data. We show that the model can solve complex degenerate mechanics design problems and determine novel material architectures across a range of hierarchical levels, providing an avenue for materials discovery and analysis. To illustrate the use case for broader possibilities, we outline a human-machine interactive MechGPT model, here trained on a set of 1,103 Wikipedia articles related to mechanics, showing how the general framework can be used not only to solve forward and inverse problems but in addition, for complex language tasks like summarization, generation of new research concepts, and knowledge extraction. Looking beyond the demonstrations reported in this paper, we discuss other opportunities in applied mechanics and general considerations about the use of large language models in modeling, design, and analysis that can span a broad spectrum of material properties from mechanical, thermal, optical, to electronic.}
}
@article{CHEN2025103068,
title = {Meet2Mitigate: An LLM-powered framework for real-time issue identification and mitigation from construction meeting discourse},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103068},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103068},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007195},
author = {Gongfan Chen and Abdullah Alsharef and Anto Ovid and Alex Albert and Edward Jaselskis},
keywords = {Construction meeting, Speaker diarization, Automatic speech recognition, Large language model, Retrieval augmented generation, Human-AI interaction},
abstract = {Construction meetings are essential for bringing together project participants to coordinate efforts, identify problems, and make decisions. Previous studies on meeting analysis relied on manual approaches to identify isolated pieces of information but struggled with providing a high-level overview that targeted real-time problem identification and resolution. Despite the rich discussions that occur, the sheer volume of information exchanged can make it difficult to discern key issues, decisions, and action items. Recent advancements in large language models (LLMs) provide sophisticated natural language processing capabilities that can effectively distill essential information and highlight actionable insights from meeting transcripts. However, these technologies are often underutilized in practice, despite their potential to significantly enhance the analysis and management of meeting data. This study introduced the Meet2Mitigate (M2M) framework, which integrates cutting-edge technologies, including speaker diarization, automatic speech recognition (ASR), LLMs, and retrieval-augmented generation (RAG) to revolutionize how construction meetings are captured and analyzed. In this framework, construction meeting recordings can be converted into a structured format, differentiated by timestamps, speakers, and corresponding contents. Different speakers’ dialogues are then summarized to extract the main project-related issues. For quick mitigation responses, this framework combines LLMs with a retrieval mechanism to access the Construction Industry Institute (CII) Best Practices (BPs) knowledge pool, generating detailed action items to drive problem-solving. The validation results demonstrated that the M2M prototype can automatically generate a tailored end-to-end problem-to-solution report in real time by only using a meeting recording file.}
}
@article{JAUHIAINEN2024262,
title = {The Metaverse: Innovations and generative AI},
journal = {International Journal of Innovation Studies},
volume = {8},
number = {3},
pages = {262-272},
year = {2024},
issn = {2096-2487},
doi = {https://doi.org/10.1016/j.ijis.2024.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096248724000183},
author = {Jussi S. Jauhiainen},
keywords = {Metaverse, Innovation, Generative AI, Collaboration, Sustainability, Creativity, ChatGPT},
abstract = {Today, the Metaverse consists of various platforms, including digital twins of the physical world as well as virtual and blended digital-material environments that offer immersive experiences for individual users. By going beyond solely physical or virtual realms, these platforms unlock new possibilities for exploration, experimentation, and interaction. This makes it possible to transcend the limitations of innovation processes confined to physical locations, so the Metaverse is thus poised to drive groundbreaking innovations. This article explores the Metaverse as an innovation platform, its opportunities and challenges, including the role of generative AI in it. It discusses how the Metaverse, as a collaboration, creativity, and technological platform, supports innovation potential. By embracing the possibilities and challenges offered by the Metaverse and leveraging the capabilities of generative AI within it, a future in which individuals can truly explore novel synergies between the physical and digital realms, thriving various kinds of innovations. It is crucial to achieve holistic sustainability impacts both within the Metaverse innovation platform and as its outputs.}
}
@article{MORENO2024916,
title = {Toward Clinical-Grade Evaluation of Large Language Models},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {118},
number = {4},
pages = {916-920},
year = {2024},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2023.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0360301623081348},
author = {Amy C. Moreno and Danielle S. Bitterman}
}
@article{WANG2024843,
title = {Large language models assisted multi-effect variants mining on cerebral cavernous malformation familial whole genome sequencing},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {843-858},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S200103702400014X},
author = {Yiqi Wang and Jinmei Zuo and Chao Duan and Hao Peng and Jia Huang and Liang Zhao and Li Zhang and Zhiqiang Dong},
keywords = {Whole genome sequencing, Cerebral cavernous malformation, Deep learning, Large language model, Natural language processing},
abstract = {Cerebral cavernous malformation (CCM) is a polygenic disease with intricate genetic interactions contributing to quantitative pathogenesis across multiple factors. The principal pathogenic genes of CCM, specifically KRIT1, CCM2, and PDCD10, have been reported, accompanied by a growing wealth of genetic data related to mutations. Furthermore, numerous other molecules associated with CCM have been unearthed. However, tackling such massive volumes of unstructured data remains challenging until the advent of advanced large language models. In this study, we developed an automated analytical pipeline specialized in single nucleotide variants (SNVs) related biomedical text analysis called BRLM. To facilitate this, BioBERT was employed to vectorize the rich information of SNVs, while a deep residue network was used to discriminate the classes of the SNVs. BRLM was initially constructed on mutations from 12 different types of TCGA cancers, achieving an accuracy exceeding 99%. It was further examined for CCM mutations in familial sequencing data analysis, highlighting an upstream master regulator gene fibroblast growth factor 1 (FGF1). With multi-omics characterization and validation in biological function, FGF1 demonstrated to play a significant role in the development of CCMs, which proved the effectiveness of our model. The BRLM web server is available at http://1.117.230.196.}
}
@article{SCHAFER2024639,
title = {BioKGrapher: Initial evaluation of automated knowledge graph construction from biomedical literature},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {639-660},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024003386},
author = {Henning Schäfer and Ahmad Idrissi-Yaghir and Kamyar Arzideh and Hendrik Damm and Tabea M.G. Pakull and Cynthia S. Schmidt and Mikel Bahn and Georg Lodde and Elisabeth Livingstone and Dirk Schadendorf and Felix Nensa and Peter A. Horn and Christoph M. Friedrich},
keywords = {Knowledge graph, Named entity recognition, Entity linking, Clinical guidelines, Software},
abstract = {Background The growth of biomedical literature presents challenges in extracting and structuring knowledge. Knowledge Graphs (KGs) offer a solution by representing relationships between biomedical entities. However, manual construction of KGs is labor-intensive and time-consuming, highlighting the need for automated methods. This work introduces BioKGrapher, a tool for automatic KG construction using large-scale publication data, with a focus on biomedical concepts related to specific medical conditions. BioKGrapher allows researchers to construct KGs from PubMed IDs. Methods The BioKGrapher pipeline begins with Named Entity Recognition and Linking (NER+NEL) to extract and normalize biomedical concepts from PubMed, mapping them to the Unified Medical Language System (UMLS). Extracted concepts are weighted and re-ranked using Kullback-Leibler divergence and local frequency balancing. These concepts are then integrated into hierarchical KGs, with relationships formed using terminologies like SNOMED CT and NCIt. Downstream applications include multi-label document classification using Adapter-infused Transformer models. Results BioKGrapher effectively aligns generated concepts with clinical practice guidelines from the German Guideline Program in Oncology (GGPO), achieving F1-Scores of up to 0.6. In multi-label classification, Adapter-infused models using a BioKGrapher cancer-specific KG improved micro F1-Scores by up to 0.89 percentage points over a non-specific KG and 2.16 points over base models across three BERT variants. The drug-disease extraction case study identified indications for Nivolumab and Rituximab. Conclusion BioKGrapher is a tool for automatic KG construction, aligning with the GGPO and enhancing downstream task performance. It offers a scalable solution for managing biomedical knowledge, with potential applications in literature recommendation, decision support, and drug repurposing.}
}
@article{HAMED2024108782,
title = {Safeguarding authenticity for mitigating the harms of generative AI: Issues, research agenda, and policies for detection, fact-checking, and ethical AI},
journal = {iScience},
volume = {27},
number = {2},
pages = {108782},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.108782},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224000038},
author = {Ahmed Abdeen Hamed and Malgorzata Zachara-Szymanska and Xindong Wu},
keywords = {Biocomputational method, Bioinformatics, Biological sciences, Computational bioinformatics, Natural sciences, Neural networks, Artificial intelligence, Artificial intelligence applications},
abstract = {Summary
As the influence of transformer-based approaches in general and generative artificial intelligence (AI) in particular continues to expand across various domains, concerns regarding authenticity and explainability are on the rise. Here, we share our perspective on the necessity of implementing effective detection, verification, and explainability mechanisms to counteract the potential harms arising from the proliferation of AI-generated inauthentic content and science. We recognize the transformative potential of generative AI, exemplified by ChatGPT, in the scientific landscape. However, we also emphasize the urgency of addressing associated challenges, particularly in light of the risks posed by disinformation, misinformation, and unreproducible science. This perspective serves as a response to the call for concerted efforts to safeguard the authenticity of information in the age of AI. By prioritizing detection, fact-checking, and explainability policies, we aim to foster a climate of trust, uphold ethical standards, and harness the full potential of AI for the betterment of science and society.}
}
@article{SONG2025129019,
title = {Unsupervised fuzzy temporal knowledge graph entity alignment via joint fuzzy semantics learning and global structure learning},
journal = {Neurocomputing},
volume = {617},
pages = {129019},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129019},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224017909},
author = {Jingni Song and Luyi Bai and Xuanxuan An and Longlong Zhou},
keywords = {Entity alignment, Fuzzy temporal knowledge graph, Fuzzy semantics learning, Global structure learning, Unsupervised},
abstract = {Temporal Knowledge Graph Entity Alignment (TKGEA) aims to identify the equivalent entities between different Temporal Knowledge Graphs (TKGs), which is important to knowledge fusion. The current mainstream TKGEA models are supervised embedding-based models that rely on pre-aligned seeds and implicitly encode structural information into entity embedding space for identifying equivalent entities. To deal with the TKGs structural information, some models use Graph Neural Network (GNN) encoding. But they ignore the design of decoders, failing to fully leverage the TKGs structural information. In addition, they primarily focus on crisp TKGs with clear entity semantics. However, many real-world TKGs exhibit fuzzy semantics. This fuzzy information makes existing TKGEA models face the challenge of handling the fuzzy semantics when aligning the equivalent fuzzy entities. To solve the above problems, we propose a novel unsupervised Fuzzy Temporal Knowledge Graphs Entity Alignment (EA) framework that jointly performs Fuzzy Semantics Learning and Global Structure Learning, namely FTFS. In this framework, we convert the EA task into an unsupervised optimal transport task between two intra-graph matrices, eliminating the necessity for pre-aligned seeds and thereby avoiding intensive labor. Since we further consider the relation between graph structure and entities during the optimal-transport-based decoder module, it can make better use of the global structural information rather than simply encoding it implicitly into the embedding space. Moreover, unlike TKGEA models, which use binary classification to represent temporal relational facts, we introduce fuzzy semantics learning to embed membership degrees of fuzzy temporal relational facts. Extensive experiments on five FTKG datasets show that our unsupervised method is superior to the state-of-the-art EA methods.}
}
@article{PHOGAT202555,
title = {ZFP-CanPred: Predicting the effect of mutations in zinc-finger proteins in cancers using protein language models},
journal = {Methods},
volume = {235},
pages = {55-63},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000295},
author = {Amit Phogat and Sowmya Ramaswamy Krishnan and Medha Pandey and M. Michael Gromiha},
keywords = {ZFP-CanPred, Zinc-fingers, Cancer, Driver, Neutral, Mutations, Neural network, Protein language model},
abstract = {Zinc-finger proteins (ZNFs) constitute the largest family of transcription factors and play crucial roles in various cellular processes. Missense mutations in ZNFs significantly alter protein-DNA interactions, potentially leading to the development of various types of cancers. This study presents ZFP-CanPred, a novel deep learning-based model for predicting cancer-associated driver mutations in ZNFs. The representations derived from protein language models (PLMs) from the structural neighbourhood of mutated sites were utilized to train ZFP-CanPred for differentiating between cancer-causing and neutral mutations. ZFP-CanPred, achieved a superior performance with an accuracy of 0.72, F1-score of 0.79, and area under the Receiver Operating Characteristics (ROC) Curve (AUC) of 0.74, on an independent test set. In a comparative analysis against 11 existing prediction tools using a curated dataset of 331 mutations, ZFP-CanPred demonstrated the highest AU-ROC of 0.74, outperforming both generic and cancer-specific methods. The model’s balanced performance across specificity and sensitivity addresses a significant limitation of current methodologies. The source code and other related files are available on GitHub at https://github.com/amitphogat/ZFP-CanPred.git. We envisage that the present study contributes to understand the oncogenic processes and developing targeted therapeutic strategies.}
}
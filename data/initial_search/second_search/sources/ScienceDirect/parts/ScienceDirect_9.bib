@article{WANG2024123736,
title = {An exploration method for technology forecasting that combines link prediction with graph embedding: A case study on blockchain},
journal = {Technological Forecasting and Social Change},
volume = {208},
pages = {123736},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123736},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524005341},
author = {Liang Wang and Munan Li},
keywords = {Technology forecasting, Topic recognition, Link prediction, Graph representation learning, Emerging technology, Blockchain},
abstract = {To keep pace with the latest technological changes and advancements, predicting future technological trends and topics has become a critical approach for high-tech companies and policy-making institutions. In this paper, we proposed an explorative method that integrates link prediction and Node2Vec graph embedding to predict future technology topics using co-occurrence data from patent keywords. Specifically, this method collects and preprocesses patent datasets, constructs network graphs that depict relationships among different technology topics, and builds a supervised link prediction model based on the time series of the graph to identify future technology graphs. Furthermore, node2vec graph embedding is conducted to obtain node vector representations, and then the clustering algorithms can be improved to identify the relevant topics, which could be interpreted as future technology. A case study on blockchain is conducted to validate the feasibility and practicality of the method to demonstrate the application of the method. Through the comparison of machine learning methods, we selected the Random Forest (RF) model, which presents the highest accuracy, for our experiments. The results show that the proposed method can be used to effectively visualize potential future topics related to a specific technology. Compared to traditional methods such as Latent Dirichlet Allocation (LDA), our method can identify more unique and differentiated technological topics, significantly reducing topic overlap. Additionally, the reported method can illustrate the internal relationships of topics through subgraphs, helping readers better understand the core concepts of each topic and vividly displaying the structure and composition of the topics. Furthermore, the proposed method can also depict potential relationships between different technology topics, which can facilitate the visualization of new directions of research and development.}
}
@article{LUH2025104287,
title = {Gamifying information security: Adversarial risk exploration for IT/OT infrastructures},
journal = {Computers & Security},
volume = {151},
pages = {104287},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104287},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005935},
author = {Robert Luh and Sebastian Eresheim and Paul Tavolato and Thomas Petelin and Simon Gmeiner and Andreas Holzinger and Sebastian Schrittwieser},
keywords = {Hacking, Security game, Model, Gamification},
abstract = {Today’s interconnected IT and OT infrastructure faces an array of cyber threats from diverse actors with varying motivations and capabilities. The increasing complexity of exposed systems, coupled with adversaries’ sophisticated technical arsenals, poses significant challenges for organizations seeking to defend against these attacks. Understanding the relationship between specific attack techniques and effective technical, organizational and human-centric mitigation measures remains elusive, as does grasping the underlying principles of information security and how they may be applied to cyber defense. In response to these challenges, we propose a gamified metamodel that combines well-established frameworks, including MITRE ATT&CK, D3FEND, CAPEC, and the NIST SP 800-53 security standard. The programmatic implementation of the model, “PenQuest”, combines elements of game theory with cybersecurity concepts to enhance risk assessment and training for IT practitioners and security engineers. In PenQuest, participants engage in a digital battle — attackers attempt to compromise an abstracted IT infrastructure, while defenders work to prevent or mitigate the threat. Bot opponents and the technical foundation for reinforcement learning enable future automated strategy inference. This paper provides an in-depth exploration of the metamodel, the game’s components and features built to translate cybersecurity principles into strategy game rules, and the technical implementation of a mature, ready-to-use education and risk exploration solution. Future work will focus on further improving the attack likelihood and detection chance algorithms for seamless risk assessment.}
}
@article{WANG2024127709,
title = {Triple alignment-enhanced complex question answering over knowledge bases},
journal = {Neurocomputing},
volume = {588},
pages = {127709},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127709},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224004806},
author = {Dong Wang and Sihang Zhou and Jian Huang and Xiangrong Ni},
keywords = {Program induction, Complex question answering, Triple alignment, Function misalignment, Argument ambiguity},
abstract = {Program induction is a crucial paradigm for complex question answering over knowledge bases. In the existing learning framework, the predicted program is required to strictly align (word-by-word) with a gold program, which could cause over penalization for minor deviations. Meanwhile, due to the existence of synonyms, program induction question answering often fails to retrieve answer from the knowledge base because individual function argument cannot perfectly replicate the target argument. To address above misalignment problems, we propose a triple alignment-enhanced complex question answering (TACQA) method by incorporating global token alignment, function alignment, and argument alignment. First, apart from the classical global token alignment, the predicted functions are extracted and aligned separately with the gold functions, enabling efficient learning of implicit structural information related to the query framework of program from input questions. Second, an argument alignment is introduced to correct the ambiguous function arguments, which enhances the disambiguation processing efficiency of multi-argument by optimizing candidate pool construction and similarity calculation. The experiments on KQA Pro show that our method consistently outperforms the SOTA methods, demonstrating the effectiveness of triple alignment processing mechanism for simultaneously addressing function misalignment and argument ambiguity in program induction and further improving the model performance.}
}
@article{VODYAHO2024350,
title = {Continuous agile cyber–physical systems architectures based on digital twins},
journal = {Future Generation Computer Systems},
volume = {153},
pages = {350-359},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004326},
author = {Alexander Vodyaho and Nataly Zhukova and Radhakrishnan Delhibabu and Alexey Subbotin},
keywords = {Compute continuum, Digital twin, Digital twin networks, Digital threads, Model synthesis, Continuous architecture, Agile architecture, cyber-physical system},
abstract = {Modern cyber-physical systems, for the most part, are large-scale multilevel heterogeneous distributed systems that integrate subsystems of different kinds and are built on the Internet of Things platforms, where system structure and behavior are not constant. Managing such systems and keeping them in working condition throughout their lifetime is a difficult task. The proposed article discusses one of the possible approaches to solving this problem, based on the use of well-known continuous and agile architecture paradigms. However, there are currently no effective mechanisms for implementing these paradigms. The proposed article suggests a new approach to implementing continuous agile architectures by utilizing digital twins and proposes a reference architecture for a run-time dynamic digital twin. This method is unique because it builds a series of dynamic digital twins that model the system in real time, utilizing data about system events. Build the first models using the models used in earlier stages of the system lifecycle. This gives the following opportunities: i) a way to use dynamic digital twins to implement the continuous agile architecture paradigm; ii) a generalized three-level model of the life cycle of the continuous agile architecture; iii) a reference architecture for dynamic digital twins; and iv) a set of models that are all about using dynamic digital twins. The suggested approach enables the management of heterogeneous multilevel cyber-physical systems with variable structure and behavior variability.}
}
@article{SAADY2025103066,
title = {Implementation of artificial intelligence approaches in oncology clinical trials: A systematic review},
journal = {Artificial Intelligence in Medicine},
volume = {161},
pages = {103066},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103066},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000016},
author = {Marwa Saady and Mahmoud Eissa and Ahmed S. Yacoub and Ahmed B. Hamed and Hassan Mohamed El-Said Azzazy},
keywords = {Artificial intelligence, Machine learning, Deep learning, Oncology clinical trials},
abstract = {Introduction
There is a growing interest in leveraging artificial intelligence (AI) technologies to enhance various aspects of clinical trials. The goal of this systematic review is to assess the impact of implementing AI approaches on different aspects of oncology clinical trials.
Methods
Pertinent keywords were used to find relevant articles published in PubMed, Scopus, and Google Scholar databases, which described the clinical application of AI approaches. A quality evaluation utilizing a customized checklist specifically adapted was conducted. This study is registered with PROSPERO (CRD42024537153).
Results
Out of the identified 2833 studies, 72 studies satisfied the inclusion criteria. Clinical Trial Enrollment & Eligibility were among the most commonly studied clinical trial aspects with 30 papers. The prediction of outcomes was covered in 25 studies of which 15 addressed the prediction of patients' survival and 10 addressed the prediction of drug outcomes. The trial design was studied in 10 articles. Three studies addressed each of the personalized treatments and decision-making, while one addressed data management. The results demonstrate using AI in cancer clinical trials has the potential to increase clinical trial enrollment, predict clinical outcomes, improve trial design, enhance personalized treatments, and increase concordance in decision-making. Additionally, automating some areas and tasks, clinical trials were made more efficient, and human error was minimized. Nevertheless, concerns and restrictions related to the application of AI in clinical studies are also noted.
Conclusion
AI tools have the potential to revolutionize the design, enrollment rate, and outcome prediction of oncology clinical trials.}
}
@article{TU2024965,
title = {Architecture for data-centric and semantic-enhanced industrial metaverse: Bridging physical factories and virtual landscape},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {965-979},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001134},
author = {Xinyi Tu and Riku Ala-Laurinaho and Chao Yang and Juuso Autiosalo and Kari Tammi},
keywords = {Industrial metaverse, Virtual–physical continuum, Digital twins, Extended reality, Architecture, Industry 5.0},
abstract = {The metaverse paradigm has recently captured increasing scholarly and industrial attention, particularly within the scope of human-centric Industry 5.0. In this context, the metaverse promises a transformative confluence of the physical and digital realms, offering unparalleled avenues for human augmentation in industrial applications. Yet, while several conceptual metaverse architectures and illustrative case studies have emerged, they scarcely delve deep into the nuanced practice of cultivating the industrial metaverse for factory-scale applications. Addressing this research gap, this work introduces a novel architecture for a data-centric and semantic-enhanced industrial metaverse. The architecture intricately weaves the physical factory domain with the metaverse, fortified by a suite of ten modules, facilitating data flow and knowledge synchronization with the integration of digital twins and semantic models. The practical application and relevance of this architecture are further accentuated through a case study focused on in-plant material flow tracking. Emerging results underline that our architecture encapsulates the essential components for constructing a factory-scale industrial metaverse. Future research will be geared towards a comprehensive validation of the proposed metaverse architecture, culminating in tangible implementations across diverse industrial contexts.}
}
@article{SHI2024121488,
title = {Integrity verification for scientific papers: The first exploration of the text},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121488},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121488},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019905},
author = {Xiang Shi and Yinpeng Liu and Jiawei Liu and Qikai Cheng and Wei Lu},
keywords = {Integrity verification, Scientific paper, Named entity recognition, Joint model},
abstract = {Scientific papers, as pivotal tools for academic communication, should be articulated with clarity and precision to ensure the effective conveyance of scholarly ideas and to prevent reader confusion. Yet, many such papers conspicuously lack in-depth research, and their core content is often ambiguously presented. This pattern poses a significant impediment to the progressive evolution of science and technology. While numerous researchers have recognized this widespread challenge, a holistic theoretical or methodological solution remains elusive in the academic realm. To bridge this gap, we introduce the INTEGrity vERification (INTEGER) task. This task aids researchers in assessing the integrity of their papers by verifying the clarity of each knowledge unit. To implement this task on text, we propose a multi-task learning model that utilizes the Tucker decomposition and span-level attention mechanism to identify terms and their integrity precisely. More specifically, to provide insights into the INTEGER task and validate the effectiveness of the proposed model, we collect 8076 sentences and construct three new datasets containing various types of terms and descriptions in different domains. Extensive experimental results show that our proposed model has an average performance improvement of 1.1% F1 over the three datasets compared to a series of state-of-the-art baseline methods.}
}
@article{PU2024128442,
title = {Graph neural network based intelligent tutoring system: A survey},
journal = {Neurocomputing},
volume = {610},
pages = {128442},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128442},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401213X},
author = {Juhua Pu and Shufei Li and Meng Guo and Xi Chen and Zhang Xiong},
keywords = {Graph neural network, Intelligent tutoring system, Personalized learning, Representation Learning, Overview},
abstract = {Online education is developing rapidly driven by artificial intelligence technology. The massive learning resources lead to information overload and low resource utilization. Intelligent tutoring system (ITS) plays a vital role in the education platform, providing personalized learning services for students. The data obtained from the online education platform has complex correlations, which can be potentially transformed into multi-level graph structures. In recent years, graph neural networks (GNNs) have been tried to be introduced into intelligent learning services due to their superior performance in processing graph-structured data. This paper aims to provide researchers and engineers with a general overview of modeling processes and techniques for intelligent learning services based on GNNs. Through a careful review of the advanced models published between 2019 and 2023, existing research primarily focuses on four detailed areas within the smart services scenario. The GNN models involved are systematically classified, and the principles, pioneers and variants of various models are summarized in detail. Simultaneously, this paper analyzes the applications, the specific problems to be solved, and the technologies and innovations of graph-based models in the four key areas. In addition, we examine the commonly used datasets and evaluation metrics in the field of education. Finally, the current challenges and future development trends are summarized to provide comprehensive and in-depth guidance for research in related fields.}
}
@article{ALTURAYEIF2025125525,
title = {EASE: An enhanced active learning framework for aspect-based sentiment analysis based on sample diversity and data augmentation},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125525},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125525},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424023923},
author = {Nouf Alturayeif and Irfan Ahmad},
keywords = {Aspect-based sentiment analysis, Active learning, Deep learning},
abstract = {Aspect-Based Sentiment Analysis (ABSA) has received considerable attention in recent studies. Powerful pre-trained models were proposed which can be fine-tuned for many Natural Language Processing (NLP) tasks, including ABSA. However, fine-tuning these models needs a relatively large amount of labeled data. In this research, we propose EASE; an active learning framework to minimize manual labeling effort. We extend the active learning technique by incorporating the concept of sample diversity where similar samples are not selected for labeling. Furthermore, we maximize the utility of these samples by incorporating data augmentation. EASE was evaluated on three benchmark ABSA datasets from three different domains. The results show that the reduction of the number of needed labeled samples ranges from 88% to 94% among the three datasets while maintaining accuracy. Our results show that active learning is an effective approach to reduce manual labeling effort while maintaining comparable performance. Moreover, it is possible to reduce the number of labeled data even further by incorporating sample diversity and data augmentation while maintaining performance.}
}
@incollection{2024281,
title = {Nomenclature},
editor = {Shufei Li and Pai Zheng and Lihui Wang},
booktitle = {Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing},
publisher = {Elsevier},
pages = {281-283},
year = {2024},
isbn = {978-0-443-13943-7},
doi = {https://doi.org/10.1016/B978-0-44-313943-7.00020-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044313943700020X}
}
@article{XI2025301846,
title = {Towards a joint semantic analysis in mobile forensics environments},
journal = {Forensic Science International: Digital Investigation},
volume = {52},
pages = {301846},
year = {2025},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301846},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001732},
author = {Jian Xi and Melanie Siegel and Dirk Labudde and Michael Spranger},
keywords = {Semantic analysis, Mobile forensics, Topic modeling, Natural language processing, Multimodal machine learning, Communication analysis, Text mining, Semantic network},
abstract = {In recent years, mobile devices have become the dominant communication medium in our daily lives. This trend is also evident in the planning, arranging, and committing of criminal activities, particularly in organized crime. Accordingly, mobile devices have become an essential source of evidence for data analysts or investigators, especially in Law Enforcement Agencies (LEAs). However, communication via mobile devices generates vast amounts of data, rendering manual analysis impractical and resulting in growing backlogs of evidence awaiting analysis process, which can take months to years, thereby hindering investigations and trials. The automatic analysis of textual chat messages falls short because communication is not limited to the single modality, such as text, but instead spans multiple modalities, including voice messages, pictures, videos, and sometimes various messengers (channels). These modalities frequently overlap or interchange within the same communication, further complicating the analysis process. To achieve a correct and comprehensive understanding of such communication, it is essential to consider all modalities and channels through a consistent joint semantic analysis. This paper introduces a novel mobile forensics approach that enables efficient assessment of mobile data without losing semantic consistency by unifying semantic concepts across different modalities and channels. Additionally, a knowledge-guided topic modeling approach is proposed, integrating expertise into the investigation process to effectively examine large volumes of noisy mobile data. In this way, investigators can quickly identify evidentiary parts of the communication and completely facilitate reconstructing the course of events.}
}
@article{ABADIE2024123202,
title = {A shared journey: Experiential perspective and empirical evidence of virtual social robot ChatGPT's priori acceptance},
journal = {Technological Forecasting and Social Change},
volume = {201},
pages = {123202},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.123202},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523008879},
author = {Amelie Abadie and Soumyadeb Chowdhury and Sachin Kumar Mangla},
keywords = {Social robot, ChatGPT, Priori acceptance, UTAUT, Consumer value creation framework, Managerial usage intention},
abstract = {Due to recent technological advancements, social robots are becoming increasingly prevalent in the consumer space. ChatGPT, a virtual social robot, has captured significant attention from the mass media and academic practitioners alike since its release in November 2022. This attention arises from its remarkable capabilities, as well as potential challenges it poses to society and various business sectors. In light of these developments, we developed a theoretical model based on the Unified Theory of Acceptance and Use of Technology and a consumer value typology centred around consumer experiences to examine the influence of experiential factors on the intention to use ChatGPT and subsequently collaborating with it for co-creating content among business managers. To test this model, we conducted a survey of 195 business managers in the UK and employed partial PLS-structural equation modelling for analysis. Our findings indicate that the efficiency, excellence, meaningfulness of recommendations, and conversational ability of ChatGPT will influence the behavioural intention to use it during the priori acceptance stage. Based on these findings, we suggest that organisations should thoughtfully consider and strategize the deployment of ChatGPT applications to ensure their acceptance, eventual adoption, and subsequent collaboration between ChatGPT and managers for content creation or problem-solving.}
}
@incollection{2023243,
title = {Index},
editor = {Venu Govindaraju and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {48},
pages = {243-252},
year = {2023},
booktitle = {Deep Learning},
issn = {0169-7161},
doi = {https://doi.org/10.1016/S0169-7161(23)00019-6},
url = {https://www.sciencedirect.com/science/article/pii/S0169716123000196}
}
@article{ZHANG2024101738,
title = {Integrating multi-omics to unravel host-microbiome interactions in inflammatory bowel disease},
journal = {Cell Reports Medicine},
volume = {5},
number = {9},
pages = {101738},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101738},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124004683},
author = {Yiran Zhang and John P. Thomas and Tamas Korcsmaros and Lejla Gul},
abstract = {Summary
The gut microbiome is crucial for nutrient metabolism, immune regulation, and intestinal homeostasis with changes in its composition linked to complex diseases like inflammatory bowel disease (IBD). Although the precise host-microbial mechanisms in disease pathogenesis remain unclear, high-throughput sequencing have opened new ways to unravel the role of interspecies interactions in IBD. Systems biology—a holistic computational framework for modeling complex biological systems—is critical for leveraging multi-omics datasets to identify disease mechanisms. This review highlights the significance of multi-omics data in IBD research and provides an overview of state-of-the-art systems biology resources and computational tools for data integration. We explore gaps, challenges, and future directions in the research field aiming to uncover novel biomarkers and therapeutic targets, ultimately advancing personalized treatment strategies. While focusing on IBD, the proposed approaches are applicable for other complex diseases, like cancer, and neurodegenerative diseases, where the microbiome has also been implicated.}
}
@article{MAKAROV2024108632,
title = {Good machine learning practices: Learnings from the modern pharmaceutical discovery enterprise},
journal = {Computers in Biology and Medicine},
volume = {177},
pages = {108632},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108632},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007170},
author = {Vladimir Makarov and Christophe Chabbert and Elina Koletou and Fotis Psomopoulos and Natalja Kurbatova and Samuel Ramirez and Chas Nelson and Prashant Natarajan and Bikalpa Neupane},
keywords = {Artificial intelligence, Machine learning, Pharmaceutical, Drug discovery, Best practice, Life sciences},
abstract = {Machine Learning (ML) and Artificial Intelligence (AI) have become an integral part of the drug discovery and development value chain. Many teams in the pharmaceutical industry nevertheless report the challenges associated with the timely, cost effective and meaningful delivery of ML and AI powered solutions for their scientists. We sought to better understand what these challenges were and how to overcome them by performing an industry wide assessment of the practices in AI and Machine Learning. Here we report results of the systematic business analysis of the personas in the modern pharmaceutical discovery enterprise in relation to their work with the AI and ML technologies. We identify 23 common business problems that individuals in these roles face when they encounter AI and ML technologies at work, and describe best practices (Good Machine Learning Practices) that address these issues.}
}
@article{JIN2023114038,
title = {Building a deep learning-based QA system from a CQA dataset},
journal = {Decision Support Systems},
volume = {175},
pages = {114038},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114038},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001136},
author = {Sol Jin and Xu Lian and Hanearl Jung and Jinsoo Park and Jihae Suh},
keywords = {Question answering (QA) system, Community question answering (CQA), BERT, T5},
abstract = {A man-made machine-reading comprehension (MRC) dataset is necessary to train the answer extraction part of existing Question Answering (QA) systems. However, a high-quality and well-structured dataset with question-paragraph-answer pairs is not usually found in the real world. Furthermore, updating or building an MRC dataset is a challenging and costly affair. To address these shortcomings, we propose a QA system that uses a large-scale English Community Question Answering (CQA) dataset (i.e., Stack Exchange) composed of 3,081,834 question-answer pairs. The QA system adopts a classifier-retriever-summarizer structure design. The question classifier and the answer retriever part are based on a Bidirectional Encoder Representations from Transformers (BERT) Natural Language Processing (NLP) model by Google, and the summarizer part introduces a deep learning-based Text-to-Text Transfer Transformer (T5) model to summarize the long answers. We instantiated the proposed QA system with 140 topics from the CQA dataset (including topics such as biology, law, politics, etc.) and conducted human and automatic evaluations. Our system presented encouraging results, considering that it provides high-quality answers to the questions in the test set and satisfied the requirements to develop a QA system without MRC datasets. Our results show the potential of building automatic and high-performance QA systems without being limited by man-made datasets, a significant step forward in the research of open-domain or specific-domain QA systems.}
}
@article{COBANAJ2024113504,
title = {Advancing equitable and personalized cancer care: Novel applications and priorities of artificial intelligence for fairness and inclusivity in the patient care workflow},
journal = {European Journal of Cancer},
volume = {198},
pages = {113504},
year = {2024},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2023.113504},
url = {https://www.sciencedirect.com/science/article/pii/S0959804923008067},
author = {Marisa Cobanaj and Chiara Corti and Edward C. Dee and Lucas McCullum and Laura Boldrini and Ilana Schlam and Sara M. Tolaney and Leo A. Celi and Giuseppe Curigliano and Carmen Criscitiello},
keywords = {Artificial intelligence, Bias, Decision support, Equity, Inclusivity, Precision medicine},
abstract = {Patient care workflows are highly multimodal and intertwined: the intersection of data outputs provided from different disciplines and in different formats remains one of the main challenges of modern oncology. Artificial Intelligence (AI) has the potential to revolutionize the current clinical practice of oncology owing to advancements in digitalization, database expansion, computational technologies, and algorithmic innovations that facilitate discernment of complex relationships in multimodal data. Within oncology, radiation therapy (RT) represents an increasingly complex working procedure, involving many labor-intensive and operator-dependent tasks. In this context, AI has gained momentum as a powerful tool to standardize treatment performance and reduce inter-observer variability in a time-efficient manner. This review explores the hurdles associated with the development, implementation, and maintenance of AI platforms and highlights current measures in place to address them. In examining AI’s role in oncology workflows, we underscore that a thorough and critical consideration of these challenges is the only way to ensure equitable and unbiased care delivery, ultimately serving patients’ survival and quality of life.}
}
@article{SINGHAL2023765,
title = {Opportunities and challenges for biomarker discovery using electronic health record data},
journal = {Trends in Molecular Medicine},
volume = {29},
number = {9},
pages = {765-776},
year = {2023},
issn = {1471-4914},
doi = {https://doi.org/10.1016/j.molmed.2023.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1471491423001417},
author = {P. Singhal and A.L.M. Tan and T.G. Drivas and K.B. Johnson and M.D. Ritchie and B.K. Beaulieu-Jones},
keywords = {electronic health records, biomarker discovery, phenotyping, precision medicine},
abstract = {Electronic health records (EHRs) have become increasingly relied upon as a source for biomedical research. One important research application of EHRs is the identification of biomarkers associated with specific patient states, especially within complex conditions. However, using EHRs for biomarker identification can be challenging because the EHR was not designed with research as the primary focus. Despite this challenge, the EHR offers huge potential for biomarker discovery research to transform our understanding of disease etiology and treatment and generate biological insights informing precision medicine initiatives. This review paper provides an in-depth analysis of how EHR data is currently used for phenotyping and identifying molecular biomarkers, current challenges and limitations, and strategies we can take to mitigate challenges going forward.}
}
@article{JIAO20241,
title = {A Comprehensive Survey on Deep Learning Multi-Modal Fusion: Methods, Technologies and Applications},
journal = {Computers, Materials and Continua},
volume = {80},
number = {1},
pages = {1-35},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.053204},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824005216},
author = {Tianzhe Jiao and Chaopeng Guo and Xiaoyue Feng and Yuming Chen and Jie Song},
keywords = {Multi-modal fusion, representation, translation, alignment, deep learning, comparative analysis},
abstract = {Multi-modal fusion technology gradually become a fundamental task in many fields, such as autonomous driving, smart healthcare, sentiment analysis, and human-computer interaction. It is rapidly becoming the dominant research due to its powerful perception and judgment capabilities. Under complex scenes, multi-modal fusion technology utilizes the complementary characteristics of multiple data streams to fuse different data types and achieve more accurate predictions. However, achieving outstanding performance is challenging because of equipment performance limitations, missing information, and data noise. This paper comprehensively reviews existing methods based on multi-modal fusion techniques and completes a detailed and in-depth analysis. According to the data fusion stage, multi-modal fusion has four primary methods: early fusion, deep fusion, late fusion, and hybrid fusion. The paper surveys the three major multi-modal fusion technologies that can significantly enhance the effect of data fusion and further explore the applications of multi-modal fusion technology in various fields. Finally, it discusses the challenges and explores potential research opportunities. Multi-modal tasks still need intensive study because of data heterogeneity and quality. Preserving complementary information and eliminating redundant information between modalities is critical in multi-modal technology. Invalid data fusion methods may introduce extra noise and lead to worse results. This paper provides a comprehensive and detailed summary in response to these challenges.}
}
@article{CHAKRABORTY2024102295,
title = {The changing scenario of drug discovery using AI to deep learning: Recent advancement, success stories, collaborations, and challenges},
journal = {Molecular Therapy - Nucleic Acids},
volume = {35},
number = {3},
pages = {102295},
year = {2024},
issn = {2162-2531},
doi = {https://doi.org/10.1016/j.omtn.2024.102295},
url = {https://www.sciencedirect.com/science/article/pii/S2162253124001823},
author = {Chiranjib Chakraborty and Manojit Bhattacharya and Sang-Soo Lee and Zhi-Hong Wen and Yi-Hao Lo},
keywords = {MT: Bioinformatics, drug molecules, artificial intelligence, drug discovery, deep learning, pharmaceutics},
abstract = {Due to the transformation of artificial intelligence (AI) tools and technologies, AI-driven drug discovery has come to the forefront. It reduces the time and expenditure. Due to these advantages, pharmaceutical industries are concentrating on AI-driven drug discovery. Several drug molecules have been discovered using AI-based techniques and tools, and several newly AI-discovered drug molecules have already entered clinical trials. In this review, we first present the data and their resources in the pharmaceutical sector for AI-driven drug discovery and illustrated some significant algorithms or techniques used for AI and ML which are used in this field. We gave an overview of the deep neural network (NN) models and compared them with artificial NNs. Then, we illustrate the recent advancement of the landscape of drug discovery using AI to deep learning, such as the identification of drug targets, prediction of their structure, estimation of drug-target interaction, estimation of drug-target binding affinity, design of de novo drug, prediction of drug toxicity, estimation of absorption, distribution, metabolism, excretion, toxicity; and estimation of drug-drug interaction. Moreover, we highlighted the success stories of AI-driven drug discovery and discussed several collaboration and the challenges in this area. The discussions in the article will enrich the pharmaceutical industry.}
}
@article{BAO2024102854,
title = {Platform service portfolio management (PSPM) of social digitalization platform for cloud-based collaborative product development ecosystem: A structural approach},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102854},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102854},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624005020},
author = {Yuguang Bao and Xianyu Zhang and Zhihua Chen and Tongtong Zhou and Xinguo Ming},
keywords = {Social digitalization platform, Platform service portfolio management, Business-technology-system alignment, System analysis, Heterogeneous causal correlations, HINGS-TOPSIS method},
abstract = {In the context of digital transformation, social digitalization platform (SDP) emerges and play an important and irreplaceable role in the whole industrial ecosystems. The main value creation purpose of SDP is to develop general technical service portfolios and customized solutions. For SDPs, platform service portfolio management (PSPM) is a really difficult problem. The decision must dynamically respond to the complex external environments of business requirement, technology evolution, and system innovation. The traditional experience-driven decision-making process confuses various factors together, and it is difficult to produce scientific decisions continuously and effectively. To help these platform-kind firms make wise decision, we put forward a novel and practical decision-making methodological framework for PSPM issues. Firstly, a general system architecture model, namely Business-Digitalization-System (BDS) model, is proposed based on system engineering, which helps platform service component identification more refined. Next, the PSPM problem is formalized and modelled as a causal relation graph-based intertwined system analysis procedure. A novel platform service portfolio evaluation method is developed to obtain the priority of the identified service alternatives. The approach can effectively handle the component system importance and its heterogeneous causal interdependencies simultaneously. Meanwhile, the combinatorial manipulation of subjective judgements and objective measure improves the reasonability and efficiency within a multi-stakeholder group. Finally, a case study in the constructive industry is presented and the results of method comparisons show the feasibility and advantages of the methodology.}
}
@article{GAO2024188,
title = {Hypergraph Computation},
journal = {Engineering},
volume = {40},
pages = {188-201},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924002510},
author = {Yue Gao and Shuyi Ji and Xiangmin Han and Qionghai Dai},
keywords = {High-order correlation, Hypergraph structure modeling, Hypergraph semantic computing, Efficient hypergraph computing, Hypergraph computation framework},
abstract = {Practical real-world scenarios such as the Internet, social networks, and biological networks present the challenges of data scarcity and complex correlations, which limit the applications of artificial intelligence. The graph structure is a typical tool used to formulate such correlations, it is incapable of modeling high-order correlations among different objects in systems; thus, the graph structure cannot fully convey the intricate correlations among objects. Confronted with the aforementioned two challenges, hypergraph computation models high-order correlations among data, knowledge, and rules through hyperedges and leverages these high-order correlations to enhance the data. Additionally, hypergraph computation achieves collaborative computation using data and high-order correlations, thereby offering greater modeling flexibility. In particular, we introduce three types of hypergraph computation methods: ① hypergraph structure modeling, ② hypergraph semantic computing, and ③ efficient hypergraph computing. We then specify how to adopt hypergraph computation in practice by focusing on specific tasks such as three-dimensional (3D) object recognition, revealing that hypergraph computation can reduce the data requirement by 80% while achieving comparable performance or improve the performance by 52% given the same data, compared with a traditional data-based method. A comprehensive overview of the applications of hypergraph computation in diverse domains, such as intelligent medicine and computer vision, is also provided. Finally, we introduce an open-source deep learning library, DeepHypergraph (DHG), which can serve as a tool for the practical usage of hypergraph computation.}
}
@article{ZHANG2025104065,
title = {Empowering crisis information extraction through actionability event schemata and domain-adaptive pre-training},
journal = {Information & Management},
volume = {62},
number = {1},
pages = {104065},
year = {2025},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.104065},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624001472},
author = {Yuhao Zhang and Siaw Ling Lo and Phyo Yi Win Myint},
keywords = {Actionability extraction, Social media crisis detection, Multi-task learning, Domain-adaptive pre-training},
abstract = {One of the persistent challenges in crisis detection is inferring actionable information to support emergency response. Existing methods focus on situational awareness but often lack actionable insights. This study proposes a holistic approach to implementing an actionability extraction system on social media, including requirement gathering, selection of machine learning tasks, data preparation, and integration with existing resources, providing guidance for governments, civil services, emergency workers, and researchers on supplementing existing channels with actionable information from social media. Our solution leverages an actionability schema and domain-adaptive pre-training, improving upon the state-of-the-art model by 5.5 % and 10.1 % in micro and macro F1 scores.}
}
@article{MORENOBAREA2025109576,
title = {Named entity recognition for de-identifying Spanish electronic health records},
journal = {Computers in Biology and Medicine},
volume = {185},
pages = {109576},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109576},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524016615},
author = {Francisco J. Moreno-Barea and Guillermo López-García and Héctor Mesa and Nuria Ribelles and Emilio Alba and José M. Jerez and Francisco J. Veredas},
keywords = {Named entity recognition, Natural language processing, De-identification, Electronic health records, Spanish},
abstract = {Background and objectives:
There is an increasing and renewed interest in Electronic Health Records (EHRs) as a substantial information source for clinical decision making. Consequently, automatic de-identification of EHRs is an indispensable task, since their dissociation from personal data is a necessary prerequisite for their dissemination. Nevertheless, the bulk of prior research in this domain has been conducted using English EHRs, given the limited availability of annotated corpora in other languages, including Spanish.
Methods:
In this study, the automatic de-identification of medical documents in Spanish was explored. A private corpus comprising 599 genuine clinical cases was annotated with eight different categories of protected health information. The prediction problem was approached as a named entity recognition task and two deep learning-based methodologies were developed. The first strategy was based on recurrent neural networks (RNN) and the second, an end-to-end approach, was based on Transformers. In addition, we have implemented a procedure to expand the amount of texts employed for model training.
Results:
Our findings demonstrate that Transformers surpass RNNs in the de-identification of clinical data in Spanish. Particularly noteworthy is the excellent performance of the XLM-RoBERTa large Transformer, achieving a rigorous strict-match micro-average of 0.946 for precision, 0.954 for recall, and an F1 score of 0.95 when applied to the amplified version of the corpus. Furthermore, a web-based application has been created to assist specialized clinicians in de-identifying EHRs through the aid of the implemented models.
Conclusion:
The study’s conclusions showcase the practical applicability of the state-of-the-art Transformers models for precise de-identification of clinical notes in real-world medical settings in Spanish, with the potential to improve performance if continual pre-training strategies are implemented.}
}
@article{ZHANG2024103812,
title = {Chinese nested entity recognition method for the finance domain based on heterogeneous graph network},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103812},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103812},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001717},
author = {Han Zhang and Yiping Dang and Yazhou Zhang and Siyuan Liang and Junxiu Liu and Lixia Ji},
keywords = {Chinese finance domain, Nested named entity recognition, Heterogeneous graphs, Expert knowledge},
abstract = {In the finance domain, nested named entities recognition has become a hot topic in named entity recognition tasks. Traditional nested entity recognition methods easily ignore the dependency relationships between entities, and these methods are mostly suitable for English general domain. Therefore, we propose a Chinese nested entity recognition method for the finance domain based on heterogeneous graph network(HGFNER). This method consists of two parts: the boundary division model of candidate entities and the internal relationship graph model of candidate entities. First, the boundary division model of candidate entities that introduces expert knowledge is used to partition the flat entities contained in the text and segment the text to address issues such as long entity boundaries and strong domain features in the Chinese finance domain. Then, by using heterogeneous graphs to represent the internal structure of entities from both spatial and syntactic dependencies to achieve the goal of learning dependency relationships between entities from multiple perspectives. Meanwhile, so as not to affect the operational efficiency of the model, we also propose a fast matching algorithm DAAC_BM for n-gram sequences in domain dictionaries to solve the problems of memory overflow and space waste faced by multi-pattern fast matching algorithms in Chinese matching. In addition, we propose a Chinese nested entity dataset CFNE for the financial field, which, as far as we know, is the first publicly available annotated dataset in the field. HGFNER achieves state-of-the-art macro-F1 value on CFNE, reaching 86.41%.}
}
@article{XU2025102721,
title = {Interpretability research of deep learning: A literature survey},
journal = {Information Fusion},
volume = {115},
pages = {102721},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102721},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004998},
author = {Biao Xu and Guanci Yang},
keywords = {Deep learning, Interpretability, Active explanations, Passive explanations, Explainable artificial intelligence},
abstract = {Deep learning (DL) has been widely used in various fields. However, its black-box nature limits people's understanding and trust in its decision-making process. Therefore, it becomes crucial to research the DL interpretability, which can elucidate the model's decision-making processes and behaviors. This review provides an overview of the current status of interpretability research. First, the DL's typical models, principles, and applications are introduced. Then, the definition and significance of interpretability are clarified. Subsequently, some typical interpretability algorithms are introduced into four groups: active, passive, supplementary, and integrated explanations. After that, several evaluation indicators for interpretability are briefly described, and the relationship between interpretability and model performance is explored. Next, the specific applications of some interpretability methods/models in actual scenarios are introduced. Finally, the interpretability research challenges and future development directions are discussed.}
}
@article{SHEN2024111304,
title = {Unsupervised multilingual machine translation with pretrained cross-lingual encoders},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111304},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111304},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123010523},
author = {Yingli Shen and Wei Bao and Ge Gao and Maoke Zhou and Xiaobing Zhao},
keywords = {Unsupervised learning, Multilingual machine translation, Cross-lingual encoders},
abstract = {Multilingual Neural Machine Translation (MNMT) has recently made great progress in training models that can translate between multiple languages. However, MNMT faces a significant challenge due to the lack of sufficient parallel corpora for all language pairs. Unsupervised machine translation methods, which utilize monolingual data, have emerged as a solution to this challenge. In this paper, we propose a method that leverages cross-lingual encoders, such as XLM-R, in an unsupervised manner (i.e., using monolingual data and bilingual dictionaries) to train a MNMT model. Our method initializes the MNMT model with a pre-trained cross-lingual encoder and employs two levels of alignment to further align the representation space in MNMT model. Experimental results demonstrate that our method outperforms strong baseline systems and exhibits robust domain and language transfer capabilities while preserving the performance of the original pre-trained encoder on other downstream tasks.}
}
@article{SUN2024102823,
title = {A label information fused medical image report generation framework},
journal = {Artificial Intelligence in Medicine},
volume = {150},
pages = {102823},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102823},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000654},
author = {Shuifa Sun and Zhoujunsen Mei and Xiaolong Li and Tinglong Tang and Zhanglin Su and Yirong Wu},
keywords = {Medical image, Text generation, Attention mechanism, Multi-modal feature fusion, Feature extraction},
abstract = {Medical imaging is an important tool for clinical diagnosis. Nevertheless, it is very time-consuming and error-prone for physicians to prepare imaging diagnosis reports. Therefore, it is necessary to develop some methods to generate medical imaging reports automatically. Currently, the task of medical imaging report generation is challenging in at least two aspects: (1) medical images are very similar to each other. The differences between normal and abnormal images and between different abnormal images are usually trivial; (2) unrelated or incorrect keywords describing abnormal findings in the generated reports lead to mis-communications. In this paper, we propose a medical image report generation framework composed of four modules, including a Transformer encoder, a MIX-MLP multi-label classification network, a co-attention mechanism (CAM) based semantic and visual feature fusion, and a hierarchical LSTM decoder. The Transformer encoder can be used to learn long-range dependencies between images and labels, effectively extract visual and semantic features of images, and establish long-term dependent relationships between visual and semantic information to accurately extract abnormal features from images. The MIX-MLP multi-label classification network, the co-attention mechanism and the hierarchical LSTM network can better identify abnormalities, achieving visual and text alignment fusion and multi-label diagnostic classification to better facilitate report generation. The results of the experiments performed on two widely used radiology report datasets, IU X-RAY and MIMIC-CXR, show that our proposed framework outperforms current report generation models in terms of both natural linguistic generation metrics and clinical efficacy assessment metrics. The code of this work is available online at https://github.com/watersunhznu/LIFMRG.}
}
@article{ZHANG2024121211,
title = {Multi-information interaction graph neural network for joint entity and relation extraction},
journal = {Expert Systems with Applications},
volume = {235},
pages = {121211},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121211},
url = {https://www.sciencedirect.com/science/article/pii/S095741742301713X},
author = {Yini Zhang and Yuxuan Zhang and Zijing Wang and Huanchun Peng and Yongsheng Yang and Yuanxiang Li},
keywords = {Joint entity and relation extraction, Graph neural network, Transformer, Overlapping triplets, Distant supervision},
abstract = {Overlap situation where different triplets share entities or relations is a common challenge in joint entity and relation extraction task. On the one hand, there is strong correlation between overlapping triplets. On the other hand, most of the existing large-scale training data come from distant supervision, which introduces incomplete annotations. These practical problems make the information interaction between triplets particularly important. However, there are two problems with the existing methods: (i) the neglect of information interaction between different triplets; (ii) the limited information utilization caused by the specific decoding order. To solve the above problems, we decompose decoding and information interaction. Specifically, entity and relation proposals are obtained by a proposal generator, then a multi-information interaction graph neural network with parallel decoder is proposed to complete the joint extraction task. In this way, the inherent decoding order is broken to achieve the purpose of fully exploiting multi-information interaction across triplets and within triplets. Experimental results show that our proposed model outperforms previous work, especially in the case of incomplete annotations.}
}
@article{AALEALI2024100049,
title = {Machine learning advancements in organic synthesis: A focused exploration of artificial intelligence applications in chemistry},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {1},
pages = {100049},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100049},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000071},
author = {Rizvi Syed {Aal E Ali} and Jiaolong Meng and Muhammad Ehtisham Ibraheem Khan and Xuefeng Jiang},
keywords = {Artificial intelligence, Chemical selectivity, Retrosynthesis prediction, Catalyst design, Material design},
abstract = {Artificial intelligence (AI) is driving a revolution in chemistry, reshaping the landscape of molecular design. This review explores AI’s pivotal roles in the field of organic synthesis applications. AI accurately predicts reaction outcomes, controls chemical selectivity, simplifies synthesis planning, accelerates catalyst discovery, and fuels material innovation and so on. It seamlessly integrates data-driven algorithms with chemical intuition to redefine molecular design. As AI chemistry advances, it promises accelerated research, sustainability, and innovative solutions to chemistry’s pressing challenges. The fusion of AI and chemistry is poised to shape the field’s future profoundly, offering new horizons in precision and efficiency. This review encapsulates the transformation of AI in chemistry, marking a pivotal moment where algorithms and data converge to revolutionize the world of molecules.}
}
@article{JIA2025126130,
title = {Joint entity and relation extraction with table filling based on graph convolutional Networks},
journal = {Expert Systems with Applications},
volume = {266},
pages = {126130},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126130},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402997X},
author = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
keywords = {Graph convolutional networks, Graph construction, Joint entity and relation extraction, Table filling},
abstract = {Information extraction involves extracting structured information from text, which can be categorized into named entity recognition (NER) and relation extraction (RE). Recent successful works address the dependency between NER and RE using a filling table approach, but they overlook the long-distance dependencies between cells in the table, which may not adequately understand the semantics between distant elements in the text. To address this limitation, we introduce an innovative approach known as Table Filling with Graph Convolutional Networks (TableF-GCN) for joint NER and RE. First, TableF-GCN constructs a graph that captures the interaction between cells within the table. Additionally, it leverages GCNs to encode the long-distance dependency within the graph through multi-hop propagation, thereby enabling the acquisition of local and global contextualized embeddings for each node in the graph. Experimental results are conducted on joint NER and RE extraction, ablation study, hyper parameter setting, and a case study on three widely utilized datasets. The comparative analysis demonstrates TableF-GCN outperforms the baselines in recall, but not necessarily in precision. Fortunately, when consolidating the outcomes from the ConLL04 and ADE datasets, TableF-GCN showcases overall improvement through statistical analysis.}
}
@article{WANG2025101600,
title = {Integrating artificial intelligence in energy transition: A comprehensive review},
journal = {Energy Strategy Reviews},
volume = {57},
pages = {101600},
year = {2025},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2024.101600},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X24003092},
author = {Qiang Wang and Yuanfan Li and Rongrong Li},
keywords = {Artificial intelligence, Energy transition, Clean energy supply, Demand-side management, Technological innovation, Smart grids},
abstract = {The global energy transition, driven by the imperative to mitigate climate change, demands innovative solutions to address the technical, economic, and social challenges of decarbonization. Artificial intelligence (AI) has emerged as a transformative technology in this domain, offering tools to enhance each link in the energy system. This comprehensive review examines the current state of AI applications across key energy transition domains, including renewable energy deployment, energy efficiency, grid stability, and smart grid integration. The study identifies the pivotal role of AI in accelerating the adoption of intermittent renewable energy sources like solar and wind, managing demand-side dynamics with advanced forecasting and optimization, and enabling energy storage and distribution innovations such as vehicle-to-grid systems and hybrid energy solutions. It also highlights the potential of AI to advance energy system stability, address cybersecurity risks, and promote equitable and sustainable energy systems. Despite these advancements, challenges remain, including data quality and accessibility, system interoperability, scalability, and concerns regarding privacy and ethics. By synthesizing recent research and practical case studies, this paper provides insights into the opportunities and limitations of AI-driven energy transformation and offers strategic recommendations to guide future research, development, and policy-making. This review highlights that AI is not just a tool but a transformative catalyst, reshaping global energy systems into equitable, resilient, and sustainable frameworks, essential for achieving a net-zero future.}
}
@article{DIKMEN2025104251,
title = {Automated construction contract analysis for risk and responsibility assessment using natural language processing and machine learning},
journal = {Computers in Industry},
volume = {166},
pages = {104251},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104251},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000168},
author = {Irem Dikmen and Gorkem Eken and Huseyin Erol and M. Talat Birgonul},
keywords = {Automated contract review, Natural Language Processing (NLP), Machine Learning (ML), Artificial Intelligence (AI), Text classification, Construction risk management},
abstract = {Construction contracts contain critical risk-related information that requires in-depth examination, yet tight schedules for bidding limit the possibility of comprehensive review of extensive documents manually. This research aims to develop models for automating the review of construction contracts to extract information on risk and responsibility that will provide inputs for risk management plans. Models were trained on 2268 sentences from International Federation of Consulting Engineers templates and tested on an actual construction project contract containing 1217 sentences. A taxonomy classified sentences into Heading, Definition, Obligation, Risk, and Right categories with related parties of Contractor, Employer, and Shared. Twelve models employing diverse Natural Language Processing vectorization techniques and Machine Learning algorithms were implemented and benchmarked based on accuracy and F1 score. Binary classification of sentence types and an ensemble method integrating top models were further applied to improve performance. The best model achieved 89 % accuracy for sentence types and 83 % for related parties, demonstrating the capabilities of automated contract review for identification of risk and responsibilities. Adopting the proposed approach can significantly expedite contract reviews to support risk management activities, bid preparation processes and prevent disputes caused by overlooking risks and responsibilities.}
}
@article{MOSCATO2024112682,
title = {ALDANER: Active Learning based Data Augmentation for Named Entity Recognition},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112682},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112682},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013169},
author = {Vincenzo Moscato and Marco Postiglione and Giancarlo Sperlì and Andrea Vignali},
keywords = {Data augmentation, Named Entity Recognition, Active Learning},
abstract = {Training Named Entity Recognition (NER) models typically necessitates the use of extensively annotated datasets. This requirement presents a significant challenge due to the labor-intensive and costly nature of manual annotation, especially in specialized domains such as medicine and finance. To address data scarcity, two strategies have emerged as effective: (1) Active Learning (AL), which autonomously identifies samples that would most enhance model performance if annotated, and (2) data augmentation, which automatically generates new samples. However, while AL reduces human effort, it does not eliminate it entirely, and data augmentation often leads to incomplete and noisy annotations, presenting new hurdles in NER model training. In this study, we integrate AL principles into a data augmentation framework, named Active Learning-based Data Augmentation for NER (ALDANER), to prioritize the selection of informative samples from an augmented pool and mitigate the impact of noisy annotations. Our experiments across various benchmark datasets and few-shot scenarios demonstrate that our approach surpasses several data augmentation baselines, offering insights into promising avenues for future research.}
}
@article{DING2024123308,
title = {Popularity prediction with semantic retrieval for news recommendation},
journal = {Expert Systems with Applications},
volume = {247},
pages = {123308},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123308},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424001738},
author = {Yuhan Ding and Bang Wang and Xiangyang Cui and Minghua Xu},
keywords = {News recommendation, Popularity, Semantic retrieval, Hypergraph},
abstract = {News recommendation (NR) is critical for helping users to navigate the vast amount of information on online news platforms. However, the key challenges of tackling the cold-start problem, comprehensively modeling user interests and accurately encoding news semantics in NR have not yet been effectively addressed by existing methods. In this paper, we propose to alleviate the cold-start problem by exploiting news popularity. We also argue that the browsing history of a user can be organized as a personalized hypergraph to augment user interest modeling. Furthermore, we suggest enriching the representation of a candidate news article by retrieving its similar news. Our proposed NR method, named Popularity Prediction with Semantic Retrieval (PPSR), features a retrieval-driven popularity predictor that leverages click records to predict news popularity by incorporating semantic retrieval. It also comprises a hypergraph-based user encoder that mines the rich and diverse relatedness among news in a user’s browsing history. In addition, it designs a semantic-enhanced news encoder that retrieves and utilizes similar news in the training set to enrich semantic encoding for candidate news. Experiments on real-world datasets show that our PPSR can outperform the state-of-the-art methods in terms of higher recommendation accuracy and can well mitigate both the user-level and news-level cold-start problems.}
}
@article{MA2025112700,
title = {Semantic-based topic model for public opinion analysis in sudden-onset disasters},
journal = {Applied Soft Computing},
volume = {170},
pages = {112700},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112700},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625000110},
author = {Yulong Ma and Xinsheng Zhang and Runzhou Wang},
keywords = {Sudden-onset disaster, Topic model, Latent Dirichlet allocation, Distributional semantics},
abstract = {Sudden-onset disasters have put forward more stringent requirements for the government to carry out public opinion analysis work. However, most existing topic models ignore the contextual semantics of disaster texts, and fail to balance the robustness and the training cost. To address these issues, a neural clustering topic model is proposed in this work. The topic probability distribution of the LDA model is integrated with the distribution semantic vector generated by a lite BERT. The fused vectors are reconstructed by a nonlinear manifold learning algorithm, and re-clustered into topics by a mini-batch based k-means++ algorithm. Compared to state-of-the-art models on three sudden-onset disaster datasets, the proposed model shows an increase of 1.79 % in average topic coherence and 33.87 % in topic diversity. Meanwhile, the inference time is reduced by 84.09 % on average. The visual study of the latent process of the proposed model reflects that its ability to compact intra-cluster vector distances and sparse inter-cluster vector distances is the potential reason for its better performance. It can be considered that the application of the proposed model can help the government enhance its ability to manage negative public opinions in sudden-onset disasters.}
}
@incollection{2024377,
title = {Index},
editor = {Vijai Singh},
series = {Progress in Molecular Biology and Translational Science},
publisher = {Academic Press},
volume = {205},
pages = {377-388},
year = {2024},
booktitle = {New Approach for Drug Repurposing Part A},
issn = {1877-1173},
doi = {https://doi.org/10.1016/S1877-1173(24)00111-X},
url = {https://www.sciencedirect.com/science/article/pii/S187711732400111X}
}
@article{JIANG2025104278,
title = {Textual adversarial attacks in cybersecurity named entity recognition},
journal = {Computers & Security},
volume = {150},
pages = {104278},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104278},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005844},
author = {Tian Jiang and Yunqi Liu and Xiaohui Cui},
keywords = {Cyber Threat Intelligence, Named Entity Recognition, Fine-tuned models, Adversarial examples, Word substitution, Adversarial detection},
abstract = {In the cybersecurity domain, Cyber Threat Intelligence (CTI) includes procedures that lead to textual reports and different types of pieces of information and evidence on cyber threats. To better understand the behaviors of attackers and construct attack graphs, identifying attack-relevant entities in diverse CTI texts precisely and efficiently becomes more important, and Named Entity Recognition (NER) models can help extract entities automatically. However, such fine-tuned models are usually vulnerable to adversarial attacks. In this paper, we first construct an attack framework that can explore textual adversarial attacks in the cybersecurity NER task by generating adversarial CTI texts. Then, we analyze the most important parts of speech (POSs) from the perspective of grammar, and propose a word-substitution-based attack method. To confront adversarial attacks, we also introduce a method to detect potential adversarial examples. Experimental results show that cybersecurity NER models are also vulnerable to adversarial attacks. Among all attack methods, our method can generate adversarial texts that keep a balanced performance in several aspects. Furthermore, adversarial examples generated by all attack methods perform well in the study of transferability, and they can help improve the robustness of NER models through adversarial training. On the defense side, our detection method is simple but effective against multiple types of textual adversarial attacks.}
}
@article{TCHOUKA2024200416,
title = {Differentially private de-identifying textual medical document is compliant with challenging NLP analyses: Example of privacy-preserving ICD-10 code association},
journal = {Intelligent Systems with Applications},
volume = {23},
pages = {200416},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200416},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000905},
author = {Yakini Tchouka and Jean-François Couchot and David Laiymani and Philippe Selles and Azzedine Rahmani},
keywords = {De-identification, Clinical data, Local differential privacy, Metric-privacy, Natural language processing, ICD-10 code association, Machine learning},
abstract = {Medical research plays a crucial role within scientific research. Technological advancements, especially those related to the rise of machine learning, pave the way for the exploration of medical issues that were once beyond reach. Unstructured textual data, such as correspondence between doctors, operative reports, etc., often serve as a starting point for many medical applications. However, for obvious privacy reasons, researchers do not legally have the right to access these documents as long as they contain sensitive data, as defined by regulations like GDPR (General Data Protection Regulation) or HIPAA (Health Insurance Portability and Accountability Act). De-identification, meaning the detection, removal or substitution of all sensitive information, is therefore a necessary step to facilitate the sharing of these data between the medical field and research. Over the past decade, various approaches have been proposed to de-identify medical textual data. However, while entity detection is a well-known task in the natural language processing field, it presents some specific challenges in the medical context. Moreover, existing substitution methods proposed in the literature often pay little attention to the medical relevance of de-identified data or are not very resilient to attacks. This paper addresses these challenges. Firstly, an efficient system for detecting sensitive entities in French medical data and then accurately substitute them was implemented. Secondly, robust strategies for generating substitutes that incorporate the medical utility of the data were provided, thereby minimizing the difference in utility between the original and de-identified data, and that mathematically ensure privacy protection. Thirdly, the utility of the de-identification system in a context of ICD-10 code association was evaluated. Finally, various systems developed to tackle ICD-10 code association were presented while providing a state-of-the-art model in French.}
}
@article{GUO2025107024,
title = {Counterfactual learning for higher-order relation prediction in heterogeneous information networks},
journal = {Neural Networks},
volume = {183},
pages = {107024},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024009535},
author = {Xuan Guo and Jie Li and Pengfei Jiao and Wang Zhang and Tianpeng Li and Wenjun Wang},
keywords = {Heterogeneous information networks, Higher-order relation prediction, Counterfactual learning, Network representation learning},
abstract = {Heterogeneous Information Networks (HINs) play a crucial role in modeling complex social systems, where predicting missing links/relations is a significant task. Existing methods primarily focus on pairwise relations, but real-world scenarios often involve multi-entity interactions. For example, in academic collaboration networks, an interaction occurs between a paper, a conference, and multiple authors. These higher-order relations are prevalent but have been underexplored. Moreover, existing methods often neglect the causal relationship between the global graph structure and the state of relations, limiting their ability to capture the fundamental factors driving relation prediction. In this paper, we propose HINCHOR, an end-to-end model for higher-order relation prediction in HINs. HINCHOR introduces a higher-order structure encoder to capture multi-entity proximity information. Then, it focuses on a counterfactual question: “If the global graph structure were different, would the higher-order relation change?” By presenting a counterfactual data augmentation module, HINCHOR utilizes global structure information to generate counterfactual relations. Through counterfactual learning, HINCHOR estimates causal effects while predicting higher-order relations. The experimental results on four constructed benchmark datasets show that HINCHOR outperforms existing state-of-the-art methods.}
}
@article{WANG2024103785,
title = {DCTM: Dual Contrastive Topic Model for identifiable topic extraction},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103785},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103785},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001456},
author = {Rui Wang and Peng Ren and Xing Liu and Shuyu Chang and Haiping Huang},
keywords = {Contrastive learning, Neural-based topic model, Topic modeling},
abstract = {The recent advanced Contrastive Neural Topic Model (CNTM) was proposed to tackle topic collapse through document-level contrastive learning. However, limited by its usage of the Logistic-Normal prior in topic space and document level contrastive learning, it is less capable of disentangling semantically similar topics. To address the limitation, we propose a novel Dual Contrastive Topic Model (DCTM) that utilizes the Dirichlet prior to capture interpretable patterns. Besides, it incorporates dual (document-level and topic-level) contrastive learning on the topic distribution matrix which helps generate discriminative topic representations and mine identifiable topics. Our proposed DCTM outperforms the state-of-the-art neural topic models in terms of topic coherence and diversity, which is verified by extensive experimentation on three publicly available text corpora. In detail, the proposed DCTM surpasses baselines on almost all the used topic coherence metrics (CP, CA, NPMI for 20Newsgroups, CP, CA, NPMI and UCI for Grolier and DBPedia), and it also obtains higher topic diversity with 1 datasets respectively. Moreover, when performing text clustering, DCTM also achieves significant improvements, with observed increases of more than 1% (20Newsgroups) and 6% (DBPedia) in accuracy.}
}
@article{JIA2024111545,
title = {Document-level relation extraction with global and path dependencies},
journal = {Knowledge-Based Systems},
volume = {289},
pages = {111545},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111545},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001801},
author = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
keywords = {Relation extraction, Global dependency, Multi-hop path, Path representation},
abstract = {Document-level relation extraction (RE) focuses on extracting relations for each entity pair in the same sentence or across different sentences of a document. Several existing methodologies aim to capture the intricate interactions among entities across a document by constructing diverse document graphs. However, these graphs frequently cannot sufficiently model the intricate global interactions and concurrent explicit path reasoning. Therefore, we introduce a distinctive graph-based model designed to assimilate global and path dependencies within a document for document-level RE, termed graph-based global and path dependencies (GGP). Specifically, the global dependency component captures interactions between mentions, entities, sentences and, the document through two interconnected graphs: the mention-level graph and the entity-level graph (ELG). To integrate relevant paths essential for the designated entity pair, the path dependency component consolidates information from various multi-hop paths of the target entity pair through an attention mechanism on the ELG. In addition, we devised an innovative method for learning path representation, which encapsulates relations and intermediate entities within the multi-hop path in the ELG. Comprehensive experiments conducted on standard document-level RE and CDR datasets reveal the following key findings: (i) GGP achieves an Ign F1 score of 59.98%, surpassing baselines by 0.61% on the test set; and (ii) the integration of various features derived from entities, sentences, documents, and paths enhances GGP's performance in document-level RE.}
}
@incollection{2025251,
title = {Index},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {251-254},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.09991-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962099915}
}
@incollection{2025425,
title = {Index},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {425-430},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.20001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460200011}
}
@article{FARISCO2024106714,
title = {Is artificial consciousness achievable? Lessons from the human brain},
journal = {Neural Networks},
volume = {180},
pages = {106714},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106714},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006385},
author = {Michele Farisco and Kathinka Evers and Jean-Pierre Changeux},
keywords = {Brain, Consciousness, Artificial intelligence, Neuromorphic computing, Robotics, Cognition, Neuroscience},
abstract = {We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model or as a benchmark. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of human-like conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (i.e., structural and architectural) and extrinsic (i.e., related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make human-like conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it cannot be theoretically excluded that AI research can develop partial or potentially alternative forms of consciousness that are qualitatively different from the human form, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word “consciousness” for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify which level and/or type of consciousness AI research aims to develop, as well as what would be common versus differ in AI conscious processing compared to human conscious experience.}
}
@article{LONGO202364,
title = {A framework for cognitive chatbots based on abductive–deductive inference},
journal = {Cognitive Systems Research},
volume = {81},
pages = {64-79},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000359},
author = {Carmelo Fabio Longo and Paolo Marco Riela and Daniele Francesco Santamaria and Corrado Santoro and Antonio Lieto},
keywords = {Chatbot, Question answering, Artificial intelligence, First-order logic, Cognitive architectures, Meta-reasoning},
abstract = {This paper presents a framework based on natural language processing and first-order logic aiming at instantiating cognitive chatbots. The proposed framework leverages two types of knowledge bases interacting with each other in a meta-reasoning process. The first one is devoted to the reactive interactions within the environment, while the second one to conceptual reasoning. The latter exploits a combination of axioms represented with rich semantics and abduction as pre-stage of deduction, dealing also with some of the state-of-the-art issues in the natural language ontology domain. As a case study, a Telegram chatbot system has been implemented, supported by a module which automatically transforms polar and wh-questions into one or more likely assertions, so as to infer Boolean values or snippets with variable length as factoid answer. The conceptual knowledge base is organized in two layers, representing both long- and short-term memory. The knowledge transition between the two layers is achieved by leveraging both a greedy algorithm and the engine’s features of a NoSQL database, with promising timing performance if compared with the adoption of a single layer. Furthermore, the implemented chatbot only requires the knowledge base in natural language sentences, avoiding any script updates or code refactoring when new knowledge has to income. The framework has been also evaluated as cognitive system by taking into account the state-of-the art criteria: the results show that AD-Caspar is an interesting starting point for the design of psychologically inspired cognitive systems, endowed of functional features and integrating different types of perception.}
}
@incollection{LEE202513,
title = {Chapter 2 - What if we have Metaverse GPT? Content singularity and human-Metaverse interaction in the AIGC era},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {13-28},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.00015-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962000152},
author = {Lik-Hang Lee and Peng Yuan Zhou and Chaoning Zhang and Simo Hosio},
keywords = {AI-generated content (AIGC), Metaverse, Human-technology interaction, Mixed reality, Content singularity},
abstract = {The global Metaverse development faces a “cooldown moment,” while the academia and industry attention moves from the Metaverse to AI-generated content (AIGC) in 2023. Nonetheless, the current discussion rarely considers the connection between AIGC and the Metaverse. We can imagine the Metaverse, i.e., immersive cyberspace, as the black void of space where AIGC can offer content and facilitate diverse user needs. As such, this article argues that AIGC can be a vital technological enabler for the Metaverse. The article first provides a retrospect of recent history's major pitfall of the Metaverse applications. Second, we discuss from a user-centric perspective how the Metaverse development will likely accelerate with AIGC. Next, the article conjectures future scenarios that leverage the combination of the Metaverse and AIGC. Finally, we present and advocate for an AI-generated Metaverse (AIGM) framework for energizing the creation of Metaverse content in the AIGC era.}
}
@article{WU2024103772,
title = {Fuser: An enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103772},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103772},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001328},
author = {Fan Wu and Bin Gao and Xiaoou Pan and Linlin Li and Yujiao Ma and Shutian Liu and Zhengjun Liu},
keywords = {Hateful memes detection, Multimodal fusion, Congruent reinforced perceptron, Main semantic, Auxiliary context},
abstract = {As a multimodal form of hate speech on social media, hateful memes are more aggressive and cryptic threats to the real life of humans. Automatic detection of hateful memes is crucial, but the images and texts in most memes are only weakly consistent or even irrelevant. Although existing works have achieved the initial goal of detecting hateful memes with pre-trained models, they are limited to monolithic inference methods while ignoring the semantic differences between multimodal representations. To strengthen the comprehension and reasoning of the hidden meaning behind the memes by combining real-world knowledge, we propose an enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection. Inspired by the human cognitive mechanism, we first divide the extracted multisource representations into main semantics and auxiliary contexts based on their strength and relevance, and then precode them into lightly correlated embeddings with unified spatial dimensions via a novel prefix uniform layer, respectively. To jointly learn the intrinsic correlation between primary and secondary semantics, a congruent reinforced perceptron with brain-like perceptual integration is designed to seamlessly fuse multimodal representations in a shared latent space while maintaining the feature integrity in the sub-fusion space, thereby implicitly reasoning about the subtle metaphors behind the memes. Extensive experiments on four benchmark datasets fully demonstrate the effectiveness and superiority of our architecture compared with previous state-of-the-art methods.}
}
@article{SUN2025130361,
title = {Deep reinforcement learning-based influence maximization for heterogeneous hypergraphs},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {660},
pages = {130361},
year = {2025},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2025.130361},
url = {https://www.sciencedirect.com/science/article/pii/S0378437125000135},
author = {Yanhao Sun and Jie Wu and Nuan Song and Tianwei Lin and Longxiang Li and Dong Li},
keywords = {Influence maximization, Diffusion model, Heterogeneous hypergraphs, Deep reinforcement learning},
abstract = {In the field of social computing, understanding and optimizing information dissemination within social networks is crucial. To effectively model the complex higher-order relationships that exist in these networks, hypergraphs are employed. Current research predominantly centers on homogeneous hypergraphs, which encapsulate only a single type of relationship. However, real-world interactions often encompass more intricate and diverse higher-order relationships, which can be captured well by heterogeneous hypergraphs. This paper delves into the challenge of influence maximization within such heterogeneous hypergraphs. Specifically, we first introduce HLabel-LT, an innovative diffusion model designed for heterogeneous hypergraphs, which integrates multiple relationship types to enhance simulation accuracy. Furthermore, we propose a novel seed nodes selection framework, Heterogeneous Influence Maximization with Deep Reinforcement Learning (HIMH-DRL), which harnesses the unique adaptive trial-and-error learning capabilities of deep reinforcement learning to optimize influence spread in heterogeneous hypergraphs. To facilitate the training demands of this framework, we also present a new hypergraph generation model, HyperFF-Label, used to create artificially synthesized heterogeneous hypergraphs. Extensive experiments across various real-world datasets demonstrate that our methodology not only broadens the understanding of hypergraph dynamics but also markedly enhances the efficiency and effectiveness of influence propagation compared to traditional methods. This study underscores the potential of exploiting heterogeneity in hypergraphs to devise more effective information dissemination strategies in social networks.}
}
@incollection{MARTINIS2024,
title = {Natural Language Processing Approaches in Bioinformatics},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00179-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001792},
author = {Maria Chiara Martinis and Zucco Chiara},
keywords = {Active learning, Biomedical natural language processing, Semantical analysis and Syntactical analysis},
abstract = {In this article, we provide an overview of the natural language processing and its bioinformatics applications. We describe the historical evolution of NLP, and summarize the common NLP sub-problems in the field, as well as their research progress in the biomedical domain. In addition, we discuss an advanced topic of applying active learning methods to the NLP systems, to solve the practical issue of lacking training data in the field.}
}
@article{SONG2024100268,
title = {IDL-LTSOJ: Research and implementation of an intelligent online judge system utilizing DNN for defect localization},
journal = {High-Confidence Computing},
pages = {100268},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100268},
url = {https://www.sciencedirect.com/science/article/pii/S2667295224000710},
author = {Lihua Song and Ying Han and Yufei Guo and Chenying Cai},
keywords = {Online Judge (OJ) system, Fine-grained defect localization, Deep neural network, Task scheduling},
abstract = {The evolution of artificial intelligence has thrust the Online Judge (OJ) systems into the forefront of research, particularly within programming education, with a focus on enhancing performance and efficiency. Addressing the shortcomings of the current OJ systems in coarse defect localization granularity and heavy task scheduling architecture, this paper introduces an innovative Integrated Intelligent Defect Localization and Lightweight Task Scheduling Online Judge (IDL-LTSOJ) system. Firstly, to achieve token-level fine-grained defect localization, a Deep Fine-Grained Defect Localization (Deep-FGDL) deep neural network model is developed. By integrating Bidirectional Long Short-Term Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU), this model extracts fine-grained information from the abstract syntax tree (AST) of code, enabling more accurate defect localization. Subsequently, we propose a lightweight task scheduling architecture to tackle issues, such as limited concurrency in task evaluation and high equipment costs. This architecture integrates a Kafka messaging system with an optimized task distribution strategy to enable concurrent execution of evaluation tasks, substantially enhancing system evaluation efficiency. The experimental results demonstrate that the Deep-FGDL model improves the accuracy by 35.9% in the Top-20 rank compared to traditional machine learning benchmark methods for fine-grained defect localization tasks. Moreover, the lightweight task scheduling strategy notably reduces response time by nearly 6000ms when handling 120 task volumes, which represents a significant improvement in evaluation efficiency over centralized evaluation methods.}
}
@article{HAIR2025115047,
title = {Connecting the dots in neuroscience research: The future of evidence synthesis},
journal = {Experimental Neurology},
volume = {384},
pages = {115047},
year = {2025},
issn = {0014-4886},
doi = {https://doi.org/10.1016/j.expneurol.2024.115047},
url = {https://www.sciencedirect.com/science/article/pii/S001448862400373X},
author = {Kaitlyn Hair and María Arroyo-Araujo and Sofija Vojvodic and Maria Economou and Charis Wong and Francesca Tinsdeall and Sean Smith and Torsten Rackoll and Emily S. Sena and Sarah K. McCann},
keywords = {Systematic review, Evidence synthesis, Evidence ecosystem, Artificial intelligence, Research improvement, Data sharing, Meta-analysis},
abstract = {Making progress in neuroscience research involves learning from existing data. In this perspective piece, we explore the potential of a data-driven evidence ecosystem to connect all primary data streams, and synthesis efforts to inform evidence-based research and translational success from bench to bedside. To enable this transformation, we set out how we can produce evidence designed with evidence curation in mind. All data should be findable, understandable, and easily synthesisable, using a combination of human and machine effort. This will require shifts in research culture and tailored infrastructure to support rapid dissemination, data sharing, and transparency. We also discuss improvements in the way we can synthesise evidence to better inform primary research, including the potential of emerging technologies, big-data approaches, and breaking down research silos. Through a case study in stroke research, one of the most well-established areas for synthesis efforts, we demonstrate the progress in implementing elements of this ecosystem, with an emphasis on the need for coordinated efforts between laboratory researchers and synthesists.}
}
@article{LIU2024e32093,
title = {GlyReShot: A glyph-aware model with label refinement for few-shot Chinese agricultural named entity recognition},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32093},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32093},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024081246},
author = {Haitao Liu and Jihua Song and Weiming Peng},
keywords = {Agricultural named entity recognition, Few-shot learning, Glyph features, Agricultural text mining},
abstract = {Chinese agricultural named entity recognition (NER) has been studied with supervised learning for many years. However, considering the scarcity of public datasets in the agricultural domain, exploring this task in the few-shot scenario is more practical for real-world demands. In this paper, we propose a novel model named GlyReShot, integrating the knowledge of Chinese character glyph into few-shot NER models. Although the utilization of glyph has been proven successful in supervised models, two challenges still persist in the few-shot setting, i.e., how to obtain glyph representations and when to integrate them into the few-shot model. GlyReShot handles the two challenges by introducing a lightweight glyph representation obtaining module and a training-free label refinement strategy. Specifically, the glyph representations are generated based on the descriptive sentences by filling the predefined template. As most steps come before training, this module aligns well with the few-shot setting. Furthermore, by computing the confidence values for draft predictions, the refinement strategy selectively utilizes the glyph information only when the confidence values are relatively low, thus mitigating the influence of noise. Finally, we annotate a new agricultural NER dataset and the experimental results demonstrate effectiveness of GlyReShot for few-shot Chinese agricultural NER.}
}
@article{YIN2024108548,
title = {Enhancing bibliographic reference parsing with contrastive learning and prompt learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108548},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108548},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624007061},
author = {Zhen Yin and Shenghua Wang},
keywords = {Contrastive learning, Prompt learning, Information extraction, Bibliographic reference},
abstract = {Bibliographic references, typically comprising author names, journal titles, paper titles, and publication dates, play a vital role in academic research. Accurately identifying these structured pieces of information from references is a crucial step in developing intelligent bibliographic management systems. However, existing methods often rely on extensive high-quality training data. To mitigate the reliance on extensive training data, we propose a method that integrates prompt learning and contrastive learning for extracting structured information from bibliographic references, named CONT_Prompt_ParseRef. This approach aims to utilize contrastive learning to deepen the understanding of different metadata label types and employ prompt learning to provide specific guidelines for processing and recognition. We constructed a dataset comprising 12,000 samples, available in both Chinese and English versions. The experimental results on this bilingual dataset demonstrate the model's superior performance over existing techniques. Notably, CONT_Prompt_ParseRef shows remarkable robustness in low-resource environments, particularly in scenarios with limited training data, both contrastive and prompt learning play pivotal roles in label extraction from bibliographic references. The ablation study illustrates that omitting either component leads to a decline in performance, with contrastive learning being slightly more influential.}
}
@article{CHEN2024,
title = {Digital Information Ecosystems in Modern Care Coordination and Patient Care Pathways and the Challenges and Opportunities for AI Solutions},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/60258},
url = {https://www.sciencedirect.com/science/article/pii/S143888712400894X},
author = {You Chen and Christoph U Lehmann and Bradley Malin},
keywords = {patient care pathway, care journey, care coordination, digital information ecosystem, digital technologies, artificial intelligence, information interoperability, information silos, workload, information retrieval, care transitions, patient-reported outcome measures, clinical workflow, usability, user experience workflow, health care information systems, networks of health care professionals, patient information flow},
abstract = {The integration of digital technologies into health care has significantly enhanced the efficiency and effectiveness of care coordination. Our perspective paper explores the digital information ecosystems in modern care coordination, focusing on the processes of information generation, updating, transmission, and exchange along a patient’s care pathway. We identify several challenges within this ecosystem, including interoperability issues, information silos, hard-to-map patient care journeys, increased workload on health care professionals, coordination and communication gaps, and compliance with privacy regulations. These challenges are often associated with inefficiencies and diminished care quality. We also examine how emerging artificial intelligence (AI) tools have the potential to enhance the management of patient information flow. Specifically, AI can boost interoperability across diverse health systems; optimize and monitor patient care pathways; improve information retrieval and care transitions; humanize health care by integrating patients’ desired outcomes and patient-reported outcome measures; and optimize clinical workflows, resource allocation, and digital tool usability and user experiences. By strategically leveraging AI, health care systems can establish a more robust and responsive digital information ecosystem, improving care coordination and patient outcomes. This perspective underscores the importance of continued research and investment in AI technologies in patient care pathways. We advocate for a thoughtful integration of AI into health care practices to fully realize its potential in revolutionizing care coordination.}
}
@article{LUO2024124631,
title = {Graph Contrastive Topic Model},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124631},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124631},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424014982},
author = {Zheheng Luo and Lei Liu and Sophia Ananiadou and Qianqian Xie},
keywords = {Neural topic modelling, Contrastive learning, Graph neural networks, Document representations},
abstract = {Contrastive learning has recently been introduced into neural topic models (NTMs) to improve latent semantic discovery, but existing methods suffer from the sample bias problem owing to word frequency-based sampling strategy, which may result in false negative samples with similar semantics to the prototypes. We propose the novel graph contrastive neural topic model (GCTM), based on the graph-based sampling strategy, guided by the in-depth correlation and irrelevance information among documents and words. We model the input document as the document word bipartite graph (DWBG) and construct positive and negative word co-occurrence graphs (WCGs), to capture in-depth semantic correlation and irrelevance among words. Based on the DWBG and WCGs, we design the document-word information propagation (DWIP) process to perform the edge perturbation of DWBG, based on multi-hop correlations/irrelevance among documents and words. This yields the desired negative and positive samples, which are utilized for GCL together with the prototypes to improve learning document topic representations and latent topics. Experiments on several benchmark datasets demonstrate the effectiveness of our method for topic coherence and document representation learning compared with existing state-of-the-art methods.}
}
@article{FRICONNET202542,
title = {Phenomenal consciousness is alien to us: SETI and the fermi paradox seen through the prism of illusionism and attention schema theory},
journal = {Acta Astronautica},
volume = {226},
pages = {42-49},
year = {2025},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2024.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0094576524005976},
author = {Guillaume Friconnet},
keywords = {Extra-terrestrial intelligence, Fermi paradox, Consciousness, Illusionism, Attention schema theory},
abstract = {Illusionism is an eliminativist position about qualia stating that phenomenal consciousness is nothing more than an introspective illusion. The attention schema theory (AST) relates this philosophical stance to a large body of experimental data and states that phenomenal consciousness arises from an internal model of attention control. In this paper, I intend to show that AST and illusionism have significant implications both in the search for extra-terrestrial intelligence and in the explanation of Fermi paradox. Firstly, on the basis of findings concerning the evolutionary history of phenomenal consciousness on Earth, I argue that extraterrestrial biological life is likely to experience phenomenality. In the second part, I set AST in the context of a post-biological universe, where artificial intelligence (AI) is the dominant form of intelligence. I argue that phenomenal consciousness is probably present in these entities, and that they could even be super-conscious. Finally, I show that because phenomenality grounds value, illusionism has profound revisionary consequences in the field of ethics. This reconsideration of the justifiability of our values paves the way to AI misalignment and may be the source of neocatastrophic scenarios that explain to Fermi paradox.}
}
@article{DARVISHI2024104967,
title = {Impact of AI assistance on student agency},
journal = {Computers & Education},
volume = {210},
pages = {104967},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104967},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523002440},
author = {Ali Darvishi and Hassan Khosravi and Shazia Sadiq and Dragan Gašević and George Siemens},
keywords = {AI in education, Student agency, Peer feedback, Educational technology},
abstract = {AI-powered learning technologies are increasingly being used to automate and scaffold learning activities (e.g., personalised reminders for completing tasks, automated real-time feedback for improving writing, or recommendations for when and what to study). While the prevailing view is that these technologies generally have a positive effect on student learning, their impact on students’ agency and ability to self-regulate their learning is under-explored. Do students learn from the regular, detailed and personalised feedback provided by AI systems, and will they continue to exhibit similar behaviour in the absence of assistance? Or do they instead continue to rely on AI assistance without learning from it? To contribute to filling this research gap, we conducted a randomised controlled experiment that explored the impact of AI assistance on student agency in the context of peer feedback. With 1625 students across 10 courses, an experiment was conducted using peer review. During the initial four-week period, students were guided by AI features that utilised techniques such as rule-based suggestion detection, semantic similarity, and comparison with previous comments made by the reviewer to enhance their submissions if the feedback provided was deemed insufficiently detailed or general in nature. Over the following four weeks, students were divided into four different groups: control (AI) received prompts, (NR) received no prompts, (SR) received self-monitoring checklists in place of AI prompts, and (SAI) had access to both AI prompts and self-monitoring checklists. Results of the experiment suggest that students tended to rely on rather than learn from AI assistance. If AI assistance was removed, self-regulated strategies could help fill the gap but were not as effective as AI assistance. Results also showed that hybrid human-AI approaches that complement AI assistance with self-regulated strategies (SAI) were not more effective than AI assistance on its own. We conclude by discussing the broader benefits, challenges and implications of relying on AI assistance in relation to student agency in a world where we learn, live and work with AI.}
}
@article{FAROOQ2024891,
title = {Artificial intelligence in plant breeding},
journal = {Trends in Genetics},
volume = {40},
number = {10},
pages = {891-908},
year = {2024},
issn = {0168-9525},
doi = {https://doi.org/10.1016/j.tig.2024.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168952524001677},
author = {Muhammad Amjad Farooq and Shang Gao and Muhammad Adeel Hassan and Zhangping Huang and Awais Rasheed and Sarah Hearne and Boddupalli Prasanna and Xinhai Li and Huihui Li},
keywords = {artificial intelligence, plant breeding, genetic gain, big data, deep learning},
abstract = {Harnessing cutting-edge technologies to enhance crop productivity is a pivotal goal in modern plant breeding. Artificial intelligence (AI) is renowned for its prowess in big data analysis and pattern recognition, and is revolutionizing numerous scientific domains including plant breeding. We explore the wider potential of AI tools in various facets of breeding, including data collection, unlocking genetic diversity within genebanks, and bridging the genotype–phenotype gap to facilitate crop breeding. This will enable the development of crop cultivars tailored to the projected future environments. Moreover, AI tools also hold promise for refining crop traits by improving the precision of gene-editing systems and predicting the potential effects of gene variants on plant phenotypes. Leveraging AI-enabled precision breeding can augment the efficiency of breeding programs and holds promise for optimizing cropping systems at the grassroots level. This entails identifying optimal inter-cropping and crop-rotation models to enhance agricultural sustainability and productivity in the field.}
}
@article{SANEMETERIODELAPARTE2024101030,
title = {Spatio-temporal semantic data management systems for IoT in agriculture 5.0: Challenges and future directions},
journal = {Internet of Things},
volume = {25},
pages = {101030},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.101030},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523003530},
author = {Mario {San Emeterio de la Parte} and José-Fernán Martínez-Ortega and Pedro Castillejo and Néstor Lucas-Martínez},
keywords = {Data science, Internet of Things (IoT), Big data, Agriculture, Spatio-temporal semantic data management system (STSDMS)},
abstract = {The Agri-Food sector is in a stressful situation due to the high demand for food from the growing population around the world. The agricultural sector is facing a challenging situation; it must increase production and reduce its impact on the environment by appropriately allocating resources, adapting to climate change, and avoiding food waste. Agriculture 5.0, as the fifth agricultural evolution, aims to offer a perfect symbiosis between agriculture, advanced technologies, and sustainability. The most advanced technologies in automation, monitoring, and decision support are driven by the collection and processing of large volumes of agricultural data, such as weather information, farm machinery, soil and crop conditions, and marketing demand for higher profits. Taking advantage of the technological paradigm of the Internet of Things, agricultural data provides information on spatial, temporal, and semantic dimensions. Spatio-temporal semantic data management systems have become the cornerstone for the achievement of Agriculture 5.0 through advanced Internet of Things technologies. This paper aims to review the current literature on spatio-temporal semantic data management systems for Agriculture 5.0. This paper uses a systematic literature review technique to study eleven representative spatio-temporal semantic data management systems. A comprehensive evaluation of the aspects of interoperability, accessibility, scalability, real-time operation capability, etc. is carried out. Based on the evaluation results, future challenges are detected and development trends and possible improvements are proposed for future research. Finally, a distributed architecture capable of satisfying the above needs and challenges is proposed. The paper aims to inspire further research and development efforts to improve the efficiency, accessibility, and performance of spatio-temporal semantic data management systems.}
}
@article{YANG2025129089,
title = {Few-shot cyberviolence intent classification with Meta-learning AutoEncoder based on adversarial domain adaptation},
journal = {Neurocomputing},
volume = {620},
pages = {129089},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129089},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224018605},
author = {Shun Yang and YaJun Du and ShangYi Du and XianYong Li and XiaoLiang Chen and YanLi Li and ChunZhi Xie and Jia Liu},
keywords = {Few-shot learning, Meta-learning, Intent classification, Cyberviolence, AutoEncoder},
abstract = {The phenomenon of cyberviolence has become a critical issue in online security, drawing attention from various stakeholders. A major shortcoming in the previous works is the limitation of using simple methods like ”yes” or ”no” to evaluate cyberviolence utterances, which can significantly restrict netizens’ free speech. Therefore, we provide a novel strategy for detecting cyberviolence utterances based on the user’s real intent. Fine-grained cyberviolence intents are complex, leading to texts that share similar syntactic structures and semantics but differ in intent category. The previous method did not consider this issue. To address this, in this paper, we propose a Meta-learning AutoEncoder (MetaAE) based on adversarial domain adaptation. The goal is to comprehend and learn the inherent logical rules and important semantic knowledge of cyberviolence utterances, specifically targeting fine-grained cyberviolence intent problems. Specifically, we use the autoencoder structure to help the model implement self-supervised learning. This enables the model to comprehend the inherent logical structure of texts with different intent categories and helps the model learn important semantic knowledge of the text during the encoder compression process. At the same time, to solve the problem of overfitting in small samples and multidomain cyberviolence utterance, we introduce domain adversarial learning to align domain features and enhance model robustness. Experimental results on both a real cyberviolence intent classification dataset and a public dataset demonstrate significant improvements. On 5-way 1-shot and 5-shot Chinese and English cyberviolence datasets, MetaAE improved the accuracy by approximately 7.23%, 8.27%, 7.22%, and 5%, respectively. In the public dataset, MetaAE improved accuracy by approximately 2.53% on 5-way 5-shot.11Our code is available at https://github.com/YS19999/Meta-learning-AutoEncoder.}
}
@article{CALISKAN20234895,
title = {Metadata integrity in bioinformatics: Bridging the gap between data and knowledge},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {4895-4913},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023003616},
author = {Aylin Caliskan and Seema Dangwal and Thomas Dandekar},
keywords = {Meta-data, Error, Annotation, Error-transfer, Wrong labelling, Patient data, Control group, Tools overview},
abstract = {In the fast-evolving landscape of biomedical research, the emergence of big data has presented researchers with extraordinary opportunities to explore biological complexities. In biomedical research, big data imply also a big responsibility. This is not only due to genomics data being sensitive information but also due to genomics data being shared and re-analysed among the scientific community. This saves valuable resources and can even help to find new insights in silico. To fully use these opportunities, detailed and correct metadata are imperative. This includes not only the availability of metadata but also their correctness. Metadata integrity serves as a fundamental determinant of research credibility, supporting the reliability and reproducibility of data-driven findings. Ensuring metadata availability, curation, and accuracy are therefore essential for bioinformatic research. Not only must metadata be readily available, but they must also be meticulously curated and ideally error-free. Motivated by an accidental discovery of a critical metadata error in patient data published in two high-impact journals, we aim to raise awareness for the need of correct, complete, and curated metadata. We describe how the metadata error was found, addressed, and present examples for metadata-related challenges in omics research, along with supporting measures, including tools for checking metadata and software to facilitate various steps from data analysis to published research.}
}
@incollection{WOLLOWSKI202515,
title = {2 - Toward a new foundation for AI},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {15-39},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044329246000002X},
author = {Michael Wollowski},
keywords = {Autonomy, Convolutional neural networks as pattern recognizers, Foundation of artificial intelligence, Neural networks, Pattern recognition, Predicted impact of artificial intelligence, Transformers as pattern recognizers},
abstract = {We argue that artificial neural networks (NNs) should be the foundation of AI. We reach back to some early writings on AI in which the earlier authors argued for such an approach. We explain that modern NNs, in particular convolutional neural networks (CNNs), and transformer architectures satisfy the requirements laid out by early workers and critics of AI. We review current practice of teaching AI and contrast those to current successes in NNs and their projected future. We explain limitations of current NNs and highlight approaches designed to overcome them. We show in some detail how current NN architectures suit the desiderata posited by early researchers in the field. We end the chapter by cautioning the desire to build autonomous systems and recommend an approach that ensures that such machines are beneficial.}
}
@article{MARTINSON20242695,
title = {Artificial Intelligence and Machine Learning for Inborn Errors of Immunity: Current State and Future Promise},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
volume = {12},
number = {10},
pages = {2695-2704},
year = {2024},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2024.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S2213219824008286},
author = {Alexandra K. Martinson and Aaron T. Chin and Manish J. Butte and Nicholas L. Rider},
keywords = {Inborn errors of immunity, Artificial intelligence, Machine learning, Phenotyping, Electronic health records, Clinical decision support systems},
abstract = {Artificial intelligence (AI) and machine learning (ML) research within medicine has exponentially increased over the last decade, with studies showcasing the potential of AI/ML algorithms to improve clinical practice and outcomes. Ongoing research and efforts to develop AI-based models have expanded to aid in the identification of inborn errors of immunity (IEI). The use of larger electronic health record data sets, coupled with advances in phenotyping precision and enhancements in ML techniques, has the potential to significantly improve the early recognition of IEI, thereby increasing access to equitable care. In this review, we provide a comprehensive examination of AI/ML for IEI, covering the spectrum from data preprocessing for AI/ML analysis to current applications within immunology, and address the challenges associated with implementing clinical decision support systems to refine the diagnosis and management of IEI.}
}
@article{GE2023104458,
title = {Few-shot learning for medical text: A review of advances, trends, and opportunities},
journal = {Journal of Biomedical Informatics},
volume = {144},
pages = {104458},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104458},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300179X},
author = {Yao Ge and Yuting Guo and Sudeshna Das and Mohammed Ali Al-Garadi and Abeed Sarker},
keywords = {Few-shot learning, Natural language processing, Machine learning, Biomedical informatics},
abstract = {Background:
Few-shot learning (FSL) is a class of machine learning methods that require small numbers of labeled instances for training. With many medical topics having limited annotated text-based data in practical settings, FSL-based natural language processing (NLP) holds substantial promise. We aimed to conduct a review to explore the current state of FSL methods for medical NLP.
Methods:
We searched for articles published between January 2016 and October 2022 using PubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. We also searched the preprint servers (e.g., arXiv, medRxiv, and bioRxiv) via Google Scholar to identify the latest relevant methods. We included all articles that involved FSL and any form of medical text. We abstracted articles based on the data source, target task, training set size, primary method(s)/approach(es), and evaluation metric(s).
Results:
Fifty-one articles met our inclusion criteria—all published after 2018, and most since 2020 (42/51; 82%). Concept extraction/named entity recognition was the most frequently addressed task (21/51; 41%), followed by text classification (16/51; 31%). Thirty-two (61%) articles reconstructed existing datasets to fit few-shot scenarios, and MIMIC-III was the most frequently used dataset (10/51; 20%). 77% of the articles attempted to incorporate prior knowledge to augment the small datasets available for training. Common methods included FSL with attention mechanisms (20/51; 39%), prototypical networks (11/51; 22%), meta-learning (7/51; 14%), and prompt-based learning methods, the latter being particularly popular since 2021. Benchmarking experiments demonstrated relative underperformance of FSL methods on biomedical NLP tasks.
Conclusion:
Despite the potential for FSL in biomedical NLP, progress has been limited. This may be attributed to the rarity of specialized data, lack of standardized evaluation criteria, and the underperformance of FSL methods on biomedical topics. The creation of publicly-available specialized datasets for biomedical FSL may aid method development by facilitating comparative analyses.}
}
@article{SALIHOGLU20241376,
title = {Cat-E: A comprehensive web tool for exploring cancer targeting strategies},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {1376-1386},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024000771},
author = {Rana Salihoglu and Johannes Balkenhol and Gudrun Dandekar and Chunguang Liang and Thomas Dandekar and Elena Bencurova},
keywords = {Network analysis, Protein interaction, Cancer pathways, Cancer, Oncolytic virus, Immune modulation},
abstract = {Identifying potential cancer-associated genes and drug targets from omics data is challenging due to its diverse sources and analyses, requiring advanced skills and large amounts of time. To facilitate such analysis, we developed Cat-E (Cancer Target Explorer), a novel R/Shiny web tool designed for comprehensive analysis with evaluation according to cancer-related omics data. Cat-E is accessible at https://cat-e.bioinfo-wuerz.eu/. Cat-E compiles information on oncolytic viruses, cell lines, gene markers, and clinical studies by integrating molecular datasets from key databases such as OvirusTB, TCGA, DrugBANK, and PubChem. Users can use all datasets and upload their data to perform multiple analyses, such as differential gene expression analysis, metabolic pathway exploration, metabolic flux analysis, GO and KEGG enrichment analysis, survival analysis, immune signature analysis, single nucleotide variation analysis, dynamic analysis of gene expression changes and gene regulatory network changes, and protein structure prediction. Cancer target evaluation by Cat-E is demonstrated here on lung adenocarcinoma (LUAD) datasets. By offering a user-friendly interface and detailed user manual, Cat-E eliminates the need for advanced computational expertise, making it accessible to experimental biologists, undergraduate and graduate students, and oncology clinicians. It serves as a valuable tool for investigating genetic variations across diverse cancer types, facilitating the identification of novel diagnostic markers and potential therapeutic targets.}
}
@article{JIANG2024102049,
title = {Artificial intelligence and automation to power the future of chemistry},
journal = {Cell Reports Physical Science},
volume = {5},
number = {7},
pages = {102049},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.102049},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424003187},
author = {Xuefeng Jiang and Sanzhong Luo and Kuangbiao Liao and Shan Jiang and Jing Ma and Jun Jiang and Zhigang Shuai},
abstract = {In our traditional impression of chemical laboratories, researchers wear white coats and safety goggles to conduct experiments. However, many recent developments in the field make use of autonomous synthesis robots with integrated artificial intelligence (AI)-driven machine-learning units. These benchtop devices might outperform human chemists in terms of speed and accuracy, which could accelerate the discovery of molecules and materials for various applications. In this Voices piece, we ask a panel of experts from institutes in China: How are AI and automation shaping the future of chemistry?}
}
@article{RAVI2024200456,
title = {Ideological orientation and extremism detection in online social networking sites: A systematic review},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200456},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200456},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001303},
author = {Kamalakkannan Ravi and Jiann-Shiun Yuan},
keywords = {Extremism detection, Machine learning, Natural language processing, Predictive models, Social media, User-generated content},
abstract = {The rise of social networking sites has reshaped digital interactions, becoming fertile grounds for extremist ideologies, notably in the United States. Despite previous research, understanding and tackling online ideological extremism remains challenging. In this context, we conduct a systematic literature review to comprehensively analyze existing research and offer insights for both researchers and policymakers. Spanning from 2005 to 2023, our review includes 110 primary research articles across platforms like Twitter (X), Facebook, Reddit, TikTok, Telegram, and Parler. We observe a diverse array of methodologies, including natural language processing (NLP), machine learning (ML), deep learning (DL), graph-based methods, dictionary-based methods, and statistical approaches. Through synthesis, we aim to advance understanding and provide actionable recommendations for combating ideological extremism effectively on online social networking sites.}
}
@article{WANG2025109835,
title = {WCG-VMamba: A multi-modal classification model for corn disease},
journal = {Computers and Electronics in Agriculture},
volume = {230},
pages = {109835},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109835},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924012262},
author = {Haoyang Wang and Mingfang He and Minge Zhu and Genhua Liu},
keywords = {Corn disease, Image-text pairs, Classification, Multimodal, VMamba},
abstract = {Corn is one of the important food crops and industrial raw materials. However, maize diseases have seriously affected its yield and quality. In order to effectively identify maize diseases, digital image processing technology has been widely used in the agricultural field. The classification of diseases based on digital images enables early detection of corn diseases, reducing farmers’ losses. Although existing corn disease identification methods have made significant progress using deep learning technology for digital image processing, most of these technologies rely on single-modal data for identification and lack the connection between images and texts. To solve this problem, this paper proposes a cross-modal feature alignment fusion model called WCG-VMamba. Firstly, we propose a wavelet visual Mamba (WAVM) network, which integrates the advantages of different visual coding strategies and can reduce the influence of intrinsic noise and other factors on the validity of image features during extraction. Then, we introduce the Cross Modal Alignment Transformer (CMAT), which interacts image features with text features to capture their semantic correlation and determine the weight distribution of image and text features in the fusion process. We then use Transformer coding blocks to fuse features. Finally, the Gaussian Random Walk Duck Swarm Algorithm (GRW-DSA) is proposed to reduce errors in the duck swarm exploration process through Gaussian Random Walk, aiming to find the optimal learning rate. Experiments on self-built datasets and two common datasets show that WCG-VMamba can be effectively used in the task of corn disease classification. Compared with other excellent models such as MobileViT, MobilenetV3, SwinT, and DINOV2, better results have been achieved, our model achieves a recognition accuracy as high as 96.97%, proving its important practical application in promoting agricultural cross-modal models and corn disease control.}
}
@article{ZHOU2024127424,
title = {Two stages prompting for few-shot multi-intent detection},
journal = {Neurocomputing},
volume = {579},
pages = {127424},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127424},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224001954},
author = {Xingfa Zhou and Lan Yang and Xin Wang and Huayi Zhan and Rui Sun},
keywords = {Multi-intent detection, Few-shot learning, Prompt, Self-attention},
abstract = {This paper focuses on multi-intent detection in the few-shot scenario. Most prior works for multi-intent detection pick intent labels when estimated label-instance relevance scores are above a given threshold. However, these methods often perform poorly since it is nontrivial to precisely estimate the label-instance relevance scores and set an appropriate threshold in the few-shot scenario. In addition, these methods overlook the correlation among intents, which is vital to multi-intent detection. In light of these, we propose a prompt-based fine-tuning (PFT) method to tackle the issue of multi-intent detection in the few-shot scenario. Specifically, we first construct a prompt template to predict the number of intents. According to the number of intents, we then construct an intent prompt template to identify intents. To capture the correlation among intents, we also introduce a multi-view multi-head self-attention scheme based on PFT. Experimental results on two datasets demonstrate that our proposed method significantly outperforms the baselines.}
}
@article{DEWI20244195,
title = {Adjusted Reasoning Module for Deep Visual Question Answering Using Vision Transformer},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {4195-4216},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.057453},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008403},
author = {Christine Dewi and Hanna Prillysca Chernovita and Stephen Abednego Philemon and Christian {Adi Ananta} and Abbott Po Shun Chen},
keywords = {VQA, vision transformer, multimodal data, deep learning},
abstract = {Visual Question Answering (VQA) is an interdisciplinary artificial intelligence (AI) activity that integrates computer vision and natural language processing. Its purpose is to empower machines to respond to questions by utilizing visual information. A VQA system typically takes an image and a natural language query as input and produces a textual answer as output. One major obstacle in VQA is identifying a successful method to extract and merge textual and visual data. We examine “Fusion” Models that use information from both the text encoder and picture encoder to efficiently perform the visual question-answering challenge. For the transformer model, we utilize BERT and RoBERTa, which analyze textual data. The image encoder designed for processing image data utilizes ViT (Vision Transformer), Deit (Data-efficient Image Transformer), and BeIT (Image Transformers). The reasoning module of VQA was updated and layer normalization was incorporated to enhance the performance outcome of our effort. In comparison to the results of previous research, our proposed method suggests a substantial enhancement in efficacy. Our experiment obtained a 60.4% accuracy with the PathVQA dataset and a 69.2% accuracy with the VizWiz dataset.}
}
@article{MAI2025104125,
title = {RAF-AG: Report analysis framework for attack path generation},
journal = {Computers & Security},
volume = {148},
pages = {104125},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104125},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004309},
author = {Khang Mai and Jongmin Lee and Razvan Beuran and Ryosuke Hotchi and Sian En Ooi and Takayuki Kuroda and Yasuo Tan},
keywords = {Automated report analysis, Attack path generation, Graph alignment, Weak supervision, MITRE ATT&CK, Cybersecurity},
abstract = {Information sharing is a key practice in cybersecurity for coping with the ever-changing cyberattacks that are targeting computer systems. Thus, when cyber incidents happen, cyber threat intelligence (CTI) reports are prepared and shared among cybersecurity practitioners to help them get up-to-date information about those incidents. However, reading and analyzing the report text to comprehend the included information is a cumbersome process. Although techniques based on deep learning were proposed to speed up report analysis in order to obtain the enclosed essential information, such as attack path, training data insufficiency makes these methods inefficient in practical circumstances. This paper presents RAF-AG, a report analysis framework for attack path generation. To analyze CTI reports, RAF-AG utilizes the sentence dependency tree for entity and relation extraction, and a weak supervision approach for entity labeling. This is followed by graph building and graph alignment for generating the attack paths. Our approach resolves the data insufficiency problem in the cybersecurity domain by lowering the need for expert involvement. We evaluated RAF-AG by comparing the generated attack paths with those produced by AttacKG, a state-of-the-art automatic report analysis framework. RAF-AG was able to identify cyberattack steps by matching their appearance order inside the report, and link them with techniques from the MITRE ATT&CK knowledge base with an improved F1 score compared to AttacKG (0.708 versus 0.393).}
}
@article{SHIDIK2024100358,
title = {Indonesian disaster named entity recognition from multi source information using bidirectional LSTM (BiLSTM)},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {3},
pages = {100358},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100358},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124001525},
author = {Guruh Fajar Shidik and Filmada Ocky Saputra and Galuh Wilujeng Saraswati and Nurul Anisa Sri Winarsih and Muhammad Syaifur Rohman and Ricardus Anggi Pramunendar and Edi Jaya Kusuma and Danny Oka Ratmana and Valentijn Venus and Pulung Nurtantio Andono and Zainal Arifin Hasibuan},
keywords = {Disaster emergency information, Named entity recognition, Deep learning, BiLSTM networks, Random oversampling},
abstract = {Precise logistic support is essential after a disaster occurs. It must be timely, accurate, targeted, and based on existing needs. However, obtaining sufficient and accurate information related to logistic distribution locations remains a key problem. Therefore, implementing Named Entity Recognition (NER) can address this issue. In recent years, news coverage through Indonesian digital news media and social media accounts has emerged as a promising source for building a disaster data corpus. This study implemented NER to extract and identify named entities from text-based information, particularly from Indonesian digital news media. In addition to using regular entities from the NER standard, this study introduced new entities specialized for disaster-related information, including DISASTER, SCALE, SUPPLIES, CASUALTIES, and OUTSIDE. The new disaster corpus in the Indonesian language for the NER model was obtained with an imbalanced dataset composition. To overcome this problem, random oversampling was applied. This study also utilized the BiLSTM model to recognize each entity in new textual information, evaluating its performance when the proposed Indonesian disaster corpus was used as a training reference in the deep learning model. Several optimization algorithms applied in BiLSTM were evaluated. The results showed improved BiLSTM performance using Adam optimization and a balanced corpus. Performance indicators achieved were 93.4 %, 82.4 %, and 87.5 % for precision, recall, and F1-score, respectively. The BiLSTM network captured long-range dependencies in sequential data provided by NER. Oversampling ensured that the proposed NER model could precisely recognize all entities and reduce biased results. Thus, the BiLSTM method can better identify entities in the textual corpus of Indonesian disaster-related online news.}
}
@article{XIE2025108561,
title = {Do discussions in human-computer communities trigger group polarization? Insights from the media evocation paradigm},
journal = {Computers in Human Behavior},
volume = {165},
pages = {108561},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108561},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225000081},
author = {Zehang Xie and Shuoshuo Li and Wu Li},
keywords = {Group polarization, AIGC, Human-computer communities, Media evocation paradigm, Spiral of silence},
abstract = {This study investigates group polarization and the spiral of silence within human-computer communities, particularly as intelligent chatbots become increasingly integrated into online interactions. Grounded in the media evocation paradigm, two experiments were conducted: Experiment 1, using a 4 × 4 design, explored how varying stances held by humans and chatbots influence group polarization, revealing distinct polarization mechanisms based on the differing stances within online communities. Experiment 2 employed a 3 × 4 design to examine the impact of human identifiability on the spiral of silence, finding that higher identifiability led to increased conformity to majority opinions, while full anonymity intensified the spiral of silence, especially when chatbots held strong stances. These results contribute to the understanding of group polarization, the media evocation paradigm, and the spiral of silence within human-computer communities.}
}
@article{LIN2023101830,
title = {Automated scholarly paper review: Concepts, technologies, and challenges},
journal = {Information Fusion},
volume = {98},
pages = {101830},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101830},
url = {https://www.sciencedirect.com/science/article/pii/S156625352300146X},
author = {Jialiang Lin and Jiaxin Song and Zhangping Zhou and Yidong Chen and Xiaodong Shi},
keywords = {Automated scholarly paper review, Peer review, Academic publishing, Natural language processing, Artificial intelligence},
abstract = {Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in academic publishing. However, criticisms have long been leveled at this mechanism, mostly because of its poor efficiency and low reproducibility. Recent years have seen the application of artificial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and preliminary implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major difficulties lie in inadequate data, imperfect document parsing and representation, defective human–computer interaction, and flawed deep logical reasoning. Moreover, we point out the future directions and discuss the possible moral and ethical issues of ASPR. In the foreseeable future, ASPR and peer review will coexist in a reinforcing manner before ASPR is able to fully undertake the reviewing workload from humans.}
}
@article{ALFARHOOD202453,
title = {CAML: A Context-Aware Metric Learning approach for improved recommender systems},
journal = {Alexandria Engineering Journal},
volume = {100},
pages = {53-60},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824004952},
author = {Sultan Alfarhood and Meshal Alfarhood},
keywords = {Recommender systems, Collaborative filtering, Metric learning, Attention mechanism},
abstract = {The primary goal of recommender systems is to identify and propose items that users might find appealing. A large number of these systems are heavily dependent on explicit interactions between the user and the item, which can often be infrequent. In this work, we introduce a unique model known as Context-Aware Metric Learning (CAML), designed to enhance the effectiveness of recommendations. The CAML model utilizes an attentive autoencoder to extract latent features from contextual context and incorporates these features into a metric learning framework. In particular, these extracted features act as a Gaussian prior for the embeddings of the items, thereby enhancing the precision of their positioning in the latent space. This integration not only boosts the precision of the recommendations but also increases computational efficiency, rendering CAML appropriate for both offline and online application scenarios. Our model’s evaluation on two real-world datasets reveals that it outperforms several existing baseline models, including those that do not incorporate contextual information such as CML and CPE, as well as other contextual recommendation models like CDL, CATA, and CML+F.}
}
@article{PRATT2023103217,
title = {Bringing advanced technology to strategic decision-making: The Decision Intelligence/Data Science (DI/DS) Integration framework},
journal = {Futures},
volume = {152},
pages = {103217},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103217},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723001222},
author = {Lorien Pratt and Christophe Bisson and Thierry Warin},
keywords = {Strategic decision making, Decision intelligence, Corporate foresight, Artificial intelligence, Complexity, Kahneman’s systems thinking},
abstract = {There is a widespread stated desire amongst both public and private organizations worldwide to engage in more significant “evidence-based reasoning” and to be more “data-driven.” We argue that these two goals are proxies for the often-unstated goal of improving the exploration of possible futures as foresights that could lead to better strategic decisions and improved business outcomes. From this perspective, data and analytics hold great promise and are necessary—but not sufficient—for improving strategic decision-making. Something more is needed to realize this potential. We specify how to fill this gap using an integration framework between technology and decision-makers, which is especially appropriate in complex and/or volatile environments. Our solution—which comprises a methodology as well as a software architecture—therefore unifies not only human decision makers to technology but each other and also integrates several disciplines that have been hitherto unnecessarily separated. Thereby, it could help organizations to address increasing challenges better as well as improve the exploration of possible futures.}
}
@article{XING2024102534,
title = {Voices in the digital storm: Unraveling online polarization with ChatGPT},
journal = {Technology in Society},
volume = {77},
pages = {102534},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102534},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24000824},
author = {Yunfei Xing and Justin Zuopeng Zhang and Guangqing Teng and Xiaotang Zhou},
keywords = {ChatGPT, Opinion polarization, social media, BERTopic, BERTSentiment},
abstract = {ChatGPT, an esteemed natural language processing model, has demonstrated remarkable capabilities in intelligent text generation, interactive conversation, and myriad additional tasks. The utilization of ChatGPT has generated a wide debate among users with different attitudes on social media platforms, culminating in the phenomenon of polarization. Based on confirmation bias theory, this paper presented a theoretical framework that elucidates the process of online polarization. Subsequently, we develop the sentiment classification (BERTSentiment) and topic identification (BERTopic) model leveraging the pre-trained BERT (Bidirectional Encoder Representations from Transformers) model. To empirically investigate the public sentiment regarding ChatGPT, an in-depth study was conducted on the X platform. The results indicate that although a small portion of users (approximately 10%) express negative sentiments regarding ChatGPT's ethical considerations, functionality, and accuracy, the majority of users exhibit either positive or neutral views. Among the public concerns, AI and bot functions, response quality, instant messaging, enterprise applications, and technological aspects emerge as the most prominent topics. This study sheds light on public perceptions regarding the progress and integration of emerging technologies. Moreover, it introduces a fresh data mining perspective that enhances our understanding of polarization in the context of social media research.}
}
@article{HE2025103901,
title = {Few-shot cross domain event discovery in narrative text},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103901},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103901},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002607},
author = {Ruifang He and Fei Huang and Jinsong Ma and Jinpeng Zhang and Yongkai Zhu and Shiqi Zhang and Jie Bai},
keywords = {Event discovery, Few-shot domain adaptation, Positive–negative balanced sampling, Parameter adapter},
abstract = {Cross-domain event detection presents notable challenges in the form of data scarcity, and existing few-shot algorithms only consider events whose types are predefined, resulting in low coverage or excessive trivial identification results. To address this issue, this paper proposes the task Few-shot Cross Domain Event Discovery, which includes two subtasks: Domain Event Discovery and Few-shot Domain Adaptation. The former aims to identify the type-agnostic event triggers, and the latter completes domain adaptation with only a few annotated domain samples. Additionally, we introduce a positive–negative balanced sampling mechanism and a novel domain parameter adapter for these two subtasks, respectively. Extensive experiments on the DuEE dataset and the ACE2005 dataset show that our proposed method outperforms the current state-of-the-art method by 6.3% in Mix-F1 score on average. Moreover, we achieve SOTA performance in all domains of the DuEE dataset.}
}
@article{FRANCIA2025100310,
title = {Automating materiality assessment with a data-driven document-based approach},
journal = {International Journal of Information Management Data Insights},
volume = {5},
number = {1},
pages = {100310},
year = {2025},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100310},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000995},
author = {Matteo Francia and Enrico Gallinucci and Matteo Golfarelli},
keywords = {Materiality assessment, Decision support system, Information retrieval, Sustainability reporting},
abstract = {Materiality assessment is a critical process for companies to understand the interest perceived by its stakeholders towards topics related to environmental, social, and governance issues. Materiality assessment helps companies define their growth and communicative strategies; recently, it has become crucial within sustainability reporting, i.e., the practice of annually declaring the activities conducted to pursue economic growth in a sustainable way for society. In this paper, we propose a data-driven and automated approach to carry out materiality assessment. Stakeholders’ perception of important topics is obtained by analyzing relevant textual documents (e.g., company reports, press releases, social media posts), identifying mentions of potentially interesting topics, and converting them to scores that produce materiality rankings or matrices. An iterative methodology is proposed to incrementally carry out materiality assessment by progressively building the domain knowledge required to automate the process. Efficiency and effectiveness evaluations are carried out in a real-world scenario.}
}
@article{JIANG2025100566,
title = {AI drug discovery tools and analysis technology: New methods aid in studying the compatibility of Traditional Chinese Medicine},
journal = {Pharmacological Research - Modern Chinese Medicine},
volume = {14},
pages = {100566},
year = {2025},
issn = {2667-1425},
doi = {https://doi.org/10.1016/j.prmcm.2024.100566},
url = {https://www.sciencedirect.com/science/article/pii/S2667142524002082},
author = {Qiwu Jiang and Suhan Yang and Shan He and Fei Li},
keywords = {Traditional Chinese Medicine compatibility, AI drug discovery tool, Combination prediction, Compatibility mechanisms, Compatibility ratio optimization},
abstract = {Introduction
The compatibility of Traditional Chinese Medicine (TCM) holds the potential for reducing toxicity and enhancing efficacy, serving as a crucial guide for the clinical application of TCM. In recent years, the development of artificial intelligence (AI) drug discovery tools has introduced novel approaches for analyzing the multichemical components of TCM, thereby saving time and efforts in experiments.
Methods
The keywords "Traditional Chinese Medicine" and "Artificial Intelligence", "Traditional Chinese Medicine" and "drug compatibility" were searched across various literature databases, including Web of Science, Google Scholar, PubMed, and Elsevier. Over 100 articles were reviewed, and after narrowing the selection to those focused on compatibility, the chosen studies were carefully analyzed to summarize the latest developments for this review.
Results
The review introduce AI drug discovery tools, including virtual screening, target prediction, ADMET prediction, and data mining, along with their roles in studying TCM compatibility. The results further provide insights of AI's application in TCM combination prediction, TCM compatibility mechanisms, and optimization of TCM compatibility ratio within the TCM compatibility research field.
Discussion
Traditional Chinese Medicine uses holistic formulas involving multiple components, targets, and pathways for disease treatment, but scientific explanations of these formulas are limited. AI aids TCM research by predicting combinations, mechanisms, and optimizing ratios, which improves efficiency and reducing costs. However, AI predictions may not be definitely accurate, and traditional expertise is still essential for validation. Future applications of AI in TCM require improved tools and collaboration between AI and TCM researchers.}
}
@article{LV2024115460,
title = {iSUMO-RsFPN: A predictor for identifying lysine SUMOylation sites based on multi-features and feature pyramid networks},
journal = {Analytical Biochemistry},
volume = {687},
pages = {115460},
year = {2024},
issn = {0003-2697},
doi = {https://doi.org/10.1016/j.ab.2024.115460},
url = {https://www.sciencedirect.com/science/article/pii/S0003269724000046},
author = {Zhe Lv and Xin Wei and Siqin Hu and Gang Lin and Wangren Qiu},
keywords = {SUMOylation, Feature extraction, Deep learning, Ensemble learning, Pre-training model},
abstract = {SUMOylation is a protein post-translational modification that plays an essential role in cellular functions. For predicting SUMO sites, numerous researchers have proposed advanced methods based on ordinary machine learning algorithms. These reported methods have shown excellent predictive performance, but there is room for improvement. In this study, we constructed a novel deep neural network Residual Pyramid Network (RsFPN), and developed an ensemble deep learning predictor called iSUMO-RsFPN. Initially, three feature extraction methods were employed to extract features from samples. Following this, weak classifiers were trained based on RsFPN for each feature type. Ultimately, the weak classifiers were integrated to construct the final classifier. Moreover, the predictor underwent systematically testing on an independent test dataset, where the results demonstrated a significant improvement over the existing state-of-the-art predictors. The code of iSUMO-RsFPN is free and available at https://github.com/454170054/iSUMO-RsFPN.}
}
@article{XIAO2024107442,
title = {MSGVUL: Multi-semantic integration vulnerability detection based on relational graph convolutional neural networks},
journal = {Information and Software Technology},
volume = {170},
pages = {107442},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107442},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000478},
author = {Wei Xiao and Zhengzhang Hou and Tao Wang and Chengxian Zhou and Chao Pan},
keywords = {Vulnerability detection, Code representation, Program slicing, Graph convolutional neural networks},
abstract = {Software security has drawn extensive attention as software projects have grown increasingly large and complex. Since the traditional manual or equipment vulnerability detection technology cannot meet today's software development needs, there is a recognized need to create more effective techniques to address security issues. Although various vulnerability detection systems have been proposed, most are based only on serialization or graph representation, to inadequate effect. We propose a system, MSGVUL, that provides superior vulnerability detection using a new multi-semantic approach. MSGVUL uses versatile and efficient code slicing employing a search algorithm based on sensitive data and functions and innovatively constructs an SSVEC model to fully integrate the semantic and structural information into the code. We also developed a novel BAG model, made up of BAP and PAG frameworks, that enables the hierarchical extraction of code vulnerability representations from the graph and sequence levels. The MSGVUL model is evaluated on slice-level and function-level vulnerability datasets, and the results demonstrate that the MSGVUL method outperforms other state-of-the-art methods.}
}
@incollection{2024243,
title = {Index},
editor = {Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth},
booktitle = {Trolley Crash},
publisher = {Academic Press},
pages = {243-248},
year = {2024},
isbn = {978-0-443-15991-6},
doi = {https://doi.org/10.1016/B978-0-44-315991-6.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159916000212}
}
@article{LI2024104621,
title = {Artificial intelligence-powered pharmacovigilance: A review of machine and deep learning in clinical text-based adverse drug event detection for benchmark datasets},
journal = {Journal of Biomedical Informatics},
volume = {152},
pages = {104621},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104621},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400039X},
author = {Yiming Li and Wei Tao and Zehan Li and Zenan Sun and Fang Li and Susan Fenton and Hua Xu and Cui Tao},
keywords = {Pharmacovigilance, Machine learning/Deep learning, Adverse drug event (ADE) extraction, named-entity recognition (NER), Relation extraction (RE), Natural language processing (NLP)},
abstract = {Objective
The primary objective of this review is to investigate the effectiveness of machine learning and deep learning methodologies in the context of extracting adverse drug events (ADEs) from clinical benchmark datasets. We conduct an in-depth analysis, aiming to compare the merits and drawbacks of both machine learning and deep learning techniques, particularly within the framework of named-entity recognition (NER) and relation classification (RC) tasks related to ADE extraction. Additionally, our focus extends to the examination of specific features and their impact on the overall performance of these methodologies. In a broader perspective, our research extends to ADE extraction from various sources, including biomedical literature, social media data, and drug labels, removing the limitation to exclusively machine learning or deep learning methods.
Methods
We conducted an extensive literature review on PubMed using the query “(((machine learning [Medical Subject Headings (MeSH) Terms]) OR (deep learning [MeSH Terms])) AND (adverse drug event [MeSH Terms])) AND (extraction)”, and supplemented this with a snowballing approach to review 275 references sourced from retrieved articles.
Results
In our analysis, we included twelve articles for review. For the NER task, deep learning models outperformed machine learning models. In the RC task, gradient Boosting, multilayer perceptron and random forest models excelled. The Bidirectional Encoder Representations from Transformers (BERT) model consistently achieved the best performance in the end-to-end task. Future efforts in the end-to-end task should prioritize improving NER accuracy, especially for 'ADE' and 'Reason'.
Conclusion
These findings hold significant implications for advancing the field of ADE extraction and pharmacovigilance, ultimately contributing to improved drug safety monitoring and healthcare outcomes.}
}
@article{GACHKAR2025144448,
title = {Text-based algorithms for automating life cycle inventory analysis in building sector life cycle assessment studies},
journal = {Journal of Cleaner Production},
volume = {486},
pages = {144448},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.144448},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624038976},
author = {Sadaf Gachkar and Darya Gachkar and Erfan Ghofrani and Antonio {García Martínez} and Cecilio {Angulo Bahón}},
keywords = {Life cycle assessment, Inventory data collection, Text-based algorithms, Sustainability, Artificial intelligence, Natural language processing},
abstract = {Life Cycle Assessment (LCA) is essential for evaluating the environmental impact of sustainable activities in industry. Despite its importance, there exist challenges negatively impacting its deployment, particularly the time-consuming process of gathering inventory data. This research introduces a novel framework that leverages advanced text-based algorithms from Natural Language Processing (NLP), significantly enhancing the efficiency of data collection in LCA studies. Focusing on the inventory phase, the novelty of this research lies in its ability to reduce data collection time by an estimated 80%–90% compared to conventional methods and improve accuracy by directly extracting materials from bills of quantities (BoQs), which usually list all the construction materials. While our methodology shows promise, it faces challenges due to project complexity, particularly the need for consistent terminology between BoQ and reference databases, though future advancements in matching algorithms may enhance our approach’s efficiency. Real-world case studies demonstrate the framework’s effectiveness, offering flexibility across industries and system complexities.}
}
@article{SULIS2024105949,
title = {Introduction for computer law and security review: special issue “knowledge management for law”},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105949},
year = {2024},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2024.105949},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000165},
author = {Emilio Sulis and Luigi Di Caro and Rohan Nanda}
}
@article{CHAMAKURI2025100343,
title = {A systematic review on recent methods on deep learning for automatic detection of Alzheimer's disease},
journal = {Medicine in Novel Technology and Devices},
volume = {25},
pages = {100343},
year = {2025},
issn = {2590-0935},
doi = {https://doi.org/10.1016/j.medntd.2024.100343},
url = {https://www.sciencedirect.com/science/article/pii/S2590093524000596},
author = {Radhakrishna Chamakuri and Hyma Janapana},
keywords = {Alzheimer disease diagnosis, Artificial intelligence, Deep learning, Ensemble learning, Machine learning},
abstract = {Alzheimer's disease (AD) is the most frequent cause of dementia, however, and it is caused by a number of different disorders. With regard to the elderly population all over the world, Alzheimer's disease is the seventh largest cause of mortality, disability, and reliance. Depression, social isolation, inactivity, alcohol, smoking, obesity, diabetes, high blood pressure, and age are all variables that can increase the likelihood of getting dementia. Other risk factors include social isolation, depression, and smoking. A diagnosis of Alzheimer's disease at an earlier stage may improve the odds of receiving care and therapy. Medical professionals often diagnose AD based on a limited number of symptoms. On the other hand, it is now possible to identify and categorize Alzheimer's disease (AD) because of technological advancements such as artificial intelligence (AI). However, to identify the current AI-enabled approaches, we must conduct an investigation into the state of the art. This breakthrough in diagnosis methodologies will enable the development of the Clinical Decision Support System (CDSS), capable of automatically diagnosing Alzheimer's disease (AD) without human expertise. In this publication, we conduct a systematic review of sixty research articles previously reviewed by other researchers. The systematic review sheds light on the synthesis of new knowledge and ideas. This study discusses the current approaches for machine learning, deep learning methods, ensemble models, transfer learning, and methods used for early Alzheimer's disease diagnosis. This paper provides answers to a large number of research issues and synthesizes fresh information that is helpful to the reader on many elements of AI-enabled approaches for Alzheimer's disease diagnosis. In addition, it has the potential to stimulate additional research into more effective methods of computer-based intelligent identification of Alzheimer's disease.}
}
@incollection{BRUTZMAN2024189,
title = {Chapter Eleven - Designing meaningful metrics to demonstrate ethical supervision of autonomous systems: How do you measure that?},
editor = {Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth},
booktitle = {Trolley Crash},
publisher = {Academic Press},
pages = {189-208},
year = {2024},
isbn = {978-0-443-15991-6},
doi = {https://doi.org/10.1016/B978-0-44-315991-6.00017-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159916000170},
author = {Don Brutzman and Curtis Blais},
keywords = {Ethics, ethical AI, autonomy, metrics, negligence, human–machine teams, Autonomous Vehicle Command Language (AVCL), Mission Execution Ontology (MEO), Dimensions of Autonomous Decision Making (DADM), TestDevOps, virtual environments, trust},
abstract = {Design and testing of meaningful metrics for artificial intelligence (AI) guiding ethical robots holds fundamental importance for useful progress and trustable operations. Moral responsibility and authority for ethical behaviors by remote autonomous systems ultimately lies with the humans responsible for unleashing individual robots. Lines of success or failure are sharply defined when delegating tasks to robots which have the capacity for life-saving or lethal force. Goals, constraints, and metrics that are commonly defined and shared by humans and robots can be mutually understood, formally verifiable as consistent, and further testable in repeatable ways. Metrics for AI are essential, as illustrated by the diverse topics explored throughout this book. It is interesting that commonplace gaps in applied AI often derive from “Here are the measurements we know how to take” which are too easily over-extrapolated or over-simplified into conclusions matching prior preconceptions. In other words, legacy metrics are appealing but might not broadly apply to general situations. We assert that necessary subsequent questions are “How do we define meaningful objectives and outcomes for a current autonomous system?” and “How do we measure those characteristics that indicate expected success/failure?” Since testing drives system evolution, such questions then become “Once we can measure meaningful results, how do we assemble exemplars into test suites that confirm successful completion across ongoing system lifecycles?” This chapter explores potential design principles for metrics that test ethical AI systems, in both real and virtual system frameworks. Such comprehensive test frameworks are essential for achieving meaningful human authority over autonomous robots. Final success is measurable when trust is achieved.}
}
@article{ASLAM2023110494,
title = {Attention-based multimodal sentiment analysis and emotion recognition using deep neural networks},
journal = {Applied Soft Computing},
volume = {144},
pages = {110494},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110494},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623005124},
author = {Ajwa Aslam and Allah Bux Sargano and Zulfiqar Habib},
keywords = {Sentiment analysis, Emotion recognition, Multimodal attention, Deep neural networks},
abstract = {There has been a growing interest in multimodal sentiment analysis and emotion recognition in recent years due to its wide range of practical applications. Multiple modalities allow for the integration of complementary information, improving the accuracy and precision of sentiment and emotion recognition tasks. However, working with multiple modalities presents several challenges, including handling data source heterogeneity, fusing information, aligning and synchronizing modalities, and designing effective feature extraction techniques that capture discriminative information from each modality. This paper introduces a novel framework called “Attention-based Multimodal Sentiment Analysis and Emotion Recognition (AMSAER)” to address these challenges. This framework leverages intra-modality discriminative features and inter-modality correlations in visual, audio, and textual modalities. It incorporates an attention mechanism to facilitate sentiment and emotion classification based on visual, textual, and acoustic inputs by emphasizing relevant aspects of the task. The proposed approach employs separate models for each modality to automatically extract discriminative semantic words, image regions, and audio features. A deep hierarchical model is then developed, incorporating intermediate fusion to learn hierarchical correlations between the modalities at bimodal and trimodal levels. Finally, the framework combines four distinct models through decision-level fusion to enable multimodal sentiment analysis and emotion recognition. The effectiveness of the proposed framework is demonstrated through extensive experiments conducted on the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. The results confirm a notable performance improvement compared to state-of-the-art methods, attaining 85% and 93% accuracy for sentiment analysis and emotion classification, respectively. Additionally, when considering class-wise accuracy, the results indicate that the “angry” emotion and “positive” sentiment are classified more effectively than the other emotions and sentiments, achieving 96.80% and 93.14% accuracy, respectively.}
}
@article{LU2024102380,
title = {Privacy-preserving data integration and sharing in multi-party IoT environments: An entity embedding perspective},
journal = {Information Fusion},
volume = {108},
pages = {102380},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102380},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524001581},
author = {Junyu Lu and Henry Leung and Nan Xie},
keywords = {Data integration & sharing, Privacy preservation, Entity embedding},
abstract = {The increasing prevalence of IoT applications highlights the urgency for insightful data fusion and information acquisition, boosting data integration and sharing needs. However, challenges arise in multi-party data sharing due to inherent data heterogeneity and privacy concerns. To address these issues, this paper discusses the feasibility of using embedding vectors as the semantic representation, aiming to enhance interoperability across diverse data sources and lay the foundation for natural language-based data querying. At the specific method level, this paper proposes an improved entity tree embedding algorithm to reduce information loss and ameliorate the representation of entity semantics. Additionally, a privacy preservation mechanism based on the entity embedding approach is introduced to provide privacy protection for text-based data. Experimental results on address data demonstrate the mechanism’s efficacy in achieving privacy protection comparable to the widely adopted 2D Laplace plane noise method. Furthermore, incorporating the entity tree embedding into the privacy mechanism could yield more robust and reasonable results regarding location privacy and service quality, signifying the validity of the entity embedding results.}
}
@article{GARG2025112969,
title = {ATSumm: Auxiliary information enhanced approach for abstractive disaster tweet summarization with sparse training data},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {112969},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.112969},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000176},
author = {Piyush Kumar Garg and Roshni Chakraborty and Sourav Kumar Dandapat},
keywords = {Disasters, Abstractive summarization, Social media, Crisis scenario, Pointer generator network model, Deep learning, Data sparsity},
abstract = {The abundance of situational information on Twitter poses a challenge for users to manually discern vital and relevant information during disasters. A concise and human-interpretable overview of this information can aid decision makers implement efficient and quick disaster responses. Existing abstractive summarization approaches can be categorized as sentence- or key-phrase-based. This study focused on a sentence-based approach, which is typically implemented as a dual-phase procedure. The initial phase, known as the extractive phase, involves the identification of the most relevant tweets. The subsequent phase, which is referred to as the abstractive phase, generates a more human-interpretable summary. In this study, we adopted a methodology from prior research for the extractive phase. Most existing approaches employ deep learning-based frameworks for the abstractive phase of summarization. Such frameworks can either be pre-trained or require training from scratch. However, to achieve an appropriate level of performance, it is imperative to have substantial training data for both methods, which are not readily available. This study proposed an abstractive tweet summarizer (ATSumm) that effectively addresses the issue of data sparsity using auxiliary information. We introduced the auxiliary pointer generator network (AuxPGN) model, which utilizes a unique attention mechanism called key-phrase attention. This attention mechanism incorporates auxiliary information in the form of key-phrases and their corresponding importance scores from the input tweets. We evaluated the proposed approach through comparisons with 10 state-of-the-art approaches across 13 disaster datasets. The evaluation results indicated that ATSumm achieved superior performance compared with state-of-the-art approaches, with an improvement of 4−80% in the ROUGE-N F1-score.}
}
@article{MALBURG2023106727,
title = {Converting semantic web services into formal planning domain descriptions to enable manufacturing process planning and scheduling in industry 4.0},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106727},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106727},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623009119},
author = {Lukas Malburg and Patrick Klein and Ralph Bergmann},
keywords = {Semantic web services, Industry 4.0, Automated planning, Planning domain definition language, Cyber-physical workflows},
abstract = {To build intelligent manufacturing systems that react flexibly in case of failures or unexpected circumstances, manufacturing capabilities of production systems must be utilized as much as possible. Artificial Intelligence (AI) and, in particular, automated planning can contribute to this by enabling flexible production processes. To efficiently leverage automated planning, an almost complete planning domain description of the real-world is necessary. However, creating such planning descriptions is a demanding and error-prone task that requires high manual efforts even for domain experts. In addition, maintaining the encoded knowledge is laborious and, thus, can lead to outdated domain descriptions. To reduce the high efforts, already existing knowledge can be reused and transformed automatically into planning descriptions to benefit from organization-wide knowledge engineering activities. This paper presents a novel approach that reduces the described efforts by reusing existing knowledge for planning and scheduling in Industry 4.0 (I4.0). For this purpose, requirements for developing a converter that transforms existing knowledge are derived from literature. Based on these requirements, the SWS2PDDL converter is developed that transforms the knowledge into formal Planning Domain Definition Language (PDDL) descriptions. The approach’s usefulness is verified by a practical evaluation with a near real-world application scenario by generating failures in a physical smart factory and evaluating the generated re-planned production processes. When comparing the resulting plan quality to those achieved by using a manually modeled planning domain by a domain expert, the automatic transformation by SWS2PDDL leads to comparable or even better results without requiring the otherwise high manual modeling efforts.}
}
@article{TSIRMPAS2024108231,
title = {Neural natural language processing for long texts: A survey on classification and summarization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108231},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108231},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003890},
author = {Dimitrios Tsirmpas and Ioannis Gkionis and Georgios Th. Papadopoulos and Ioannis Mademlis},
keywords = {Natural language processing, Long document, Document classification, Document summarization, Sentiment analysis, Deep neural networks},
abstract = {The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded online renders automated understanding of lengthy texts a critical issue. Relevant applications include automated Web mining, legal document review, medical records analysis, financial reports analysis, contract management, environmental impact assessment, news aggregation, etc. Despite the relatively recent development of efficient algorithms for analyzing long documents, practical tools in this field are currently flourishing. This article serves as an entry point into this dynamic domain and aims to achieve two objectives. First of all, it provides an introductory overview of the relevant neural building blocks, serving as a concise tutorial for the field. Secondly, it offers a brief examination of the current state-of-the-art in two key long document analysis tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Consequently, this article presents an introductory exploration of document-level analysis, addressing the primary challenges, concerns, and existing solutions. Finally, it offers a concise definition of “long text/document”, presents an original overarching taxonomy of common deep neural methods for long document analysis and lists publicly available annotated datasets that can facilitate further research in this area.}
}
@article{BAO2024110119,
title = {Further expansion from smart manufacturing system (SMS) to social smart manufacturing system (SSMS) based on industrial internet},
journal = {Computers & Industrial Engineering},
volume = {191},
pages = {110119},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110119},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224002407},
author = {Yuguang Bao and Xianyu Zhang and Chengjun Wang and Xinguo Ming},
keywords = {Smart Manufacturing System, Social Smart Manufacturing System, Social Manufacturing, Industrial Internet, System Architecture, Testbed Validation},
abstract = {The Industrial Internet (II) is driving the evolution of the manufacturing industry characterized by socially-engagement, servitization, universal interaction, and connection. With a single technical perspective, the requirements of “Sociality” and “Smartness” for future factories are often confused. The motivation of this study is to fill the existing gaps and provide practical knowledge for exploring how to design, synthesize, implement, and deploy a social smart manufacturing system (SSMS). Through an in-depth industrial practice investigation based on cooperative inquiry, an innovative factory model, namely II-supported SSMS, was proposed and synthesized for establishing a socialized aerospace part machining service ecosystem. The II-supported SSMS has been implemented and demonstrated via testbed validation. Furthermore, useful and portable practical knowledge was elicited from the industrial practice. A detailed six-step system architecture and a structural and hierarchical implementation strategy were presented, revealing the reference subsystem, integration logic, main technology tenants, and critical implementation processes of II-supported SSMS. Finally, the practical value and useful insights of II-supported SSMS were analyzed from the aspects of functionality, efficiency, flexibility, smartness, anthropocentric production, interconnectedness, collaborative intelligence, and scalability and portability. The potential risks, challenges, and barriers also were discussed.}
}
@incollection{OLENIK2024,
title = {Machine Learning and Omic Data for Prediction of Health and Chronic Diseases},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00284-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002840},
author = {Mark Olenik and Handan Melike Dönertaş},
keywords = {Chronic diseases, Health prediction, Machine learning, Omic data},
abstract = {Machine learning (ML) combined with diverse omic datasets offers transformative potential for predicting health outcomes and chronic diseases. By leveraging diverse omic data, ML models can identify biomarkers, enhance diagnostic accuracy, and enable personalized treatments. This chapter introduces the fundamental concepts of ML, key omic data sources, and the challenges associated with ML and omic-based disease prediction. Advances in technology, large-scale datasets, interpretable ML algorithms, and the digitization of healthcare are poised to revolutionize medical science, paving the way for precision medicine and early disease detection.}
}
@article{FORTH2023100263,
title = {BIM4EarlyLCA: An interactive visualization approach for early design support based on uncertain LCA results using open BIM},
journal = {Developments in the Built Environment},
volume = {16},
pages = {100263},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100263},
url = {https://www.sciencedirect.com/science/article/pii/S266616592300145X},
author = {Kasimir Forth and Alexander Hollberg and André Borrmann},
keywords = {LCA, BIM, Design decision support, Early design stages},
abstract = {To meet the European climate goals in the building sector, a holistic optimization of embodied greenhouse gas (GHG) emissions using the method of life cycle assessments (LCA) are necessary. The early design stages have high impact on the final performance of the buildings and are characterized by high uncertainty due to the lack of information and not yet taken decisions. Furthermore, most current LCA approaches based on Building Information Models (BIM) require high expertise and experience in both BIM and LCA and do not follow an intuitive visualization approach for other stakeholders and non-experts. This paper presents a novel design-decision-making approach for reducing embodied GHG emissions by interactive, model-based visualizations of uncertain LCA results. The proposed workflow is based on open BIM data formats, such as Industry Foundation Classes (IFC) and BIM Collaboration Format (BCF), and is developed for decision support for non-LCA experts in the early design stages. With the help of a user study, the prototypical implementation is tested by 103 participants with different levels of experience in BIM and LCA based on a case study. We evaluate the proposed approach regarding the support of open BIM data formats, different LCA visualization strategies, and the intuitiveness of different approaches to visualizing uncertain LCA results. The user study results show a broad acceptance and need for open BIM data formats and model-based LCA visualization but less for visualizing uncertainties, which needs further research. In conclusion, this interactive, model-based visualization approach using color coding supports non-LCA experts in the design decision-making process in early design stages.}
}
@article{LUO2025128768,
title = {A bi-consolidating model for joint relational triple extraction},
journal = {Neurocomputing},
volume = {614},
pages = {128768},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128768},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401539X},
author = {Xiaocheng Luo and Yanping Chen and Ruixue Tang and Caiwei Yang and Ruizhang Huang and Yongbin Qin},
keywords = {Pixel difference convolutions, Attention mechanism, Relational triple extraction, Joint entity and relation extraction},
abstract = {Current methods to extract relational triples directly make a prediction based on a possible entity pair in a raw sentence without depending on entity recognition. The task suffers from a serious semantic overlapping problem, in which several relation triples may share one or two entities in a sentence. In this paper, based on a two-dimensional sentence representation, a bi-consolidating model is proposed to address this problem by simultaneously reinforcing the local and global semantic features relevant to a relation triple. This model consists of a local consolidation component and a global consolidation component. The first component uses a pixel difference convolution to enhance semantic information of a possible triple representation from adjacent regions and mitigate noise in neighboring neighbors. The second component strengthens the triple representation based a channel attention and a spatial attention, which has the advantage to learn remote semantic dependencies in a sentence. They are helpful to improve the performance of both entity identification and relation type classification in relation triple extraction. After evaluated on several publish datasets, the bi-consolidating model achieves competitive performance. Analytical experiments demonstrate the effectiveness of our model for relational triple extraction and give motivation for other natural language processing tasks.}
}
@incollection{MIEVILLE2024,
title = {Modern Automation in Organic Synthesis Laboratories},
booktitle = {Reference Module in Chemistry, Molecular Sciences and Chemical Engineering},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-409547-2},
doi = {https://doi.org/10.1016/B978-0-323-96025-0.00047-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323960250000478},
author = {Pascal Miéville and Florian de Nanteuil},
keywords = {Automation, Autonomous laboratory, Data science, Flow chemistry, High throughput experimentation, Integration, Orchestration, Robotics, Sample transfer, Self-driving laboratory},
abstract = {The exponential growth of automation within organic chemistry has unlocked many opportunities for scientists worldwide. The chapter aims to provide a comprehensive understanding of the transformative potential and the complex landscape created by the evolution of automation in organic chemistry. It presents an overview dedicated to exploring the global evolution of automation in experimental chemistry, focusing on its pivotal concepts, cutting-edge technologies, and diverse applications that drive this new branch of experimental sciences. The integration of robotics, automation, global data management, and machine learning algorithms within the domain of organic synthesis signifies a transformative leap in laboratory practices. New tools harmonize to execute an extensive range of synthetic tasks, promising reproducibility, precision, and scalability in handling compounds and performing reactions. This approach optimizes molecule production, resulting in accelerated discovery processes, heightened efficiency, and a drastic reduction in errors. The chapter dives into various components underpinning this era of organic synthesis automation, encompassing the historical progression, current state-of-the-art in instrumentation, experimental coding, data processing automation, and the role of artificial intelligence in predictive modeling of reaction outcomes. Moreover, it explores the persistent challenges posed by these new technologies, addressing issues such as solid sampling, global laboratory integration, and examines implications touching on ethical considerations, safety concerns, and sustainability aspects.}
}
@incollection{ELLSWORTH2024239,
title = {Acronyms},
editor = {Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth},
booktitle = {Trolley Crash},
publisher = {Academic Press},
pages = {239-241},
year = {2024},
isbn = {978-0-443-15991-6},
doi = {https://doi.org/10.1016/B978-0-44-315991-6.00020-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159916000200},
author = {Shannon Ellsworth and Hsin-Fu ‘Sinker’ Wu}
}
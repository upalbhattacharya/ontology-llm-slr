@article{SULASTRI2024101979,
title = {Transforming towards inclusion-by-design: Information system design principles shaping data-driven financial inclusiveness},
journal = {Government Information Quarterly},
volume = {41},
number = {4},
pages = {101979},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101979},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000716},
author = {Reni Sulastri and Marijn Janssen and Ibo {van de Poel} and Aaron Ding},
keywords = {Digital governance, Inclusion, Lending systems, Value-based requirements, Inclusion by design, System-level transformation},
abstract = {Digitalization and datafication of financial systems result in more efficiency, but might also result in the exclusions of certain groups. Governments are looking for ways to increase inclusions and leave no one behind. For this, they must govern an organizational ecosystem of public and private parties. We derive value-based requirements through a systematic research methodology and iteratively refine design principles for achieving inclusivity goals. This refinement process is enriched by interviews with field experts, leading to the formulation of key Design principles: the essential role of inclusive metrics, leveraging alternative data sources, ensuring transparency in loan processes and the ability for decision contestation, providing tailored credit solutions, and maintaining long-term system sustainability. The government's role is to ensure a level playing field where all parties have equal access to the data. Following the principles ensures that exclusion and discrimination become visible and can be avoided. This study underscores the necessity for system-level transformations, inclusion-by-design, and advocacy for a new system design complemented by regulatory updates, new data integration, inclusive AI, and organizational collaborative shifts. These principles can also be used in different data-driven governance situations.}
}
@article{GHOSH2024729,
title = {Feeding the wrath with myelin},
journal = {Trends in Immunology},
volume = {45},
number = {10},
pages = {729-731},
year = {2024},
note = {Special issue: Neuroimmunology – II},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2024.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1471490624002126},
author = {Sourav Ghosh and Carla V. Rothlin},
abstract = {Kloosterman and colleagues studied molecular and cellular changes during radiation therapy and disease recurrence across molecular subtypes of glioblastoma. They uncovered a distinct immune–cancer cell metabolic crosstalk during proneural/oligodendrocyte progenitor cell-like to mesenchymal-like transition, wherein macrophages feed on cholesterol-rich myelin debris to provide lipids to mesenchymal tumor cells, thereby fueling glioblastoma growth.}
}
@article{HARDINGLARSEN2024108459,
title = {Protein representations: Encoding biological information for machine learning in biocatalysis},
journal = {Biotechnology Advances},
volume = {77},
pages = {108459},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108459},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024001538},
author = {David Harding-Larsen and Jonathan Funk and Niklas Gesmar Madsen and Hani Gharabli and Carlos G. Acevedo-Rocha and Stanislav Mazurenko and Ditte Hededam Welner},
keywords = {Machine learning, Biocatalysis, Protein representations, Enzyme engineering, Representation learning, Protein dynamics, Predictive models},
abstract = {Enzymes offer a more environmentally friendly and low-impact solution to conventional chemistry, but they often require additional engineering for their application in industrial settings, an endeavour that is challenging and laborious. To address this issue, the power of machine learning can be harnessed to produce predictive models that enable the in silico study and engineering of improved enzymatic properties. Such machine learning models, however, require the conversion of the complex biological information to a numerical input, also called protein representations. These inputs demand special attention to ensure the training of accurate and precise models, and, in this review, we therefore examine the critical step of encoding protein information to numeric representations for use in machine learning. We selected the most important approaches for encoding the three distinct biological protein representations — primary sequence, 3D structure, and dynamics — to explore their requirements for employment and inductive biases. Combined representations of proteins and substrates are also introduced as emergent tools in biocatalysis. We propose the division of fixed representations, a collection of rule-based encoding strategies, and learned representations extracted from the latent spaces of large neural networks. To select the most suitable protein representation, we propose two main factors to consider. The first one is the model setup, which is influenced by the size of the training dataset and the choice of architecture. The second factor is the model objectives such as consideration about the assayed property, the difference between wild-type models and mutant predictors, and requirements for explainability. This review is aimed at serving as a source of information and guidance for properly representing enzymes in future machine learning models for biocatalysis.}
}
@article{HASSANI2024102136,
title = {A systematic review of data fusion techniques for optimized structural health monitoring},
journal = {Information Fusion},
volume = {103},
pages = {102136},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102136},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004529},
author = {Sahar Hassani and Ulrike Dackermann and Mohsen Mousavi and Jianchun Li},
keywords = {Structural health monitoring, Raw data fusion, Feature fusion, Decision fusion, Deep learning, Machine learning},
abstract = {Advancements in structural health monitoring (SHM) techniques have spiked in the past few decades due to the rapid evolution of novel sensing and data transfer technologies. This development has facilitated the simultaneous recording of a wide range of data, which could contain abundant damage-related features. Concurrently, the age of omnipresent data started with massive amounts of SHM data collected from large-size heterogeneous sensor networks. The abundance of information from diverse sources needs to be aggregated to enable robust decision-making strategies. Data fusion is the process of integrating various data from heterogeneous sources to produce more useful, accurate, and reliable information about system behavior. This paper reviews recent developments in data fusion techniques applied to SHM systems. The theoretical concepts, applications, benefits, and limitations of current methods and challenges in SHM are presented, and future trends in data fusion methods are discussed. Furthermore, a set of criteria is proposed to evaluate contents and information from original and review papers in this field, and a road map is provided discussing possible future work.}
}
@article{BUTZ2025105948,
title = {Contextualizing predictive minds},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {168},
pages = {105948},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105948},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004172},
author = {Martin V. Butz and Maximilian Mittenbühler and Sarah Schwöbel and Asya Achimova and Christian Gumbsch and Sebastian Otte and Stefan Kiebel},
keywords = {Prediction, Cognitive modeling, Events, Active inference, Learning, Behavior, Free energy, Abstraction, Context inference, Deep learning},
abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.}
}
@article{SORBELLO2023,
title = {Artificial Intelligence–Enabled Software Prototype to Inform Opioid Pharmacovigilance From Electronic Health Records: Development and Usability Study},
journal = {JMIR AI},
volume = {2},
year = {2023},
issn = {2817-1705},
doi = {https://doi.org/10.2196/45000},
url = {https://www.sciencedirect.com/science/article/pii/S2817170523000261},
author = {Alfred Sorbello and Syed Arefinul Haque and Rashedul Hasan and Richard Jermyn and Ahmad Hussein and Alex Vega and Krzysztof Zembrzuski and Anna Ripple and Mitra Ahadpour},
keywords = {electronic health records, pharmacovigilance, artificial intelligence, real world data, EHR, natural language, software application, drug, Food and Drug Administration, deep learning},
abstract = {Background
The use of patient health and treatment information captured in structured and unstructured formats in computerized electronic health record (EHR) repositories could potentially augment the detection of safety signals for drug products regulated by the US Food and Drug Administration (FDA). Natural language processing and other artificial intelligence (AI) techniques provide novel methodologies that could be leveraged to extract clinically useful information from EHR resources.
Objective
Our aim is to develop a novel AI-enabled software prototype to identify adverse drug event (ADE) safety signals from free-text discharge summaries in EHRs to enhance opioid drug safety and research activities at the FDA.
Methods
We developed a prototype for web-based software that leverages keyword and trigger-phrase searching with rule-based algorithms and deep learning to extract candidate ADEs for specific opioid drugs from discharge summaries in the Medical Information Mart for Intensive Care III (MIMIC III) database. The prototype uses MedSpacy components to identify relevant sections of discharge summaries and a pretrained natural language processing (NLP) model, Spark NLP for Healthcare, for named entity recognition. Fifteen FDA staff members provided feedback on the prototype’s features and functionalities.
Results
Using the prototype, we were able to identify known, labeled, opioid-related adverse drug reactions from text in EHRs. The AI-enabled model achieved accuracy, recall, precision, and F1-scores of 0.66, 0.69, 0.64, and 0.67, respectively. FDA participants assessed the prototype as highly desirable in user satisfaction, visualizations, and in the potential to support drug safety signal detection for opioid drugs from EHR data while saving time and manual effort. Actionable design recommendations included (1) enlarging the tabs and visualizations; (2) enabling more flexibility and customizations to fit end users’ individual needs; (3) providing additional instructional resources; (4) adding multiple graph export functionality; and (5) adding project summaries.
Conclusions
The novel prototype uses innovative AI-based techniques to automate searching for, extracting, and analyzing clinically useful information captured in unstructured text in EHRs. It increases efficiency in harnessing real-world data for opioid drug safety and increases the usability of the data to support regulatory review while decreasing the manual research burden.}
}
@article{MARTORELLI2024,
title = {Multiple graphical views for automatically generating SQL for the MycoDiversity DB; making fungal biodiversity studies accessible},
journal = {Biodiversity Data Journal},
volume = {12},
year = {2024},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.12.e119660},
url = {https://www.sciencedirect.com/science/article/pii/S1314283624001763},
author = {Irene Martorelli and Aram Pooryousefi and Haike {van Thiel} and Floris J Sicking and Guus J Ramackers and Vincent Merckx and Fons J Verbeek},
keywords = {fungal distribution, fungal biodiversity, biogeography, environmental DNA, mycodiversity, geospatial maps, information visualisation, dynamic hierarchical data, accessibility, reusability, database, FAIR data, controlled vocabulary terms},
abstract = {Fungi is a highly diverse group of eukaryotic organisms that live under an extremely wide range of environmental conditions. Nowadays, there is a fundamental focus on observing how biodiversity varies on different spatial scales, in addition to understanding the environmental factors which drive fungal biodiversity. Metabarcoding is a high-throughput DNA sequencing technology that has positively contributed to observing fungal communities in environments. While the DNA sequencing data generated from metabarcoding studies are available in public archives, this valuable data resource is not directly usable for fungal biodiversity investigation. Additionally, due to its fragmented storage and distributed nature, it is not immediately accessible through a single user interface. We developed the MycoDiversity DataBase User Interface (https://mycodiversity.liacs.nl) to provide direct access and retrieval of fungal data that was previously inaccessible in the public domain. The user interface provides multiple graphical views of the data components used to reveal fungal biodiversity. These components include reliable geo-location terms, the reference taxonomic scientific names associated with fungal species and the standard features describing the environment where they occur. Direct observation of the public DNA sequencing data in association with fungi is accessible through SQL search queries created by interactively manipulating topological maps and dynamic hierarchical tree views. The search results are presented in configurable data table views that can be downloaded for further use. With the MycoDiversity DataBase User Interface, we make fungal biodiversity data accessible, assisting researchers and other stakeholders in using metabarcoding studies for assessing fungal biodiversity.}
}
@article{ROOD20244520,
title = {Toward a foundation model of causal cell and tissue biology with a Perturbation Cell and Tissue Atlas},
journal = {Cell},
volume = {187},
number = {17},
pages = {4520-4545},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.07.035},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424008298},
author = {Jennifer E. Rood and Anna Hupalowska and Aviv Regev},
abstract = {Summary
Comprehensively charting the biologically causal circuits that govern the phenotypic space of human cells has often been viewed as an insurmountable challenge. However, in the last decade, a suite of interleaved experimental and computational technologies has arisen that is making this fundamental goal increasingly tractable. Pooled CRISPR-based perturbation screens with high-content molecular and/or image-based readouts are now enabling researchers to probe, map, and decipher genetically causal circuits at increasing scale. This scale is now eminently suitable for the deployment of artificial intelligence and machine learning (AI/ML) to both direct further experiments and to predict or generate information that was not—and sometimes cannot—be gathered experimentally. By combining and iterating those through experiments that are designed for inference, we now envision a Perturbation Cell Atlas as a generative causal foundation model to unify human cell biology.}
}
@incollection{GUDIVADA202527,
title = {Chapter 2 - Data analytics: fundamentals},
editor = {Mashrur Chowdhury and Kakan Dey and Amy Apon},
booktitle = {Data Analytics for Intelligent Transportation Systems (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {27-66},
year = {2025},
isbn = {978-0-443-13878-2},
doi = {https://doi.org/10.1016/B978-0-443-13878-2.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443138782000126},
author = {Venkat N. Gudivada},
keywords = {Data analytics, data science, data mining, clustering, classification, machine learning, OLAP},
abstract = {This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, and big data analytics to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open-source tools and resources for developing data analytics systems are listed. The chapter concludes by indicating emerging trends in data analytics.}
}
@incollection{FARISCO2024191,
title = {Chapter Ten - The ethical implications of indicators of consciousness in artificial systems},
editor = {Marcello Ienca and Georg Starke},
series = {Developments in Neuroethics and Bioethics},
publisher = {Academic Press},
volume = {7},
pages = {191-204},
year = {2024},
booktitle = {Brains and Machines: Towards a Unified Ethics of AI and Neuroscience},
issn = {2589-2959},
doi = {https://doi.org/10.1016/bs.dnb.2024.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S2589295924000146},
author = {Michele Farisco},
keywords = {Consciousness, Artificial consciousness, Indicators of consciousness, AI ethics, Moral status},
abstract = {The prospect of artificial consciousness raises theoretical, technical and ethical challenges which converge on the core issue of how to eventually identify and characterize it. In order to provide an answer to this question, I propose to start from a theoretical reflection about the meaning and main characteristics of consciousness. On the basis of this conceptual clarification it is then possible to think about relevant empirical indicators (i.e. features that facilitate the attribution of consciousness to the system considered) and identify key ethical implications that arise. In this chapter, I further elaborate previous work on the topic, presenting a list of candidate indicators of consciousness in artificial systems and introducing an ethical reflection about their potential implications. Specifically, I focus on two main ethical issues: the conditions for considering an artificial system as a moral subject; and the need for a non-anthropocentric approach in reflecting about the science and the ethics of artificial consciousness.}
}
@article{TURON2025100118,
title = {The path to adoption of open source AI for drug discovery in Africa},
journal = {Artificial Intelligence in the Life Sciences},
volume = {7},
pages = {100118},
year = {2025},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2024.100118},
url = {https://www.sciencedirect.com/science/article/pii/S2667318524000254},
author = {Gemma Turon and Miquel Duran-Frigola}
}
@article{EICHELBERGER2025107650,
title = {Industry 4.0/IIoT Platforms for manufacturing systems — A systematic review contrasting the scientific and the industrial side},
journal = {Information and Software Technology},
volume = {179},
pages = {107650},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107650},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002556},
author = {Holger Eichelberger and Christian Sauer and Amir Shayan Ahmadian and Christian Kröher},
keywords = {Industry 4.0, Industrial Internet of Things (IIoT), Cyber–Physical Production Systems (CPPS), Software platforms, Systematic literature review},
abstract = {Context:
IIoT, Industry 4.0 or CPPS software platforms are cornerstones of smart manufacturing production systems. Such platforms integrate machines, IIoT and edge devices, realize distributed (management) functionality and provide the basis for user-defined IIoT applications. Individual instances in research and industrial practice do share commonalities while they also differ significantly.
Objective:
A detailed overview of the platform landscape is fundamental for innovative research. However, actual surveys and literature reviews concentrate on specific aspects and usually focus only on the research works, neglecting specific aspects of the industrial use of IIoT platforms. We aim at a systematic overview of the functionalities and approaches of scientific and industrial IIoT platforms along 16 analysis dimensions and thereby exposing gaps between the focuses of research on IIoT platforms and actual industrial IIoT platforms in use. By doing so we are able to highlight future areas of interest to research as well as indicating potentially over-researched areas which are of less interest in actual industrial IIoT platforms.
Method:
We combine a systematic literature review of scientific IIoT platform research with a systematic analysis of industrial IIoT platforms.
Results:
We start off with 1620 research papers plus 70 from snowballing that we systematically filter down to 36 papers (plus 11 added by a SLR update) providing sufficient information for a data extraction, which we analyze along 16 topics to extract actual capabilities and differences of relevant platform approaches. In a second step, we contrast these results with an analysis of 21 industrial platforms.
Conclusion:
Similar approaches, differences and topics for future are exhibited. In comparison with 21 industrial platforms along the same analysis topics, we distill various commonalities, differences, trends and gaps.}
}
@article{CHAKRABORTY2024100164,
title = {From machine learning to deep learning: Advances of the recent data-driven paradigm shift in medicine and healthcare},
journal = {Current Research in Biotechnology},
volume = {7},
pages = {100164},
year = {2024},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2023.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590262823000461},
author = {Chiranjib Chakraborty and Manojit Bhattacharya and Soumen Pal and Sang-Soo Lee},
keywords = {Deep learning, Machine learning, Artificial intelligence, Medicine and health care},
abstract = {The medicine and healthcare sector has been evolving and advancing very fast. The advancement has been initiated and shaped by the applications of data-driven, robust, and efficient machine learning (ML) to deep learning (DL) technologies. ML in the medical sector is developing quickly, causing rapid progress, reshaping medicine, and improving clinician and patient experiences. ML technologies evolved into data-hungry DL approaches, which are more robust and efficient in dealing with medical data. This article reviews some critical data-driven aspects of machine intelligence in the medical field. In this direction, the article illustrated the recent progress of data-driven medical science using ML to DL in two categories: firstly, the recent development of data science in medicine with the use of ML to DL and, secondly, the chabot technologies in healthcare and medicine, particularly on ChatGPT. Here, we discuss the progress of ML, DL, and the transition requirements from ML to DL. To discuss the advancement in data science, we illustrate prospective studies of medical image data, newly evolved DL interpretation data from EMR or EHR, big data in personalized medicine, and dataset shifts in artificial intelligence (AI). Simultaneously, the article illustrated recently developed DL-enabled ChatGPT technology. Finally, we summarize the broad role of ML and DL in medicine and the significant challenges for implementing recent ML to DL technologies in healthcare. The overview of the data-driven paradigm shift in medicine using ML to DL technologies in the article will benefit researchers immensely.}
}
@article{DANESHFAR2024109288,
title = {Image captioning by diffusion models: A survey},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109288},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109288},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624014465},
author = {Fatemeh Daneshfar and Ako Bartani and Pardis Lotfi},
keywords = {Image captioning, Diffusion models, Image-to-text, Survey, Implemented artificial intelligence, Application of artificial intelligence},
abstract = {Diffusion models are increasingly favored over traditional approaches like generative adversarial networks (GANs) and auto-regressive transformers due to their remarkable generative capabilities. They demonstrate outstanding performance not solely limited to image generation and manipulation but also in text-related tasks. Despite this, existing surveys tend to concentrate on the utilization of diffusion models solely for image generation, ignoring their potential in image captioning. To address this oversight, our paper provides an exhaustive examination of image-to-text diffusion models within the landscape of artificial intelligence (AI) and generative computing, filling a critical void in the literature. Starting with an overview of basic diffusion model principles, we explore into the enhancements brought by conditioning or guidance and the implemented AI. We then present a taxonomy and review of cutting-edge methods in diffusion-based image captioning. Additionally, we explore applications beyond image-to-text generation, such as image-guided creative generation, text editing, and the application of AI. We also cover existing evaluation metrics, software and libraries, as well as challenges and future directions in the field.}
}
@article{PADMANABHANPOTI2024100086,
title = {Enabling affordances for AI Governance},
journal = {Journal of Responsible Technology},
volume = {18},
pages = {100086},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100086},
url = {https://www.sciencedirect.com/science/article/pii/S266665962400012X},
author = {Siri {Padmanabhan Poti} and Christopher J Stanton},
keywords = {Governance, Explainability, Interpretability, Assurance, Technical debt, Shift-left, Behavior tree},
abstract = {Organizations dealing with mission-critical AI based autonomous systems may need to provide continuous risk management controls and establish means for their governance. To achieve this, organizations are required to embed trustworthiness and transparency in these systems, with human overseeing and accountability. Autonomous systems gain trustworthiness, transparency, quality, and maintainability through the assurance of outcomes, explanations of behavior, and interpretations of intent. However, technical, commercial, and market challenges during the software development lifecycle (SDLC) of autonomous systems can lead to compromises in their quality, maintainability, interpretability and explainability. This paper conceptually models transformation of SDLC to enable affordances for assurance, explanations, interpretations, and overall governance in autonomous systems. We argue that opportunities for transformation of SDLC are available through concerted interventions such as technical debt management, shift-left approach and non-ephemeral artifacts. This paper contributes to the theory and practice of governance of autonomous systems, and in building trustworthiness incrementally and hierarchically.}
}
@article{BADAMI2023102231,
title = {Adaptive search query generation and refinement in systematic literature review},
journal = {Information Systems},
volume = {117},
pages = {102231},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102231},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923000674},
author = {Maisie Badami and Boualem Benatallah and Marcos Baez},
keywords = {Systematic reviews, Query enrichment, Query adaptation, Reinforcement learning, Word embedding},
abstract = {Systematic literature reviews (SLRs) are a central part of evidence-based research, which involves collecting and integrating empirical evidence on specific research questions. A key step in this process is building Boolean search queries, which are at the core of information retrieval systems that support literature search. This involves turning general research aims into specific search terms that can be combined into complex Boolean expressions. Researchers must build and refine search queries to ensure they have sufficient coverage and properly represent the literature. In this paper, we propose an adaptive query generation and refinement pipeline for SLR search that uses reinforcement learning to learn the optimal modifications to a query based on feedback from researchers about its performance. Empirical evaluations with 10 SLR datasets showed our approach achieves comparable performance to queries manually composed by SLR authors. We also investigate the impact of design decisions on the performance of the query generation and refinement pipeline. Specifically, we study the effects of the type of input seed, the use of general versus domain-specific word embedding models, the sampling strategy for relevance feedback, and number of iterations in the refinement process. Our results provide insights into the effects of these choices on the pipeline’s performance.}
}
@article{PAL2023100247,
title = {Automated vision-based construction progress monitoring in built environment through digital twin},
journal = {Developments in the Built Environment},
volume = {16},
pages = {100247},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100247},
url = {https://www.sciencedirect.com/science/article/pii/S2666165923001291},
author = {Aritra Pal and Jacob J. Lin and Shang-Hsien Hsieh and Mani Golparvar-Fard},
abstract = {Effective progress monitoring is ineviTable for completing the construction of building and infrastructure projects successfully. In this digital transformation era, with the data-centric management and control approach, the effectiveness of monitoring methods is expected to improve dramatically. ”Digital Twin,” which creates a bidirectional communication flow between a physical entity and its digital counterpart, is found to be a crucial enabling technology for information-aware decision-making systems in manufacturing and other automotive industries. Recognizing the benefits of this technology in production management in construction, researchers have proposed Digital Twin Construction (DTC). DTC leverages building information modeling technology and processes, lean construction practices, on-site digital data collection mechanisms, and Artificial Intelligence (AI) based data analytics for improving construction production planning and control processes. Progress monitoring, a key component in construction production planning and control, can significantly benefit from DTC. However, some knowledge gaps still need to be filled for the practical implementation of DTC for progress monitoring in the built environment domain. This research reviews the existing vision-based progress monitoring methods, studies the evolution of automated vision-based construction progress monitoring research, and highlights the methodological and technological knowledge gaps that must be addressed for DTC-based predictive progress monitoring. Subsequently, it proposes a framework for closed-loop construction control through DTC. Finally, the way forward for fully automated, real-time construction progress monitoring built upon the DTC concept is proposed.}
}
@article{SCHWOERER2024102936,
title = {‘International, intersectional and interdisciplinary’ – Gender and feminist studies degree descriptions and logics of representation in marketised English higher education},
journal = {Women's Studies International Forum},
volume = {105},
pages = {102936},
year = {2024},
issn = {0277-5395},
doi = {https://doi.org/10.1016/j.wsif.2024.102936},
url = {https://www.sciencedirect.com/science/article/pii/S0277539524000748},
author = {Lili Schwoerer},
keywords = {Gender studies, Commodification, Marketisation, Neoliberal university, Difference, Discourse analysis},
abstract = {This article explores how the academic field of gender and feminist studies in England represents itself, by drawing on a discourse analysis of online descriptions from websites of all gender and feminist studies degree programmes and departments in English universities, all but one of which are graduate degrees. Foregrounding the context of the neoliberal university, in which feminist and gender knowledge is simultaneously marginalised and mainstreamed, the article asks how representations of the field are shaped by the marketisation of higher education. This analysis reveals a disjuncture between two representative logics: while most feminist, gender studies and queer scholarship relies on anti-essentialist epistemologies and ontologies, the dominant logic of representation in contemporary universities understands difference as static and representable. This representability enables and is in turn facilitated by marketisation.}
}
@article{BEKAMIRI2024123536,
title = {PatentSBERTa: A deep NLP based hybrid model for patent distance and classification using augmented SBERT},
journal = {Technological Forecasting and Social Change},
volume = {206},
pages = {123536},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123536},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524003329},
author = {Hamid Bekamiri and Daniel S. Hain and Roman Jurowetzki},
keywords = {Technological distance, Patent classification, Deep NLP, Augmented SBERT, Hybrid model, Model explainability},
abstract = {This study presents an efficient approach for utilizing text data to calculate patent-to-patent (p2p) technological similarity and proposes a hybrid framework for leveraging the resulting p2p similarity in applications such as semantic search and automated patent classification. To achieve this, we create embeddings using Sentence-BERT (SBERT) on patent claims. For domain adaptation of the general SBERT model, we implement an augmented approach to fine-tune SBERT using in-domain supervised patent claims data. The study utilizes SBERT's efficiency in creating embedding distance measures to map p2p similarity in large sets of patent data. We demonstrate applications of the framework for the use case of automated patent classification with a simple K Nearest Neighbors (KNN) model that predicts assigned Cooperative Patent Classification (CPC) based on the class assignment of the K patents with the highest p2p similarity. The results show that p2p similarity captures technological features in terms of CPC overlap, and the approach is useful for automatic patent classification based on text data. Moreover, the presented classification framework is simple, and the results are easy to interpret and evaluate by end-users via instance-based explanations. The study performs an out-of-sample model validation, predicting all assigned CPC classes on the subclass (663) level with an F1 score of 66 %, outperforming the current state-of-the-art in text-based multi-label patent classification. The study also discusses the applicability of the presented framework for semantic intellectual property (IP) search, patent landscaping, and technology mapping. Finally, the study outlines a future research agenda to leverage multi-source patent embeddings, evaluate their appropriateness across applications, and improve and validate patent embeddings by creating domain-expert curated Semantic Textual Similarity (STS) benchmark datasets.}
}
@incollection{2024285,
title = {Index},
editor = {Shufei Li and Pai Zheng and Lihui Wang},
booktitle = {Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing},
publisher = {Elsevier},
pages = {285-293},
year = {2024},
isbn = {978-0-443-13943-7},
doi = {https://doi.org/10.1016/B978-0-44-313943-7.00018-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139437000181}
}
@article{DANGSAWANG2024100408,
title = {A machine learning approach for detecting customs fraud through unstructured data analysis in social media},
journal = {Decision Analytics Journal},
volume = {10},
pages = {100408},
year = {2024},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2024.100408},
url = {https://www.sciencedirect.com/science/article/pii/S2772662224000122},
author = {Bundidth Dangsawang and Siranee Nuchitprasitchai},
keywords = {Logistic Regression, Long short-term memory, Gated Recurrent Unit, Unstructured data, Customs duties, Commercial goods},
abstract = {Goods and services are sold through social media by individuals not authorized as legitimate dealers, resulting in lost taxes and customs duties to governments. This study proposes a model called SHIELD for detecting these violations through unstructured data in social media. The process involves collecting 2,373,570 records of commercial goods from social media platforms such as Twitter and Facebook in three phases. In Phase 1, keywords for labeling are collected for text classification. Three categories of results are defined: Red Line for smuggled goods, unpaid duty, prohibited goods, and restricted goods; Green Line for non-commercial goods; and Inspect for goods that cannot be identified from the text and require further investigation. Phase 2 and Phase 3 use keywords to detect smugglers from unstructured social media data for labeling grouped by three algorithms of Logistic Regression (LR), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM), employed to classify imported illegal products. The results of all tests show that the LSTM technique had the best accuracy of 99.44% and the best average F1 score of 90.55%. Using algorithms and techniques such as LR, GRU, and LSTM demonstrates the potential of machine learning and natural language processing in detecting illegal activities and promoting economic security.}
}
@article{YANG2024103823,
title = {Span-level bidirectional retention scheme for aspect sentiment triplet extraction},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103823},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103823},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001821},
author = {Xuan Yang and Tao Peng and Haijia Bi and Jiayu Han},
keywords = {Sentiment parsing, Triplet extraction, Relation detection, Semantic transfer},
abstract = {The objective of the Aspect Sentiment Triplet Extraction (ASTE) task is to identify triplets of (aspect, opinion, sentiment) from user-generated reviews. The current study does not extensively integrate the interaction between word pairs and aspect-opinion pairs during the learning process at the granularity of sentence analysis. Furthermore, the bidirectional inference for the triplet, along with the parallel computing approach for long-span texts, also fail to achieve efficient unification. We introduce a new perspective: Span-level Bidirectional Retention Scheme(SBRS) for Aspect Sentiment Triplet Extraction model. The model comprises two pathways. The first pathway involves extracting effective aspect-opinion pair outcomes via two progressive submodules that operate on words and word pairs at varying scales. Building on the first pathway, the second pathway senses the interaction information of word pairs through bidirectional recursion and combines an efficient parallel computing approach. This combination allows the model to utilize three features – context, semantics, and relationship – to accurately identify the sentimental orientation. Thus, the two pathways facilitate the learning of relation-aware representations of word pairs. We carried out experiments on two public datasets, showing an average enhancement of 3.34% and 1.72% in F1 scores compared to the most recent baselines models, and multiple experiments from diverse angles proved the model’s superiority.}
}
@incollection{2023323,
title = {Index},
editor = {Tung-Hung Su and Jia-Horng Kao},
booktitle = {Artificial Intelligence, Machine Learning, and Deep Learning in Precision Medicine in Liver Diseases},
publisher = {Academic Press},
pages = {323-333},
year = {2023},
isbn = {978-0-323-99136-0},
doi = {https://doi.org/10.1016/B978-0-323-99136-0.20001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991360200019}
}
@article{GETULI2025105897,
title = {Parametric design methodology for developing BIM object libraries in construction site modeling},
journal = {Automation in Construction},
volume = {170},
pages = {105897},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105897},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006332},
author = {Vito Getuli and Alessandro Bruttini and Farzad Rahimian},
keywords = {BIM, Object library, Construction site, Parametric design, Modeling, planning, And product information},
abstract = {The adoption of Building Information Modeling (BIM) in construction site layout planning and activity scheduling faces challenges due to the lack of standardized approaches for digitally reproducing and organizing site elements that meet information requirements of diverse regulatory frameworks and stakeholders' use cases. This paper addresses the question of how to streamline the development of BIM objects for construction site modeling by proposing a vendor-neutral parametric design methodology and introduces a dedicated hierarchical structure for BIM object libraries to support users in their implementation. The methodology includes a six-step process for creating informative content, parametric geometries, and documentation, and is demonstrated through the development and implementation of a construction site BIM object library suitable for the Italian context. This approach fills a gap in BIM object development standards and offers a foundation for future research, benefiting practitioners and industry stakeholders involved in BIM-based site layout modeling and activity planning.}
}
@incollection{SCARCELLO2024,
title = {Artificial Intelligence},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00109-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001093},
author = {Francesco Scarcello and Simona Nisticò and Luigi Palopoli},
keywords = {Agents, Artificial intelligence, Knowledge representation, Logic, Machine learning, Natural language processing, Planning, Reasoning, Robotics, Vision},
abstract = {Artificial Intelligence is going to support every human activity, from communication to healthcare, business, entertaining, and so forth. It is a very active field of research with an uncountable spectrum of applications. A picture of the field is provided, with a brief overview of history, recent achievements, and directions to detailed information on the various facets of AI.}
}
@article{BRANNSTROM2025109325,
title = {Goal-hiding information-seeking dialogues: A formal framework},
journal = {International Journal of Approximate Reasoning},
volume = {177},
pages = {109325},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109325},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24002123},
author = {Andreas Brännström and Virginia Dignum and Juan Carlos Nieves},
keywords = {Formal dialogues, Quantitative argumentation, Information extraction, Human-agent interaction, Theory of mind},
abstract = {We consider a type of information-seeking dialogue between a seeker agent and a respondent agent, where the seeker estimates the respondent to not be willing to share a particular set of sought-after information. Hence, the seeker postpones (hides) its goal topic, related to the respondent's sensitive information, until the respondent is perceived as willing to talk about it. In the intermediate process, the seeker opens other topics to steer the dialogue tactfully towards the goal. Such dialogue strategies, which we refer to as goal-hiding strategies, are common in diverse contexts such as criminal interrogations and medical assessments, involving sensitive topics. Conversely, in malicious online interactions like social media extortion, similar strategies might aim to manipulate individuals into revealing information or agreeing to unfavorable terms. This paper proposes a formal dialogue framework for understanding goal-hiding strategies. The dialogue framework uses Quantitative Bipolar Argumentation Frameworks (QBAFs) to assign willingness scores to topics. An initial willingness for each topic is modified by considering how topics promote (support) or demote (attack) other topics. We introduce a method to identify relations among topics by considering a respondent's shared information. Finally, we introduce a gradual semantics to estimate changes in willingness as new topics are opened. Our formal analysis and empirical evaluation show the system's compliance with privacy-preserving safety properties. A formal understanding of goal-hiding strategies opens up a range of practical applications; For instance, a seeker agent may plan with goal-hiding to enhance privacy in human-agent interactions. Similarly, an observer agent (third-party) may be designed to enhance social media security by detecting goal-hiding strategies employed by users' interlocutors.}
}
@article{JOVEJUNCA2024101043,
title = {Genomic architecture of carcass and pork traits and their association with immune capacity},
journal = {animal},
volume = {18},
number = {1},
pages = {101043},
year = {2024},
issn = {1751-7311},
doi = {https://doi.org/10.1016/j.animal.2023.101043},
url = {https://www.sciencedirect.com/science/article/pii/S1751731123003609},
author = {T. Jové-Juncà and D. Crespo-Piazuelo and O. González-Rodríguez and M. Pascual and C. Hernández-Banqué and J. Reixach and R. Quintanilla and M. Ballester},
keywords = {Carcass quality, Genetic correlations, Immunity, pH, Pig},
abstract = {Carcass and pork traits have traditionally been considered of prime importance in pig breeding programmes. However, the changing conditions in modern farming, coupled with antimicrobial resistance issues, are raising the importance of health and robustness-related traits. Here, we explore the genetic architecture of carcass and pork traits and their relationship with immunity phenotypes in a commercial Duroc pig population. A total of nine traits related to fatness, lean content and meat pH were measured at slaughter (∼190 d of age) in 378 pigs previously phenotyped (∼70 d of age) for 36 immunity-related traits, including plasma concentrations of immunoglobulins, acute-phase proteins, leukocytes subpopulations and phagocytosis. Our study showed medium to high heritabilities and strong genetic correlations between fatness, lean content and meat pH at 24 h postmortem. Genetic correlations were found between carcass and pork traits and white blood cells. pH showed strong positive genetic correlations with leukocytes and eosinophils, and strong negative genetic correlations with haemoglobin, haematocrit and cytotoxic T cell proportion. In addition, genome-wide association studies (GWASs) pointed out four significantly associated genomic regions for lean meat percentages in different muscles, ham fat, backfat thickness, and semimembranosus pH at 24 h. The functional annotation of genes located in these regions reported a total of 14 candidate genes, with BGN, DPP10, LEPR, LEPROT, PDE4B and SLC6A8 being the strongest candidates. After performing an expression GWAS for the expression of these genes in muscle, two signals were detected in cis for the BGN and SLC6A8 genes. Our results indicate a genetic relationship between carcass fatness, lean content and meat pH with a variety of immunity-related traits that should be considered to improve immunocompetence without impairing production traits.}
}
@article{BIMPAS2024110156,
title = {Leveraging pervasive computing for ambient intelligence: A survey on recent advancements, applications and open challenges},
journal = {Computer Networks},
volume = {239},
pages = {110156},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110156},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623006011},
author = {Athanasios Bimpas and John Violos and Aris Leivadeas and Iraklis Varlamis},
keywords = {Artificial intelligence, Ubiquitous computing, Smart environments, Context awareness, Internet of Things, Data mining},
abstract = {The advent of pervasive computing and ambient intelligence has opened up new possibilities for the development of intelligent systems that can adapt to the needs of their users. This paper provides a comprehensive survey of recent advancements in this field, focusing on the different techniques, tools, and applications that have been developed to leverage the benefits of pervasive computing for ambient intelligence. We discuss the various challenges and opportunities that arise when designing such systems and provide a critical evaluation of the current state-of-the-art. In particular, we examine the role of machine learning, data analytics, and context awareness in the development of intelligent systems, and we investigate the impact of these technologies on the quality of life of their users. The paper also presents a discussion on the open challenges in this field, such as ensuring user privacy, security, and trust, designing effective user interfaces, and dealing with the complexity of large-scale systems. Finally, we conclude with an outlook on the future of ambient intelligence and its potential impact on society, and we highlight some of the emerging research directions that are likely to shape the field in the coming years.}
}
@incollection{YOSHINORI2024161,
title = {Chapter Eight - Prediction of mitochondrial targeting signals and their cleavage sites},
editor = {Nils Wiedemann},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {706},
pages = {161-192},
year = {2024},
booktitle = {Mitochondrial Translocases Part A},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2024.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0076687924003628},
author = {Fukasawa Yoshinori and Kenichiro Imai and Paul Horton},
abstract = {In this chapter we survey prediction tools and computational methods for the prediction of amino acid sequence elements which target proteins to the mitochondria. We will primarily focus on the prediction of N-terminal mitochondrial targeting signals (MTSs) and their N-terminal cleavage sites by mitochondrial peptidases. We first give practical details useful for using and installing some prediction tools. Then we describe procedures for preparing datasets of MTS containing proteins for statistical analysis or development of new prediction methods. Following that we lightly survey some of the computational techniques used by prediction tools. Finally, after discussing some caveats regarding the reliability of such methods to predict the effects of mutations on MTS function; we close with a discussion of possible future directions of computer prediction methods related to mitochondrial proteins.}
}
@article{OHSAWA20244843,
title = {Semantic Cells: Evolutional process for item sense disambiguation},
journal = {Procedia Computer Science},
volume = {246},
pages = {4843-4852},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.350},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023755},
author = {Yukio Ohsawa and Dingming Xue and Kaira Sekiguchi},
keywords = {evolutionary computing, word-sense disambiguation, item-sense disambiguation, bio-inspired computation},
abstract = {Previous models for learning the semantic vectors of items and their groups, such as words, sentences, nodes, and graphs, using distributed representation have been based on the assumption that the basic sense of an item corresponds to one vector composed of dimensions corresponding to hidden contexts in the target real world, from which multiple senses of the item are obtained by conforming to lexical databases or adapting to the context. However, there may be multiple senses of an item, which are hardly assimilated and change or evolve dynamically following the contextual shift even within a document or a restricted period. This is a process similar to the evolution or adaptation of a living entity with/to environmental shifts. Setting the scope of disambiguation of items for sensemaking, the author presents a method in which a word or item in the data embraces multiple semantic vectors that evolve via interaction with others, similar to a cell embracing chromosomes crossing over with each other. We obtained a preliminary result: the role of a word that evolves to acquire the largest or lower-middle variance of semantic vectors tends to be explainable by the author of the text.}
}
@article{CHAKRABARTY2024498,
title = {Imaging Analytics using Artificial Intelligence in Oncology: A Comprehensive Review},
journal = {Clinical Oncology},
volume = {36},
number = {8},
pages = {498-513},
year = {2024},
note = {Advances in Imaging for Oncology},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2023.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0936655523003345},
author = {N. Chakrabarty and A. Mahajan},
keywords = {Artificial intelligence, cancer, deep learning, diagnosis-genomic mutations-outcome prediction},
abstract = {The present era has seen a surge in artificial intelligence-related research in oncology, mainly using deep learning, because of powerful computer hardware, improved algorithms and the availability of large amounts of data from open-source domains and the use of transfer learning. Here we discuss the multifaceted role of deep learning in cancer care, ranging from risk stratification, the screening and diagnosis of cancer, to the prediction of genomic mutations, treatment response and survival outcome prediction, through the use of convolutional neural networks. Another role of artificial intelligence is in the generation of automated radiology reports, which is a boon in high-volume centres to minimise report turnaround time. Although a validated and deployable deep-learning model for clinical use is still in its infancy, there is ongoing research to overcome the barriers for its universal implementation and we also delve into this aspect. We also briefly describe the role of radiomics in oncoimaging. Artificial intelligence can provide answers pertaining to cancer management at baseline imaging, saving cost and time. Imaging biobanks, which are repositories of anonymised images, are also briefly described. We also discuss the commercialisation and ethical issues pertaining to artificial intelligence. The latest generation generalist artificial intelligence model is also briefly described at the end of the article. We believe this article will not only enrich knowledge, but also promote research acumen in the minds of readers to take oncoimaging to another level using artificial intelligence and also work towards clinical translation of such research.}
}
@article{2025107047,
title = {List of Editorial Board Members},
journal = {Neural Networks},
volume = {181},
pages = {107047},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107047},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024009766}
}
@article{BERNDT2024622,
title = {A Platform Architecture for Data- and AI-supported human-centred Zero Defect Manufacturing for Sustainable Production⁎⁎The ZERO³ project is funded as a lead project as part of the “Production of the Future – 43rd Call” program of the Austrian Research Promotion Agency (FFG) and the Federal Ministry for Climate Protection, Environment, Energy, Mobility, Innovation and Technology (BMK). – (FFG No.: FO999896399).},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {622-627},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.231},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324016598},
author = {René Berndt and Doriana Cobârzan and Eva Eggeling},
keywords = {Zero defect manufacturing, Sustainable production, Circular economy, Knowledge Management, Decision Support System},
abstract = {The challenge for manufacturing companies lies in efficiently adapting to economic, ecological, and social sustainability while maintaining a competitive edge in the global market, despite the rapid advancements in information, communication, and resource-efficient production processes utilizing robotics and AI-based methods. The key question is: How can knowledge regarding feasibility and implementation possibilities be effectively transferred? In our paper, we propose a platform architecture designed to facilitate knowledge exchange and transfer between technology providers and the manufacturing industry. The platform facilitates the dissemination of innovative methods and technologies, improving collaboration and operational efficiency It will also serve as a repository for insights and experiences from technology implementation, making this knowledge accessible for internal and industry-wide use. The goal is to create a sustainable ecosystem for continuous improvement and competitive advantage in manufacturing, by matching technologies and methodologies to specific production needs using algorithms and tracking sustainability metrics.}
}
@article{FIELDS2025256,
title = {Thoughts and thinkers: On the complementarity between objects and processes},
journal = {Physics of Life Reviews},
volume = {52},
pages = {256-273},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2025.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571064525000089},
author = {Chris Fields and Michael Levin},
keywords = {Active inference, Cognitive light cone, Emergence, Evo/devo/eco, Multiscale competency architecture, Niche construction, Semantics},
abstract = {We argue that “processes versus objects” is not a useful dichotomy. There is, instead, substantial theoretical utility in viewing “objects” and “processes” as complementary ways of describing persistence through time, and hence the possibility of observation and manipulation. This way of thinking highlights the role of memory as an essential resource for observation, and makes it clear that “memory” and “time” are also mutually inter-defined, complementary concepts. We formulate our approach in terms of the Free Energy Principle (FEP) of Friston and colleagues and the fundamental idea from quantum theory that physical interactions can be represented by linear operators. Following Levin (2024) [30], we emphasize that memory is, first and foremost, an interpretative function, from which the idea of memory as a record, at some level of accuracy, of past events is derivative. We conclude that the distinction between objects and processes is always contrived, and always misleading, and that science would be better served by abandoning it entirely.}
}
@article{COLTHER2024100625,
title = {Artificial intelligence: Driving force in the evolution of human knowledge},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100625},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100625},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001641},
author = {Cristian Colther and Jean Pierre Doussoulin},
keywords = {Artificial intelligence, Evolution knowledge, Noosphere, Ethical considerations, Future scenarios},
abstract = {This article proposes that artificial intelligence (AI) is positioned as a key driver of a new evolutionary stage of human knowledge, complementing human intelligence and facilitating the creation and development of sophisticated collective intelligence, defined as the noosphere, understood as the sphere of collective human thought. The study reveals several key insights into the transformative potential of AI, including its capacity to accelerate, mediate, and diffuse human knowledge. It concludes that AI not only catalyzes the existence of the noosphere but also redefines the structures and mechanisms through which human knowledge is expanded and democratized. Additionally, the document presents potential risks and significant ethical, social, and legal challenges of an AI-mediated noosphere, offering recommendations and a research agenda around the topic, and limitations and proposals for improvement to be considered in the future.}
}
@incollection{2024275,
title = {Index},
editor = {Shai Ben-David and Giuseppe Curigliano and David Koff and Barbara Alicja Jereczek-Fossa and Davide {La Torre} and Gabriella Pravettoni},
booktitle = {Artificial Intelligence for Medicine},
publisher = {Academic Press},
pages = {275-281},
year = {2024},
series = {Advanced Studies in Complex Systems: Theory and Applications},
isbn = {978-0-443-13671-9},
doi = {https://doi.org/10.1016/B978-0-443-13671-9.09992-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443136719099920}
}
@article{ZUO2024124884,
title = {Collaborative trajectory representation for enhanced next POI recommendation},
journal = {Expert Systems with Applications},
volume = {256},
pages = {124884},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124884},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424017512},
author = {Jiankai Zuo and Yaying Zhang},
keywords = {Next POI recommendation, Trajectory similarity, Attention mechanism, Representation learning},
abstract = {Point-of-Interest (POI) recommendation stands as the cornerstone within a variety of location-based applications and services that intend to anticipate upcoming movements that users may be interested in. The current state-of-the-art methods have effectively explored spatio-temporal contextual features and users’ long-term and short-term preference patterns. Nevertheless, most existing work lacks the ability to effectively capture group movement patterns from trajectory collaboration. Additionally, they pay close attention to the accuracy of personalized recommendations, neglecting recommendation diversity, which refers to offering broader location options that extend beyond a user’s typical preferences, thereby avoiding excessive homogeneity or repetition. To address these gaps, this study proposes a Collaborative Trajectory Representation model (CTRNext), which enhances the diversity of recommendations while maintaining the precision of personalized preferences. To be specific, we first design two trajectory embedding layers to extract joint semantic interactions and the explicit spatiotemporal context-aware representation. Then, a trajectory semantic similarity calculation module that captures collaborative signals from potentially similar-minded users and eliminates barriers caused by trajectory length is proposed. Next, the implicit correlation and further updated representation between different check-in records are achieved through a multi-head self-attention aggregation module. Finally, we put forward a dual-driven user preference matching module to generate the preference-based next POI recommendation while enhancing diversity. Our approach demonstrates its remarkable recommendation accuracy through extensive experimentation on four real-world datasets, surpassing the performance of state-of-the-art methodologies.}
}
@article{HIMEUR2025102742,
title = {Applications of knowledge distillation in remote sensing: A survey},
journal = {Information Fusion},
volume = {115},
pages = {102742},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102742},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005207},
author = {Yassine Himeur and Nour Aburaed and Omar Elharrouss and Iraklis Varlamis and Shadi Atalla and Wathiq Mansoor and Hussain Al-Ahmad},
keywords = {Knowledge distillation, Model compression, Model and data distillation, Remote sensing, Urban planning and precision agriculture},
abstract = {With the ever-growing complexity of models in the field of remote sensing (RS), there is an increasing demand for solutions that balance model accuracy with computational efficiency. Knowledge distillation (KD) has emerged as a powerful tool to meet this need, enabling the transfer of knowledge from large, complex models to smaller, more efficient ones without significant loss in performance. This review article provides an extensive examination of KD and its innovative applications in RS. KD, a technique developed to transfer knowledge from a complex, often cumbersome model (teacher) to a more compact and efficient model (student), has seen significant evolution and application across various domains. Initially, we introduce the fundamental concepts and historical progression of KD methods. The advantages of employing KD are highlighted, particularly in terms of model compression, enhanced computational efficiency, and improved performance, which are pivotal for practical deployments in RS scenarios. The article provides a comprehensive taxonomy of KD techniques, where each category is critically analyzed to demonstrate the breadth and depth of the alternative options, and illustrates specific case studies that showcase the practical implementation of KD methods in RS tasks, such as instance segmentation and object detection. Further, the review discusses the challenges and limitations of KD in RS, including practical constraints and prospective future directions, providing a comprehensive overview for researchers and practitioners in the field of RS. Through this organization, the paper not only elucidates the current state of research in KD but also sets the stage for future research opportunities, thereby contributing significantly to both academic research and real-world applications.}
}
@article{NOREEN2024109003,
title = {Mono-lingual text reuse detection for the Urdu language at lexical level},
journal = {Engineering Applications of Artificial Intelligence},
volume = {136},
pages = {109003},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624011618},
author = {Ayesha Noreen and Iqra Muneer and Rao Muhammad Adeel Nawab},
keywords = {Text reuse detection, Urdu text reuse detection, Derived, Non-derived, Lexical level},
abstract = {Text reuse is the process of creating new texts from pre-existing ones. In recent years, Urdu Text Reuse Detection (U-TRD) has garnered the attention of researchers due to the ready availability of digital text all over the internet, which can be copied or paraphrased from other sources without proper attribution, making it easier to reuse but challenging to detect. Previous studies have explored the issue of U-TRD at the phrasal, sentence/passage, and document levels, using benchmark corpora and methods. However, the problem of U-TRD has not been investigated at the lexical level in terms of corpora and methods. To address this research gap, our study has developed a large benchmark corpus manually annotated at the lexical level. This corpus consists of 22,184 text pairs categorized into two levels of rewrite: (1) Derived (8,660) and (2) Non-Derived (13,524). Additionally, our research has involved the development, application, evaluation, and comparison of a range of methods, including baseline methods (uni-gram overlap and word embedding-based methods), along with state-of-the-art transformer-based methods and feature-fusion-based methods, using the proposed UTRD-Lex-23 corpus. Our study concludes that one of our proposed feature-fusion methods outperforms all other methods. The model we propose, which combines seven different Sentence Transformers (ST) (each producing 768 dimension vectors) with one uni-gram (at word level) and sixteen different features extracted from four different Word Embedding (WE) based models (yielding 300 dimension vectors), achieves an F1 score of 0.70601 using 10-fold cross validation. To foster and promote research in Urdu (a low-resourced language) proposed corpus will be freely and publicly available for research purposes.}
}
@article{ATMAKURU2025102673,
title = {Artificial intelligence-based suicide prevention and prediction: A systematic review (2019–2023)},
journal = {Information Fusion},
volume = {114},
pages = {102673},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102673},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004512},
author = {Anirudh Atmakuru and Alen Shahini and Subrata Chakraborty and Silvia Seoni and Massimo Salvi and Abdul Hafeez-Baig and Sadaf Rashid and Ru San Tan and Prabal Datta Barua and Filippo Molinari and U Rajendra Acharya},
keywords = {Artificial intelligence, Natural language processing, Machine learning, Mental health, Suicide prevention, Suicide prediction},
abstract = {Suicide is a major global public health concern, and the application of artificial intelligence (AI) methods, such as natural language processing (NLP), machine learning (ML), and deep learning (DL), has shown promise in advancing suicide prediction and prevention efforts. Recent advancements in AI – particularly NLP and DL have opened up new avenues of research in suicide prediction and prevention. While several papers have reviewed specific detection techniques like NLP or DL, there has been no recent study that acts as a one-stop-shop, providing a comprehensive overview of all AI-based studies in this field. In this work, we conduct a systematic literature review to identify relevant studies published between 2019 and 2023, resulting in the inclusion of 156 studies. We provide a comprehensive overview of the current state of research conducted on AI-driven suicide prevention and prediction, focusing on different data types and AI techniques employed. We discuss the benefits and challenges of these approaches and propose future research directions to improve the practical application of AI in suicide research. AI is highly capable of improving the accuracy and efficiency of risk assessment, enabling personalized interventions, and enhancing our understanding of risk and protective factors. Multidisciplinary approaches combining diverse data sources and AI methods can help identify individuals at risk by analyzing social media content, patient histories, and data from mobile devices, enabling timely intervention. However, challenges related to data privacy, algorithmic bias, model interpretability, and real-world implementation must be addressed to realize the full potential of these technologies. Future research should focus on integrating prediction and prevention strategies, harnessing multimodal data, and expanding the scope to include diverse populations. Collaboration across disciplines and stakeholders is essential to ensure that AI-driven suicide prevention and prediction efforts are ethical, culturally sensitive, and person-centered.}
}
@article{MUSTAFA2024101450,
title = {Using social Cognitive theory to reengage dormant users in question and answer Communities: A case study of active StackOverflow participants},
journal = {Electronic Commerce Research and Applications},
volume = {68},
pages = {101450},
year = {2024},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2024.101450},
url = {https://www.sciencedirect.com/science/article/pii/S1567422324000954},
author = {Sohaib Mustafa and Wen Zhang and Muhammad {Mateen Naveed} and Dur e Adan},
keywords = {Low participation, Active participants, Dormant users, Peer recognition, Q&A communities},
abstract = {Online question-and-answer communities are seriously threatened by low user participation. There is currently a rare comprehensive study on the knowledge contribution pattern of consistently active participants and the moderating role of peer recognition, which can help improve low participation and reengage inactive users, despite researchers having examined the various facets of knowledge contribution and made helpful suggestions. As per the self-determination and social cognitive theory, the communal environment impacts peers and imitates role models or reliable sources in their involvement patterns. We have examined StackOverflow’s most reliable active users from 2010 to 2020 using the social cognition and self-determination theories to use the findings to reactivate dormant users. We have used a two-step dynamic system GMM model to get robust and reliable findings. The research discovered that peer repudiation, reputation, and online social interactions favorably affect the contributed knowledge. However, knowledge-seeking and earning virtual badges such as gold and bronze usually negatively impact it. Furthermore, it was revealed that the effect of virtual badges on contributed knowledge was positively moderated by peer recognition. However, peer recognition reduces the benefits of social interaction and reputation on the contributed knowledge. The study’s findings advance the body of knowledge and provide thorough management implications for raising low participation, reengaging inactive users, and cultivating a culture of innovative sharing of knowledge.}
}
@article{ZHANG2024102413,
title = {A survey of route recommendations: Methods, applications, and opportunities},
journal = {Information Fusion},
volume = {108},
pages = {102413},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102413},
url = {https://www.sciencedirect.com/science/article/pii/S156625352400191X},
author = {Shiming Zhang and Zhipeng Luo and Li Yang and Fei Teng and Tianrui Li},
keywords = {Urban computing, Route recommendation, Spatio-temporal learning, Deep learning},
abstract = {Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens’ travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: (1) Methodology-wise. We categorize a large volume of classic methods and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. (2) Application-wise. We present numerous novel applications related to route commendation within urban computing scenarios. (3) We discuss current problems and challenges and envision several promising research directions. We believe that this survey can help relevant researchers quickly familiarize themselves with the current state of route recommendation research and then direct them to future research trends.}
}
@article{ANANIKOV2024100075,
title = {Top 20 influential AI-based technologies in chemistry},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100075},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100075},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000332},
author = {Valentine P. Ananikov},
keywords = {Chemical science, Artificial Intelligence, Digital technologies, Research methodology, Machine Learning, Chemical industry, Digital twins, Blockchain, Large data},
abstract = {The beginning and ripening of digital chemistry is analyzed focusing on the role of artificial intelligence (AI) in an expected leap in chemical sciences to bring this area to the next evolutionary level. The analytic description selects and highlights the top 20 AI-based technologies and 7 broader themes that are reshaping the field. It underscores the integration of digital tools such as machine learning, big data, digital twins, the Internet of Things (IoT), robotic platforms, smart control of chemical processes, virtual reality and blockchain, among many others, in enhancing research methods, educational approaches, and industrial practices in chemistry. The significance of this study lies in its focused overview of how these digital innovations foster a more efficient, sustainable, and innovative future in chemical sciences. This article not only illustrates the transformative impact of these technologies but also draws new pathways in chemistry, offering a broad appeal to researchers, educators, and industry professionals to embrace these advancements for addressing contemporary challenges in the field.}
}
@article{FENZA2024127951,
title = {Robustness of models addressing Information Disorder: A comprehensive review and benchmarking study},
journal = {Neurocomputing},
volume = {596},
pages = {127951},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127951},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224007227},
author = {Giuseppe Fenza and Vincenzo Loia and Claudio Stanzione and Maria {Di Gisi}},
keywords = {Information Disorder, Explainable artificial intelligence, Adversarial attacks, Robustness},
abstract = {Machine learning and deep learning models are increasingly susceptible to adversarial attacks, particularly in critical areas like cybersecurity and Information Disorder. This study provides a comprehensive evaluation of model Robustness against such attacks across key tasks well-assessed in Information Disorder literature: Toxic Speech Detection, Sentiment Analysis, Propaganda Detection, and Hate Speech Detection. Rigorous experiments conducted across 13 models and 12 diverse datasets highlight significant vulnerabilities. The methodological framework implements adversarial attacks that strategically manipulates model inputs based on keyword significance, identified using the LIME method, an advanced explainable AI technique. The evaluation measures Robustness primarily through accuracy of the models and attack success rates. The experiments reveal that current models display inconsistent resistance to adversarial manipulations, underscoring an urgent need for developing more sophisticated defensive strategies. The study sheds light on the critical weaknesses in existing models and charts a course for future research to fortify AI resilience against evolving cyber threats. The findings advocate for a paradigm shift in model training and development to prioritize adversarial Robustness, ensuring that AI systems are equipped to handle real-world adversarial scenarios effectively.}
}
@incollection{2024552,
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {552-598},
year = {2024},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.09001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895090015}
}
@article{LEE2023100174,
title = {High-level implementable methods for automated building code compliance checking},
journal = {Developments in the Built Environment},
volume = {15},
pages = {100174},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100174},
url = {https://www.sciencedirect.com/science/article/pii/S266616592300056X},
author = {Jin-Kook Lee and Kyunghyun Cho and Hyeokjin Choi and Soohyung Choi and Sumin Kim and Seung Hyun Cha},
keywords = {High-level method, Implementable method, Automated building code compliance checking, Building permit, Building information modeling (BIM)},
abstract = {This paper presents an approach for defining high-level implementable methods to improve their low-level rule-checking procedures. This is part of an effort to develop challenging building information modeling (BIM)-enabled applications for building permits through automated code compliance checking. The main approach described here aims to alleviate the time-consuming and error-prone tasks of translating natural language into explicitly defined rules. An explicit expression of design requirements is key to building projects involving collaboration between architects, code experts, and developers. To maximize the generalization, neutralization, and reusability of the given rules in natural (written) language, we propose a series of high-level implementable computer programming methods (operators); these can be beneficial for translating verb phrases in building act sentences with minimal ambiguity as well as representing the peculiar properties of building objects without conflicts. Lastly, we demonstrate its application in code compliance checking by employing KBimCode and the developed rule-checking software.}
}
@incollection{ANJUM2025349,
title = {Chapter 19 - Artificial intelligence and deep learning in single-cell omics data analysis: A case study},
editor = {Khalid Raza},
booktitle = {Deep Learning in Genetics and Genomics},
publisher = {Academic Press},
pages = {349-383},
year = {2025},
isbn = {978-0-443-27523-4},
doi = {https://doi.org/10.1016/B978-0-443-27523-4.00007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044327523400007X},
author = {Zubina Anjum and Waniya Khalid and Gurupriya Takkar and Pakhi Chhetri and Khalid Raza},
keywords = {Biological data analysis, Multiomics, Next-generation sequencing, Single cell},
abstract = {The advent of single-cell genomics has led to a new era of biological exploration, allowing researchers to investigate the intricacies of cellular heterogeneity deeply. The ability of artificial intelligence (AI)-driven models to unveil hidden patterns in single-cell expression data, providing the classification of cell types based on gene expression profiles. Deep learning methods in single-cell analysis offer a comprehensive view of cellular biology. This chapter explores the significant role of AI and deep learning methodologies in the analysis and interpretation of single-cell genomic data. Various deep learning models along with their roles in single-cell analysis are discussed. Furthermore, the chapter highlights the role of AI and deep learning in connecting complex model outputs with biological understanding, reshaping our knowledge of cellular complexity, and driving groundbreaking discoveries in disease biology and personalized medicine. Furthermore, challenges and future research directions have been discussed.}
}
@article{MICHALOWSKI2024104681,
title = {Provision and evaluation of explanations within an automated planning-based approach to solving the multimorbidity problem},
journal = {Journal of Biomedical Informatics},
volume = {156},
pages = {104681},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104681},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000996},
author = {Martin Michalowski and Szymon Wilk and Wojtek Michalowski and Malvika Rao and Marc Carrier},
keywords = {Explainability, Automated planning, Multimorbidity, Clinical decision support},
abstract = {The multimorbidity problem involves the identification and mitigation of adverse interactions that occur when multiple computer interpretable guidelines are applied concurrently to develop a treatment plan for a patient diagnosed with multiple diseases. Solving this problem requires decision support approaches which are difficult to comprehend for physicians. As such, the rationale for treatment plans generated by these approaches needs to be provided.
Objective:
To develop an explainability component for an automated planning-based approach to the multimorbidity problem, and to assess the fidelity and interpretability of generated explanations using a clinical case study.
Methods:
The explainability component leverages the task-network model for representing computer interpretable guidelines. It generates post-hoc explanations composed of three aspects that answer why specific clinical actions are in a treatment plan, why specific revisions were applied, and how factors like medication cost, patient’s adherence, etc. influence the selection of specific actions. The explainability component is implemented as part of MitPlan, where we revised our planning-based approach to support explainability. We developed an evaluation instrument based on the system causability scale and other vetted surveys to evaluate the fidelity and interpretability of its explanations using a two dimensional comparison study design.
Results:
The explainability component was implemented for MitPlan and tested in the context of a clinical case study. The fidelity and interpretability of the generated explanations were assessed using a physician-focused evaluation study involving 21 participants from two different specialties and two levels of experience. Results show that explanations provided by the explainability component in MitPlan are of acceptable fidelity and interpretability, and that the clinical justification of the actions in a treatment plan is important to physicians.
Conclusion:
We created an explainability component that enriches an automated planning-based approach to solving the multimorbidity problem with meaningful explanations for actions in a treatment plan. This component relies on the task-network model to represent computer interpretable guidelines and as such can be ported to other approaches that also use the task-network model representation. Our evaluation study demonstrated that explanations that support a physician’s understanding of the clinical reasons for the actions in a treatment plan are useful and important.}
}
@article{BERGER2024106003,
title = {Towards reusable building blocks for agent-based modelling and theory development},
journal = {Environmental Modelling & Software},
volume = {175},
pages = {106003},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106003},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224000641},
author = {Uta Berger and Andrew Bell and C. Michael Barton and Emile Chappin and Gunnar Dreßler and Tatiana Filatova and Thibault Fronville and Allen Lee and Emiel {van Loon} and Iris Lorscheid and Matthias Meyer and Birgit Müller and Cyril Piou and Viktoriia Radchuk and Nicholas Roxburgh and Lennart Schüler and Christian Troost and Nanda Wijermans and Tim G. Williams and Marie-Christin Wimmler and Volker Grimm},
keywords = {Individual-based modelling, Theory development, Complex adaptive systems, Software engineering, Best practices},
abstract = {Despite the increasing use of standards for documenting and testing agent-based models (ABMs) and sharing of open access code, most ABMs are still developed from scratch. This is not only inefficient, but also leads to ad hoc and often inconsistent implementations of the same theories in computational code and delays progress in the exploration of the functioning of complex social-ecological systems (SES). We argue that reusable building blocks (RBBs) known from professional software development can mitigate these issues. An RBB is a submodel that represents a particular mechanism or process that is relevant across many ABMs in an application domain, such as plant competition in vegetation models, or reinforcement learning in a behavioural model. RBBs need to be distinguished from modules, which represent entire subsystems and include more than one mechanism and process. While linking modules faces the same challenges as integrating different models in general, RBBs are “atomic” enough to be more easily re-used in different contexts. We describe and provide examples from different domains for how and why building blocks are used in software development, and the benefits of doing so for the ABM community and to individual modellers. We propose a template to guide the development and publication of RBBs and provide example RBBs that use this template. Most importantly, we propose and initiate a strategy for community-based development, sharing and use of RBBs. Individual modellers can have a much greater impact in their field with an RBB than with a single paper, while the community will benefit from increased coherence, facilitating the development of theory for both the behaviour of agents and the systems they form. We invite peers to upload and share their RBBs via our website - preferably referenced by a DOI (digital object identifier obtained e.g. via Zenodo). After a critical mass of candidate RBBs has accumulated, feedback and discussion can take place and both the template and the scope of the envisioned platform can be improved.}
}
@incollection{SUN20241,
title = {Chapter 1 - History of graph computing and graph databases},
editor = {Ricky Sun},
booktitle = {The Essential Criteria of Graph Databases},
publisher = {Elsevier},
pages = {1-32},
year = {2024},
isbn = {978-0-443-14162-1},
doi = {https://doi.org/10.1016/B978-0-443-14162-1.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443141621000040},
author = {Ricky Sun},
keywords = {Big data, Graph thinking, Graph database, Graph computing, Knowledge graph, Network analysis},
abstract = {This chapter introduces the core concept throughout this book—graph thinking. Concrete and visualized real-world examples were given in the first section to facilitate the readers to understand the depth and breadth of the concept, and how to put it to work. The first section is completed with a review of the historical development of graph theory and technologies. The second section gives an overview of how data processing technologies and frameworks have evolved from relational databases to big-data frameworks and eventually to graph databases, and insights into their differences. The final section focuses on introducing the amazing and unprecedented capabilities of graph databases, again, with real-world practical examples. This section ended with a comparison of graph computing and graph databases, hoping to clarify any potential confusion between the two topics.}
}
@article{SHENG2024569,
title = {Artificial intelligence for diabetes care: current and future prospects},
journal = {The Lancet Diabetes & Endocrinology},
volume = {12},
number = {8},
pages = {569-595},
year = {2024},
issn = {2213-8587},
doi = {https://doi.org/10.1016/S2213-8587(24)00154-2},
url = {https://www.sciencedirect.com/science/article/pii/S2213858724001542},
author = {Bin Sheng and Krithi Pushpanathan and Zhouyu Guan and Quan Hziung Lim and Zhi Wei Lim and Samantha Min Er Yew and Jocelyn Hui Lin Goh and Yong Mong Bee and Charumathi Sabanayagam and Nick Sevdalis and Cynthia Ciwei Lim and Chwee Teck Lim and Jonathan Shaw and Weiping Jia and Elif Ilhan Ekinci and Rafael Simó and Lee-Ling Lim and Huating Li and Yih-Chung Tham},
abstract = {Summary
Artificial intelligence (AI) use in diabetes care is increasingly being explored to personalise care for people with diabetes and adapt treatments for complex presentations. However, the rapid advancement of AI also introduces challenges such as potential biases, ethical considerations, and implementation challenges in ensuring that its deployment is equitable. Ensuring inclusive and ethical developments of AI technology can empower both health-care providers and people with diabetes in managing the condition. In this Review, we explore and summarise the current and future prospects of AI across the diabetes care continuum, from enhancing screening and diagnosis to optimising treatment and predicting and managing complications.}
}
@article{RAJ2024201309,
title = {Classify Alzheimer genes association using Naïve Bayes algorithm},
journal = {Human Gene},
volume = {41},
pages = {201309},
year = {2024},
issn = {2773-0441},
doi = {https://doi.org/10.1016/j.humgen.2024.201309},
url = {https://www.sciencedirect.com/science/article/pii/S2773044124000536},
author = {Sushrutha Raj and Anchal Vishnoi and Alok Srivastava},
keywords = {Disease gene associations, Alzheimer's candidate genes, Machine learning, Text mining, Text classification, Cross validation},
abstract = {Background
Alzheimer's disease, the most common form of dementia, accounts for 60–80% of cases and its prevalence is projected to increase as aging populations grow. By 2050, the number of individuals with Alzheimer's and dementia worldwide is expected to reach 152 million. Genetics plays a significant role, contributing to about 70% of the overall risk, underscoring the importance of understanding the genetic basis for developing targeted interventions. This study presents a system that combines text mining and machine learning techniques to identify and prioritize prospective candidate genes for Alzheimer's and further classifies them into three association classes with weights.
Methods
The machine learning-based classifier was trained over a meticulously curated gold standard dataset and then rigorously validated utilizing a 10-fold cross-validation method, demonstrating its consistency across all the folds of the data. This developed ensemble learning system categorizes PubMed abstracts into three distinct groups: Yes, No, and Ambiguous using text mining and a Bayesian classification algorithm. The system further predicts disease-gene associations over unknown disease-specific prediction data by using the developed classifier.
Results
With an average accuracy of 87.33% and confidence level of 90.10% +/− 0.142, the protocol effectively extracted 2031 associated genes, of which 1162, 489 and 1439 belong to positive, negative and ambiguous classes respectively at the threshold of 0.9. In comparison between the established disease gene databases, our system identified 915 positive genes that had not been previously reported. One can use these positive genes for in-depth understanding and ambiguous genes for further exploration of their association with Alzheimer's disease.
Conclusions
The system's ability to generate accurate predictions demonstrates its robustness and provides valuable insights into the genetic factors of Alzheimer's disease. Consequently, this study contributes to existing knowledge and paves the way for future research in this field.}
}
@article{QIAN2024104453,
title = {Evaluating resilience of urban lifelines against flooding in China using social media data},
journal = {International Journal of Disaster Risk Reduction},
volume = {106},
pages = {104453},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104453},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924002152},
author = {Jiale Qian and Yunyan Du and Fuyuan Liang and Jiawei Yi and Nan Wang and Wenna Tu and Sheng Huang and Tao Pei and Ting Ma and Keith Burghardt and Kristina Lerman},
keywords = {Early warning, Flood, Resilience, Rumor spreading, Social media, Urban lifelines},
abstract = {Urban lifelines are the backbone of fundamental services that require stability to enable all other aspects of society to function during natural hazards. However, few studies have focused on measuring lifeline performance and resilience to hazards, especially from the public perspective. In this study, taking flood as the research object, we developed an enhanced urban lifelines classification scheme that integrates the Federal Emergency Management Agency (FEMA)'s lifeline framework with the co-occurrence relationships of keywords derived from 430 million Weibo posts. Leveraging this framework, we formulated two key indicators: Public Concern Ratio and Public Emotion Ratio, to evaluate the resilience of urban lifelines during the 2017 and 2020 flood in China. The results demonstrate the robust resilience of urban lifelines against flooding, with notable improvements over time. The study also identifies certain vulnerable lifelines, notably in the ability of early warning and control of rumor spreading, which often lead to an increase in social media posts expressing negative emotions during flooding. These areas are pinpointed for necessary enhancements. Employing a data-driven methodology, the study provides a novel and insightful approach to assessing urban lifeline resilience against flooding.}
}
@incollection{2025247,
title = {Index},
editor = {Himanshu Arora},
booktitle = {Artificial Intelligence in Urologic Malignancies},
publisher = {Academic Press},
pages = {247-258},
year = {2025},
isbn = {978-0-443-15504-8},
doi = {https://doi.org/10.1016/B978-0-443-15504-8.00014-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443155048000144}
}
@article{WU2025107203,
title = {Multi-knowledge informed deep learning model for multi-point prediction of Alzheimer’s disease progression},
journal = {Neural Networks},
volume = {185},
pages = {107203},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107203},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025000826},
author = {Kai Wu and Hong Wang and Feiyan Feng and Tianyu Liu and Yanshen Sun},
keywords = {Alzheimer’s disease (AD) progression, Multi-point prediction, Multi-knowledge informed, Dual path feature extraction, Visual feature},
abstract = {The diagnosis of Alzheimer’s disease (AD) based on visual features-informed by clinical knowledge has achieved excellent results. Our study endeavors to present an innovative and detailed deep learning framework designed to accurately predict the progression of Alzheimer’s disease. We propose Mul-KMPP, a Multi-Knowledge Informed Deep Learning Model for Multi-Point Prediction of AD progression, intended to facilitate precise assessments of AD progression in older adults. Firstly, we designed a dual-path methodology to capture global and local brain characteristics for visual feature extraction (utilizing MRIs). Then, we developed a diagnostic module before the prediction module, leveraging AAL (Anatomical Automatic Labeling) knowledge. Following this, predictions are informed by clinical insights. For this purpose, we devised a new composite loss function, including diagnosis loss, prediction loss, and consistency loss of the two modules. To validate our model, we compiled a dataset comprising 819 samples and the results demonstrate that our Mul-KMPP model achieved an accuracy of 86.8%, sensitivity of 86.1%, specificity of 92.1%, and area under the curve (AUC) of 95.9%, significantly outperforming several competing diagnostic methods at every time point. The source code for our model is available at https://github.com/Camelus-to/Mul-KMPP.}
}
@article{DEASY2024379,
title = {Data Science Opportunities To Improve Radiotherapy Planning and Clinical Decision Making},
journal = {Seminars in Radiation Oncology},
volume = {34},
number = {4},
pages = {379-394},
year = {2024},
note = {Future of Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2024.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053429624000638},
author = {Joseph O. Deasy},
abstract = {Radiotherapy aims to achieve a high tumor control probability while minimizing damage to normal tissues. Personalizing radiotherapy treatments for individual patients, therefore, depends on integrating physical treatment planning with predictive models of tumor control and normal tissue complications. Predictive models could be improved using a wide range of rich data sources, including tumor and normal tissue genomics, radiomics, and dosiomics. Deep learning will drive improvements in classifying normal tissue tolerance, predicting intra-treatment tumor changes, tracking accumulated dose distributions, and quantifying the tumor response to radiotherapy based on imaging. Mechanistic patient-specific computer simulations (‘digital twins’) could also be used to guide adaptive radiotherapy. Overall, we are entering an era where improved modeling methods will allow the use of newly available data sources to better guide radiotherapy treatments.}
}
@article{BARBEROAPARICIO2024102035,
title = {Addressing data scarcity in protein fitness landscape analysis: A study on semi-supervised and deep transfer learning techniques},
journal = {Information Fusion},
volume = {102},
pages = {102035},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102035},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003512},
author = {José A. Barbero-Aparicio and Alicia Olivares-Gil and Juan J. Rodríguez and César García-Osorio and José F. Díez-Pastor},
keywords = {Bioinformatics, Machine learning, Transfer learning, Semi-supervised learning, Protein fitness prediction, Small datasets},
abstract = {This paper presents a comprehensive analysis of deep transfer learning methods, supervised methods, and semi-supervised methods in the context of protein fitness prediction, with a focus on small datasets. The analysis includes the exploration of the combination of different data sources to enhance the performance of the models. While deep learning and deep transfer learning methods have shown remarkable performance in situations with abundant data, this study aims to address the more realistic scenario faced by wet lab researchers, where labeled data is often limited. The novelty of this work lies in its examination of deep transfer learning in the context of small datasets and its consideration of semi-supervised methods and multi-view strategies. While previous research has extensively explored deep transfer learning in large dataset scenarios, little attention has been given to its efficacy in small dataset settings or its comparison with semi-supervised approaches. Our findings suggest that deep transfer learning, exemplified by ProteinBERT, shows promising performance in this context compared to the rest of the methods across various evaluation metrics, not only in small dataset contexts but also in large dataset scenarios. This highlights the robustness and versatility of deep transfer learning in protein fitness prediction tasks, even with limited labeled data. The results of this study shed light on the potential of deep transfer learning as a state-of-the-art approach in the field of protein fitness prediction. By leveraging pre-trained models and fine-tuning them on small datasets, researchers can achieve competitive performance surpassing traditional supervised and semi-supervised methods. These findings provide valuable insights for wet lab researchers who face the challenge of limited labeled data, enabling them to make informed decisions when selecting the most effective methodology for their specific protein fitness prediction tasks. Additionally, the study investigated the combination of two different sources of information (encodings) through our enhanced semi-supervised methods, yielding noteworthy results improving their base model and providing valuable insights for further research. The presented analysis contributes to a better understanding of the capabilities and limitations of different learning approaches in small dataset scenarios, ultimately aiding in the development of improved protein fitness prediction methods.}
}
@article{LIU2024102592,
title = {A review of digital twin capabilities, technologies, and applications based on the maturity model},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102592},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102592},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002404},
author = {Yang Liu and Jun Feng and Jiamin Lu and Siyuan Zhou},
keywords = {Digital twin, DT, Maturity model, Digital twin capabilities, Technologies and applications},
abstract = {The advanced stage of Industry 4.0 is characterized by the integration and interaction between physical and virtual spaces, and Digital Twin (DT) technology, congruent with this vision, has garnered extensive attention and has undergone large-scale implementation. Yet, in the practical implementation of Digital Twin projects, several issues persist: ① How to formulate reasonable task objectives and action plans before project implementation? ② how to determine and assess the development level of the digital twin during project implementation? ③ How to evaluate the effectiveness of digital twin after project completion and how to enhance improvement in the next steps? Consequently, a methodological model is urgently needed to evaluate the development process of Digital Twins, offering a benchmark for their design, development, and appraisal. To address these issues, this paper introduces a five-level Digital Twin Maturity Model (DTMM), which systematically aligns DT capabilities, phased objectives, and technical requirements within a unified framework, creating a theoretical system capable of assessing DT’s developmental level and specifying its construction trajectory. Further, this paper catalogs supporting tools aligned with the technical specifications stipulated in DTMM’s functional capabilities, aiding developers in devising implementation strategies. Additionally, it scrutinizes the application status across six DT vertical sectors, conducts maturity evaluations, and confirms the efficacy of the proposed model. The conclusion can be drawn that DT is still in its embryonic phase. This work aspires to assist project managers and public policymakers gain a more objective understanding of Digital Twin, offering references to facilitate their positive development and broader implementation.}
}
@incollection{2025299,
title = {Index},
editor = {Miltiadis D. Lytras and Abdulrahman Housawi and Basim S. Alsaywid and Naif Radi Aljohani},
booktitle = {Next Generation eHealth},
publisher = {Academic Press},
pages = {299-305},
year = {2025},
series = {Next Generation Technology Driven Personalized Medicine And Smart Healthcare},
isbn = {978-0-443-13619-1},
doi = {https://doi.org/10.1016/B978-0-443-13619-1.20001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443136191200019}
}
@article{LEE2024108219,
title = {GraphRec-based Korean expert recommendation using author contribution index and the paper abstracts in marine},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108219},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108219},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003774},
author = {Jeong-Wook Lee and Jae-Hoon Kim},
keywords = {Recommendation system, Marine expert recommendation system, Graph neural networks},
abstract = {Expert recommendation systems recommend specialized experts in a particular field to users based on the knowledge of those experts. However, these systems are limited by the number of experts available and the potential for subjective evaluation, which may result in inappropriate recommendations. Furthermore, we explore the evolution from traditional to deep learning-based recommendation systems, emphasizing graph-based recommendation systems. Nonetheless, deep learning-based systems require large amounts of data, and marine expert recommendation training data are scarce. To address these issues, we constructed and utilized marine expert data in this study. The dataset contains abstracts of marine-related papers and information on their authors. Graphs were generated by assessing the similarity among the abstracts, representing them in a graph format indicative of this similarity, and using the author contribution index to depict the relationship between the abstracts and their respective authors. Various similarity methods and abstract embedding techniques were experimentally explored to realize performance optimization. In the experiments, the optimized model achieved a mean absolute error of 0.7556 and a root-mean-squared error of 1.0421. Notably, this study highlights the limitations of traditional evaluation metrics and proposes the averaged mean reciprocal rank as a suitable alternative. This metric facilitates the quantitative evaluation of model performance on newly created data, obviating a comparison model. Finally, applying the newly constructed data to the GraphRec model by using their graphical representation significantly improves the system performance.}
}
@article{CEN2024102032,
title = {Towards interpretable imaging genomics analysis: Methodological developments and applications},
journal = {Information Fusion},
volume = {102},
pages = {102032},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102032},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003482},
author = {Xiaoping Cen and Wei Dong and Wei Lv and Yi Zhao and Fred Dubee and Alexios-Fotios A. Mentis and Dragomirka Jovic and Huanming Yang and Yixue Li},
keywords = {Interpretability, Imaging genomics, Genotype-phenotype association, Precision medicine},
abstract = {Identifying the relationship between imaging features and genetic variation (a term coined“imaging genomics”) offers valuable insight into the pathogenesis of cancer, as well as cognitive and psychiatric disorders. However, how to integrate -omics and imaging data in a biologically meaningful approach to both provide pathobiological underpinning and achieve clinical applications remain challenging. In this review, we aim to discuss the difficulties in combining different sources of data and explaining the genotype-phenotype association, and we attempt to discuss the potential applications of imaging genomics in several (patho)physiological questions to better understand human physiology and disease. Future efforts on deciphering the genotype-phenotype landscapes by interpretable imaging genomics analyses are needed to provide new insights into molecular biology and molecular medicine.}
}
@article{HENRIQUE2024100043,
title = {Trust in artificial intelligence: Literature review and main path analysis},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100043},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100043},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000033},
author = {Bruno Miranda Henrique and Eugene Santos},
keywords = {Artificial intelligence, Trust, Trust calibration, Literature review, Main path analysis},
abstract = {Artificial Intelligence (AI) is present in various modern systems, but it is still subject to acceptance in many fields. Medical diagnosis, autonomous driving cars, recommender systems and robotics are examples of areas in which some humans distrust AI technology, which ultimately leads to low acceptance rates. Conversely, those same applications can have humans who over rely on AI, acting as recommended by the systems with no criticism regarding the risks of a wrong decision. Therefore, there is an optimal balance with respect to trust in AI, achieved by calibration of expectations and capabilities. In this context, the literature about factors influencing trust in AI and its calibration is scattered among research fields, with no objective summaries of the overall evolution of the theme. In order to close this gap, this paper contributes a literature review of the most influential papers on the subject of trust in AI, selected by quantitative methods. It also proposes a Main Path Analysis of the literature, highlighting how the theme has evolved over the years. As results, researchers will find an overview on trust in AI based on the most important papers objectively selected and also tendencies and opportunities for future research.}
}
@article{MOSQUERA2024107492,
title = {Understanding the landscape of software modelling assistants for MDSE tools: A systematic mapping},
journal = {Information and Software Technology},
volume = {173},
pages = {107492},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107492},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000971},
author = {David Mosquera and Marcela Ruiz and Oscar Pastor and Jürgen Spielberger},
keywords = {Modelling assistance, Model-driven development, Systematic mapping, State of the practice, Low code, No-code},
abstract = {Context
Model Driven Software Engineering (MDSE) and low-code/no-code software development tools promise to increase quality and productivity by modelling instead of coding software. One of the major advantages of modelling software is the increased possibility of involving diverse stakeholders since it removes the barrier of being IT experts to actively participate in software production processes. From an academic and industry point of view, the main question remains: What has been proposed to assist humans in software modelling tasks?
Objective
In this paper, we systematically elucidate the state of the art in assistants for software modelling and their use in MDSE and low-code/no-code tools.
Method
We conducted a systematic mapping to review the state of the art and answer the following research questions: i) how is software modelling assisted? ii) what goals and limitations do existing modelling assistance proposals report? iii) which evaluation metrics and target users do existing modelling assistance proposals consider? For this purpose, we selected 58 proposals from 3.176 screened records and reviewed 17 MDSE and low-code/no-code tools from main market players published by the Gartner Magic Quadrant.
Result
We clustered existing proposals regarding their modelling assistance strategies, goals, limitations, evaluation metrics, and target users, both in research and practice.
Conclusions
We found that both academic and industry proposals recognise the value of assisting software modelling. However, documentation about MDSE assistants’ limitations, evaluation metrics, and target users is scarce or non-existent. With the advent of artificial intelligence, we expect more assistants for MDSE and low-code/no-code software development will emerge, making imperative the need for well-founded frameworks for designing modelling assistants focused on addressing target users’ needs and advancing the state of the art.}
}
@article{SICILIANI2023102284,
title = {AI-based decision support system for public procurement},
journal = {Information Systems},
volume = {119},
pages = {102284},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102284},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923001205},
author = {Lucia Siciliani and Vincenzo Taccardi and Pierpaolo Basile and Marco {Di Ciano} and Pasquale Lops},
keywords = {E-procurement, Data analysis, Data visualisation, Natural language processing, Semantic search, Decision support systems},
abstract = {Tenders are powerful means of investment of public funds and represent a strategic development resource. Thus, improving the efficiency of procuring entities and developing evaluation models turn out to be essential to facilitate e-procurement procedures. With this contribution, we introduce our research to create a supporting system for the decision-making and monitoring process during the entire course of investments and contracts. This system employs artificial intelligence techniques based on natural language processing, focused on providing instruments for extracting useful information from both structured and unstructured (i.e., text) data. Therefore, we developed a framework based on a web app that provides integrated tools such as a semantic search engine, a summariser, an open information extraction engine in the form of triples (subject–predicate–object) for tender documents, and dashboards for analysing tender data.}
}
@article{KHALID2024102344,
title = {Repairing raw metadata for metadata management},
journal = {Information Systems},
volume = {122},
pages = {102344},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102344},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924000024},
author = {Hiba Khalid and Esteban Zimányi},
keywords = {Data preparation, Metadata management, Metadata representation, Metadata categorization},
abstract = {With the exponential growth of data production, the generation of metadata has become an integral part of the process. Metadata plays a crucial role in facilitating enhanced data analytics, data integration, and resource management by offering valuable insights. However, inconsistencies arise due to deviations from standards in metadata recording, including missing attribute information, publishing URLs, and provenance. Furthermore, the recorded metadata may exhibit inconsistencies, such as varied value formats, special characters, and inaccurately entered values. Addressing these inconsistencies through metadata preparation can greatly enhance the user experience during data management tasks. This paper introduces MDPrep, a system that explores the usability and applicability of data preparation techniques in improving metadata quality. Our approach involves three steps: (1) detecting and identifying problematic metadata elements and structural issues, (2) employing a keyword-based approach to enhance metadata elements and a syntax-based approach to rectify structural metadata issues, and (3) comparing the outcomes to ensure improved readability and reusability of prepared metadata files.}
}
@article{VELDHUIS2025100708,
title = {Critical Artificial Intelligence literacy: A scoping review and framework synthesis},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100708},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000771},
author = {Annemiek Veldhuis and Priscilla Y. Lo and Sadhbh Kenny and Alissa N. Antle},
keywords = {Artificial intelligence, Critical literacy, AI ethics, AI literacy, Computational empowerment, Literature review},
abstract = {The proliferation of Artificial Intelligence (AI) in everyday life raises concerns for children, other marginalized groups, and the general public. As new AI implementations continue to emerge, it is crucial to enable children to engage critically with AI. Critical literacy objectives and practices can encourage children to question, critique, and transform the social, political, cultural, and ethical implications of AI. As an initial step towards critical AI education, we conducted a 10-year scoping review to identify publications reporting on activities that engage children, between the ages of 5 and 18, to address the critical implications of AI. Our review identifies a wide range of participants, content, and pedagogical approaches. Through framework synthesis guided by an established critical literacy model, we examine the critical literacy learning objectives embedded in the reported activities and propose a critical AI literacy framework. This paper outlines future opportunities for critical AI literacies in the field of child–computer interaction including inspiring new learning activities, encouraging inclusive perspectives, and supporting pragmatic curriculum integration.}
}
@article{DENG2025104132,
title = {Edge-featured multi-hop attention graph neural network for intrusion detection system},
journal = {Computers & Security},
volume = {148},
pages = {104132},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104132},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004371},
author = {Ping Deng and Yong Huang},
keywords = {Multi-hop attention, Graph neural networks, Intrusion detection, Internet of Things},
abstract = {With the development of the Internet, the application of computer technology has rapidly become widespread, driving the progress of Internet of Things (IoT) technology. The attacks present on networks have become more complex and stealthy. However, traditional network intrusion detection systems with singular functions are no longer sufficient to meet current demands. While some machine learning-based network intrusion detection systems have emerged, traditional machine learning methods cannot effectively respond to the complex and dynamic nature of network attacks. Intrusion detection systems utilizing deep learning can better enhance detection capabilities through diverse data learning and training. To capture the topological relationships in network data, using graph neural networks (GNNs) is most suitable. Most existing GNNs for intrusion detection use multi-layer network training, which may lead to over-smoothing issues. Additionally, current intrusion detection solutions often lack efficiency. To mitigate the issues mentioned above, this paper proposes an Edge-featured Multi-hop Attention Graph Neural Network for Intrusion Detection System (EMA-IDS), aiming to improve detection performance by capturing more features from data flows. Our method enhances computational efficiency through attention propagation and integrates node and edge features, fully leveraging data characteristics. We carried out experiments on four public datasets, which are NF-CSE-CIC-IDS2018-v2, NF-UNSW-NB15-v2, NF-BoT-IoT, and NF-ToN-IoT. Compared with existing models, our method demonstrated superior performance.}
}
@article{TROUMPOUKIS2024505,
title = {European AI and EO convergence via a novel community-driven framework for data-intensive innovation},
journal = {Future Generation Computer Systems},
volume = {160},
pages = {505-521},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24003133},
author = {Antonis Troumpoukis and Iraklis Klampanos and Despina-Athanasia Pantazi and Mohanad Albughdadi and Vasileios Baousis and Omar Barrilero and Alexandra Bojor and Pedro Branco and Lorenzo Bruzzone and Andreina Chietera and Philippe Fournand and Richard Hall and Michele Lazzarini and Adrian Luna and Alexandros Nousias and Christos Perentis and George Petrakis and Dharmen Punjani and David Röbl and George Stamoulis and Eleni Tsalapati and Indrė Urbanavičiūtė and Giulio Weikmann and Xenia Ziouvelou and Marcin Ziółkowski and Manolis Koubarakis and Vangelis Karkaletsis},
keywords = {Artificial Intelligence, Earth observation, DIAS, Applications, Case-study, Methodology, Platform},
abstract = {Artificial Intelligence (AI) represents a collection of tools and methodologies that have the potential to revolutionise various aspects of human activity. Earth observation (EO) data, including satellite and in-situ, are essential in a number of high impact applications, ranging from security and energy to agriculture and health. In this paper, we present the AI4Copernicus framework for bridging the two domains within the European context to enable data-centred innovation. In order to achieve this goal, AI4Copernicus has developed and enriches the European AI-on-demand platform with a number of application bootstrapping services and tools to accelerate uptake and innovation, whilst it provides integration over AI-on-Demand services and the Copernicus ecosystem, targeting the highly successful Data and Information Access Service (DIAS) Cloud platforms. More specifically, by employing procedures for onboarding and validating models and tools, and by utilising a host of meticulously reviewed and supervised open calls-enabled projects, and containerisation best-practices, AI4Copernicus deployed and made available several products on DIAS platforms. Moreover, these products and resources have been made available on the AI-on-Demand platform catalogue for discovery, use and further development. The AI4Copernicus framework is being used by a number of business-driven projects and SMEs spanning several application domains. This article provides an overview of the European AI and EO context as well as the AI4Copernicus technological framework and tools offered. Further, we present real world use-cases as well as a community-centred evaluation of our framework based on usage and feedback received from several projects.}
}
@article{ZHANG2025111920,
title = {M3NetFlow: A multi-scale multi-hop graph AI model for integrative multi-omic data analysis},
journal = {iScience},
pages = {111920},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111920},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225001804},
author = {Heming Zhang and S. Peter Goedegebuure and Li Ding and David DeNardo and Ryan C. Fields and Michael Province and Yixin Chen and Philip Payne and Fuhai Li},
abstract = {Summary
Multi-omic data-driven studies are at the forefront of precision medicine by characterizing complex disease signaling systems across multiple views and levels. The integration and interpretation of multi-omic data are critical for identifying disease targets and deciphering disease signaling pathways. However, it remains an open problem due to the complex signaling interactions among many proteins. Herein, we propose a Multi-scale Multi-hop Multi-omic Network Flow model, M3NetFlow, to facilitate both hypothesis-guided and generic multi-omic data analysis tasks. We evaluated M3NetFlow using two independent case studies: 1) uncovering mechanisms of synergy of drug combinations (hypothesis/anchor-target guided multi-omic analysis), and 2) identifying biomarkers of Alzheimer ’s disease (generic multi-omic analysis). The evaluation and comparison results showed M3NetFlow achieved the best prediction accuracy and identified a set of drug combination synergy and disease associated targets. The model can be directly applied to other multi-omic data-driven studies. The code is publicly accessible at: https://github.com/FuhaiLiAiLab/M3NetFlow}
}
@article{WANG2024105602,
title = {Proactive safety hazard identification using visual–text semantic similarity for construction safety management},
journal = {Automation in Construction},
volume = {166},
pages = {105602},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105602},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003388},
author = {Yiheng Wang and Bo Xiao and Ahmed Bouferguene and Mohamed Al-Hussein},
keywords = {Behavior-based safety, Computer vision, Natural language processing, Image captioning, Automatic safety hazard identification},
abstract = {Automated safety management in construction can reduce injuries by identifying hazardous postures, actions, and missing personal protective equipment (PPE). However, existing computer vision (CV) methods have limitations in connecting recognition results to text-based safety rules. To address this issue, this paper presents a multi-modal framework that bridges the gap between construction image monitoring and safety knowledge. The framework includes an image processing module that utilizes CV and dense image captioning techniques, and a text processing module that employs natural language processing for semantic similarity evaluation. Experiments showed a mean average precision of 49.6% in dense captioning and an F1 score of 74.3% in hazard identification. While the proposed framework demonstrates a promising multi-modal approach towards automated safety hazard identification and reasoning, improvements in dataset size and model performance are still needed to enhance its effectiveness in real-world applications.}
}
@article{DAS2024382,
title = {Towards the development of an explainable e-commerce fake review index: An attribute analytics approach},
journal = {European Journal of Operational Research},
volume = {317},
number = {2},
pages = {382-400},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724001826},
author = {Ronnie Das and Wasim Ahmed and Kshitij Sharma and Mariann Hardey and Yogesh K. Dwivedi and Ziqi Zhang and Chrysostomos Apostolidis and Raffaele Filieri},
keywords = {Fake reviews, Amazon, Risk analysis, AI explainability, BERT, Topic model indexing, LIME confidence score},
abstract = {Instruments of corporate risk and reputation assessment tools are quintessentially developed on structured quantitative data linked to financial ratios and macroeconomics. An emerging stream of studies has challenged this norm by demonstrating improved risk assessment and model prediction capabilities through unstructured textual corporate data. Fake online consumer reviews pose serious threats to a business’ competitiveness and sales performance, directly impacting revenue, market share, brand reputation and even survivability. Research has shown that as little as three negative reviews can lead to a potential loss of 59.2 % of customers. Amazon, as the largest e-commerce retail platform, hosts over 85,000 small-to-medium-size (SME) retailers (UK), selling over fifty percent of Amazon products worldwide. Despite Amazon's best efforts, fake reviews are a growing problem causing financial and reputational damage at a scale never seen before. While large corporations are better equipped to handle these problems more efficiently, SMEs become the biggest victims of these scam tactics. Following the principles of attribute (AA) and responsible (RA) analytics, we present a novel hybrid method for indexing enterprise risk that we call the Fake Review Index (RFRI). The proposed modular approach benefits from a combination of structured review metadata and semantic topic index derived from unstructured product reviews. We further apply LIME to develop a Confidence Score, demonstrating the importance of explainability and openness in contemporary analytics within the OR domain. Transparency, explainability and simplicity of our roadmap to a hybrid modular approach offers an attractive entry platform for practitioners and managers from the industry.}
}
@article{ZHU2023100680,
title = {Transformers and their application to medical image processing: A review},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {16},
number = {4},
pages = {100680},
year = {2023},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2023.100680},
url = {https://www.sciencedirect.com/science/article/pii/S1687850723001589},
author = {Dongmei Zhu and Dongbo Wang},
keywords = {Transformer, Image processing, Image classification, Image segmentation, Image reconstruction},
abstract = {Transformers perform well in natural language processing tasks and have made many breakthroughs in computer vision. In medical image processing, transformers are successfully used in image segmentation, classification, reconstruction, and diagnosis. In this paper, we mainly expound on the transformer principle and its application in medical imaging. Specifically, we first introduce the basic principles and model structure of transformers. Then, we summarize the improvement mechanism of the transformer's network including combining the transformer with the Unet network, creating a transformer lightweight variant network, strengthening the cross-fast link mechanism, and building a large model with the transformer as the skeleton. Second, extensive discussion is given to medical image segmentation, reconstruction, classification, and other applications. Finally, the main challenges transformers face in the medical image processing field and future development prospects. Furthermore, we systematically summarize the latest research progress of transformers and their application in medical image processing, which has significant reference value for transformer research in the medical field.}
}
@article{WANG2024111950,
title = {A comprehensive survey on interactive evolutionary computation in the first two decades of the 21st century},
journal = {Applied Soft Computing},
volume = {164},
pages = {111950},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111950},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624007245},
author = {Yanan Wang and Yan Pei},
keywords = {Interactive evolutionary computation, Evolutionary computation, Computational intelligence, Humanized computational intelligence, Human-machine interaction},
abstract = {Interactive evolutionary computation (IEC) has demonstrated significant success in addressing numerous real-world problems that are challenging to quantify mathematically or are inadequately evaluated using conventional computational models. This success arises from IEC’s ability to effectively amalgamate evolutionary computation (EC) algorithms with expert knowledge and user preferences. These problems encompass the creative and personalized generation of products, art, and sound; the design optimization of communication systems, environments, and pharmaceuticals; and expert support in areas such as portfolio selection and hearing aid fitting, among others. Despite significant advancements in IEC over the past two decades, no major comprehensive survey encompassing all aspects of IEC research has been conducted since 2001. This article aims to address this gap by providing a comprehensive survey and an enriched definition and scope of IEC, along with innovative ideas for future research in this field. The proposed IEC definition more clearly reflects the mechanism and current research status of the IEC. Additionally, the survey categorizes IEC research into five distinct directions from a problem-oriented perspective: interactive evolutionary computation algorithms, IEC algorithm improvements, evolutionary multi-objective optimization (EMO) with IEC, human perception studies with IEC, and IEC applications. Each direction is meticulously explored, elucidating its contents and key features, while providing a concise summary of pertinent IEC studies. Finally, the survey investigates several promising future trends in IEC, analyzing them through the lens of these five directions and considering the current perspective of computational intelligence, artificial intelligence, and human-machine interaction.}
}
@article{DUAN2024100145,
title = {Human–robot object handover: Recent progress and future direction},
journal = {Biomimetic Intelligence and Robotics},
volume = {4},
number = {1},
pages = {100145},
year = {2024},
issn = {2667-3797},
doi = {https://doi.org/10.1016/j.birob.2024.100145},
url = {https://www.sciencedirect.com/science/article/pii/S2667379724000032},
author = {Haonan Duan and Yifan Yang and Daheng Li and Peng Wang},
keywords = {Human–robot interactions, Object handover},
abstract = {Human–robot object handover is one of the most primitive and crucial capabilities in human–robot collaboration. It is of great significance to promote robots to truly enter human production and life scenarios and serve human in numerous tasks. Remarkable progressions in the field of human–robot object handover have been made by researchers. This article reviews the recent literature on human–robot object handover. To this end, we summarize the results from multiple dimensions, from the role played by the robot (receiver or giver), to the end-effector of the robot (parallel-jaw gripper or multi-finger hand), to the robot abilities (grasp strategy or motion planning). We also implement a human–robot object handover system for anthropomorphic hand to verify human–robot object handover pipeline. This review aims to provide researchers and developers with a guideline for designing human–robot object handover methods.}
}
@article{LIN2025216436,
title = {Deep learning-assisted methods for accelerating the intelligent screening of novel 2D materials: New perspectives focusing on data collection and description},
journal = {Coordination Chemistry Reviews},
volume = {529},
pages = {216436},
year = {2025},
issn = {0010-8545},
doi = {https://doi.org/10.1016/j.ccr.2025.216436},
url = {https://www.sciencedirect.com/science/article/pii/S0010854525000062},
author = {Yuandong Lin and Ji Ma and Yong-Guang Jia and Chongchong Yu and Jun-Hu Cheng},
keywords = {2D materials, Deep learning, Data collections, Data descriptions, Material screenings},
abstract = {Since the isolation of graphene, the interest in two-dimensional (2D) materials has been steadily growing thanks to their unique chemical and physical properties, as well as their potential for various applications. Deep learning (DL), currently one of the most sophisticated machine learning (ML) models, is emerging as a highly effective tool for intelligently investigating and screening 2D materials. The utilization of abundant data sources, appropriate descriptors, and neural networks enables the prediction of the structural and physicochemical properties of undiscovered 2D materials based on DL. Specifically, high-quality and well-described data plays a crucial role in effective model training, accurate predictions, and the discovery of new 2D materials. It also promotes reproducibility, collaboration, and continuous improvement within this field. This tutorial review is dedicated to an examination of the characterization, prediction, and discovery of 2D materials facilitated by various DL techniques. It focuses on the perspective of data collection and description, aiming to provide a clearer understanding of underlying principles and predicting outcomes. In addition, it also offers insights into future research prospects. The growing acceptance of DL is set to accelerate and transform the study of 2D materials.}
}
@article{XU2024104120,
title = {On implementing autonomous supply chains: A multi-agent system approach},
journal = {Computers in Industry},
volume = {161},
pages = {104120},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104120},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000484},
author = {Liming Xu and Stephen Mak and Maria Minaricova and Alexandra Brintrup},
keywords = {Autonomous supply chain, Multi-agent system, Autonomous agents, Perishable foods, Resilience},
abstract = {Trade restrictions, the COVID-19 pandemic, and geopolitical conflicts have significantly exposed vulnerabilities within traditional global supply chains. These events underscore the need for organisations to establish more resilient and flexible supply chains. To address these challenges, the concept of the autonomous supply chain (ASC), characterised by predictive and self-decision-making capabilities, has recently emerged as a promising solution. However, research on ASCs is relatively limited, with no existing studies specifically focusing on their implementations. This paper aims to address this gap by presenting an implementation of ASC using a multi-agent approach. It presents a methodology for the analysis and design of such an agent-based ASC system (A2SC). This paper provides a concrete case study, the autonomous meat supply chain, which showcases the practical implementation of the A2SC system using the proposed methodology. Additionally, a system architecture and a toolkit for developing such A2SC systems are presented. Despite limitations, this work demonstrates a promising approach for implementing an effective ASC system.}
}
@article{PILARIO2024100163,
title = {Teaching classical machine learning as a graduate-level course in chemical engineering: An algorithmic approach},
journal = {Digital Chemical Engineering},
volume = {11},
pages = {100163},
year = {2024},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2024.100163},
url = {https://www.sciencedirect.com/science/article/pii/S2772508124000255},
author = {Karl Ezra Pilario},
keywords = {Chemical engineering education, Machine learning, Data science, Artificial intelligence, Bridging topics, Roadmap},
abstract = {The demand for engineering graduates with technical skills in data science, machine learning (ML), and artificial intelligence (AI) is now growing. Chemical engineering (ChemE) departments around the world are currently addressing this skills gap by instituting AI or ML elective courses in their program. However, designing such a course is difficult since the issue of which ML models to teach and the depth of theory to be discussed remains unclear. In this paper, we present a graduate-level ML course particularly designed such that students will be able to apply ML for research in ChemE. To achieve this, the course intends to cover a wide selection of ML models with emphasis on their motivations, derivations, and training algorithms, followed by their applications to ChemE-related data sets. We argue that this algorithmic approach to teaching ML can help broaden the capabilities of students since they can judge for themselves which tool to use when, even for problems outside the process industries, or they can modify the methods to test novel ideas. We found that students remain engaged in the mathematical details as long as every topic is properly motivated and the gaps in the required statistical and computer science concepts are filled. Hence, this paper also presents a roadmap of ML topics, their motivations, and bridging topics that can be followed by instructors. Lastly, we report anonymized student feedback on this course which is being offered at the Department of Chemical Engineering, University of the Philippines, Diliman.}
}
@article{LU2025102747,
title = {Multimodal dual perception fusion framework for multimodal affective analysis},
journal = {Information Fusion},
volume = {115},
pages = {102747},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102747},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005256},
author = {Qiang Lu and Xia Sun and Yunfei Long and Xiaodi Zhao and Wang Zou and Jun Feng and Xuxin Wang},
keywords = {Multimodal sentiment analysis, Sarcasm detection, Fake news detection, Multimodal affective analysis, Multimodal dual perception fusion},
abstract = {The misuse of social platforms and the difficulty in regulating post contents have culminated in a surge of negative sentiments, sarcasms, and the rampant spread of fake news. In response, Multimodal sentiment analysis, sarcasm detection and fake news detection based on image and text have attracted considerable attention recently. Due to that these areas share semantic and sentiment features and confront related fusion challenges in deciphering complex human expressions across different modalities, integrating these multimodal classification tasks that share commonalities across different scenarios into a unified framework is expected to simplify research in sentiment analysis, and enhance the effectiveness of classification tasks involving both semantic and sentiment modeling. Therefore, we consider integral components of a broader spectrum of research known as multimodal affective analysis towards semantics and sentiment, and propose a novel multimodal dual perception fusion framework (MDPF). Specifically, MDPF contains three core procedures: (1) Generating bootstrapping language-image Knowledge to enrich origin modality space, and utilizing cross-modal contrastive learning for aligning text and image modalities to understand underlying semantics and interactions. (2) Designing dynamic connective mechanism to adaptively match image-text pairs and jointly employing gaussian-weighted distribution to intensify semantic sequences. (3) Constructing a cross-modal graph to preserve the structured information of both image and text data and share information between modalities, while introducing sentiment knowledge to refine the edge weights of the graph to capture cross-modal sentiment interaction. We evaluate MDPF on three publicly available datasets across three tasks, and the empirical results demonstrate the superiority of our proposed model.}
}
@incollection{2024197,
title = {Index},
editor = {Cecilio Angulo and Alejandro Chacón and Pere Ponsa},
booktitle = {Cognitive Assistant Supported Human-Robot Collaboration},
publisher = {Academic Press},
pages = {197-204},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-22135-4},
doi = {https://doi.org/10.1016/B978-0-44-322135-4.00020-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443221354000201}
}
@article{LIU2025119522,
title = {Intelligent visual analysis of accident behavior and mechanism inherent in ship collision accident data},
journal = {Ocean Engineering},
volume = {315},
pages = {119522},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119522},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824028609},
author = {Tao Liu and Hao Hong and Jihong Chen and Yaqin Zhang and Kejun Zhao and Maowen Liu and Jinxian Weng and Wen Liu},
keywords = {Ship collision accident, Visual analysis, Co-occurrence pattern, Collision risk, Accident similarity, Graph neural network},
abstract = {The analysis of ship collision accident data plays a crucial role in ensuring maritime transportation safety. This paper proposes an intelligent visual analysis-based method focusing on the co-occurrence patterns, collision risk, similarity analysis, and classification of ship collision data. Based on data from 1573 maritime accidents that occurred globally between 2010 and 2023, this paper first designs co-occurrence patterns to identify regularities between accident attributes. Second, a fusion model combining fuzzy comprehensive evaluation and logistic regression is developed to calculate ship collision risk, and the model is applied to reconstruct and visually analyze the collision process. Third, the similarity between two collision accidents is analyzed based on Sentence-BERT algorithm, and a heterogeneous graph neural network model is constructed for accident classification. Finally, all the analysis contents are integrated into the visual analysis system of the ship collision accident data to illustrate the accident behavior and mechanism inherent in ship collision accident data. The rich content within the visualization system provides trainee crew members with a more comprehensive understanding of accidents and serves as a reference for maritime management agencies to analyze similar and related accidents, derive lessons learned, identify high-risk areas, and formulate targeted risk management strategies.}
}
@incollection{CHANG20243,
title = {Chapter 1 - Introduction to artificial intelligence for cardiovascular clinicians},
editor = {Anthony C. Chang and Alfonso Limon},
booktitle = {Intelligence-Based Cardiology and Cardiac Surgery},
publisher = {Academic Press},
pages = {3-120},
year = {2024},
series = {Intelligence-Based Medicine: Subspecialty Series},
isbn = {978-0-323-90534-3},
doi = {https://doi.org/10.1016/B978-0-323-90534-3.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390534300010X},
author = {Anthony C. Chang and Alfonso Limon},
keywords = {Artificial intelligence, Cardiovascular clinicians, Deep learning technology, Human-machine intelligence continuum, Machine learning, Neuroscience},
abstract = {The impressive gains in deep learning (DL) started in 2012 and its successful utilization in image interpretation have led to the current momentum for artificial intelligence (AI) awareness and adoption. In 2016, Google DeepMind's AlphaGo software soundly defeated the best human Go champion Lee Sedol to introduce the capability of DL outside of image interpretation. More recently, there have been impressive exponential advances in natural language processing with transformer tools such as GPT-3, GPT-4, and now ChatGPT. DeepMind and its AlphaFold AI tool has been able to predict the three-dimensional (3D) structure of proteins since 2021 and was Science magazine's “Breakthrough of the Year.” All of these AI accomplishments heralded the recent new era in AI. Major universities with AI departments (such as Stanford, MIT, and Carnegie Mellon) and technology giants (such as IBM, Apple, Facebook, and Microsoft in the United States as well as other large companies such as Baidu, Alibaba, and Tencent [BAT] in China) are all fervidly exploring real-life applications of AI. There is also a movement to democratize AI so that “no-code platforms” can accommodate people who do not know how to code [1].}
}
@article{JU2024106207,
title = {A Comprehensive Survey on Deep Graph Representation Learning},
journal = {Neural Networks},
volume = {173},
pages = {106207},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106207},
url = {https://www.sciencedirect.com/science/article/pii/S089360802400131X},
author = {Wei Ju and Zheng Fang and Yiyang Gu and Zequn Liu and Qingqing Long and Ziyue Qiao and Yifang Qin and Jianhao Shen and Fang Sun and Zhiping Xiao and Junwei Yang and Jingyang Yuan and Yusheng Zhao and Yifan Wang and Xiao Luo and Ming Zhang},
keywords = {Deep learning on graphs, Graph representation learning, Graph neural network, Survey},
abstract = {Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future.}
}
@article{AMIN20201472,
title = {Establishing trustworthiness and authenticity in qualitative pharmacy research},
journal = {Research in Social and Administrative Pharmacy},
volume = {16},
number = {10},
pages = {1472-1482},
year = {2020},
issn = {1551-7411},
doi = {https://doi.org/10.1016/j.sapharm.2020.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1551741119309155},
author = {Mohamed Ezzat Khamis Amin and Lotte Stig Nørgaard and Afonso M. Cavaco and Matthew J. Witry and Lisa Hillman and Alina Cernasev and Shane P. Desselle},
abstract = {Spurred by the value it can add, the use of qualitative research methods has been steadily growing by social pharmacy researchers around the globe, either separately or as part of mixed methods research projects. Given this increase, it is important to provide guidance to assist researchers in ensuring quality when employing such methods. This commentary addresses both theoretical fundamentals as well as practical aspects of establishing quality in qualitative social pharmacy research. More specifically, it provides an explanation of each of the criteria of trustworthiness proposed by Lincoln and Guba (credibility, transferability, dependability and confirmability) and different techniques used in establishing them. It also provides a brief overview of authenticity, a more recent and less widely used set of criteria that involve demonstrating fairness, ontological authenticity, educative authenticity, catalytic authenticity, and tactical authenticity. For each of these terms, the commentary provides a definition, how it applies to social pharmacy research, and guidance on when and how to use them. These are accompanied by examples from the pharmacy literature where the criteria have been used. The commentary ends by providing a summary of competing viewpoints of establishing quality in the published literature while inviting the reader to reflect on how the presented criteria would apply to different qualitative research projects.}
}
@article{DAKAKNI2023100179,
title = {Artificial intelligence in the L2 classroom: Implications and challenges on ethics and equity in higher education: A 21st century Pandora's box},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100179},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100179},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000589},
author = {Deema Dakakni and Nehme Safa},
keywords = {Artificial intelligence, Ethical uses of AI, L2 classrooms, Digital technology and L2 learning, Chatbot GPT and ethics, Writing mills},
abstract = {The purpose of this research was to investigate attitudes of both students and teachers concerning Artificial Intelligence (AI) tools in the L2 classroom. The study was a descriptive, qualitative, mixedmethods case study whose data were taken from a purposive, convenient sample at a private, English-speaking university during the Summer Semester 2023 in Beirut, Lebanon. Data collection primarily involved an online survey on Google forms which was given to a sample of 49 students taking a research-based English 202 course of which 46 were completed. Afterwards, six English teachers and six students were chosen based on their voluntary will to participate in individual interviews for the former and semi-structured focus group interviews for the latter. The findings revealed that approximately 85% of students did indeed use AI unethically to get ideas for their assignments, assist them in their projects' “blue-prints” or do their assignments/projects altogether. The findings also revealed that a “love/hate” relationship seemed to dictate students' relationships with AI, where students did indeed make use of AI but were distrusting of it for privacy and equity concerns. Finally, findings also revealed that most of the interviewed instructors' readiness to undergo training for AI was more to monitor students' potential misuse of it. The article purposes a suggestive revamping of course learning objectives due to students' inclinations to misuse AI to do their coursework with 89.4% of students willing to use AI to complete their coursework should university punitive measures be removed; furthermore, the article equally proposes future research investigating the impact and use of AI in the higher educational classroom on student performance and that it be used with a “grain of salt” as it may unleash a Pandora's box of future generations graduating without the necessary know-how in delicate professions of medicine, nursing, engineering, architecture among others.}
}
@article{TRAPPEY201738,
title = {Exploring 4G patent and litigation informatics in the mobile telecommunications industry},
journal = {World Patent Information},
volume = {50},
pages = {38-51},
year = {2017},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0172219016300692},
author = {Charles V. Trappey and Amy J.C. Trappey},
keywords = {Mobile telecommunications, Patents, Knowledge e-discovery, Formal concept analysis, Ontology},
abstract = {Patent informatics are often analysed for IP protections, particularly in high-tech industries. This research develops a computer-supported generic methodology for discovering evolutions and linkages between litigations and disputed patents. The IP litigations in mobile telecommunications are used as the case study. An ontology framework representing the 4G domain knowledge is defined first. Then, a modified formal concept analysis (MFCA) approach is developed to discover the evolutionary linkages of legal cases and their disputed patents. In addition to citation-based patent analysis, this research provides a new approach in identifying legal and technical evolutions for future R&D planning and IP strategies.}
}
@article{ZHANG2024104613,
title = {iCORPP: Interleaved commonsense reasoning and probabilistic planning on robots},
journal = {Robotics and Autonomous Systems},
volume = {174},
pages = {104613},
year = {2024},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104613},
url = {https://www.sciencedirect.com/science/article/pii/S092188902300252X},
author = {Shiqi Zhang and Piyush Khandelwal and Peter Stone},
keywords = {Integrated Reasoning and Planning, Commonsense reasoning, Planning under uncertainty, Autonomous Robots, Markov Decision Processes, POMDPs},
abstract = {Robot sequential decision-making in the real world is a challenge because it requires the robots to simultaneously reason about the current world state and dynamics, while planning actions to accomplish complex tasks. On the one hand, declarative languages and reasoning algorithms support representing and reasoning with commonsense knowledge. But these algorithms are not good at planning actions toward maximizing cumulative reward over a long, unspecified horizon. On the other hand, probabilistic planning frameworks, such as Markov decision processes (MDPs) and partially observable MDPs (POMDPs), support planning to achieve long-term goals under uncertainty. But they are ill-equipped to represent or reason about knowledge that is not directly related to actions. In this article, we present an algorithm, called iCORPP, to simultaneously estimate the current world state, reason about world dynamics, and construct task-oriented controllers. In this process, robot decision-making problems are decomposed into two interdependent (smaller) subproblems that focus on reasoning to “understand the world” and planning to “achieve the goal” respectively. The developed algorithm has been implemented and evaluated both in simulation and on real robots using everyday service tasks, such as indoor navigation, and dialog management. Results show significant improvements in scalability, efficiency, and adaptiveness, compared to competitive baselines including handcrafted action policies.}
}
@article{CHEN2025109698,
title = {High-order complementary cloud application programming interface recommendation with logical reasoning for incremental development},
journal = {Engineering Applications of Artificial Intelligence},
volume = {140},
pages = {109698},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109698},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624018566},
author = {Zhen Chen and Denghui Xie and Xiaolong Wang and Dianlong You and Limin Shen},
keywords = {Recommendation systems, Cloud application programming interface recommendation, Complementary recommendation, Logical reasoning},
abstract = {Cloud application programming interface, as the best carrier for service delivery, data exchange, and capability replication, has been an indispensable element of innovation in today’s app-driven world. However, it is difficult for developers to select the suitable one when facing the sea of cloud application programming interfaces. Existing researches focus on generating single-function and high-quality recommendation lists, while ignoring developers’ needs for high-order complementary cloud application programming interfaces in incremental development. In this paper, we present a high-order complementary cloud application programming interface recommendation with logical reasoning. Firstly, we conduct data analysis to demonstrate the necessity of recommending high-order complementary cloud application programming interfaces and the existence of substitute noise. Secondly, a logical reasoning network is designed using projection, intersection, and negation three logic operators, wherein high-order complementary relations are mined and substitute noises are eliminated. Then, the cloud application programming interface base vector that is complementary but not substitute to the query set is generated, and Kullback–Leibler divergence is subsequently introduced to generate complementary recommendation results. Finally, experimental results demonstrate the superiority of our approach in low-, high-, and hybrid-order complementary recommendation scenarios, and there is a significant increase in hit rate, normalize discounted cumulative gain, mean reciprocal rank, and substitute degree by 11.43%/4.86%, 10.08%/4.28%, 7.50%/2.67%, and 36.33%/32.35% on ProgrammableWeb and Huawei AppGallery datasets respectively. The proposed approach is not only more likely to produce diversified results that meet developers’ needs but also help providers better formulate pricing strategies to achieve combined sales and improve revenue.}
}
@article{ZHONG202352,
title = {Enhancing head and neck tumor management with artificial intelligence: Integration and perspectives},
journal = {Seminars in Cancer Biology},
volume = {95},
pages = {52-74},
year = {2023},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2023.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X23001086},
author = {Nian-Nian Zhong and Han-Qi Wang and Xin-Yue Huang and Zi-Zhan Li and Lei-Ming Cao and Fang-Yi Huo and Bing Liu and Lin-Lin Bu},
keywords = {Artificial intelligence, Head and neck tumor, Machine learning, Neural network, Deep learning, Computer vision, Prediction model},
abstract = {Head and neck tumors (HNTs) constitute a multifaceted ensemble of pathologies that primarily involve regions such as the oral cavity, pharynx, and nasal cavity. The intricate anatomical structure of these regions poses considerable challenges to efficacious treatment strategies. Despite the availability of myriad treatment modalities, the overall therapeutic efficacy for HNTs continues to remain subdued. In recent years, the deployment of artificial intelligence (AI) in healthcare practices has garnered noteworthy attention. AI modalities, inclusive of machine learning (ML), neural networks (NNs), and deep learning (DL), when amalgamated into the holistic management of HNTs, promise to augment the precision, safety, and efficacy of treatment regimens. The integration of AI within HNT management is intricately intertwined with domains such as medical imaging, bioinformatics, and medical robotics. This article intends to scrutinize the cutting-edge advancements and prospective applications of AI in the realm of HNTs, elucidating AI’s indispensable role in prevention, diagnosis, treatment, prognostication, research, and inter-sectoral integration. The overarching objective is to stimulate scholarly discourse and invigorate insights among medical practitioners and researchers to propel further exploration, thereby facilitating superior therapeutic alternatives for patients.}
}
@article{DWIVEDI2024102750,
title = {“Real impact”: Challenges and opportunities in bridging the gap between research and practice – Making a difference in industry, policy, and society},
journal = {International Journal of Information Management},
volume = {78},
pages = {102750},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102750},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223001317},
author = {Yogesh K. Dwivedi and Anand Jeyaraj and Laurie Hughes and Gareth H. Davies and Manju Ahuja and Mousa Ahmed Albashrawi and Adil S. Al-Busaidi and Salah Al-Sharhan and Khalid Ibrahim Al-Sulaiti and Levent Altinay and Shem Amalaya and Sunil Archak and María Teresa Ballestar and Shonil A. Bhagwat and Anandhi Bharadwaj and Amit Bhushan and Indranil Bose and Pawan Budhwar and Deborah Bunker and Alexandru Capatina and Lemuria Carter and Ioanna Constantiou and Crispin Coombs and Tom Crick and Csaba Csáki and Yves Darnige and Rahul Dé and Rick Delbridge and Rameshwar Dubey and Robin Gauld and Ravi Kumar Gutti and Marié Hattingh and Arve Haug and Leeya Hendricks and Airo Hino and Cathy H.C. Hsu and Netta Iivari and Marijn Janssen and Ikram Jebabli and Paul Jones and Iris Junglas and Abhishek Kaushik and Deepak Khazanchi and Mitsuru Kodama and Sascha Kraus and Vikram Kumar and Christian Maier and Tegwen Malik and Machdel Matthee and Ian P. McCarthy and Marco Meier and Bhimaraya Metri and Adrian Micu and Angela-Eliza Micu and Santosh K. Misra and Anubhav Mishra and Tonja Molin-Juustila and Leif Oppermann and Nicholas O’Regan and Abhipsa Pal and Neeraj Pandey and Ilias O. Pappas and Andrew Parker and Kavita Pathak and Daniel Pienta and Ariana Polyviou and Ramakrishnan Raman and Samuel Ribeiro-Navarrete and Paavo Ritala and Michael Rosemann and Suprateek Sarker and Pallavi Saxena and Daniel Schlagwein and Hergen Schultze and Chitra Sharma and Sujeet Kumar Sharma and Antonis Simintiras and Vinay Kumar Singh and Hanlie Smuts and John Soldatos and Manoj Kumar Tiwari and Jason Bennett Thatcher and Cristina Vanberghen and Ákos Varga and Polyxeni Vassilakopoulou and Viswanath Venkatesh and Giampaolo Viglia and Tim Vorley and Michael Wade and Paul Walton},
keywords = {Academic impact, Implications for practice, Relevance, Research benefits, Research contribution, Research impact},
abstract = {Achieving impact from academic research is a challenging, complex, multifaceted, and interconnected topic with a number of competing priorities and key performance indicators driving the extent and reach of meaningful and measurable benefits from research. Academic researchers are incentivised to publish their research in high-ranking journals and academic conferences but also to demonstrate the impact of their outputs through metrics such as citation counts, altmetrics, policy and practice impacts, and demonstrable institutional decision-making influence. However, academic research has been criticized for: its theoretical emphasis, high degree of complexity, jargon-heavy language, disconnect from industry and societal needs, overly complex and lengthy publishing timeframe, and misalignment between academic and industry objectives. Initiatives such as collaborative research projects and technology transfer offices have attempted to deliver meaningful impact, but significant barriers remain in the identification and evaluation of tangible impact from academic research. This editorial focusses on these aspects to deliver a multi-expert perspective on impact by developing an agenda to deliver more meaningful and demonstrable change to how “impact” can be conceptualized and measured to better align with the aims of academia, industry, and wider society. We present the 4D model - Design, Deliver, Disseminate, and Demonstrate - to provide a structured approach for academia to better align research endeavors with practice and deliver meaningful, tangible benefits to stakeholders.}
}
@article{GROVER2024477,
title = {Global Workforce and Access: Demand, Education, Quality},
journal = {Seminars in Radiation Oncology},
volume = {34},
number = {4},
pages = {477-493},
year = {2024},
note = {Future of Radiation Oncology},
issn = {1053-4296},
doi = {https://doi.org/10.1016/j.semradonc.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1053429624000547},
author = {Surbhi Grover and Laurence Court and Sheldon Amoo-Mitchual and John Longo and Danielle Rodin and Aba Anoa Scott and Yolande Lievens and Mei Ling Yap and May Abdel-Wahab and Peter Lee and Ekaterina Harsdorf and Jamal Khader and Xun Jia and Manjit Dosanjh and Ahmed Elzawawy and Taofeeq Ige and Miles Pomper and David Pistenmaa and Patricia Hardenbergh and Daniel G Petereit and Michele Sargent and Kristin Cina and Benjamin Li and Yavuz Anacak and Chuck Mayo and Sainikitha Prattipati and Nwamaka Lasebikan and Katharine Rendle and Donna O'Brien and Eugenia Wendling and C. Norman Coleman},
abstract = {There has long existed a substantial disparity in access to radiotherapy globally. This issue has only been exacerbated as the growing disparity of cancer incidence between high-income countries (HIC) and low and middle-income countries (LMICs) widens, with a pronounced increase in cancer cases in LMICs. Even within HICs, iniquities within local communities may lead to a lack of access to care. Due to these trends, it is imperative to find solutions to narrow global disparities. This requires the engagement of a diverse cohort of stakeholders, including working professionals, non-governmental organizations, nonprofits, professional societies, academic and training institutions, and industry. This review brings together a diverse group of experts to highlight critical areas that could help reduce the current global disparities in radiation oncology. Advancements in technology and treatment, such as artificial intelligence, brachytherapy, hypofractionation, and digital networks, in combination with implementation science and novel funding mechanisms, offer means for increasing access to care and education globally. Common themes across sections reveal how utilizing these new innovations and strengthening collaborative efforts among stakeholders can help improve access to care globally while setting the framework for the next generation of innovations.}
}
@article{OFFENHUBER2023264,
title = {Reconsidering Representation in College Design Curricula},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {264-282},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
author = {Dietmar Offenhuber and Joy Mountford},
keywords = {Representation, Data, Models, Maps, Visualization, Sensory modalities},
abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.}
}
@article{SHARMA2024111934,
title = {A survey on machine learning techniques applied to source code},
journal = {Journal of Systems and Software},
volume = {209},
pages = {111934},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111934},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003291},
author = {Tushar Sharma and Maria Kechagia and Stefanos Georgiou and Rohit Tiwari and Indira Vats and Hadi Moazen and Federica Sarro},
keywords = {Machine learning for software engineering, Source code analysis, Deep learning, Datasets, Tools},
abstract = {The advancements in machine learning techniques have encouraged researchers to apply these techniques to a myriad of software engineering tasks that use source code analysis, such as testing and vulnerability detection. Such a large number of studies hinders the community from understanding the current research landscape. This paper aims to summarize the current knowledge in applied machine learning for source code analysis. We review studies belonging to twelve categories of software engineering tasks and corresponding machine learning techniques, tools, and datasets that have been applied to solve them. To do so, we conducted an extensive literature search and identified 494 studies. We summarize our observations and findings with the help of the identified studies. Our findings suggest that the use of machine learning techniques for source code analysis tasks is consistently increasing. We synthesize commonly used steps and the overall workflow for each task and summarize machine learning techniques employed. We identify a comprehensive list of available datasets and tools useable in this context. Finally, the paper discusses perceived challenges in this area, including the availability of standard datasets, reproducibility and replicability, and hardware resources. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{STEPHENSON20233156,
title = {Physical Laboratory Automation in Synthetic Biology},
journal = {ACS Synthetic Biology},
volume = {12},
number = {11},
pages = {3156-3169},
year = {2023},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.3c00345},
url = {https://www.sciencedirect.com/science/article/pii/S2161506323003285},
author = {Ashley Stephenson and Lauren Lastra and Bichlien Nguyen and Yuan-Jyue Chen and Jeff Nivala and Luis Ceze and Karin Strauss},
keywords = {automation, synthetic biology, standardization, robotics, microfluidics, design-build-test-learn},
abstract = {Synthetic Biology has overcome many of the early challenges facing the field and is entering a systems era characterized by adoption of Design-Build-Test-Learn (DBTL) approaches. The need for automation and standardization to enable reproducible, scalable, and translatable research has become increasingly accepted in recent years, and many of the hardware and software tools needed to address these challenges are now in place or under development. However, the lack of connectivity between DBTL modules and barriers to access and adoption remain significant challenges to realizing the full potential of lab automation. In this review, we characterize and classify the state of automation in synthetic biology with a focus on the physical automation of experimental workflows. Though fully autonomous scientific discovery is likely a long way off, impressive progress has been made toward automating critical elements of experimentation by combining intelligent hardware and software tools. It is worth questioning whether total automation that removes humans entirely from the loop should be the ultimate goal, and considerations for appropriate automation versus total automation are discussed in this light while emphasizing areas where further development is needed in both contexts.
}
}
@article{HARGREAVES2024301844,
title = {DFPulse: The 2024 digital forensic practitioner survey},
journal = {Forensic Science International: Digital Investigation},
volume = {51},
pages = {301844},
year = {2024},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301844},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001719},
author = {Christopher Hargreaves and Frank Breitinger and Liz Dowthwaite and Helena Webb and Mark Scanlon},
keywords = {Digital forensics, Practitioner survey, Challenges, Future directions, Artificial intelligence},
abstract = {This paper reports on the largest survey of digital forensic practitioners to date (DFPulse) conducted from March to May 2024 resulting in 122 responses. The survey collected information about practitioners' operating environments, the technologies they encounter, investigative techniques they use, the challenges they face, the degree to which academic research is accessed and useful to the practitioner community, and their suggested future research directions. The paper includes quantitative and qualitative results from the survey and a discussion of the implications for academia, the improvements that can be made, and future research directions.}
}
@article{ZHAO2023100521,
title = {Toward parallel intelligence: An interdisciplinary solution for complex systems},
journal = {The Innovation},
volume = {4},
number = {6},
pages = {100521},
year = {2023},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2023.100521},
url = {https://www.sciencedirect.com/science/article/pii/S2666675823001492},
author = {Yong Zhao and Zhengqiu Zhu and Bin Chen and Sihang Qiu and Jincai Huang and Xin Lu and Weiyi Yang and Chuan Ai and Kuihua Huang and Cheng He and Yucheng Jin and Zhong Liu and Fei-Yue Wang},
abstract = {The growing complexity of real-world systems necessitates interdisciplinary solutions to confront myriad challenges in modeling, analysis, management, and control. To meet these demands, the parallel systems method rooted in the artificial systems, computational experiments, and parallel execution (ACP) approach has been developed. The method cultivates a cycle termed parallel intelligence, which iteratively creates data, acquires knowledge, and refines the actual system. Over the past two decades, the parallel systems method has continuously woven advanced knowledge and technologies from various disciplines, offering versatile interdisciplinary solutions for complex systems across diverse fields. This review explores the origins and fundamental concepts of the parallel systems method, showcasing its accomplishments as a diverse array of parallel technologies and applications while also prognosticating potential challenges. We posit that this method will considerably augment sustainable development while enhancing interdisciplinary communication and cooperation.}
}
@article{LIU2024102423,
title = {Artificial intelligence for production, operations and logistics management in modular construction industry: A systematic literature review},
journal = {Information Fusion},
volume = {109},
pages = {102423},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102423},
url = {https://www.sciencedirect.com/science/article/pii/S156625352400201X},
author = {Qiurui Liu and Yanfang Ma and Lin Chen and Witold Pedrycz and Mirosław J. Skibniewski and Zhen-Song Chen},
keywords = {Literature review, Bibliometric analysis, Production management, Operations management, Logistics management, Artificial intelligence, Modular construction industry},
abstract = {Artificial intelligence (AI) has garnered significant attention within the modular construction industry, emerging as a prominent frontier development trend. A comprehensive and systematic analysis is required to gain a thorough understanding of the existing literature on the use of AI in the management of production, operations, and logistics within the modular construction industry. This review delves into the various aspects of AI implementation in this sector, adopting a critical perspective. The objective of this paper is to analyze the progress, suitability, and research patterns in the field of AI for the management of productions, operations, and logistics within the modular construction industry. First, a concise overview of AI technologies pertaining to the contemporary research on the production, operations and logistics management of the modular construction industry is provided. Second, a bibliometric analysis is performed to provide a comprehensive overview of the existing publications pertaining to this subject matter. Subsequently, this paper presents literature reviews and outlines future directions for each component, specifically AI in the context of production management, operations management, and logistics management within the modular construction industry. The review provides a valuable knowledge base and roadmap to guide future research and development efforts in AI-enhanced modular construction management.}
}
@article{LAREYRE202357,
title = {Comprehensive Review of Natural Language Processing (NLP) in Vascular Surgery},
journal = {EJVES Vascular Forum},
volume = {60},
pages = {57-63},
year = {2023},
issn = {2666-688X},
doi = {https://doi.org/10.1016/j.ejvsvf.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666688X23000758},
author = {Fabien Lareyre and Bahaa Nasr and Arindam Chaudhuri and Gilles {Di Lorenzo} and Mathieu Carlier and Juliette Raffort},
keywords = {Artificial Intelligence, Literature search, Natural Language Processing, Vascular surgery},
abstract = {Objective
The use of Natural Language Processing (NLP) has attracted increased interest in healthcare with various potential applications including identification and extraction of health information, development of chatbots and virtual assistants. The aim of this comprehensive literature review was to provide an overview of NLP applications in vascular surgery, identify current limitations, and discuss future perspectives in the field.
Data sources
The MEDLINE database was searched on April 2023.
Review methods
The database was searched using a combination of keywords to identify studies reporting the use of NLP and chatbots in three main vascular diseases. Keywords used included Natural Language Processing, chatbot, chatGPT, aortic disease, carotid, peripheral artery disease, vascular, and vascular surgery.
Results
Given the heterogeneity of study design, techniques, and aims, a comprehensive literature review was performed to provide an overview of NLP applications in vascular surgery. By enabling identification and extraction of information on patients with vascular diseases, such technology could help to analyse data from healthcare information systems to provide feedback on current practice and help in optimising patient care. In addition, chatbots and NLP driven techniques have the potential to be used as virtual assistants for both health professionals and patients.
Conclusion
While Artificial Intelligence and NLP technology could be used to enhance care for patients with vascular diseases, many challenges remain including the need to define guidelines and clear consensus on how to evaluate and validate these innovations before their implementation into clinical practice.}
}
@article{SANTOS2025107568,
title = {Software solutions for newcomers’ onboarding in software projects: A systematic literature review},
journal = {Information and Software Technology},
volume = {177},
pages = {107568},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107568},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001733},
author = {Italo Santos and Katia Romero Felizardo and Igor Steinmacher and Marco A. Gerosa},
keywords = {Systematic literature review, Software projects, Open source software, Onboarding, Turnover, Tool, Newcomers, Novices},
abstract = {Context:
Newcomers joining an unfamiliar software project face numerous barriers; therefore, effective onboarding is essential to help them engage with the team and develop the behaviors, attitudes, and skills needed to excel in their roles. However, onboarding can be a lengthy, costly, and error-prone process. Software solutions can help mitigate these barriers and streamline the process without overloading senior members.
Objective:
This study aims to identify the state-of-the-art software solutions for onboarding newcomers.
Methods:
We conducted a systematic literature review (SLR) to answer six research questions.
Results:
We analyzed 32 studies about software solutions for onboarding newcomers and yielded several key findings: (1) a range of strategies exists, with recommendation systems being the most prevalent; (2) most solutions are web-based; (3) solutions target a variety of onboarding aspects, with a focus on process; (4) many onboarding barriers remain unaddressed by existing solutions; (5) laboratory experiments are the most commonly used method for evaluating these solutions; and (6) diversity and inclusion aspects primarily address experience level.
Conclusion:
We shed light on current technological support and identify research opportunities to develop more inclusive software solutions for onboarding. These insights may also guide practitioners in refining existing platforms and onboarding programs to promote smoother integration of newcomers into software projects.}
}
@article{REDDY20244081,
title = {A Fusion Model for Personalized Adaptive Multi-Product Recommendation System Using Transfer Learning and Bi-GRU},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {4081-4107},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.057071},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008361},
author = {Buchi Reddy Ramakantha Reddy and Ramasamy Lokesh Kumar},
keywords = {Personalized recommendation systems, transfer learning, bidirectional gated recurrent units (Bi-GRU), performance metrics, adaptive systems, product reviews},
abstract = {Traditional e-commerce recommendation systems often struggle with dynamic user preferences and a vast array of products, leading to suboptimal user experiences. To address this, our study presents a Personalized Adaptive Multi-Product Recommendation System (PAMR) leveraging transfer learning and Bi-GRU (Bidirectional Gated Recurrent Units). Using a large dataset of user reviews from Amazon and Flipkart, we employ transfer learning with pre-trained models (AlexNet, GoogleNet, ResNet-50) to extract high-level attributes from product data, ensuring effective feature representation even with limited data. Bi-GRU captures both spatial and sequential dependencies in user-item interactions. The innovation of this study lies in the innovative feature fusion technique that combines the strengths of multiple transfer learning models, and the integration of an attention mechanism within the Bi-GRU framework to prioritize relevant features. Our approach addresses the classic recommendation systems that often face challenges such as cold start along with data sparsity difficulties, by utilizing robust user and item representations. The model demonstrated an accuracy of up to 96.9%, with precision and an F1-score of 96.2% and 96.97%, respectively, on the Amazon dataset, significantly outperforming the baselines and marking a considerable advancement over traditional configurations. This study highlights the effectiveness of combining transfer learning with Bi-GRU for scalable and adaptive recommendation systems, providing a versatile solution for real-world applications.}
}
@incollection{2023503,
title = {Index},
editor = {Anthony C. Chang and Alfonso Limon},
booktitle = {Intelligence-Based Cardiology and Cardiac Surgery},
publisher = {Academic Press},
pages = {503-513},
year = {2023},
series = {Intelligence-Based Medicine: Subspecialty Series},
isbn = {978-0-323-90534-3},
doi = {https://doi.org/10.1016/B978-0-323-90534-3.20001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905343200012}
}@article{SOHAIL2023105949,
title = {Data-driven approaches for road safety: A comprehensive systematic literature review},
journal = {Safety Science},
volume = {158},
pages = {105949},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105949},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522002880},
author = {Ammar Sohail and Muhammad Aamir Cheema and Mohammed Eunus Ali and Adel N. Toosi and Hesham A. Rakha},
keywords = {Road safety, Crash prevention, Crash prediction & detection, Road surface condition, Driver & pedestrian behavior, Traffic & congestion},
abstract = {Road crashes cost over a million lives each year. Consequently, researchers and transport engineers continue their efforts to improve road safety and minimize road crashes. With the increasing availability of various sensor technologies to capture road safety-related data and the recent breakthrough in modern data-driven techniques, in particular Machine Learning and Deep Learning techniques, data-driven road safety research has gained significant attention in the past few years. As road safety involves a number of different aspects, including road infrastructure (e.g., surface conditions), road user behaviors (e.g., driver/pedestrian behavior), and traffic congestion, critically reviewing all these major aspects and their relationships with road crashes is a challenging task. In this paper, we present a detailed review of 70 articles, which are shortlisted from 2871 articles found by searching relevant keywords from the scopus IEEE digital library and google scholar databases. To better analyze the data-driven road safety research a number of taxonomies are first introduced to characterize data sources Equipment & sensors to capture data And methodologies to analyze and make decisions based on data. Then Based on the defined taxonomies Selected research articles covering different aspects of road safety are critically analyzed. This study highlights important directions for future work and some major challenges such as data collection Poor data quality and lack of ground truth data.}
}
@article{TIAN2024136113,
title = {Efficient sulfamethoxazole biotransformation and detoxification by newly isolated strain Hydrogenophaga sp. SNF1 via a ring ortho-hydroxylation pathway},
journal = {Journal of Hazardous Materials},
volume = {480},
pages = {136113},
year = {2024},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2024.136113},
url = {https://www.sciencedirect.com/science/article/pii/S030438942402692X},
author = {Shaohua Tian and Lelan You and Xu Huang and Chaoxiang Liu and Jian-Qiang Su},
keywords = {Sulfamethoxazole, Biodegradation, , Co-metabolism},
abstract = {Sulfonamides are frequently detected with high concentrations in various environments and was regarded as a serious environmental risk by fostering the dissemination of antibiotic resistance genes. This study for the first time reported a strain SNF1 affiliated with Hydrogenophaga can efficiently degrade sulfamethoxazole (SMX). Strain SNF1 prefers growing under extra carbon sources and neutral condition, and could degrade 500 mg/L SMX completely within 16 h. Under the conditions optimized by response surface method (3.11 g/L NaAc, 0.77 g/L (NH4)2SO4, pH = 7.53, and T = 34.38 ℃), a high removal rate constant 0.5104 /h for 50 mg/L SMX was achieved. Coupling the intermediate products identification with comparative genomic analysis, a novel SMX degradation pathway was proposed. Unlike Actinomycetota degraders, SMX was deaminized and ring ortho-hydroxylated in strain SNF1 using a Rieske dioxygenase in combination with glutamine synthetase system. Rieske dioxygenase gene expression was up-regulated by 1.09 to 6.02-fold in response to 100 mg/L SMX. When SMX is fully degraded, its antimicrobial activity drops by over 90 %, and its anticipated toxicity to aquatic organisms were overall reduced. These findings provided new insights into SMX-degrading microorganisms and mechanisms and highlighted the potential of Hydrogenophaga. sp. SNF1 for biological elimination of SMX from wastewater.}
}
@article{WILLS2021247,
title = {A genome-wide search for determinants of survival in 1926 patients with advanced colorectal cancer with follow-up in over 22,000 patients},
journal = {European Journal of Cancer},
volume = {159},
pages = {247-258},
year = {2021},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2021.09.047},
url = {https://www.sciencedirect.com/science/article/pii/S0959804921011679},
author = {Christopher Wills and Yazhou He and Matthew G. Summers and Yi Lin and Amanda I. Phipps and Katie Watts and Philip J. Law and Nada A. Al-Tassan and Timothy S. Maughan and Richard Kaplan and Richard S. Houlston and Ulrike Peters and Polly A. Newcomb and Andrew T. Chan and Daniel D. Buchanan and Steve Gallinger and Loic L. Marchand and Rish K. Pai and Qian Shi and Steven R. Alberts and Victoria Gray and Hannah D. West and Valentina Escott-Price and Malcolm G. Dunlop and Jeremy P. Cheadle},
keywords = {Colorectal cancer, GWAS, Survival, Prognostic biomarkers},
abstract = {Background
While genome-wide association studies (GWAS) have identified germline variants influencing the risk of developing colorectal cancer (CRC), there has been limited examination of the possible role of inherited variation as a determinant of patient outcome.
Patients and methods
We performed a GWAS for overall survival (OS) in 1926 patients with advanced CRC from the COIN and COIN-B clinical trials. For single nucleotide polymorphisms (SNPs) showing an association with OS (P < 1.0 × 10−5), we conducted sensitivity analyses based on the time from diagnosis to death and sought independent replications in 5675 patients from the Study of Colorectal Cancer in Scotland (SOCCS) and 16,964 patients from the International Survival Analysis in Colorectal cancer Consortium (ISACC). We analysed the Human Protein Atlas to determine if ERBB4 expression was associated with survival in 438 patients with colon adenocarcinomas.
Results
The most significant SNP associated with OS was rs79612564 in ERBB4 (hazard ratio [HR] = 1.24, 95% confidence interval [CI] = 1.16–1.32, P = 1.9 × 10−7). SNPs at 17 loci had suggestive associations for OS and all had similar effects on the time from diagnosis to death. No lead SNPs were independently replicated in the meta-analysis of all patients from SOCCS and ISACC. However, rs79612564 was significant in stage-IV patients from SOCCS (P = 2.1 × 10−2) but not ISACC (P = 0.89) and SOCCS combined with COIN and COIN-B attained genome-wide significance (P = 1.7 × 10−8). Patients with high ERBB4 expression in their colon adenocarcinomas had worse survival (HR = 1.50, 95% CI = 1.1–1.9, P = 4.6 × 10−2).
Conclusions
Genetic and expression data support a potential role for rs79612564 in the receptor tyrosine kinase ERBB4 as a predictive biomarker of survival.}
}
@article{ANSARI2023101694,
title = {Enhanced subgraph matching for large graphs using candidate region-based decomposition and ordering},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101694},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101694},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002483},
author = {Zubair Ali Ansari and Md. Aslam Parwez and Irfan Rashid Thoker and  Jahiruddin},
keywords = {Subgraph isomorphism, Graph search, Eccentricity, Candidate region ordering, Large graph, Embedding, Straggler query},
abstract = {The subgraph matching problem associated with large graphs is an emerging research challenge in graph search due to the growing size of the web, social, and metabolic graphs, and the wide availability of graph databases. Such problems involve finding all instances (aka embedding) of the small-sized query graph in the associated large-sized reference graph. Many state-of-the-art algorithms, including VF3, RI, CFL-Match, and Glasgow, exist to solve subgraph matching problem. RI is one of the fastest subgraph matching algorithms focusing mainly on time efficiency performance measures. However, other performance measures, such as the number of found instances of the query graph (embedding count), the method of ordering the query graph’s vertices, and the number of recursive calls, are crucial for the efficiency and effectiveness of the subgraph matching. In this paper, the RI+ algorithm is proposed as an enhanced version of RI, which has been designed using candidate region-based decomposition and ordering. Three novel candidate region orderings have been introduced, namely vertex-count, density, and average-path-length, based on the structural properties of the candidate regions. On empirical analysis of RI+ on real-world data sets, it was observed that RI+ shows significant improvement in efficiency and effectiveness over RI on both performance evaluation measures, namely, embedding count and search time. The influence of the proposed candidate region orderings on the search time of RI+ was also analyzed, revealing that a suitable candidate region ordering has the potential to improve the search time of the proposed algorithm.}
}
@article{GOADSBY20191081,
title = {Safety and efficacy of sphenopalatine ganglion stimulation for chronic cluster headache: a double-blind, randomised controlled trial},
journal = {The Lancet Neurology},
volume = {18},
number = {12},
pages = {1081-1090},
year = {2019},
issn = {1474-4422},
doi = {https://doi.org/10.1016/S1474-4422(19)30322-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474442219303229},
author = {Peter J Goadsby and Soma Sahai-Srivastava and Eric J Kezirian and Anne H Calhoun and David C Matthews and Peter J McAllister and Peter D Costantino and Deborah I Friedman and John R Zuniga and Laszlo L Mechtler and Saurin R Popat and Ali R Rezai and David W Dodick},
abstract = {Summary
Background
Chronic cluster headache is the most disabling form of cluster headache. The mainstay of treatment is attack prevention, but the available management options have little efficacy and are associated with substantial side-effects. In this study, we aimed to assess the safety and efficacy of sphenopalatine ganglion stimulation for treatment of chronic cluster headache.
Methods
We did a randomised, sham-controlled, parallel group, double-blind, safety and efficacy study at 21 headache centres in the USA. We recruited patients aged 22 years or older with chronic cluster headache, who reported a minimum of four cluster headache attacks per week that were unsuccessfully controlled by preventive treatments. Participants were randomly assigned (1:1) via an online adaptive randomisation procedure to either stimulation of the sphenopalatine ganglion or a sham control that delivered a cutaneous electrical stimulation. Patients and the clinical evaluator and surgeon were masked to group assignment. The primary efficacy endpoint, which was analysed with weighted generalised estimated equation logistic regression models, was the difference between groups in the proportion of stimulation-treated ipsilateral cluster attacks for which relief from pain was achieved 15 min after the start of stimulation without the use of acute drugs before that timepoint. Efficacy analyses were done in all patients who were implanted with a device and provided data for at least one treated attack during the 4-week experimental phase. Safety was assessed in all patients undergoing an implantation procedure up to the end of the open-label phase of the study, which followed the experimental phase. This trial is registered with ClinicalTrials.gov, number NCT02168764.
Findings
Between July 9, 2014, and Feb 14, 2017, 93 patients were enrolled and randomly assigned, 45 to the sphenopalatine ganglion stimulation group and 48 to the control group. 36 patients in the sphenopalatine ganglion stimulation group and 40 in the control group had at least one attack during the experimental phase and were included in efficacy analyses. The proportion of attacks for which pain relief was experienced at 15 min was 62·46% (95% CI 49·15–74·12) in the sphenopalatine ganglion stimulation group versus 38·87% (28·60–50·25) in the control group (odds ratio 2·62 [95% CI 1·28–5·34]; p=0·008). Nine serious adverse events were reported by the end of the open-label phase. Three of these serious adverse events were related to the implantation procedure (aspiration during intubation, nausea and vomiting, and venous injury or compromise). A fourth serious adverse event was an infection that was attributed to both the stimulation device and the implantation procedure. The other five serious adverse events were unrelated. There were no unanticipated serious adverse events.
Interpretation
Sphenopalatine ganglion stimulation seems efficacious and is well tolerated, and potentially offers an alternative approach to the treatment of chronic cluster headache. Further research is need to clarify its place in clinical practice.
Funding
Autonomic Technologies.}
}
@article{YELINA2024114696,
title = {Streamlined regulation of chloroplast development in the liverwort Marchantia polymorpha},
journal = {Cell Reports},
volume = {43},
number = {9},
pages = {114696},
year = {2024},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2024.114696},
url = {https://www.sciencedirect.com/science/article/pii/S2211124724010477},
author = {Nataliya E. Yelina and Eftychios Frangedakis and Zhemin Wang and Tina B. Schreier and Jenna Rever and Marta Tomaselli and Edith C.F. Forestier and Kumari Billakurthi and Sibo Ren and Yahui Bai and Julia Stewart-Wood and Jim Haseloff and Silin Zhong and Julian M. Hibberd},
keywords = {chloroplast biogenesis, chlorophyll, , green algae, transcription factors, evolution},
abstract = {Summary
Chloroplasts develop from undifferentiated plastids in response to light. In angiosperms, after the perception of light, the Elongated Hypocotyl 5 (HY5) transcription factor initiates photomorphogenesis, and two families of transcription factors known as GOLDEN2-LIKE (GLK) and GATA are considered master regulators of chloroplast development. In addition, the MIR171-targeted SCARECROW-LIKE GRAS transcription factors also impact chlorophyll biosynthesis. The extent to which these proteins carry out conserved roles in non-seed plants is not known. Using the model liverwort Marchantia polymorpha, we show that GLK controls chloroplast biogenesis, and HY5 shows a small conditional effect on chlorophyll content. Chromatin immunoprecipitation sequencing (ChIP-seq) revealed that MpGLK has a broader set of targets than has been reported in angiosperms. We also identified a functional GLK homolog in green algae. In summary, our data support the hypothesis that GLK carries out a conserved role relating to chloroplast biogenesis in land plants and green algae.}
}
@article{QUIN2024112011,
title = {A/B testing: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {211},
pages = {112011},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112011},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224000542},
author = {Federico Quin and Danny Weyns and Matthias Galster and Camila Costa Silva},
keywords = {A/B testing, Systematic literature review, A/B test engineering},
abstract = {A/B testing, also referred to as online controlled experimentation or continuous experimentation, is a form of hypothesis testing where two variants of a piece of software are compared in the field from an end user’s point of view. A/B testing is widely used in practice to enable data-driven decision making for software development. While a few studies have explored different facets of research on A/B testing, no comprehensive study has been conducted on the state-of-the-art in A/B testing. Such a study is crucial to provide a systematic overview of the field of A/B testing driving future research forward. To address this gap and provide an overview of the state-of-the-art in A/B testing, this paper reports the results of a systematic literature review that analyzed primary studies. The research questions focused on the subject of A/B testing, how A/B tests are designed and executed, what roles stakeholders have in this process, and the open challenges in the area. Analysis of the extracted data shows that the main targets of A/B testing are algorithms, visual elements, and workflow and processes. Single classic A/B tests are the dominating type of tests, primarily based in hypothesis tests. Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician. The primary types of data collected during the execution of A/B tests are product/system data, user-centric data, and spatio-temporal data. The dominating use of the test results are feature selection, feature rollout, continued feature development, and subsequent A/B test design. Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor. The main reported open problems are related to the enhancement of proposed approaches and their usability. From our study we derived three interesting lines for future research: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing.}
}
@article{STOKES2020107980,
title = {Molecular Transducers of Human Skeletal Muscle Remodeling under Different Loading States},
journal = {Cell Reports},
volume = {32},
number = {5},
pages = {107980},
year = {2020},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2020.107980},
url = {https://www.sciencedirect.com/science/article/pii/S2211124720309657},
author = {Tanner Stokes and James A. Timmons and Hannah Crossland and Thomas R. Tripp and Kevin Murphy and Chris McGlory and Cameron J. Mitchell and Sara Y. Oikawa and Robert W. Morton and Bethan E. Phillips and Steven K. Baker and Phillip J. Atherton and Claes Wahlestedt and Stuart M. Phillips},
keywords = {protein synthesis, atrophy, growth, untranslated region, skeletal muscle, hypertrophy, protein turnover, transcriptome, human, resistance exercise, unloading},
abstract = {Summary
Loading of skeletal muscle changes the tissue phenotype reflecting altered metabolic and functional demands. In humans, heterogeneous adaptation to loading complicates the identification of the underpinning molecular regulators. A within-person differential loading and analysis strategy reduces heterogeneity for changes in muscle mass by ∼40% and uses a genome-wide transcriptome method that models each mRNA from coding exons and 3′ and 5′ untranslated regions (UTRs). Our strategy detects ∼3–4 times more regulated genes than similarly sized studies, including substantial UTR-selective regulation undetected by other methods. We discover a core of 141 genes correlated to muscle growth, which we validate from newly analyzed independent samples (n = 100). Further validating these identified genes via RNAi in primary muscle cells, we demonstrate that members of the core genes were regulators of protein synthesis. Using proteome-constrained networks and pathway analysis reveals notable relationships with the molecular characteristics of human muscle aging and insulin sensitivity, as well as potential drug therapies.}
}
@article{2024S1,
title = {Abstracts},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {11, Supplement },
pages = {S1-S169},
year = {2024},
note = {Association for Molecular Pathology 2024 Annual Meeting Abstracts},
issn = {1525-1578},
doi = {https://doi.org/10.1016/S1525-1578(24)00232-0},
url = {https://www.sciencedirect.com/science/article/pii/S1525157824002320}
}
@article{LIN2025129012,
title = {Prototype matching-based meta-learning model for few-shot fault diagnosis of mechanical system},
journal = {Neurocomputing},
volume = {617},
pages = {129012},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129012},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224017831},
author = {Lin Lin and Sihao Zhang and Song Fu and Yikun Liu and Shiwei Suo and Guolei Hu},
keywords = {Fault diagnosis Meta-learning, Prototype-matching, Few-shot learning},
abstract = {The efficacy of advanced deep-learning diagnostic methods is contingent mainly upon sufficient trainable data for each fault category. However, gathering ample data in real-world scenarios is often challenging, rendering these deep-learning techniques ineffective. This paper introduces a novel Prototype Matching-based Meta-Learning (PMML) approach to address the few-shot fault diagnosis under constrained data conditions. Initially, the PMML’s feature extractor is meta-trained within the Model-Agnostic Meta-Learning framework, utilizing multiple fault classification tasks from known operational conditions in the source domain to acquire prior meta-knowledge for fault diagnosis. Subsequently, the trained feature extractor is employed to derive meta-features from few-shot samples in the target domain, and metric learning is conducted to facilitate swift and precise few-shot fault diagnosis, leveraging meta-knowledge and similarity information across sample sets. Moreover, instead of utilizing all target domain samples, the prototype of each fault category is used to capture similarity information between support and query samples. Concurrently, BiLSTM is employed to selectively embed the meta-feature prototype, enabling the extraction of more distinguishable metric features for enhanced metric learning. Finally, the effectiveness of the proposed PMML is validated through a series of comparative experiments on two fault datasets, demonstrating its outstanding performance in addressing both zero-shot and few-shot fault diagnosis challenges.}
}
@article{2021100036,
title = {Voices of GA4GH members: Collaborating in technology and policy development},
journal = {Cell Genomics},
volume = {1},
number = {2},
pages = {100036},
year = {2021},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2021.100036},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X21000434},
abstract = {The Global Alliance for Genomics and Health (GA4GH) is organized around eight Work Streams, where members develop technology standards and policy frameworks to enable the responsible sharing of human genomic and health-related data. These are implemented across 24 Driver Projects, reflecting real-world genomics initiatives. In this Voices, Cell Genomics asked GA4GH members to reflect on their engagement with GA4GH and how this has driven progress in open science and genomic medicine.}
}
@article{QIAN2024121299,
title = {Unveiling intricate transformation pathways of emerging contaminants during wastewater treatment processes through simplified network analysis},
journal = {Water Research},
volume = {253},
pages = {121299},
year = {2024},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2024.121299},
url = {https://www.sciencedirect.com/science/article/pii/S004313542400201X},
author = {Yuli Qian and Linchang Guan and Yunhao Ke and Liye Wang and Xuebing Wang and Nanyang Yu and Qingmiao Yu and Si Wei and Jinju Geng},
keywords = {Wastewater treatment processes, Structural transformation, Transformation processes, Simplified network analysis, Anti-hypertensive drugs},
abstract = {As the key stage for purifying wastewater, elimination of emerging contaminants (ECs) is found to be fairly low in wastewater treatment plants (WWTPs). However, less knowledge is obtained regarding the transformation pathways between various chemical structures of ECs under different treatment processes. This study unveiled the transformation pathways of ECs with different structures in 15 WWTPs distributed across China by simplified network analysis (SNA) we proposed. After treatment, the molecular weight of the whole component of wastewater decreased and the hydrophilicity increased. There are significant differences in the structure of eliminated, consistent and formed pollutants. Amino acids, peptides, and analogues (AAPAs) were detected most frequently and most removable. Benzenoids were refractory. Triazoles were often produced. The high-frequency reactions in different WWTPs were similar, (de)methylation and dehydration occurred most frequently. Different biological treatment processes performed similarly, while some advanced treatment processes differed, such as a significant increase of -13.976 (2HO reaction) paired mass distances (PMDs) in the chlorine alone process. Further, the common structural transformation was uncovered. 4 anti-hypertensive drugs, including irbesartan, valsartan, olmesartan, and losartan, were identified, along with 22 transformation products (TPs) of them. OH2 and H2O PMDs occurred most frequently and in 80.81 % of the parent-transformation product pairs, the intensity of the product was higher than parent in effluents, whose risk should be considered in future assessment activity. Together our results provide a macrography perspective on the transformation processes of ECs in WWTPs. In the future, selectively adopting wastewater treatment technology according to structures is conductive for eliminating recalcitrant ECs in WWTPs.}
}
@article{2019e296,
title = {Invited Faculty Abstracts from the International Neuromodulation Society’s 14th World Congress},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {22},
number = {7},
pages = {e296-e584},
year = {2019},
issn = {1094-7159},
doi = {https://doi.org/10.1111/ner.12958},
url = {https://www.sciencedirect.com/science/article/pii/S1094715921020249}
}
@article{BALATSKYI202159,
title = {Cardiac-specific β-catenin deletion dysregulates energetic metabolism and mitochondrial function in perinatal cardiomyocytes},
journal = {Mitochondrion},
volume = {60},
pages = {59-69},
year = {2021},
issn = {1567-7249},
doi = {https://doi.org/10.1016/j.mito.2021.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1567724921000982},
author = {Volodymyr V. Balatskyi and Vasyl O. Vaskivskyi and Anna Myronova and Diana Avramets and Karim {Abu Nahia} and Larysa L. Macewicz and Tetiana P. Ruban and Dar'ya Yu. Kucherenko and Oleksandr O. Soldatkin and Iryna V. Lushnikova and Galyna G. Skibo and Cecilia L. Winata and Pawel Dobrzyn and Oksana O. Piven},
keywords = {Heart, β-Catenin, Transcriptome, Glycolysis, Lipolysis, Oxidative phosphorylation, Mitochondria},
abstract = {β-Catenin signaling pathway regulates cardiomyocytes proliferation and differentiation, though its involvement in metabolic regulation of cardiomyocytes remains unknown. We used one-day-old mice with cardiac-specific knockout of β-catenin and neonatal rat ventricular myocytes treated with β-catenin inhibitor to investigate the role of β-catenin metabolism regulation in perinatal cardiomyocytes. Transcriptomics of perinatal β-catenin-ablated hearts revealed a dramatic shift in the expression of genes involved in metabolic processes. Further analysis indicated an inhibition of lipolysis and glycolysis in both in vitro and in vivo models. Finally, we showed that β-catenin deficiency leads to mitochondria dysfunction via the downregulation of Sirt1/PGC-1α pathway. We conclude that cardiac-specific β-catenin ablation disrupts the energy substrate shift that is essential for postnatal heart maturation, leading to perinatal lethality of homozygous β-catenin knockout mice.}
}
@article{LI2025101217,
title = {ALDH1L2 drives HCC progression through TAM polarization},
journal = {JHEP Reports},
volume = {7},
number = {1},
pages = {101217},
year = {2025},
issn = {2589-5559},
doi = {https://doi.org/10.1016/j.jhepr.2024.101217},
url = {https://www.sciencedirect.com/science/article/pii/S2589555924002210},
author = {Jiajun Li and Chi Zhang and Qingqing Zhou and Qinqin Long and Jiayi Chen and Lili Meng and Wei Tian and Yue Yang and Chao Ge and Yuting Su and Xi-Dai Long and Jun Wu and Hua Tian},
keywords = {ALDH1L2, Feedback loop, TAM, Crosstalk, Progression, Hepatocellular carcinoma},
abstract = {Background & Aims
Dysregulation of one-carbon metabolism is considered an early hallmark of mitochondrial dysfunction and cancer metabolism. ALDH1L2 belongs to the aldehyde dehydrogenase family and plays an important role in tumor progression. However, little is known about the precise role and underlying mechanisms of ALDH1L2 in hepatocellular carcinoma (HCC).
Methods
Immunohistochemistry, western blotting, and immunofluorescence staining were used to evaluate ALDH1L2 expression in HCC samples (n = 90) and cell lines (n = 9). A series of in vitro and in vivo assays were performed to explore the role and molecular mechanism of ALDH1L2 in HCC progression.
Results
ALDH1L2 upregulation is associated with poor prognosis in HCC (hazard ratio 1.923; 95% confidence interval 1.03–3.59; p = 0.04). ALDH1L2 promotes tumor cell proliferation and metastasis by activating NRF2/IL-6/STAT3 signaling. ALDH1L2 promotes mitochondrial respiration, increases ATP production and protects HCC cells from reactive oxygen species-induced cellular damage via NRF2 stabilization. NRF2 also directly binds to the ALDH1L2 promoter and increases ALDH1L2 transcription, thereby establishing a positive feedback loop to maintain the function of ALDH1L2. The interaction between tumor-associated macrophages and ALDH1L2-overexpressing HCC cells further promotes HCC progression. In addition, ALDH1L2 knockdown enhances the anti-HCC activity of the tyrosine kinase inhibitor sorafenib.
Conclusions
These findings provide the first evidence indicating that ALDH1L2 is directly involved in tumor progression by interacting with tumor-associated macrophages through the Jak2/STAT3 signaling pathway and that ALDH1L2 may be a target molecule for HCC therapy.
Impact and implications:
This research highlights that ALDH1L2 could serve as a predictive and prognostic marker in HCC. We found that a positive feedback loop between ALDH1L2 and NRF2 promotes HCC progression by activating the IL-6/Jak2/STAT3 signaling axis and tumor-associated macrophage polarization. In addition, we found that ALDH1L2 knockdown enhances the anti-HCC effect of sorafenib.}
}
@article{HERBENER2025103409,
title = {Are lonely youngsters turning to chatbots for companionship? The relationship between chatbot usage and social connectedness in Danish high-school students},
journal = {International Journal of Human-Computer Studies},
volume = {196},
pages = {103409},
year = {2025},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2024.103409},
url = {https://www.sciencedirect.com/science/article/pii/S1071581924001927},
author = {Arthur Bran Herbener and Malene Flensborg Damholdt},
keywords = {Chatbot, Artificial intelligence, Conversational agent, Loneliness, Perceived social support, Companionship, Friendship},
abstract = {Are lonely youngsters turning to chatbots to fill their social needs? The present research contributes to the ongoing scientific endeavors to understand the adoption of chatbots as social companions. Specifically, we examined how many Danish high-school students engage in friend-like conversations with chatbots, why they engage in such conversations, and whether this trend is associated with the sense of social connectedness (operationalized as loneliness and perceived social support). In pursuing this goal, a preregistered mixed-methods cross-sectional survey study was carried out. A total sample of 1599 students from 15 Danish high schools was collected. In total, 234 students (14.6 %) responded that they engaged in friend-like conversations with chatbots. Qualitative thematic analyses of free-text responses revealed two dominant ways of engaging with chatbots: utilitarian conversations (n = 174) and social-supportive conversations (n = 39). A major finding was that social-supportive chatbot users reported significantly more loneliness than non-chatbot users (d = 0.53) and utilitarian chatbot users (d = 0.52). Furthermore, social-supportive chatbot users also reported significantly less perceived social support than non-chatbot users (d = −0.46). Analyses also showed significant associations between higher loneliness and less perceived social support and various situational triggers for initiating conversations with chatbots, including bad mood, a need for self-disclosure, and a sense of loneliness, but not with a sense of friendship to the chatbot as a trigger of chatbot interactions. These findings suggest a trend among some socially disconnected Danish high-school students toward using chatbots to cope with negative emotions.}
}
@article{SWARNA2024111941,
title = {On the impact of multiple source code representations on software engineering tasks — An empirical study},
journal = {Journal of Systems and Software},
volume = {210},
pages = {111941},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111941},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003369},
author = {Karthik Chandra Swarna and Noble Saji Mathews and Dheeraj Vagavolu and Sridhar Chimalakonda},
keywords = {Source code representation, Abstract Syntax Tree, Control Flow Graph, Program Dependence Graph, Code embedding, Method naming},
abstract = {Efficiently representing source code is crucial for various software engineering tasks such as code classification and clone detection. Existing approaches primarily use Abstract Syntax Tree (AST), and only a few focus on semantic graphs such as Control Flow Graph (CFG) and Program Dependency Graph (PDG), which contain information about source code that AST does not. Even though some works tried to utilize multiple representations, they do not provide any insights about the costs and benefits of using multiple representations. The primary goal of this paper is to discuss the implications of utilizing multiple source code representations, specifically AST, CFG, and PDG. We modify an AST path-based approach to accept multiple representations as input to an attention-based model. We do this to measure the impact of additional representations (such as CFG and PDG) over AST. We evaluate our approach on three tasks: Method Naming, Program Classification, and Clone Detection. Our approach increases the performance on these tasks by 11% (F1), 15.7% (Accuracy), and 9.3% (F1), respectively, over the baseline. In addition to the effect on performance, we discuss timing overheads incurred with multiple representations. We envision that this work can provide a base for researchers to explore and experiment with a variety of source code representations for software engineering tasks.}
}
@article{2019101910,
title = {Selective diary of IP information related conferences, meetings, exhibitions and courses},
journal = {World Patent Information},
volume = {58},
pages = {101910},
year = {2019},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2019.101910},
url = {https://www.sciencedirect.com/science/article/pii/S0172219019300869}
}
@article{DU2023131186,
title = {Biodegradation of sulfametoxydiazine by Alcaligenes aquatillis FA: Performance, degradation pathways, and mechanisms},
journal = {Journal of Hazardous Materials},
volume = {452},
pages = {131186},
year = {2023},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2023.131186},
url = {https://www.sciencedirect.com/science/article/pii/S0304389423004685},
author = {Yuqian Du and Qilu Cheng and Mingrong Qian and Yangzhi Liu and Feng Wang and Junwei Ma and Xin Zhang and Hui Lin},
keywords = {Sulfonamide antibiotic, Biodegradation, Genomics, Transcriptome, Degradation product},
abstract = {This study reports the isolation and characterization of a novel bacterial strain Alcaligenes aquatillis FA with the ability to degrade sulfametoxydiazine (SMD), a commonly used sulfonamide antibiotic (SA) in livestock and poultry production. The biodegradation kinetics, pathways, and genomic background of SMD by FA were investigated. The results showed that strain FA had high specificity to degrade SMD, and was unable to effectively degrade its isomer, sulfamonomethoxine. The SMD biodegradation followed a first-order kinetic model with a rate constant of 27.39 mg·L-1·day-1 and a half-life of 5.98 days. The biodegradation pathways and detoxification processes of SMD were proposed based on the identification of its biodegradation byproducts and the biotoxicity assessment using both the ecological structure-activity relationship (ECOSAR) model and biological indicator. The involvement of novel degrading enzymes, such as dimethyllsulfone monooxygenase, 4-carboxymuconolactone decarboxylase, and 1,4-benzoquinone reductase, was inferred in the SMD biodegradation process. The presence of sul2 and dfrA genes in strain FA, which were constitutively expressed in its cells, suggests that multiple mechanisms were employed by the strain to resist SMD. This study provides new insights into the biodegradation of sulfonamide antibiotics (SAs) as it is the first to describe an SMD-degrading bacterium and its genetic information.}
}
@article{DASHTI2025e41874,
title = {The influence of social media applications on learning English as a second language},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e41874},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e41874},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025002543},
author = {Fatma Dashti and Hanady M. Abdulsalam},
keywords = {Social media, English language, Online learning},
abstract = {This study aims to discover how the daily usage of social media platforms affects the users’ second language learning. The study has been conducted in public high schools in Kuwait from different areas. A questionnaire has been distributed to the high school students to seek their opinions about social media usage and to test their language skills in English. The questionnaire was distributed on a random sample of 300 volunteering participants has been obtained through the online survey. An interview has been also conducted with high school English teachers to get sufficient information about the social media usage and its effect on their students. Four teachers were asked to provide random grades of their students to check how they are affected. Another questionnaire has been added to know whether the social media platforms usage during the Covid-19 Pandemic helps in the language skills. The approach that has been used to conclude our results is a mixed method of qualitative and quantitative approaches. The results have shown that the skills of the language can be improved by the usage of the social media platforms specially the listening, speaking, and reading skills. The writing skill, however have not shown any improvement after using the social media platforms. Therefore, social media applications can be a reliable tool for learning a second language but it has to be under the guidance and supervision of professional teachers.}
}
@incollection{2024I1,
title = {Index},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {I1-I32},
year = {2024},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.20001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230200015}
}
@article{XU2020326,
title = {Stock movement predictive network via incorporative attention mechanisms based on tweet and historical prices},
journal = {Neurocomputing},
volume = {418},
pages = {326-339},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.07.108},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220313060},
author = {Hongfeng Xu and Lei Chai and Zhiming Luo and Shaozi Li},
keywords = {Stock prediction, Incorporative attention, Local semantics, Contextual information},
abstract = {The recent advances usually attempt to mine the effective market information from the chaotic data and learn multilevel representations by using attention mechanisms to conduct a stock prediction task. However, such methods usually lack the full utilization of local semantic embedding which contains the abundant textual semantics information. Moreover, these models suffer from the severe noise diffusion in contextual embeddings from a sequence after passing through the RNN. The noises diffusion constrains the performance of the proposed methods. In this work, we propose a stock movement predictive network via incorporative attention mechanisms. The core innovation is that the incorporative attention combines local and contextual attention mechanisms to clean the contextual embeddings by using local semantics. As a result, the attention effectively reduce the noises in the constructed higher-level representations and enhance the performance. Moreover, the local semantics and context are merged into the constructed higher-level representations which provide more abundant local semantic and contextual information. The experimental results demonstrate the state-of-the-art performance of the proposed approach on tweet and historical price dataset.}
}
@article{GAO2021111008,
title = {Phosphoproteomic analysis of ozone stress-responsive mechanisms in grapevine identifies KEG required for stress regulation},
journal = {Plant Science},
volume = {311},
pages = {111008},
year = {2021},
issn = {0168-9452},
doi = {https://doi.org/10.1016/j.plantsci.2021.111008},
url = {https://www.sciencedirect.com/science/article/pii/S0168945221002041},
author = {Zhen Gao and Baozhen Sun and Zhengwen Chen and Heng Zhai and Yuxin Yao and Yuanpeng Du},
keywords = {Ozone, Grape, Phosphoproteomics, VvKEG, ABA},
abstract = {The environmental damage caused by ozone is of increasing concern globally. The phosphoproteomics approach was used to explore the mechanisms underlying grapevine tolerance to ozone stress and identify phosphoproteins altered by ozone treatment. Results revealed that 194 of 2275 quantitatively analyzed phosphoproteins were significantly regulated after ozone treatment. Biological pathways related to transport were significantly enriched by the differentially regulated phosphoproteins. Among these phosphoproteins, the phosphorylation of RING E3 ligase in grape (V. vinifera KEEP ON GOING, VvKEG) decreased after ozone treatment. Over-expression of VvKEG in Arabidopsis decreased abscisic acid (ABA) sensitivity and enhanced ozone tolerance. Furthermore, VvKEG interacted with the ABA-responsive transcription factor ABSCISIC ACID-INSENSITIVE3 (ABI3). The exogenous application of ABA on grapevine leaves significantly influenced chlorophyll fluorescence, chlorophyll, and malondialdehyde (MDA) contents under ozone treatment; however, treatment with 150 μmol ABA aggravated ozone stress. These results indicate that phosphorylation modification provides information on ozone-induced processes and that VvKEG plays a critical role in these processes via regulation of the ABA signaling pathway in grape.}
}
@article{HALGAMUGE2025104128,
title = {Adaptive edge security framework for dynamic IoT security policies in diverse environments},
journal = {Computers & Security},
volume = {148},
pages = {104128},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104128},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004334},
author = {Malka N. Halgamuge and Dusit Niyato},
keywords = {Cybersecurity, Edge security, IoT security, Regulatory and compliance, Policy},
abstract = {The rapid expansion of Internet of Things (IoT) technologies has introduced significant cybersecurity challenges, particularly at the network edge where IoT devices operate. Traditional security policies designed for static environments fall short of addressing the dynamic, heterogeneous, and resource-constrained nature of IoT ecosystems. Existing dynamic security policy models lack versatility and fail to fully integrate comprehensive risk assessments, regulatory compliance, and AI/ML (artificial intelligence/machine learning)-driven adaptability. We develop a novel adaptive edge security framework that dynamically generates and adjusts security policies for IoT edge devices. Our framework integrates a dynamic security policy generator, a conflict detection and resolution in policy generator, a bias-aware risk assessment system, a regulatory compliance analysis system, and an AI-driven adaptability integration system. This approach produces tailored security policies that adapt to changes in the threat landscape, regulatory requirements, and device statuses. Our study identifies critical security challenges in diverse IoT environments and demonstrates the effectiveness of our framework through simulations and real-world scenarios. We found that our framework significantly enhances the adaptability and resilience of IoT security policies. Our results demonstrate the potential of AI/ML integration in creating responsive and robust security measures for IoT ecosystems. The implications of our findings suggest that dynamic and adaptive security frameworks are essential for protecting IoT devices against evolving cyber threats, ensuring compliance with regulatory standards, and maintaining the integrity and availability of IoT services across various applications.}
}
@article{GERARDIN2024202244,
title = {Améliorer l'utilisation des données médicales : à propos d'une campagne de classification de millions de documents},
journal = {Journal of Epidemiology and Population Health},
volume = {72},
pages = {202244},
year = {2024},
note = {Congrès ÉMOIS 2024},
issn = {2950-4333},
doi = {https://doi.org/10.1016/j.jeph.2024.202244},
url = {https://www.sciencedirect.com/science/article/pii/S2950433324000545},
author = {C. Gérardin and A. Calliger and P. Wajsbürt and A. Mouchet and R. Bey},
keywords = {Entrepôt de données de santé, Documents médicaux, Traitement automatique des langues},
abstract = {Introduction
Le développement des entrepôts de données de santé a rendu possible leur utilisation secondaire notamment à des fins de recherche, sous réserve que ces données soient correctement caractérisées. En particulier, pour les documents médicaux, la connaissance de leur type : comptes-rendus (CR) hospitaliers, ordonnances, CR d'examen, etc. est d'un intérêt majeur y compris pour renseigner le parcours du patient. Néanmoins, cette information est manquante pour une majorité des documents médicaux de l'entrepôt de données de santé (EDS) de l'AP-HP (140 millions de documents collectés depuis 2017). L'objectif de notre étude est de développer et valider un nouvel algorithme de classification des documents.
Méthodes
Les types de documents d'intérêt ont été validés et définis en amont de la campagne, à partir de la “document ontology” de la terminologie LOINC. Les jeux d'entraînement (2700 documents) et de validation (500 documents) ont été annotés par une clinicienne. Le premier jeu d'entraînement a été réalisé à partir d'un clustering effectué sur une représentation vectorielle (TF-IDF) du libellé du document, correspondant à un champ éditable par le clinicien sur le logiciel source. Un sur-échantillonnage a ensuite été réalisé sur certaines unités fonctionnelles pour s'assurer la présence des types peu fréquents. L'algorithme est constitué de recherche d'expressions régulières en cascade avec deux niveaux de granularité : 6 types principaux et 43 types secondaires. Il prend en entrée des métadonnées (libellé de document, libellé de formulaire) et des données issues du texte lui-même (titre du document, corps du texte et entête du document).
Résultats
Sur les types principaux, le f1-score est de 0,76 [0,63, 0,86] sur les CR de consultation, 0,75 [0,5, 0,92] sur les CR d'examen, 0,96 [0,92, 0,98] sur les CR hospitaliers, 0,97 [0,93, 0,99] sur les autres CR (par exemple les urgences ou les RCP), 0,98 [0,97, 0,99] sur les ordonnances et 0,78 [0,71, 0.85] sur les lettres. Sur les types secondaires, à titre d'exemple, le f1-score est de 0,89 sur les CRH-HDJ, de 1 sur les CR-ANAPATH et 0,97 sur les CR paramédicaux.
Conclusion
Nous présentons un nouvel algorithme de typage de documents médicaux efficace sur un très large volume, permettant de traiter les 300 000 nouveaux documents quotidiens à l'AP-HP.}
}
@article{JIANG2025103909,
title = {An adaptive confidence-based data revision framework for Document-level Relation Extraction},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103909},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103909},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002681},
author = {Chao Jiang and Jinzhi Liao and Xiang Zhao and Daojian Zeng and Jianhua Dai},
keywords = {Noisy annotations, Adaptive data revision, Denoising training, Document level Relation Extraction},
abstract = {Noisy annotations have become a key issue limiting Document-level Relation Extraction (DocRE). Previous research explored the problem through manual re-annotation. However, the handcrafted strategy is of low efficiency, incurs high human costs and cannot be generalized to large-scale datasets. To address the problem, we construct a confidence-based Revision framework for DocRE (ReD), aiming to achieve high-quality automatic data revision. Specifically, we first introduce a denoising training module to recognize relational facts and prevent noisy annotations. Second, a confidence-based data revision module is equipped to perform adaptive data revision for long-tail distributed relational facts. After the data revision, we design an iterative training module to create a virtuous cycle, which transforms the revised data into useful training data to support further revision. By capitalizing on ReD, we propose ReD-DocRED, which consists of 101,873 revised annotated documents from DocRED. ReD-DocRED has introduced 57.1% new relational facts, and concurrently, models trained on ReD-DocRED have achieved significant improvements in F1 scores, ranging from 6.35 to 16.55. The experimental results demonstrate that ReD can achieve high-quality data revision and, to some extent, replace manual labeling.11The ReD-DocRED is available at https://github.com/jc4357/ReD-DocRED.}
}
@article{MISHRA2020105002,
title = {A bacterial phyla dataset for protein function prediction},
journal = {Data in Brief},
volume = {28},
pages = {105002},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.105002},
url = {https://www.sciencedirect.com/science/article/pii/S2352340919313575},
author = {Sarthak Mishra and Yash Pratap Rastogi and Suraiya Jabin and Punit Kaur and Mohammad Amir and Shabanam Khatoon},
keywords = {Reviewed protein, Unreviewed protein, Motif, Physicochemical features, Sequence-based features, Annotation based features, Function prediction, Molecular function},
abstract = {Protein function prediction has been the most worked upon and the most challenging problem for computational biologists. The vast majority of known proteins have yet not been characterised experimentally, and there is significant gap between their structures and functions. New un-annotated sequences are being added to the public protein databases (e.g. UniprotKB) at an enormous pace [1]. Such proteins with unknown functions might play key role in the metabolism, growth and development regulation. Thus, if functions of unknown proteins left undiscovered, researchers may skip important information(s). Based on their sequence, structure, evolutionary history, and their association with other proteins, tools of computational biology can provide insights into the function of proteins [2]. For proteins with well characterised close relatives, it is trivial to infer function. Orphan proteins without discernible sequence relatives present a greater challenge [3]. Here the task of experimental characterisation is blind and becomes unwieldy. It is highly unlikely that all known proteins will ever be completely experimentally characterised [4]. Thus, there is an emergent need to develop fast and accurate computational approaches to fulfil this requirement. Towards this end, we prepared a dataset for protein function prediction by extracting protein sequences and annotations of reviewed prokaryotic proteins (total count 323,719 as accessed on date March 10, 2019) belonging to 9 bacterial phyla Actinobacteria, Bacteroidetes, Chlamydiae, Cyanobacteria, Firmicutes, Fusobacteria, Proteobacteria, Spirochaetes and Tenericutes. Corresponding to the most frequent 1739 Gene Ontology (Molecular Function) terms, samples were filtered, and 171,212 proteins were retrieved for feature generation. The Dataset was generated by calculating the sequence, sub-sequence, physiochemical, annotation-based features for each 171,212 reviewed proteins using method in [10]. These features constitute a total of 9890 attributes for each sequence of protein along with 1739 Gene Ontology terms. Each protein sequence is assigned one or more of 1739 Gene Ontology (Molecular Function) term as its target label. The Dataset contains the Entry and Entry name of each sequence corresponding to UniprotKB Database. This dataset being huge in size (171,212 samples X 9890 features, 1739 classes with multiple values) and equipped with enough number of positive and negative samples of each 1739 class, is good for testing efficiency of any upcoming deep learning models [5]. We divided the full dataset of 171,212 reviewed proteins in the ratio 3:1 to form Train/Test dataset 1; train dataset with 128,409 samples and test dataset with 42,803 samples to facilitate training of a deep learning model. The train and test datasets are stratified to contain good proportion of each 1739 classes. We then prepared a dataset 2 of pathogenic unreviewed proteins of the 9 bacterial phyla each with 9890 features same as train/train dataset of reviewed proteins but without target labels in order to predict their functions using deep learning model proposed in [5].}
}
@article{WU2024101596,
title = {Research on personalized assessment Information of students' mathematical competency based on cognitive diagnosis},
journal = {Thinking Skills and Creativity},
volume = {53},
pages = {101596},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101596},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124001342},
author = {Xiaopeng Wu and Tianshu Xu and Rongxiu Wu},
keywords = {TIMSS, Mathematics competency, Cognitive diagnosis, Chinese students, International comparison},
abstract = {With a set of the Trends in International Mathematics and Science Study (TIMSS) 2015 test items as the assessment tool, this study constructed a mathematics competency assessment framework composed of eight attributes, and formed a Q matrix using data of 4,733 students in the four provinces (cities) of mainland China. Through model comparisons, this study selected the mixed model and made in-depth analyses of attribute mastery from four aspects: international comparative analysis, learning path analysis, learning progression construction, and personalized report development. The study found obvious advantages in most attributes of Chinese students’ basic mathematics competencies. The learning path was rich with large knowledge status distributed in the second, third, and fourth level of learning progression. It provided a basis for a detailed understanding of the basic mathematics competencies of Chinese students and their current situation in international education. It also provided a methodological basis for the cognitive diagnosis assessment of students’ mathematics competencies.}
}
@article{ABISHEKA2024102257,
title = {T-SRE: Transformer-based semantic Relation extraction for contextual paraphrased plagiarism detection},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {10},
pages = {102257},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102257},
url = {https://www.sciencedirect.com/science/article/pii/S131915782400346X},
author = {Pon Abisheka and C. Deisy and P. Sharmila},
keywords = {Plagiarism, Named Entity Recognition, Dependency Parsing, Transformer, Concept Extraction},
abstract = {Plagiarism has become a pervasive issue in academics and professionals to safeguard academic integrity and intellectual property rights. The escalating sophistication of plagiarized content through semantic manipulation and structural reorganization poses significant challenges to existing detection systems that rely primarily on lexical similarity measures. The proposed T-SRE (Transformer-based Semantic Relation Extraction), a novel framework addresses the limitations of traditional n-gram and string-matching approaches by leveraging deep semantic analysis. The proposed framework combines Dependency Parsing (DP) for syntactic relationship mapping and Named Entity Recognition (NER) for contextual entity identification, augmented by a transformer-based neural network that captures long-range contextual dependencies. This learning methodology incorporates three key components: a position-aware word reordering algorithm, Levenshtein distance metric for structural similarity, and contextual word embeddings for semantic preservation detection. The proposed T-SRE enhances text structure recognition by combining position-aware reordering with semantic preservation through ensemble learning. The system implements a hierarchical classification scheme that quantifies plagiarism severity through a four-tier taxonomy: heavy, low, non-plagiarized and verbatim copy. The Udacity benchmark dataset showcases the model’s superior detection capabilities, achieving 92% precision, 89% recall, and an F1-score of 90.5%, particularly in lightweight textual modifications.The framework achieves a granularity score of 1.28, outperforming existing approaches.}
}
@article{PERSAK201947,
title = {Beyond public punitiveness: The role of emotions in criminal law policy},
journal = {International Journal of Law, Crime and Justice},
volume = {57},
pages = {47-58},
year = {2019},
issn = {1756-0616},
doi = {https://doi.org/10.1016/j.ijlcj.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1756061618303732},
author = {Nina Persak},
keywords = {Emotion, Criminalisation, Criminal law policy, Public sentiment, Legitimacy, Justice},
abstract = {The article examines the existing and potential role of emotions in the criminal law-making and criminal policy. It aims to inspect which emotions, if any, are more acceptable for influencing criminal policy and to what extent emotions could legitimately intervene in criminalisation processes. It first analyses the ways in which emotion has already penetrated into the criminal law, criminal justice and criminalisation. Next, it inspects the various characteristics of emotions, specifically those that are central in distinguishing between good and bad candidates for influencing criminal law policy, demonstrating that certain negative, highly intense, irrational and unstable or short-lived emotions can make bad law, as do atypical cases. The article then sketches a theoretical framework, composed of the requirements that should be fulfilled before any emotion could justifiably influence criminal law-making and of the further limits to such an enterprise. It concludes with recommendations and some thoughts on further research.}
}
@article{FURUYAMA2022,
title = {Rapid Protection from COVID-19 in Nonhuman Primates Vaccinated Intramuscularly but Not Intranasally with a Single Dose of a Vesicular Stomatitis Virus-Based Vaccine},
journal = {mBio},
volume = {13},
number = {1},
year = {2022},
issn = {2150-7511},
doi = {https://doi.org/10.1128/mbio.03379-21},
url = {https://www.sciencedirect.com/science/article/pii/S2150751122007699},
author = {Wakako Furuyama and Kyle Shifflett and Amanda N. Pinski and Amanda J. Griffin and Friederike Feldmann and Atsushi Okumura and Tylisha Gourdine and Allen Jankeel and Jamie Lovaglio and Patrick W. Hanley and Tina Thomas and Chad S. Clancy and Ilhem Messaoudi and Kyle L. O’Donnell and Andrea Marzi and Anne Moscona},
keywords = {VSV, vesicular stomatitis virus, SARS-CoV-2, i.m., i.n., rhesus macaques},
abstract = {The vesicular stomatitis virus (VSV) vaccine platform rose to fame in 2019, when a VSV-based Ebola virus (EBOV) vaccine was approved by the European Medicines Agency and the U.S. Food and Drug Administration for human use against the deadly disease.
ABSTRACT
The ongoing pandemic of coronavirus (CoV) disease 2019 (COVID-19) continues to exert a significant burden on health care systems worldwide. With limited treatments available, vaccination remains an effective strategy to counter transmission of severe acute respiratory syndrome CoV 2 (SARS-CoV-2). Recent discussions concerning vaccination strategies have focused on identifying vaccine platforms, number of doses, route of administration, and time to reach peak immunity against SARS-CoV-2. Here, we generated a single-dose, fast-acting vesicular stomatitis virus (VSV)-based vaccine derived from the licensed Ebola virus (EBOV) vaccine rVSV-ZEBOV, expressing the SARS-CoV-2 spike protein and the EBOV glycoprotein (VSV-SARS2-EBOV). Rhesus macaques vaccinated intramuscularly (i.m.) with a single dose of VSV-SARS2-EBOV were protected within 10 days and did not show signs of COVID-19 pneumonia. In contrast, intranasal (i.n.) vaccination resulted in limited immunogenicity and enhanced COVID-19 pneumonia compared to results for control animals. While both i.m. and i.n. vaccination induced neutralizing antibody titers, only i.m. vaccination resulted in a significant cellular immune response. RNA sequencing data bolstered these results by revealing robust activation of the innate and adaptive immune transcriptional signatures in the lungs of i.m. vaccinated animals only. Overall, the data demonstrate that VSV-SARS2-EBOV is a potent single-dose COVID-19 vaccine candidate that offers rapid protection based on the protective efficacy observed in our study.
IMPORTANCE The vesicular stomatitis virus (VSV) vaccine platform rose to fame in 2019, when a VSV-based Ebola virus (EBOV) vaccine was approved by the European Medicines Agency and the U.S. Food and Drug Administration for human use against the deadly disease. Here, we demonstrate the protective efficacy of a VSV-EBOV-based COVID-19 vaccine against challenge in nonhuman primates (NHPs). When a single dose of the VSV-SARS2-EBOV vaccine was administered intramuscularly (i.m.), the NHPs were protected from COVID-19 within 10 days. In contrast, if the vaccine was administered intranasally, there was no benefit from the vaccine and the NHPs developed pneumonia. The i.m. vaccinated NHPs quickly developed antigen-specific IgG, including neutralizing antibodies. Transcriptional analysis highlighted the development of protective innate and adaptive immune responses in the i.m. vaccination group only.}
}
@incollection{LI2024265,
title = {Chapter 10 - Conclusions and future perspectives},
editor = {Shufei Li and Pai Zheng and Lihui Wang},
booktitle = {Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing},
publisher = {Elsevier},
pages = {265-279},
year = {2024},
isbn = {978-0-443-13943-7},
doi = {https://doi.org/10.1016/B978-0-44-313943-7.00017-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044313943700017X},
author = {Shufei Li and Pai Zheng and Lihui Wang},
keywords = {Book chapter review, Conclusions, Challenges and future perspectives, Proactive HRC, Human-centric smart manufacturing},
abstract = {Based on the elaborative exposition of Proactive HRC in the previous chapters, the concluding chapter summarizes the key discoveries and contributions made in this book. The literature analysis, connotation, methodology, and demonstration of Proactive HRC play a crucial role in today's ever-increasing transition of human-centric smart manufacturing. Meanwhile, multiple pressing challenges and future research directions of Proactive HRC are highlighted here to welcome more open discussions and in-depth research and development. These encompass a wide array of dimensions, such as the evolution of its intelligent capabilities, the human-in-the-loop control, and the development of evaluation criteria.}
}
@article{HENKE2024168455,
title = {Reporting and reproducibility: Proteomics of fish models in environmental toxicology and ecotoxicology},
journal = {Science of The Total Environment},
volume = {912},
pages = {168455},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.168455},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723070833},
author = {Abigail N. Henke and Srikhar Chilukuri and Laura M. Langan and Bryan W. Brooks},
keywords = {Proteomics, Toxicology, Fish, Precision ecotoxicology, Alternative vertebrate model},
abstract = {Environmental toxicology and ecotoxicology research efforts are employing proteomics with fish models as New Approach Methodologies, along with in silico, in vitro and other omics techniques to elucidate hazards of toxicants and toxins. We performed a critical review of toxicology studies with fish models using proteomics and reported fundamental parameters across experimental design, sample preparation, mass spectrometry, and bioinformatics of fish, which represent alternative vertebrate models in environmental toxicology, and routinely studied animals in ecotoxicology. We observed inconsistencies in reporting and methodologies among experimental designs, sample preparations, data acquisitions and bioinformatics, which can affect reproducibility of experimental results. We identified a distinct need to develop reporting guidelines for proteomics use in environmental toxicology and ecotoxicology, increased QA/QC throughout studies, and method optimization with an emphasis on reducing inconsistencies among studies. Several recommendations are offered as logical steps to advance development and application of this emerging research area to understand chemical hazards to public health and the environment.}
}
@article{METALLO2018298,
title = {Understanding business model in the Internet of Things industry},
journal = {Technological Forecasting and Social Change},
volume = {136},
pages = {298-306},
year = {2018},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518301082},
author = {Concetta Metallo and Rocco Agrifoglio and Francesco Schiavone and Jens Mueller},
keywords = {Business model, Canvas, Internet of Things},
abstract = {This research presents the results of an exploratory study of how organisations operating in the Internet of Things (IoT) industry are building and innovating their business model (BM). Using an explorative sequential approach through the multiple-case study method, we apply the “Canvas BM” framework to explore the BM of three companies operating in IoT industry, namely Intel, Solair, and Apio. The paper finds the most important building blocks - key activities, key resources, and value proposition - and most critical related factors enabling IoT-oriented organisations to create and capture value. Furthermore, our results also suggest that the main difference in the processes of BM building and innovation depend on the different capabilities and competencies possessed by organisations. This study therefore advances the theoretical understanding of the critical factors for the value creation process in the IoT industry's organisations and offers interesting implications for management theory and practice.}
}
@article{ZHANG2023103961,
title = {Infer unseen from seen: Relation regularized zero-shot visual dialog},
journal = {Journal of Visual Communication and Image Representation},
volume = {97},
pages = {103961},
year = {2023},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2023.103961},
url = {https://www.sciencedirect.com/science/article/pii/S1047320323002110},
author = {Zefan Zhang and Shun Li and Yi Ji and Chunping Liu},
keywords = {Visual dialog, Zero-shot learning, Attention},
abstract = {The Visual Dialog task requires retrieving the correct answer based on detected objects, a current question, and history dialogs. However, in real-world scenarios, most existing models face the hard-positive problem and are unable to reason about unseen features, which limits their generalization ability. To address this issue, we propose two Relation Regularized Modules (RRM) in this article. The first is the Visual Relation Regularized Module (VRRM), which seeks known visual features that have semantic relations with unknown visual features and leverages these known features to assist in understanding the unknown features. The second is the Text Relation Regularized Module (TRRM), which enhances the keywords in the answers to strengthen the understanding of unknown text features. To evaluate the effectiveness of these modules, we propose two zero-shot Visual Dialog splits for verification: Visual Zero-shot VisDial with unseen visual features and Text Zero-shot VisDial with unseen answers. Experimental results demonstrate that our proposed modules achieve state-of-the-art performance in zero-shot Visual Dialog with unseen visual features and unseen answers, while also producing comparable results on the benchmark VisDial v1.0 test dataset.}
}
@article{REN2024109631,
title = {Integrating UAV, UGV and UAV-UGV collaboration in future industrialized agriculture: Analysis, opportunities and challenges},
journal = {Computers and Electronics in Agriculture},
volume = {227},
pages = {109631},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109631},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924010226},
author = {Zhigang Ren and Han Zheng and Jian Chen and Tao Chen and Pengyang Xie and Yunzhe Xu and Jiaming Deng and Huanzhe Wang and Mingjiang Sun and Wenchi Jiao},
keywords = {UAV, UGV, UAV-UGV collaboration, Unmanned systems, Industrialized agriculture},
abstract = {Industrialized agriculture is the direction of future agricultural development, which is developing in the direction of scale, diversification, unmanned and integration. The cooperative operation of UAV, UGV and UAV-UGV is a hot topic in the field of intelligent agricultural multi-machine research. However, at present, most of the research projects have not systematically given the solutions of UAV, UGV and UAV-UGV collaborative application in the future industrialized agriculture. Therefore, we propose the development model of future industrialized agriculture, which derives the key technologies and applications of agricultural UAV, UGV and UAV-UGV collaboration. We summarize and discuss the difficulties and innovative design of the application of UAV, UGV and UAV-UGV collaboration technology in the future industrialized environment, and analyze the opportunities and challenges of the application of UAV, UGV and UAV-UGV collaboration technology in combination with future industrialized agricultural production. Finally, we describe that more technologies (multi-modal sensing technology, embodied intelligent control technology, edge computing technology, end-edge cloud collaborative management and control technology, virtual reality, augmented reality, etc.) are the future research directions for the application of UAV, UGV and UAV-UGV collaboration in industrialized agriculture.}
}
@incollection{2024II1,
title = {Index},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {II1-II29},
year = {2024},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.20002-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230200027}
}
@article{SHAMSADDINI2021508,
title = {Impact of Antibiotic Resistance Genes in Gut Microbiome of Patients With Cirrhosis},
journal = {Gastroenterology},
volume = {161},
number = {2},
pages = {508-521.e7},
year = {2021},
issn = {0016-5085},
doi = {https://doi.org/10.1053/j.gastro.2021.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S001650852100634X},
author = {Amirhossein Shamsaddini and Patrick M. Gillevet and Chathur Acharya and Andrew Fagan and Edith Gavis and Masoumeh Sikaroodi and Sara McGeorge and Alexander Khoruts and Somaya Albhaisi and Michael Fuchs and Richard K. Sterling and Jasmohan S. Bajaj},
keywords = {Ascites, Hepatic Encephalopathy, Rifaximin, Infections, Machine Learning},
abstract = {Background and aims
Cirrhosis is associated with changes in intestinal microbiota that can lead to hepatic encephalopathy (HE) and infections, especially with antibiotic-resistant organisms. However, the impact of gut microbial antibiotic resistance gene (ARG) burden on clinical outcomes is unclear. The aims of the study were to determine the impact of ARGs in cirrhosis-related gut metagenome on outcomes and disease progression, study the effect of rifaximin on ARG burden, and compare ARGs in cirrhosis with chronic kidney disease (CKD) and diabetes.
Methods
In outpatients with cirrhosis who underwent metagenomics, we evaluated change in ARG abundances with progression and their multivariable impact on 90-day hospitalizations and deaths over 1 year. We also studied ARGs pre- and 8 weeks post-rifaximin in patients with compensated cirrhosis in an open-label trial. Finally, ARGs from CKD and diabetes studies were compared with cirrhosis on machine learning.
Results
A total of 163 patients with cirrhosis (43 compensated, 20 ascites-only, 30 HE-only, 70 both) and 40 controls were included. ARG abundances were higher in cirrhosis versus controls and worsened with advancing cirrhosis severity; 44 patients were hospitalized and 14 died. ARG abundances were associated with hospitalizations and mortality while controlling for cirrhosis complications, medications, and demographics. Rifaximin trial: ARG abundance patterns were minimally affected in 19 patients post-rifaximin. CKD/diabetes comparison: ARG abundance patterns in cirrhosis are distinguishable on machine learning and include more gram-positive ARGs.
Conclusions
Cirrhosis is associated with high gut microbial ARG gene burden compared with controls, which worsens with disease progression and may be different from CKD and diabetes. ARGs are not affected by rifaximin and are associated with hospitalizations and death.}
}
@article{KALPOKIENE2023102197,
title = {Creative encounters of a posthuman kind – anthropocentric law, artificial intelligence, and art},
journal = {Technology in Society},
volume = {72},
pages = {102197},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102197},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23000027},
author = {Julija Kalpokiene and Ignas Kalpokas},
keywords = {Anthropocentrism, Artificial intelligence, Creativity, Copyright},
abstract = {Artificial Intelligence (AI) is becoming an increasingly transformative force in human life. Crucially, its impact is already extending beyond automation of routine tasks and encroaching on creativity – a domain once seen as exclusively human. Hence, this article first surveys the discriminatory and exploitative underpinnings of the anthropocentric thinking that lies beyond attempts at sidelining the creative capacities of AI. Next, four different approaches to creativity and art are analyzed, ultimately conceptualizing art-ness as externally ascribed. Ultimately, the article moves to one way of such ascription – copyrightability – demonstrating the anthropocentric thinking behind attempts to both deny and award copyright protection to AI-generated content. Moreover, it transpires that human authors are under threat whichever of such strategies ends up dominant.}
}
@article{JONES201976,
title = {Dissolving the stiff upper lip: Opportunities and challenges for the mainstreaming of therapeutic jurisprudence in the United Kingdom},
journal = {International Journal of Law and Psychiatry},
volume = {63},
pages = {76-84},
year = {2019},
note = {Therapeutic Jurisprudence: Today & Tomorrow},
issn = {0160-2527},
doi = {https://doi.org/10.1016/j.ijlp.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0160252718300359},
author = {Emma Jones and Anna Kawalek},
keywords = {Therapeutic jurisprudence, United Kingdom, Legal education, Legal profession, Problem-solving courts},
abstract = {Although therapeutic jurisprudence (“TJ”) is increasingly well-established internationally, particularly within the United States of America (“US”), to date it remains relatively unacknowledged within the United Kingdom (“UK”). This article will explore the opportunities presented within contemporary UK society for the greater promotion, and eventual mainstreaming, of TJ. It will also consider the challenges faced during this process and how best to overcome these. Its first key area of focus will be upon the potential role of legal education in the UK in educating law students (and academics) about TJ, considering which approaches are likely to be most effective in incorporating TJ perspectives, at what stage this should occur and to what extent TJ is likely to impact on the existing curricula at a time when proposed changes relating to entry into the legal profession are heavily influencing the work of Law Schools. The article will then move on to consider the receptiveness of the UK legal profession to the TJ paradigm in light of recent attempts to move to a competency-based approach to practice and to reconceptualise professionalism to meet the challenges of increasing fragmentation and corporatisation. The third key area it will explore is the UK's recent plans to reintroduce problem-solving courts (“PSCs”) into its criminal justice system. The authors will discuss the downfall of the six UK Drug Court (“DC”) pilots originally established in 2005 theorising upon their failures and reflecting upon whether the current UK criminal justice system is truly able to support a fresh round of PSC initiatives. The article will end with recommendations for ways in which the international TJ community should begin the process of mainstreaming TJ within the UK. It will conclude that there are currently significant opportunities to be utilised, but that this requires significant commitment and mobilisation amongst existing TJ scholars and practitioners.}
}
@article{CLEMENT2021721,
title = {Pleiotropic consequences of metabolic stress for the major histocompatibility complex class II molecule antigen processing and presentation machinery},
journal = {Immunity},
volume = {54},
number = {4},
pages = {721-736.e10},
year = {2021},
issn = {1074-7613},
doi = {https://doi.org/10.1016/j.immuni.2021.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1074761321000856},
author = {Cristina C. Clement and Padma P. Nanaware and Takahiro Yamazaki and Maria Pia Negroni and Karthik Ramesh and Kateryna Morozova and Sangeetha Thangaswamy and Austin Graves and Hei Jung Kim and Tsai Wanxia Li and Marco Vigano’ and Rajesh K. Soni and Massimo Gadina and Harley Y. Tse and Lorenzo Galluzzi and Paul A. Roche and Lisa K. Denzin and Lawrence J. Stern and Laura Santambrogio},
keywords = {MHC class II, diabetes, advanced glycation end products, CD4 T cells, dendritic cells, hyperinsulinemia, obesity, antigen processing and presentation, oxidative posttranslational modifications},
abstract = {Summary
Hyperglycemia and hyperlipidemia are often observed in individuals with type II diabetes (T2D) and related mouse models. One dysmetabolic biochemical consequence is the non-enzymatic reaction between sugars, lipids, and proteins, favoring protein glycation, glycoxidation, and lipoxidation. Here, we identified oxidative alterations in key components of the major histocompatibility complex (MHC) class II molecule antigen processing and presentation machinery in vivo under conditions of hyperglycemia-induced metabolic stress. These modifications were linked to epitope-specific changes in endosomal processing efficiency, MHC class II-peptide binding, and DM editing activity. Moreover, we observed some quantitative and qualitative changes in the MHC class II immunopeptidome of Ob/Ob mice on a high-fat diet compared with controls, including changes in the presentation of an apolipoprotein B100 peptide associated previously with T2D and metabolic syndrome-related clinical complications. These findings highlight a link between glycation reactions and altered MHC class II antigen presentation that may contribute to T2D complications.}
}
@article{LI2020102835,
title = {UVB induces cutaneous squamous cell carcinoma progression by de novo ID4 methylation via methylation regulating enzymes},
journal = {EBioMedicine},
volume = {57},
pages = {102835},
year = {2020},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2020.102835},
url = {https://www.sciencedirect.com/science/article/pii/S2352396420302103},
author = {Liming Li and Fengjuan Li and Yudong Xia and Xueyuan Yang and Qun Lv and Fang Fang and Qiang Wang and Wenbo Bu and Yan Wang and Ke Zhang and Yi Wu and Junfang Shen and Mingjun Jiang},
keywords = {Cutaneous squamous cell carcinoma, Ultraviolet rays, DNA (cytosine-5-)-methyltransferase, DNA-binding proteins, Ten-eleven translocation, Methylation},
abstract = {Background
Little is known about whether UVB can directly influence epigenetic regulatory pathways to induce cutaneous squamous cell carcinoma (CSCC). This study aimed to identify epigenetic-regulated signalling pathways through global methylation and gene expression profiling and to elucidate their function in CSCC development.
Methods
Global DNA methylation profiling by reduced representation bisulfite sequencing (RRBS) and genome-wide gene expression analysis by RNA sequencing (RNA-seq) in eight pairs of matched CSCC and adjacent normal skin tissues were used to investigate the potential candidate gene(s). Clinical samples, animal models, cell lines, and UVB irradiation were applied to validate the mechanism and function of the genes of interest.
Findings
We identified the downregulation of the TGF-β/BMP-SMAD-ID4 signalling pathway in CSCC and increased methylation of inhibitor of DNA binding/differentiation 4 (ID4). In normal human and mouse skin tissues and cutaneous cell lines, UVB exposure induced ID4 DNA methylation, upregulated DNMT1 and downregulated ten-eleven translocation (TETs). Similarly, we detected the upregulation of DNMT1 and downregulation of TETs accompanying ID4 DNA methylation in CSCC tissues. Silencing of DNMT1 and overexpression of TET1 and TET2 in A431 and Colo16 cells led to increased ID4 expression. Finally, we showed that overexpression of ID4 reduced cell proliferation, migration, and invasion, and increased apoptosis in CSCC cell lines and reduced tumourigenesis in mouse models.
Interpretation
The results indicate that ID4 is downregulated by UVB irradiation via DNA methylation. ID4 acts as a tumour suppressor gene in CSCC development.
Funding
CAMS Innovation Fund for Medical Sciences (CIFMS) (2016-I2M-3-021, 2017-I2M-1-017), the Natural Science Foundation of Jiangsu Province (BK20191136), and the Fundamental Research Funds for the Central Universities (3332019104).}
}
@incollection{SUN202433,
title = {Chapter 2 - Graph database basics and principles},
editor = {Ricky Sun},
booktitle = {The Essential Criteria of Graph Databases},
publisher = {Elsevier},
pages = {33-100},
year = {2024},
isbn = {978-0-443-14162-1},
doi = {https://doi.org/10.1016/B978-0-443-14162-1.00001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443141621000015},
author = {Ricky Sun},
keywords = {Storage engine, Compute engine, Native graph, Graph query language, Full-text index},
abstract = {In this chapter, the basic concepts and principles of graph database computing, storage, and query will be covered in three sections. A key concept introduced is graph computing being a first-class citizen in graph database. It is the single most important pillar to design truly high-performance (or even real-time) graph database architecture. To help readers better understand graph computing engines, core data structures, and application scenarios are introduced in the first-second. In the second section, typical storage structures considered useful for graph storage engine design are outlined particularly around B-tree, LSM-tree, Bloom Filter, Index-free Adjacency, etc. Another key concept introduced in the second section is graph data modeling, different modeling mechanisms may have different scenario and architecture implications. In the last section, five examples are offered to help readers get familiar with GQL, and understand why GQL is intended to replace SQL for the incredible capacity of GQL.}
}
@article{FEITOSA2024112112,
title = {Mining for cost awareness in the infrastructure as code artifacts of cloud-based applications: An exploratory study},
journal = {Journal of Systems and Software},
volume = {215},
pages = {112112},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112112},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001572},
author = {Daniel Feitosa and Matei-Tudor Penca and Massimiliano Berardi and Rares-Dorian Boza and Vasilios Andrikopoulos},
keywords = {Cloud computing, Cost awareness, Mining software repositories, Cloud orchestration},
abstract = {Context:
Cloud computing’s rise as the primary platform for software development and delivery is largely driven by the potential cost savings. However, it is surprising that no empirical evidence has been collected to determine whether cost awareness permeates the development process and how it manifests in practice.
Objective:
This study aims to provide empirical evidence of cost awareness by mining open source repositories of cloud-based applications. The focus is on Infrastructure-as-Code artifacts that automate software (re)deployment on the cloud.
Methods:
A systematic examination of 152735 repositories yielded 2010 relevant hits. We then analyzed 538 relevant commits and 208 relevant issues using inductive and deductive coding and corroborated findings with discussions from Stack Overflow.
Results:
The findings indicate that developers are not only concerned with the cost of their application deployments but also take actions to reduce these costs beyond selecting cheaper cloud services. We also identify research areas for future consideration.
Conclusion:
Although we focus on a particular Infrastructure-as-Code technology (Terraform), the findings can be applicable to cloud-based application development in general. The provided empirical grounding can serve developers seeking to reduce costs through service selection, resource allocation, deployment optimization, and other techniques.}
}
@incollection{2023425,
title = {Index},
editor = {Ganji Purnachandra Nagaraju and Venkatesan Amouda and Ampasala {Dinakara Rao}},
booktitle = {Computational Methods in Drug Discovery and Repurposing for Cancer Therapy},
publisher = {Academic Press},
pages = {425-433},
year = {2023},
isbn = {978-0-443-15280-1},
doi = {https://doi.org/10.1016/B978-0-443-15280-1.09993-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152801099933}
}
@article{LI2024101536,
title = {Formulating a descriptive framework and tagging system for design futures practice cases: Enabling heuristic tool development for design education and creative inspiration},
journal = {Thinking Skills and Creativity},
volume = {53},
pages = {101536},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101536},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124000749},
author = {Tiantian Li and Zhiyong Fu},
keywords = {Creativity, Design futures, Case description framework, Tagging system, Heuristic design tool, Design pedagogy},
abstract = {There has been a growing trend to extend the domain of futures studies into practical realms by integrating materials, methodologies, and perspectives from design and the arts. Design Futures” is an emerging field that arises from the interaction between design and foresight, holding great potential for expansion and development. However, there is still a lack of heuristic design tools to empower the creation of practice cases in design futures. Due to the intricacies of the sub-genres of Design Futures practice cases, our prior work included the development of a universal case description framework with a quantitative tagging system to place the different genres of cases on a common ground for study. Firstly, the study established a global descriptive framework and tagging system (4 segments, 7 primary labels, and 36 secondary labels) around 5 Design Futures sub-types through a qualitative approach. Secondly, the study invited 109 participants from five different subjects areas to conduct an empirical study to validate the proposed framework and quantify the tagging system. The results show that the rationality of the framework is preliminarily confirmed; the strength of the correlation between genres and labels is weak; in terms of the quantification of labels, the study produces 62 valid secondary labels (positive 29, negative 33), accompanied by quantitative data as the basis for recommendation, forming a creativity-enabling, content-rich labeling system that can reflect the characteristics and differences between the many genres of Design Futures. Finally, the study proposes three modes of heuristic design (confirmatory, constructive and open-ended), which provide rules to guide the use and collocation of the tagging system. The results of this research have deepened the understanding of the concepts and structures of knowledge in the field of Design Futures, which will play a more extensive role in design pedagogy and creative inspiration contexts.}
}
@article{CHEN2024103518,
title = {CTIMD: Cyber threat intelligence enhanced malware detection using API call sequences with parameters},
journal = {Computers & Security},
volume = {136},
pages = {103518},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103518},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823004285},
author = {Tieming Chen and Huan Zeng and Mingqi Lv and Tiantian Zhu},
keywords = {Malware detection, API sequence, Cyber threat intelligence, Deep learning},
abstract = {Dynamic malware analysis that monitors the sequences of API calls of the program in a sandbox has been proven to be effective against code obfuscation and unknown malware. However, most existing works ignore the run-time parameters by only considering the API names, or lack an effective way to capture the correlations between parameter values and malicious activities. In this paper, we propose CTIMD, a deep learning based dynamic malware detection method, which integrates the threat knowledge from CTIs (Cyber Threat Intelligences) into the learning on API call sequences with run-time parameters. It first extracts IOCs (Indicators of Compromise) from CTIs and uses IOCs to assist the identification of the security-sensitive levels of API calls. Then, it embeds API calls and the associated security-sensitive levels into a unified feature space. Finally, it feeds the feature vector sequences into deep neural networks to train the malware detection model. We conducted experiments on two datasets. The experiment results show that CTIMD significantly outperforms existing methods depending on raw API call sequences (F1-score is improved by 4.0 %∼41.3 %), and also has advantage over existing state-of-the-art methods that consider both API calls and run-time parameters (F1-score is improved by 1.2 %∼6.5 %).}
}
@article{PANTZKE2023104079,
title = {Processing of carbon-reinforced construction materials releases PM2.5 inducing inflammation and (secondary) genotoxicity in human lung epithelial cells and fibroblasts},
journal = {Environmental Toxicology and Pharmacology},
volume = {98},
pages = {104079},
year = {2023},
issn = {1382-6689},
doi = {https://doi.org/10.1016/j.etap.2023.104079},
url = {https://www.sciencedirect.com/science/article/pii/S1382668923000200},
author = {Jana Pantzke and Arne Koch and Elias J. Zimmermann and Narges Rastak and Svenja Offer and Christoph Bisig and Stefanie Bauer and Sebastian Oeder and Jürgen Orasche and Petra Fiala and Michael Stintz and Christopher P. Rüger and Thorsten Streibel and Sebastiano {Di Bucchianico} and Ralf Zimmermann},
keywords = {Polycyclic aromatic hydrocarbons, Bisphenol A, Carbon fibre toxicity, Air-liquid interface, Transcriptome},
abstract = {Building demolition following domestic fires or abrasive processing after thermal recycling can release particles harmful for the environment and human health. To mimic such situations, particles release during dry-cutting of construction materials was investigated. A reinforcement material consisting of carbon rods (CR), carbon concrete composite (C³) and thermally treated C³ (ttC³) were physicochemically and toxicologically analyzed in monocultured lung epithelial cells, and co-cultured lung epithelial cells and fibroblasts at the air-liquid interface. C³ particles reduced their diameter to WHO fibre dimensions during thermal treatment. Caused by physical properties or by polycyclic aromatic hydrocarbons and bisphenol A found in the materials, especially the released particles of CR and ttC³ induced an acute inflammatory response and (secondary) DNA damage. Transcriptome analysis indicated that CR and ttC³ particles carried out their toxicity via different mechanisms. While ttC³ affected pro-fibrotic pathways, CR was mostly involved in DNA damage response and in pro-oncogenic signaling.}
}
@article{MU20212087,
title = {Gene function adjustment for carbohydrate metabolism and enrichment of rumen microbiota with antibiotic resistance genes during subacute rumen acidosis induced by a high-grain diet in lactating dairy cows},
journal = {Journal of Dairy Science},
volume = {104},
number = {2},
pages = {2087-2105},
year = {2021},
issn = {0022-0302},
doi = {https://doi.org/10.3168/jds.2020-19118},
url = {https://www.sciencedirect.com/science/article/pii/S0022030220310572},
author = {Y.Y. Mu and W.P. Qi and T. Zhang and J.Y. Zhang and S.Y. Mao},
keywords = {high-grain diet, carbohydrate activity enzyme, KEGG orthologous group, antibiotic resistance gene},
abstract = {ABSTRACT
The high-grain diets fed to ruminants generally alters the structure and function of rumen microbiota, resulting in variations of rumen fermentation patterns and the occurrence of subacute rumen acidosis (SARA). To clarify the microbial mechanism for carbohydrate metabolism during SARA, 8 ruminally cannulated Holstein cows in mid lactation were selected for a 3-wk experiment. The cows were randomly divided into 2 groups, fed either a conventional diet (CON; 40% concentrate; dry matter basis) or a high-grain diet (HG; 60% concentrate; dry matter basis). Compared with the CON diet, the HG diet reduced average daily pH (5.71 vs. 6.13), acetate concentration (72.56 vs. 78.44 mM), acetate ratio (54.81 vs. 65.24%), and the ratio of the concentrations of acetate to propionate (1.87 vs. 3.21) but increased the concentrations of total volatile fatty acids (133.03 vs. 120.22 mM), propionate (41.32 vs. 24.71 mM), and valerate (2.46 vs. 1.68 mM) and the propionate ratio (30.51 vs. 20.47%). Taxonomic analysis indicated that the HG cows had a higher relative abundance of Ruminococcus, Eubacterium, Selenomonas, Ruminobacter, Succinimonas, Methanomicrobium, and Methanocaldococcus accompanied by a lower relative abundance of unclassified Firmicutes, unclassified Bacteroidetes, Bacteroides, Fibrobacter, Alistipes, Candidatus Methanoplasma, Methanomassiliicoccus, and Methanolobus. Carbohydrate-active enzyme annotation suggested that there was enriched abundance of glycosyltransferases (GT) 2, glycoside hydrolase (GH) 13, GH24, carbohydrate-binding module (CBM) 26, GH73, GH25, CBM12, GH23, GT8, CBM50, and GT9 and reduced abundance of GH78, GH31, S-layer homology, GH109, carbohydrate esterase 1, GH3, carbohydrate esterase 10, and GH43 in the HG group. Functional profiling revealed that the HG feeding mainly downregulated the pentose phosphate pathway of carbohydrate catabolism, acetate metabolism, propionate metabolism (succinate pathway), and methane metabolism, whereas it upregulated the Embden-Meyerhof-Parnas and Entner-Doudoroff pathways of glycolysis and the citrate cycle. Additionally, the HG feeding promoted the abundance of various antibiotic resistance genes and antimicrobial resistance gene families. These results elucidated the structure and function adjustment of rumen microbiota for carbohydrate metabolism and summarized the enrichment of rumen antibiotic resistance genes under the HG feeding, which expands our understanding of the mechanism underlying the response of rumen microbiota to SARA in dairy cattle.}
}
@article{ZUCCALA2017826,
title = {Enabling Energy Smart Cities through Urban Sharing Ecosystems},
journal = {Energy Procedia},
volume = {111},
pages = {826-835},
year = {2017},
note = {8th International Conference on Sustainability in Energy and Buildings, SEB-16, 11-13 September 2016, Turin, Italy},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.03.245},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217302783},
author = {Maurilio Zuccalà and Emiliano Sergio Verga},
keywords = {Digital Ecosystem, Urban Sharing Platform, API Economy, Interoperability, Smart City, Sharing Cities, Internet of Things},
abstract = {In order to build real smart cities, heterogeneous data from different sources has to be properly collected, integrated and shared. In this paper, a real district scale example of urban sharing ecosystem based on coopetition is presented. This digital ecosystem enables data sharing that can be synergically applied to different sectors relevant to the urban context, e.g., energy and transportation, in order to create innovative solutions for energy monitoring, citizen engagement, and evaluation and monitoring at district and city level.}
}
@article{SHANG2022,
title = {Scrutiny of NolA and NodD1 Regulatory Roles in Symbiotic Compatibility Unveils New Insights into Bradyrhizobium guangxiense CCBAU53363 Interacting with Peanut (Arachis hypogaea) and Mung Bean (Vigna radiata)},
journal = {Microbiology Spectrum},
volume = {11},
number = {1},
year = {2022},
issn = {2165-0497},
doi = {https://doi.org/10.1128/spectrum.02096-22},
url = {https://www.sciencedirect.com/science/article/pii/S2165049722006151},
author = {Jiao Ying Shang and Pan Zhang and Yu Wen Jia and Yi Ning Lu and Yue Wu and Shuang Ji and La Chen and En Tao Wang and Wen Xin Chen and Xin Hua Sui},
keywords = {peanut bradyrhizobia, NolA, NodD1, nodulation, compatibility},
abstract = {The main findings of this study were that we clarified that the roles and essentiality of nodD1 and nolA are host dependent. Importantly, for the first time, NolA was found to positively regulate T3SS effector gene nopP to mediate incompatibility on mung bean.
ABSTRACT
Bradyrhizobium guangxiense CCBAU53363 efficiently nodulates peanut but exhibits incompatible interaction with mung bean. By comparing the common nod region with those of other peanut bradyrhizobia efficiently nodulating these two hosts, distinctive characteristics with a single nodD isoform (nodD1) and a truncated nolA were identified. However, the regulatory roles of NodD1 and NolA and their coordination in legume-bradyrhizobial interactions remain largely unknown in terms of explaining the contrasting symbiotic compatibility. Here, we report that nolA was important for CCBAU53363 symbiosis with peanut but restricted nodulation on mung bean, while nodD1 was dispensable for CCBAU53363 symbiosis with peanut but essential for nodulation on mung bean. Moreover, nolA exerted a cumulative contribution with nodD1 to efficient symbiosis with peanut. Additionally, mutants lacking nolA delayed nodulation on peanut, and both nolA and nodD1 were required for competitive nodule colonization. It is noteworth that most of the nodulation genes and type III secretion system (T3SS)-related genes were significantly downregulated in a strain 53ΔnodD1nolA mutant compared to wild-type strain CCBAU53363, and the downregulated nodulation genes also had a greater impact than T3SS-related genes on the symbiotic defect of 53ΔnodD1nolA on peanut, which was supported by a more severe symbiotic defect induced by 53ΔnodC than that with the 53ΔnodD1nopP, 53ΔnodD1rhcJ, and 53ΔnodD1ttsI mutants. NolA did not regulate nod gene expression but did regulate the T3SS effector gene nopP in an indirect way. Meanwhile, nolA, nodW, and some T3SS-related genes besides nopP were also demonstrated as new “repressors” that seriously impaired CCBAU53363 symbiosis with mung bean. Taken together, the roles and essentiality of nolA and nodD1 in modulating symbiotic compatibility are sophisticated and host dependent.
IMPORTANCE The main findings of this study were that we clarified that the roles and essentiality of nodD1 and nolA are host dependent. Importantly, for the first time, NolA was found to positively regulate T3SS effector gene nopP to mediate incompatibility on mung bean. Additionally, NolA does not regulate nod genes, which are activated by NodD1. nolA exerts a cumulative effect with nodD1 on CCBAU53363 symbiosis with peanut. These findings shed new light on our understanding of coordinated regulation of NodD1 and NolA in peanut bradyrhizobia with different hosts.}
}
@article{2024e1,
title = {AAE24 Abstracts of Research},
journal = {Journal of Endodontics},
volume = {50},
number = {5},
pages = {e1-e45},
year = {2024},
issn = {0099-2399},
doi = {https://doi.org/10.1016/S0099-2399(24)00237-1},
url = {https://www.sciencedirect.com/science/article/pii/S0099239924002371}
}
@article{PENG2023130706,
title = {Genetic and functional characterization of multiple thermophilic organosulfur-removal systems reveals desulfurization potentials for waste residue oil cleaning},
journal = {Journal of Hazardous Materials},
volume = {446},
pages = {130706},
year = {2023},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2022.130706},
url = {https://www.sciencedirect.com/science/article/pii/S030438942202502X},
author = {Chenchen Peng and Yukun Shi and Shuo Wang and Jingjing Zhang and Xuehua Wan and Yalin Yin and Dongxu Wang and Wei Wang},
keywords = {Thermophilic bacterium, Biodesulfurization, Monooxygenases, Waste residue oil, Hazardous organosulfur},
abstract = {Heavy oil and petroleum refining residues usually contain high concentrations of recalcitrant hazardous organosulfur compounds, causing long-term serious global environmental pollution during leakage and combustion. Research conducted here identified a unique thermophilic bacterium Parageobacillus thermoglucosidasius W-36 with the notable ability of waste residue oil desulfurization, utilization and tolerance of multiplex hazardous organosulfur pollutants. Genome information mining revealed multiple desulfurization systems in three organosulfur-utilizing gene clusters. Enzymatic characterization, phylogenetic relationships, transcriptional performance and structural prediction indicated four novel key monooxygenases for diverse organosulfur removal. Importantly, all monooxygenases shared obvious commonalities in the predicted tertiary structure backbone and catalytic characteristics of C-S bond cleavage, implying the potential of genetic engineering for broad-spectrum hazardous organosulfur removal. Therefore, this work demonstrated the important application potential of thermophilic bacteria as a promising alternative biodesulfurization way for waste residue oil cleaning.}
}
@article{UGLANOV2024728,
title = {An NLP-based framework for early identification of design reliability issues from heterogeneous automotive lifecycle data},
journal = {Procedia CIRP},
volume = {128},
pages = {728-733},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.05.098},
url = {https://www.sciencedirect.com/science/article/pii/S221282712400773X},
author = {Alexey Uglanov and Felician Campean and Amr Abdullatiff and Daniel Neagu and Aleksandr Doikin and David Delaux and Pascal Bonnaud},
keywords = {automotive warranty data, natural language processing, design informatics},
abstract = {Natural Language Processing is increasingly used in different areas of design and product development with varied objectives, from enhancing productivity to embedding resilience into systems. In this paper, we introduce a framework that draws on NLP algorithms and expert knowledge for the automotive engineering domain, to extract actionable insight for system reliability improvement from data available from the operational phase of the system. Specifically, we are looking at the systematic exploration and exploitation of automotive heterogeneous data sources, including both closed-source (such as warranty records) and open-source (e.g., social networks, chatrooms, recall systems) data, to extract and classify information about faults, with predictive capability for early detection of issues. We present a preliminary NLP-based framework for enhancing system knowledge representation to increase the effectiveness and robustness of information extraction from data, and discuss the temporal alignment of data sources and insight to improve prediction ability. We demonstrate the effectiveness of the proposed framework using real-world automotive data in a recall study for a vehicle lighting system and a particular manufacturer: four recall campaigns were identified leading to corrective actions by the warranty experts.}
}
@article{WU2024141921,
title = {Integrated transcriptomics and metabolomics analyses reveal the aerobic biodegradation and molecular mechanisms of 2,3′,4,4′,5-pentachlorodiphenyl (PCB 118) in Methylorubrum sp. ZY-1},
journal = {Chemosphere},
volume = {356},
pages = {141921},
year = {2024},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2024.141921},
url = {https://www.sciencedirect.com/science/article/pii/S0045653524008142},
author = {Yuxuan Wu and Minghan Zhu and Xiaofang Ouyang and Xin Qi and Zhanyu Guo and Yibo Yuan and Zhi Dang and Hua Yin},
keywords = {, Biodegradation, Polychlorinated biphenyls, Metabolomics, Transcriptome},
abstract = {2,3′,4,4′,5-pentachlorodiphenyl (PCB 118), a highly representative PCB congener, has been frequently detected in various environments, garnering much attention across the scientific community. The degradation of highly chlorinated PCBs by aerobic microorganisms is challenging due to their hydrophobicity and persistence. Herein, the biodegradation and adaptation mechanisms of Methylorubrum sp. ZY-1 to PCB 118 were comprehensively investigated using an integrative approach that combined degradation performance, product identification, metabolomic and transcriptomic analyses. The results indicated that the highest degradation efficiency of 0.5 mg L−1 PCB 118 reached 75.66% after seven days of inoculation when the bacteria dosage was 1.0 g L−1 at pH 7.0. A total of eleven products were identified during the degradation process, including low chlorinated PCBs, hydroxylated PCBs, and ring-opening products, suggesting that strain ZY-1 degraded PCB 118 through dechlorination, hydroxylation, and ring-opening pathways. Metabolomic analysis demonstrated that the energy supply and redox metabolism of strain ZY-1 was disturbed with exposure to PCB 118. To counteract this environmental stress, strain ZY-1 adjusted both the fatty acid synthesis and purine metabolism. The analysis of transcriptomics disclosed that multiple intracellular and extracellular oxidoreductases (e.g., monooxygenase, alpha/beta hydrolase and cytochrome P450) participated in the degradation of PCB 118. Besides, active efflux of PCB 118 and its degradation intermediates mediated by multiple transporters (e.g., MFS transporter and ABC transporter ATP-binding protein) might enhance bacterial resistance against these substances. These discoveries provided the inaugural insights into the biotransformation of strain ZY-1 to PCB 118 stress, illustrating its potential in the remediation of contaminated environments.}
}
@incollection{2022xi,
title = {About the editors and authors},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {xi-xxi},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234136000021}
}
@article{GONG2024110757,
title = {Graph-based insider threat detection: A survey},
journal = {Computer Networks},
volume = {254},
pages = {110757},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110757},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624005899},
author = {Yiru Gong and Susu Cui and Song Liu and Bo Jiang and Cong Dong and Zhigang Lu},
keywords = {Insider threat analysis, Graph model, Anomaly detection, Cyber security},
abstract = {Insider threat detection has been a significant topic in recent years. However, as network technology develops, the intranet becomes more complex. Therefore, simply matching attack patterns or using traditional machine learning methods (Logistic Regression, Gaussian-NB, Random Forest, etc.) does not work well. On the other hand, the graph structure can better adapt to intranet data, thus graph-based insider threat detection methods have become mainstream. In order to study the design and effectiveness of graph-based insider threat detection, in this paper, we conduct a systematic and comprehensive survey of existing related research. Specifically, we provide a framework and a taxonomy based on the detection process, classifying existing work from three aspects: data collection, graph construction, and graph anomaly detection. We conduct a quantitative analysis of existing representative graph methods and find that the models with more information have better performance. In particular, we discuss the scalability of existing methods to large-scale networks and their feasibility in real environments. Based on the survey results, we propose 7 pain points in this field and provide specific future research directions. Our survey will provide future researchers with a complete solution.}
}
@article{DASILVA2023106732,
title = {Leishmania infantum NTPDase1 and NTPDase2 play an important role in infection and nitric oxide production in macrophages},
journal = {Acta Tropica},
volume = {237},
pages = {106732},
year = {2023},
issn = {0001-706X},
doi = {https://doi.org/10.1016/j.actatropica.2022.106732},
url = {https://www.sciencedirect.com/science/article/pii/S0001706X22004247},
author = {Walmir {da Silva} and Isadora Cunha Ribeiro and Joice de Melo Agripino and Victor Hugo Ferraz {da Silva} and Luciana Ângelo {de Souza} and Tatiana Aparecida Oliveira and Gustavo Costa Bressan and Raphael de Souza Vasconcellos and Carole Dumas and Julie Pelletier and Jean Sévigny and Barbara Papadopoulou and Juliana Lopes Rangel Fietto},
keywords = {, ENTPDases (NTPDase1 and -2), CRISPR/Cas9 deletion, Macrophage  infection, NO production, ENTPDase activity},
abstract = {Leishmania infantum, the causative agent of American Visceral Leishmaniasis (VL), is known for its ability to modulate the host immune response to its own favor. Ecto-nucleoside triphosphate diphosphohydrolase (ENTPDase) represents a family of enzymes that hydrolyze nucleotides and are involved in nucleotide-dependent biological processes. L. infantum has two ENTPDases, namely LiNTPDase1 and LiNTPDase2. Here, we used genetic tools to overexpress or abolish the expression of LiNTPDase1 and -2 to assess their role in parasite growth in culture and macrophage infection. While LiNTPDase1 or 2-overexpressing clones showed no morphological or growth changes in promastigotes, LiNTPDase2 overexpression increased macrophage adhesion and infection by 50% and 30%, respectively. The individual LiNTPDase1 and 2 knockout mutants showed lag in growth profile, which was reversed by the addition of adenine and guanine to the culture media. Moreover, the morphology of the knockout mutants even in supplemented media was changed to an amastigote-like form. The double knockout of both genes was lethal and a mechanism of compensation of deletion of one isoform was detected in these mutants. Correspondingly, the absence of LiNTPDase1 or LiNTPDase2 led to a dramatic reduction in in vitro infection (∼90%). Interestingly, nitric oxide production was decreased in both knockout mutants during infection, which suggests that both LiNTPDases can inhibit macrophage responses against the parasite. Overall, our results show important roles of LiNTPDase1 and -2 concerning in vitro macrophage infection and reinforce their use as potential targets to control Leishmania infections.}
}
@article{RIGAS2024112093,
title = {Semantic interoperability for an AI-based applications platform for smart hospitals using HL7 FHIR},
journal = {Journal of Systems and Software},
volume = {215},
pages = {112093},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112093},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001389},
author = {Emmanouil S. Rigas and Paris Lagakis and Makis Karadimas and Evangelos Logaras and Dimitra Latsou and Magda Hatzikou and Athanasios Poulakidas and Antonis Billis and Panagiotis D. Bamidis},
keywords = {Semantic, Interoperability, FHIR, Smart hospital, Digital healthcare, Artificial intelligence, Application platform},
abstract = {The digitization of the healthcare domain has the potential to drastically improve healthcare services, reduce the time to diagnosis, and lower costs. However, digital applications for the healthcare domain need to be interoperable to maximize their potential. Additionally, with the rapid expansion of Artificial Intelligence (AI) and, specifically, Machine Learning (ML), large amounts of diverse types of data are being utilized. Thus, to achieve interoperability in such applications, the adoption of common semantic data models becomes imperative. In this paper, we describe the adoption of such a common semantic data model, using the well-known Health Level Seven Fast Health Interoperability Resources (HL7 FHIR) standard, in a platform that assists in the creation and storage of a plethora of AI-based applications for several medical conditions. The FHIR server’s efficiency is being showcased by using it in an application predicting coronary artery stenosis as well as for managing the platform’s key performance indicators.}
}
@article{ZHANG2021101364,
title = {Proteomic response strategies of Pediococcus pentosaceus R1 isolated from Harbin dry sausages to oxidative stress},
journal = {Food Bioscience},
volume = {44},
pages = {101364},
year = {2021},
issn = {2212-4292},
doi = {https://doi.org/10.1016/j.fbio.2021.101364},
url = {https://www.sciencedirect.com/science/article/pii/S2212429221004892},
author = {Huan Zhang and Chao Zhang and Haotian Liu and Qian Chen and Baohua Kong},
keywords = {Oxidative stress, , Antioxidative mechanisms, Proteomics},
abstract = {The tolerance strategies of Pediococcus pentosaceus R1 to sublethal dose of H2O2 induced-oxidative stress were investigated with proteomic approaches. The results revealed that a total of 117 proteins were identified as differentially expressed proteins. Eight antioxidant enzymes were upregulated expression to defence the oxidative stress, especially manganese catalase. Besides, 14 proteins participating in carbohydrate and energy metabolism, were significantly upregulated expression (P < 0.05), in which six proteins involved in the two major pathways, pentose phosphate pathway and pyruvate metabolism. Furthermore, the results showed that the expression of four proteins related to the repair of oxidatively damaged proteins was upregulated (P < 0.05); and the upregulated expression was also found in five ATP-binding cassette transporter-related proteins, which involved in defence oxidative stress in P. pentosaceus R1 by regulating the transmembrane transport of many substrates (P < 0.05). Conversely, the expression of 13 proteins related to nucleotide biosynthesis, and four proteins participating in Fe–S cluster assembly, was significantly downregulated (P < 0.05). This work will facilitate the understanding of the antioxidative mechanisms in P. pentosaceus R1 and will provide valuable information for further engineering oxidation-tolerant strains as meat starter cultures.}
}
@article{LI2020110250,
title = {Transcriptome profiling reveals the molecular processes for survival of Lysinibacillus fusiformis strain 15-4 in petroleum environments},
journal = {Ecotoxicology and Environmental Safety},
volume = {192},
pages = {110250},
year = {2020},
issn = {0147-6513},
doi = {https://doi.org/10.1016/j.ecoenv.2020.110250},
url = {https://www.sciencedirect.com/science/article/pii/S0147651320300890},
author = {Shi-Weng Li and Yi-Xuan Huang and Meng-Yuan Liu},
keywords = {, Petroleum degradation, Transcriptome},
abstract = {A bacterial strain designated Lysinibacillus fusiformis 15-4 was isolated from oil-free soil on the Qinghai-Tibet Plateau, which can grow well utilizing petroleum hydrocarbons as a carbon source at a lower temperature. To deeply characterize the molecular adaptations and metabolic processes of this strain when grown in a petroleum-containing environment, transcriptome analysis was performed. A total of 4664 genes and the expression of 3969 genes were observed in strain 15-4. When the strain was grown in petroleum-containing medium, 2192 genes were significantly regulated, of which 1312 (60%) were upregulated and 880 (40%) were downregulated. This strain degraded and adapted to petroleum via modulation of diverse molecular processes, including improvements in transporter activity, oxidoreductase/dehydrogenase activity, two-component system/signal transduction, transcriptional regulation, fatty acid catabolism, amino acid metabolism, and environmental stress responses. Many strain-specific genes were involved in the oxidation of hydrocarbon compounds, such as several luciferase family alkane monooxygenase genes, flavin-utilizing monooxygenase family genes, and flavoprotein-like family alkanesulfonate monooxygenase genes. Several cold shock protein genes were also induced suggesting adaptation to cold environments and the potential for petroleum degradation at low temperatures. The results obtained in this study may broaden our understanding of molecular adaptation of bacteria to hydrocarbon-containing environments and may provide valuable data for further study of L. fusiformis.}
}
@article{2024S1,
title = {Abstracts of the AMP Europe 2024 Congress},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {6, Supplement },
pages = {S1-S63},
year = {2024},
note = {Abstracts of AMP Europe 2024 Congress},
issn = {1525-1578},
doi = {https://doi.org/10.1016/S1525-1578(24)00122-3},
url = {https://www.sciencedirect.com/science/article/pii/S1525157824001223}
}
@incollection{CAIONE2017357,
title = {Chapter 13 - WoX: Model-Driven Development of Web of Things Applications},
editor = {Quan Z. Sheng and Yongrui Qin and Lina Yao and Boualem Benatallah},
booktitle = {Managing the Web of Things},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {357-387},
year = {2017},
isbn = {978-0-12-809764-9},
doi = {https://doi.org/10.1016/B978-0-12-809764-9.00017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128097649000172},
author = {Adriana Caione and Alessandro Fiore and Luca Mainetti and Luigi Manco and Roberto Vergallo},
keywords = {Internet of things, Internet of everything, Web of topics, Model-driven, Cloud platforms},
abstract = {Nodes of the Internet of Things (IoT) are heterogeneous: Bluetooth Low Energy (BLE), Radio Frequency Identification (RFID), Near Field Communication (NFC), Wireless Sensors Networks (WSN), Konnex (KNX), just to name the most popular. IoT clients are heterogeneous too: mobile apps, laptops, enterprise applications, business processes instances, not to mention that even IoT nodes can be clients for other nodes. In this many-to-many relationship scenario, developing a seamless IoT system is arduous even for a specialized developer. All the more so, enable non-technical people to autonomously define innovative IoT-based scenarios is far from being trivial. This calls for the definition of a common design model shared by all the IoT stakeholder: device manufacturers, developers, stakeholders, business entities, end users. The Web of Things (WoT) paradigm has brought the IoT a step closer to the people perception, because it allows treating a networked thing as a Web resource. Nevertheless, sharing a common application layer protocol on top of the physical “things” does not guarantee that IoT application will be fast-developed, robust and easily evolvable. REST APIs definition for the IoT objects is left to the individual developer. Technological needs may vary along the application lifecycle. Stakeholders are often interested in virtual or aggregated environment features, rather than the single networked thing. To overcome these open issues, we think that it is needed an additional abstraction level between the WoT and the application layer. This should be model-driven – in order this to be adequately agreed by all the IoT stakeholders – and topic-based – because of the event-driven nature of the IoT. In this work we propose Web of Topics (WoX), a Cloud platform for the Internet of (every)Thing (IoE). WoX APIs allows companies and organisations to realise robust and high-maintainable IoT-based services, while minimising deployment costs and the time-to-market. Its model-driven approach guarantees a great end-user experience and a seamless integration among the heterogeneous IoT entities. In this book chapter we present the WoX model and the concrete architecture supporting it. As a proof of concept, in this work we also show how we implemented an original IoT scenarios using the WoX concepts, APIs and architecture: the airport short-stay parking service.}
}
@article{BELLINI2018142,
title = {Managing cloud via Smart Cloud Engine and Knowledge Base},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {142-154},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16303867},
author = {Pierfrancesco Bellini and Ivan Bruno and Daniele Cenni and Paolo Nesi},
keywords = {Knwoledge base, Smart cloud, Cloud computing, Service level agreement},
abstract = {Complexity of cloud infrastructures needs models and tools for process management, configuration, scaling, elastic computing and cloud resource health control. This paper presents a Smart Cloud Engine and solution based on a Knowledge Base, KB, with the aim of modeling cloud resources, Service Level Agreements and their evolutions, and enabling the reasoning on structures by implementing strategies of efficient smart cloud management and intelligence. The solution proposed provides formal verification and intelligence tools for cloud control. It can be easily integrated with a large range of cloud configuration manager, cloud orchestrator, and monitoring tools, since the connections with these tools are performed by using REST calls and XML files. The proposed solution has been validated in the context of large ICARO Cloud project and in the cloud facility of a national cloud service provider. Some data resulting from the validation phases have been reported and are referring to the dynamic management of real ECLAP social network http://www.eclap.eu.}
}
@article{R2021126882,
title = {Genomic characterization of Enterobacter xiangfangensis STP-3: Application to real time petroleum oil sludge bioremediation},
journal = {Microbiological Research},
volume = {253},
pages = {126882},
year = {2021},
issn = {0944-5013},
doi = {https://doi.org/10.1016/j.micres.2021.126882},
url = {https://www.sciencedirect.com/science/article/pii/S0944501321001889},
author = {Muneeswari R and Iyappan S and Swathi KV and Sudheesh KP and Rajesh T and Sekaran G and Ramani K},
keywords = {Genomic approach,  STP-3, Petroleum oil sludge bioremediation, Biosurfactant, Metabolic enzymes, Comparative genomics},
abstract = {Sustainable treatment of petroleum oil sludge still remains as a major challenge to petroleum refineries. Bioremediation is the promising technology involving bacteria for simultaneous production of biosurfactant and followed by degradation of petroleum compounds. Complete genomic knowledge on such potential microbes could accentuate its successful exploitation. The present study discusses the genomic characteristics of novel biosurfactant producing petrophilic/ petroleum hydrocarbon degrading strain, Enterobacter xiangfangensis STP-3, isolated from petroleum refinery oil sludge contaminated soil. The genome has 4,584,462 bp and 4372 protein coding sequences. Functional analysis using the RAST and KEGG databases revealed the presence of biosynthetic gene clusters linked to glycolipid and lipopeptide production and multiple key candidate genes linked with the degradation pathway of petroleum hydrocarbons. Orthology study revealed diversity in gene clusters associated to membrane transport, carbohydrate, amino acid metabolism, virulence and defence mechanisms, and nucleoside and nucleotide synthesis. The comparative analysis with 27 other genomes predicted that the core genome contributes to its inherent bioremediation potential, whereas the accessory genome influences its environmental adaptability in unconventional environmental conditions. Further, experimental results showed that E. xiangfangensis STP-3 was able to degrade PHCs by 82 % in 14 days during the bioremediation of real time petroleum oil sludge with the concomitant production of biosurfactant and metabolic enzymes, To the best of our knowledge, no comprehensive genomic study has been previously reported on the biotechnological prospective of this species.}
}@article{CHEN20244283,
title = {LKPNR: Large Language Models and Knowledge Graph for Personalized News Recommendation Framework},
journal = {Computers, Materials and Continua},
volume = {79},
number = {3},
pages = {4283-4296},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.049129},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824000225},
author = {Hao Chen and Runfeng Xie and Xiangyang Cui and Zhou Yan and Xin Wang and Zhanwei Xuan and Kai Zhang},
keywords = {Large language models, news recommendation, knowledge graphs (KG)},
abstract = {Accurately recommending candidate news to users is a basic challenge of personalized news recommendation systems. Traditional methods are usually difficult to learn and acquire complex semantic information in news texts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the long tail problem of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into traditional methods. To learn the contextual information of news text, we use LLMs’ powerful text understanding ability to generate news representations with rich semantic information, and then, the generated news representations are used to enhance the news encoding in traditional methods. In addition, multi-hops relationship of news entities is mined and the structural information of news is encoded using KG, thus alleviating the challenge of long-tail distribution. Experimental results demonstrate that compared with various traditional models, on evaluation indicators such as AUC, MRR, nDCG@5 and nDCG@10, the framework significantly improves the recommendation performance. The successful integration of LLM and KG in our framework has established a feasible way for achieving more accurate personalized news recommendation. Our code is available at https://github.com/Xuan-ZW/LKPNR.}
}
@article{WANG2025104054,
title = {A novel large-language-model-driven framework for named entity recognition},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104054},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104054},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004138},
author = {Zhenhua Wang and Huiru Chen and Guang Xu and Ming Ren},
keywords = {Large language model, Named entity recognition, In-context learning, Contrastive learning, Knowledge graph},
abstract = {Named entity recognition (NER) stands as the foundational pillar of knowledge graphs across multiple domains. Despite progress in NER using large language models (LLMs), challenges persist regarding the selection of LLMs, the retrieval of demonstrations, and the design of prompts. We introduce a novel framework for NER, termed LLMCC, which elucidates the synergistic interactions between different LLMs. Two new methods, SemnRank and InforLaw-thought, are proposed to address the issue of redundancy in demonstrations and to elevate prompt quality for boosting LLM's capabilities. Furthermore, LLMCC is trained through a new entity-aware contrastive learning. Extensive experiments across five domains confirm the competitiveness of LLMCC (surpassing ten recent studies by a margin of over 5% in F1 score), as well as the effectiveness of SemnRank and InforLaw-thought. We uncover a series of insights regarding information laws, prompting strategies, demonstration selections, and training designs. This research significantly advances the incorporation of LLMs into the construction of knowledge graphs.}
}
@article{SINGH2025100128,
title = {A survey on chatbots and large language models: Testing and evaluation techniques},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100128},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100128},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000044},
author = {Sonali Uttam Singh and Akbar Siami Namin},
keywords = {Large Language Models (LLM), Chatbot, Conversational Chatbot, Intelligent Personal Assistant (IPA), ChatGPT, Natural Language Understanding (NLU), Natural Language Processing (NLP)},
abstract = {Chatbots have been quite developed in the recent decades and evolved along with the field of Artificial Intelligence (AI), enabling powerful capabilities in tasks such as text generation and summarization, sentiment analysis, and many other interesting Natural Language Processing (NLP) based tasks. Advancements in language models (LMs), specifically LLMs, have played an important role in improving the capabilities of chatbots. This survey paper provides a comprehensive overview in chatbot with the integration of LLMs, primarily focusing on the testing, evaluation and performance techniques and frameworks associated with it. The paper discusses the foundational concepts of chatbots and their evolution, highlights the challenges and opportunities they present by reviewing the state-of-the-art papers associated with the chatbots design, testing and evaluation. The survey also delves into the key components of chatbot systems, including Natural Language Understanding (NLU), dialogue management, and Natural Language Generation (NLG), and examine how LLMs have influenced each of these components. Furthermore, the survey examines the ethical considerations and limitations associated with LLMs. The paper primarily focuses on investigating the evaluation techniques and metrics used to assess the performance and effectiveness of these language models. This paper aims to provide an overview of chatbots and highlights the need for an appropriate framework in regards to testing and evaluating these chatbots and the LLMs associated with it in order to provide efficient and proper knowledge to user and potentially improve its quality based on advancements in the field of machine learning.}
}
@article{WICKRAMASEKARA2025301859,
title = {Exploring the potential of large language models for improving digital forensic investigation efficiency},
journal = {Forensic Science International: Digital Investigation},
volume = {52},
pages = {301859},
year = {2025},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301859},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001860},
author = {Akila Wickramasekara and Frank Breitinger and Mark Scanlon},
keywords = {Digital forensics, Large language models, LLM, Investigative process, Challenges},
abstract = {The ever-increasing workload of digital forensic labs raises concerns about law enforcement's ability to conduct both cyber-related and non-cyber-related investigations promptly. Consequently, this article explores the potential and usefulness of integrating Large Language Models (LLMs) into digital forensic investigations to address challenges such as bias, explainability, censorship, resource-intensive infrastructure, and ethical and legal considerations. A comprehensive literature review is carried out, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the use of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and the possibilities of incorporating LLMs. In conclusion, the study states that the adoption of LLMs in digital forensics, with appropriate constraints, has the potential to improve investigation efficiency, improve traceability, and alleviate the technical and judicial barriers faced by law enforcement entities.}
}
@article{HATEM2024327,
title = {Up To Date: Automatic Updating Knowledge Graphs Using LLMs},
journal = {Procedia Computer Science},
volume = {244},
pages = {327-334},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030072},
author = {Shahenda Hatem and Ghada Khoriba and Mohamed H. Gad-Elrab and Mohamed ElHelw},
keywords = {Knowledge Graphs, Large Language Models, Retrieval augmented generation},
abstract = {Maintaining up-to-date knowledge graphs (KGs) is essential for enhancing the accuracy and relevance of artificial intelligence (AI) applications, especially with sensitive domains. Yet, major KGs are either manually maintained (e.g., Wikidata) or infrequently rebuilt (e.g., DBpedia & YAGO). Thus, they contain many outdated facts. The rise of Large Language Models (LLMs) reasoning and Augmented Retrieval Generation approaches (RAG) gives KGs an interface to other trusted sources. This paper introduces a methodology utilizing Large Language Models (LLMs) to validate and update KG facts automatically. In particular, we utilize LLM reasoning capabilities to determine potentially outdated facts. After that, we use RAG techniques to generate an accurate fix for the fact. Experimental results on several LLMs and real-world datasets demonstrate the ability of our approach to propose accurate fixes. In addition, our experiments highlight the efficacy of few-shot prompts over zero-shot prompts.}
}
@article{HYVONEN2025100852,
title = {Serendipitous knowledge discovery on the Web of Wisdom based on searching and explaining interesting relations in knowledge graphs},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100852},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100852},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000386},
author = {Eero Hyvönen},
keywords = {Knowledge graphs, Relational search, Knowledge discovery, Information retrieval, Large Language Models, Generative AI},
abstract = {This paper maintains that the Semantic Web is changing into a kind of Web of Wisdom (WoW) where AI-based problem solving, based on symbolic search and sub-symbolic methods, and Information Retrieval (IR) merge: IR is seen as a process for solving information-related problems of the end user with explanations, a form of knowledge discovery. As a case of example, relational search is concerned, i.e., solving problems of the type “How are X1…Xn related to Y1…Ym?”. For example: how is Pablo Picasso related to Barcelona? The idea is to find explainable “interesting” or even serendipitous associations in Knowledge Graphs (KG) and textual web contents. It is argued that domain knowledge-based symbolic methods based of KGs are needed to complement domain-agnostic graph-based methods and Generative AI (GenAI) boosted by Large Language Models (LLM). By using domain specific knowledge, it is possible to find and explain meaningful reliable textual answers, answer quantitative questions, and use data analyses and visualizations for explaining and studying the relations.}
}
@article{VALCALVO2025104042,
title = {OntoGenix: Leveraging Large Language Models for enhanced ontology engineering from datasets},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104042},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104042},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004011},
author = {Mikel Val-Calvo and Mikel {Egaña Aranguren} and Juan Mulero-Hernández and Ginés Almagro-Hernández and Prashant Deshmukh and José Antonio Bernabé-Díaz and Paola Espinoza-Arias and José Luis Sánchez-Fernández and Juergen Mueller and Jesualdo Tomás Fernández-Breis},
keywords = {Knowledge graphs, Large Language Models, Ontology engineering},
abstract = {Knowledge Graphs integrate data from multiple, heterogeneous sources, using ontologies to facilitate data interoperability. Ontology development is a resource-consuming task that requires the collaborative work of domain experts and ontology engineers. Therefore, companies invest considerable resources in order to generate and maintain Enterprise Knowledge Graphs and ontologies from large and complex datasets, most of which can be unfamiliar for ontology engineers. In this work, we study the use of Large Language Models to aid in the development of ontologies from datasets, ultimately increasing the automation of the generation of ontology-based Knowledge Graphs. As a result we have developed a structured workflow that leverages Large Language Models to enhance ontology engineering through data pre-processing, ontology planning, building, and entity improvement. Our method is also able to generate mappings and RDF data, but in this work we focus on the ontologies. The pipeline has been implemented in the OntoGenix tool. In this work we show the results of the application of OntoGenix to six datasets related to commercial activities. The findings indicate that the ontologies produced exhibit patterns of coherent modeling, and features that closely resemble those created by humans, although the most complex situations are better reflected by the ontologies developed by humans.}
}
@article{WU2024101030,
title = {Exploring the reversal curse and other deductive logical reasoning in BERT and GPT-based large language models},
journal = {Patterns},
volume = {5},
number = {9},
pages = {101030},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101030},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924001636},
author = {Da Wu and Jingye Yang and Kai Wang},
keywords = {BERT, GPT, large language model, LLM, reversal curse, auto-regressive model, bidirectional encoder, deductive logical reasoning},
abstract = {Summary
The “Reversal Curse” describes the inability of autoregressive decoder large language models (LLMs) to deduce “B is A” from “A is B,” assuming that B and A are distinct and can be uniquely identified from each other. This logical failure suggests limitations in using generative pretrained transformer (GPT) models for tasks like constructing knowledge graphs. Our study revealed that a bidirectional LLM, bidirectional encoder representations from transformers (BERT), does not suffer from this issue. To investigate further, we focused on more complex deductive reasoning by training encoder and decoder LLMs to perform union and intersection operations on sets. While both types of models managed tasks involving two sets, they struggled with operations involving three sets. Our findings underscore the differences between encoder and decoder models in handling logical reasoning. Thus, selecting BERT or GPT should depend on the task’s specific needs, utilizing BERT’s bidirectional context comprehension or GPT’s sequence prediction strengths.}
}
@article{LAVRINOVICS2025100844,
title = {Knowledge Graphs, Large Language Models, and Hallucinations: An NLP Perspective},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100844},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100844},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000301},
author = {Ernests Lavrinovics and Russa Biswas and Johannes Bjerva and Katja Hose},
keywords = {LLM, Factuality, Knowledge Graphs, Hallucinations},
abstract = {Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) based applications including automated text generation, question answering, chatbots, and others. However, they face a significant challenge: hallucinations, where models produce plausible-sounding but factually incorrect responses. This undermines trust and limits the applicability of LLMs in different domains. Knowledge Graphs (KGs), on the other hand, provide a structured collection of interconnected facts represented as entities (nodes) and their relationships (edges). In recent research, KGs have been leveraged to provide context that can fill gaps in an LLM’s understanding of certain topics offering a promising approach to mitigate hallucinations in LLMs, enhancing their reliability and accuracy while benefiting from their wide applicability. Nonetheless, it is still a very active area of research with various unresolved open problems. In this paper, we discuss these open challenges covering state-of-the-art datasets and benchmarks as well as methods for knowledge integration and evaluating hallucinations. In our discussion, we consider the current use of KGs in LLM systems and identify future directions within each of these challenges.}
}
@article{KONDINSKI20242070,
title = {Knowledge graph representation of zeolitic crystalline materials††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00166d},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {2070-2084},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00166d},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001669},
author = {Aleksandar Kondinski and Pavlo Rutkevych and Laura Pascazio and Dan N. Tran and Feroz Farazi and Srishti Ganguly and Markus Kraft},
abstract = {Zeolites are complex and porous crystalline inorganic materials that serve as hosts for a variety of molecular, ionic and cluster species. Formal, machine-actionable representation of this chemistry presents a challenge as a variety of concepts need to be semantically interlinked. This work demonstrates the potential of knowledge engineering in overcoming this challenge. We develop ontologies OntoCrystal and OntoZeolite, enabling the representation and instantiation of crystalline zeolite information into a dynamic, interoperable knowledge graph called The World Avatar (TWA). In TWA, crystalline zeolite instances are semantically interconnected with chemical species that act as guests in these materials. Information can be obtained via custom or templated SPARQL queries administered through a user-friendly web interface. Unstructured exploration is facilitated through natural language processing using the Marie System, showcasing promise for the blended large language model – knowledge graph approach in providing accurate responses on zeolite chemistry in natural language.}
}
@article{SHIM2025103001,
title = {OmEGa(Ω): Ontology-based information extraction framework for constructing task-centric knowledge graph from manufacturing documents with large language model},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103001},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624006529},
author = {Midan Shim and Hyojun Choi and Heeyeon Koo and Kaehyun Um and Kyong-Ho Lee and Sanghyun Lee},
keywords = {Ontology modeling, Manufacturing and maintenance process, Information extraction, Knowledge graph, Document understanding, Large language model},
abstract = {Manufacturing industry relies heavily on technical documents that encapsulate specialized knowledge essential for optimizing production and maintenance processes. However, extracting meaningful insights from these documents is challenging due to their complex structure, domain-specific terminology, and multimodal content, which includes text, images, and tables. Furthermore, there is a contextual gap between the generic training data of pre-trained language models (PLMs) and the specialized knowledge required for manufacturing documents. To address these issues, a Task-Centric Ontology (TCO) is designed to describe fundamental manufacturing tasks, and develop OmEGa, an Ontology-based Information Extraction Framework for Task-Centric Knowledge Graphs. OmEGa leverages large language models (LLMs) to perform instance recognition and relation classification on multimodal documents. By utilizing spatial embedding and modality linking, OmEGa addresses structural challenges, while TCO-driven reasoning mitigates contextual challenges. Experimental results demonstrate the effectiveness of OmEGa, achieving strong performance on both proprietary and open-source datasets. Additionally, a Knowledge Graph Question Answering (KGQA) system built on the extracted task-centric knowledge shows promise in enhancing communication among domain experts in the manufacturing sector.}
}
@article{HAYAWI2024101533,
title = {Generative AI and large language models: A new frontier in reverse vaccinology},
journal = {Informatics in Medicine Unlocked},
volume = {48},
pages = {101533},
year = {2024},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2024.101533},
url = {https://www.sciencedirect.com/science/article/pii/S2352914824000893},
author = {Kadhim Hayawi and Sakib Shahriar and Hany Alashwal and Mohamed Adel Serhani},
keywords = {Reverse vaccinology, Large language models (LLMs), AI, Generative AI, Vaccine candidate identification, AI ethics, Vaccines},
abstract = {Reverse vaccinology is an emerging concept in the field of vaccine development as it facilitates the identification of potential vaccine candidates. Biomedical research has been revolutionized with the recent innovations in Generative Artificial Intelligence (AI) and Large Language Models (LLMs). The intersection of these two technologies is explored in this study. In this study, the impact of Generative AI and LLMs in the field of vaccinology is explored. Through a comprehensive analysis of existing research, prospective use cases, and an experimental case study, this research highlights that LLMs and Generative AI have the potential to enhance the efficiency and accuracy of vaccine candidate identification. This work also discusses the ethical and privacy challenges, such as data consent and potential biases, raised by such applications that require careful consideration. This study paves the way for experts, researchers, and policymakers to further investigate the role and impact of Generative AI and LLM in vaccinology and medicine.}
}
@article{BADINI2025100275,
title = {Enhancing mechanical and bioinspired materials through generative AI approaches},
journal = {Next Materials},
volume = {6},
pages = {100275},
year = {2025},
issn = {2949-8228},
doi = {https://doi.org/10.1016/j.nxmate.2024.100275},
url = {https://www.sciencedirect.com/science/article/pii/S2949822824001722},
author = {Silvia Badini and Stefano Regondi and Raffaele Pugliese},
keywords = {Mechanical materials, Bioinspired materials, Additive manufacturing, Generative AI, Human-machine interaction},
abstract = {The integration of generative artificial intelligence (AI) into the design and additive manufacturing processes of mechanical and bioinspired materials has emerged as a transformative approach in engineering and material science, allowing to explore relationships across different field (e.g., mechanics-biology) or disparate domains (e.g., failure mechanics-3D printing). In addition, generative AI techniques, including generative adversarial networks (GAN), genetic algorithms, and large language models (LLMs), offer efficient and tunable solutions for optimizing material properties, reducing production costs, and accelerating the development timelines. In the field of mechanical materials design, generative AI enables the rapid generation of novel structures with enhanced mechanical performance. Instead, bioinspired materials design benefits significantly from the synergy of generative AI with bioinspired concepts and additive manufacturing. By harnessing generative algorithms and topology optimization, researchers can explore complex biological phenomena and translate them into innovative engineering solutions. Lastly, the emergence of LLMs in additive manufacturing optimization demonstrates their potential to optimize printing parameters, debug errors, and enhance productivity. This review highlights the pivotal role of generative AI in advancing materials science and engineering, unlocking new possibilities for innovation, and accelerating the development of efficient material solutions. As generative AI continues to evolve, its integration promises to revolutionize engineering design and drive the field towards unprecedented levels of efficiency, thus turns information into knowledge.}
}
@article{COLOMBO2025104082,
title = {An LLM-assisted ETL pipeline to build a high-quality knowledge graph of the Italian legislation},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104082},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104082},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500024X},
author = {Andrea Colombo and Anna Bernasconi and Stefano Ceri},
keywords = {Law, Knowledge graph, Property graph, Large language models, Data quality},
abstract = {The increasing complexity of legislative systems, characterized by an ever-growing number of laws and their interdependencies, has highlighted the utility of Knowledge Graphs (KGs) as an effective data model for organizing such information, compared to traditional methods, often based on relational models, which struggle to efficiently represent interlinked data, such as references within laws, hindering efficient knowledge discovery. A paradigm shift in modeling legislative data is already ongoing with the adoption of common international standards, predominantly XML-based, such as Akoma Ntoso (AKN) and the Legal Knowledge Interchange Format, which aim to capture fundamental aspects of laws shared across different legislations and simplify the task of creating Knowledge Graphs through the use of XML tags and identifiers. However, to enable advanced analysis and data discovery within these KGs, it is necessary to carefully check, complement, and enrich KG nodes and edges with properties, either metadata or additional derived knowledge, that enhance the quality and utility of the model, for instance, by leveraging the capabilities of state-of-the-art Large Language Models. In this paper, we present an ETL pipeline for modeling and querying the Italian legislation in a Knowledge Graph, by adopting the property graph model and the AKN standard implemented in the Italian system. The property graph model offers a good compromise between knowledge representation and the possibility of performing graph analytics, which we consider essential for enabling advanced pattern detection. Then, we enhance the KG with valuable properties by employing carefully fine-tuned open-source LLMs, i.e., BERT and Mistral-7B models, which enrich and augment the quality of the KG, allowing in-depth analysis of legislative data.}
}
@article{BUCHMANN2024102324,
title = {Large language models: Expectations for semantics-driven systems engineering},
journal = {Data & Knowledge Engineering},
volume = {152},
pages = {102324},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102324},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2400048X},
author = {Robert Buchmann and Johann Eder and Hans-Georg Fill and Ulrich Frank and Dimitris Karagiannis and Emanuele Laurenzi and John Mylopoulos and Dimitris Plexousakis and Maribel Yasmina Santos},
keywords = {Large language models, Systems engineering, Conceptual modeling, Knowledge engineering},
abstract = {The hype of Large Language Models manifests in disruptions, expectations or concerns in scientific communities that have focused for a long time on design-oriented research. The current experiences with Large Language Models and associated products (e.g. ChatGPT) lead to diverse positions regarding the foreseeable evolution of such products from the point of view of scholars who have been working with designed abstractions for most of their careers - typically relying on deterministic design decisions to ensure systems and automation reliability. Such expectations are collected in this paper in relation to a flavor of systems engineering that relies on explicit knowledge structures, introduced here as “semantics-driven systems engineering”. The paper was motivated by the panel discussion that took place at CAiSE 2023 in Zaragoza, Spain, during the workshop on Knowledge Graphs for Semantics-driven Systems Engineering (KG4SDSE). The workshop brought together Conceptual Modeling researchers with an interest in specific applications of Knowledge Graphs and the semantic enrichment benefits they can bring to systems engineering. The panel context and consensus are summarized at the end of the paper, preceded by a proposed research agenda considering the expressed positions.}
}
@article{FAN2024103646,
title = {CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models},
journal = {Information Processing & Management},
volume = {61},
number = {3},
pages = {103646},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103646},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000062},
author = {Zhanling Fan and Chongcheng Chen},
keywords = {Knowledge graph, Pretrained language models, Cultural tourism, Cultural type, ChatGPT, Travel intelligence},
abstract = {Tourism knowledge graphs lack cultural content, limiting their usefulness for cultural tourists.This paper presents the development of a cultural perspective-based knowledge graph (CuPe-KG). We evaluated fine-tuning ERNIE 3.0 (FT-ERNIE) and ChatGPT for cultural type recognition to strengthen the relationship between tourism resources and cultures. Our investigation used an annotated cultural tourism resource dataset containing 2,745 items across 16 cultural types. The results showed accuracy scores for FT-ERNIE and ChatGPT of 0.81 and 0.12, respectively, with FT-ERNIE achieving a micro-F1 score of 0.93, a 26 percentage point lead over ChatGPT's score of 0.67. These underscore FT-ERNIE's superior performance (the shortcoming is the need to annotate data) while highlighting ChatGPT's limitations because of insufficient Chinese training data and lower identification accuracy in professional knowledge. A novel ontology was designed to facilitate the construction of CuPe-KG, including elements such as cultural types, historical figures, events, and intangible cultural heritage. CuPe-KG effectively addresses cultural tourism visitors’ information retrieval needs.}
}
@article{SAHBI20243083,
title = {Automatic Ontology Population from Textual Advertisements: LLM vs. Semantic Approach},
journal = {Procedia Computer Science},
volume = {246},
pages = {3083-3092},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.364},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023925},
author = {Aya Sahbi and Céline Alec and Pierre Beust},
keywords = {Ontology Population, LLM, Textual Advertisement},
abstract = {Automatic ontology population involves identifying, extracting and integrating information from various sources to instantiate the classes and properties of an ontology, thereby building a domain Knowledge Graph (KG). In this paper, we compare two text-based ontology population techniques: KOnPoTe, a semantic approach based on textual and domain knowledge analysis, and a generative AI approach utilizing Claude, a Large Language Model (LLM). We present experiments conducted on two French sales advertisement domains: real estate and boats, and discuss the strengths and limitations of both approaches.}
}
@article{JIANG2024100723,
title = {Generating the assembly instructions of helicopter subassemblies using the hierarchical pruning strategy and large language model},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100723},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100723},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001663},
author = {Mingjie Jiang and Yu Guo and Shaohua Huang and Jun Pu},
keywords = {Assembly instruction generation, Knowledge graph, Subgraph matching, Large language model, Helicopter subassembly assembly},
abstract = {Assembly instructions are process documents in detail describing the operation steps, materials, tools, fixtures, and assembly sequences in assembly procedures. Due to assembly instructions including numerous contents, and the content being easy for workers to understand, process designers need to spend lots of time thinking and authoring assembly instructions to ensure that workers can complete the assembly task according to the assembly instructions. Focusing on the difficulties of the variety of assembly instructions and the process factors implicit in the standard languages of assembly instructions, a method of assembly instruction generation for helicopter subassemblies is proposed. First, a data representation model of multi-source heterogeneous knowledge and information based on knowledge graphs is designed and established. Then, a hierarchical pruning VF3 algorithm is presented to reuse assembly instructions according to hybrid similarity. Finally, a process factor revision model based on RoBERTa-BiLSTM-CRF is proposed to generate revised assembly instructions. Helicopter subassemblies, which contain 11,240 assembly procedures, are used to evaluate the performance of the method for generating assembly instructions. The proposed method greatly reduces the time cost of assembly instruction authoring and promotes the intelligent development of assembly process design.}
}
@article{MA2024104488,
title = {Large language models in food science: Innovations, applications, and future},
journal = {Trends in Food Science & Technology},
volume = {148},
pages = {104488},
year = {2024},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2024.104488},
url = {https://www.sciencedirect.com/science/article/pii/S092422442400164X},
author = {Peihua Ma and Shawn Tsai and Yiyang He and Xiaoxue Jia and Dongyang Zhen and Ning Yu and Qin Wang and Jaspreet K.C. Ahuja and Cheng-I Wei},
keywords = {Natural language processing, Generative AI, Pre-trained model, Large language model},
abstract = {Background
Large Language Models (LLMs) are increasingly significant in food science, transforming areas such as recipe development, nutritional analysis, food safety, and supply chain management. These models bring sophisticated decision-making, predictive analytics, and natural language processing capabilities to various aspects of food science.
Scope and approach
The review focuses on the application of LLMs in enhancing food science, with a strong emphasis on food safety, especially in contaminant detection and risk assessment. It addresses the roles of AI and LLMs in regulatory compliance and food quality control. Challenges like data biases, misinformation risks, and implementation hurdles, including data limitations and ethical concerns, are discussed. The necessity for interdisciplinary collaboration to overcome these challenges is also highlighted.
Key findings and conclusions
LLMs hold significant potential in automating processes and improving accuracy and efficiency in the global food system. Successful implementation requires continuous updates and ethical considerations. The paper provides insights for academics, industry professionals, and policymakers on the impact of LLMs in food science, emphasizing the importance of interdisciplinary efforts in this domain. Despite potential challenges, the integration of LLMs in food science promises transformative advancements.}
}
@article{ERICKSON2025100853,
title = {LLM experimentation through knowledge graphs: Towards improved management, repeatability, and verification},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100853},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100853},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000398},
author = {John S. Erickson and Henrique Santos and Vládia Pinheiro and Jamie P. McCusker and Deborah L. McGuinness},
keywords = {Generative large language models, Knowledge graphs, Retrieval-Augmented Generation, Explainability and governance in AI},
abstract = {Generative large language models (LLMs) have transformed AI by enabling rapid, human-like text generation, but they face challenges, including managing inaccurate information generation. Strategies such as prompt engineering, Retrieval-Augmented Generation (RAG), and incorporating domain-specific Knowledge Graphs (KGs) aim to address their issues. However, challenges remain in achieving the desired levels of management, repeatability, and verification of experiments, especially for developers using closed-access LLMs via web APIs, complicating integration with external tools. To tackle this, we are exploring a software architecture to enhance LLM workflows by prioritizing flexibility and traceability while promoting more accurate and explainable outputs. We describe our approach and provide a nutrition case study demonstrating its ability to integrate LLMs with RAG and KGs for more robust AI solutions.}
}
@article{SUN2025128726,
title = {SF-GPT: A training-free method to enhance capabilities for knowledge graph construction in LLMs},
journal = {Neurocomputing},
volume = {613},
pages = {128726},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128726},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014978},
author = {Lizhuang Sun and Peng Zhang and Fang Gao and Yuan An and Zhixing Li and Yuanwei Zhao},
keywords = {Knowledge graph, Triple extraction, Large language model, Knowledge fusion},
abstract = {Knowledge graphs (KGs) are constructed by extracting knowledge triples from text and fusing knowledge, enhancing information retrieval efficiency. Current methods for knowledge triple extraction include ”Pretrain and Fine-tuning” and Large Language Models (LLMs). The former shifts effort from manual extraction to dataset annotation and suffers from performance degradation with different test and training set distributions. LLMs-based methods face errors and incompleteness in extraction. We introduce SF-GPT, a training-free method to address these issues. Firstly, we propose the Entity Extraction Filter (EEF) module to filter triple generation results, addressing evaluation and cleansing challenges. Secondly, we introduce a training-free Entity Alignment Module based on Entity Alias Generation (EAG), tackling semantic richness and interpretability issues in LLM-based knowledge fusion. Finally, our Self-Fusion Subgraph strategy uses multi-response self-fusion and a common entity list to filter triple results, reducing noise from LLMs’ multi-responses. In experiments, SF-GPT showed a 55.5% increase in recall and a 32.6% increase in F1 score on the BDNC dataset compared to the UniRel model trained on the NYT dataset and achieved a 5% improvement in F1 score compared to GPT-4+EEF baseline on the WebNLG dataset in the case of a fusion round of three. SF-GPT offers a promising way to extract knowledge from unstructured information.}
}
@article{ZHANG2025104220,
title = {AttacKG+: Boosting attack graph construction with Large Language Models},
journal = {Computers & Security},
volume = {150},
pages = {104220},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104220},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005261},
author = {Yongheng Zhang and Tingwen Du and Yunshan Ma and Xiang Wang and Yi Xie and Guozheng Yang and Yuliang Lu and Ee-Chien Chang},
keywords = {Cyber threat intelligence analysis, Attack graph construction, Large Language Models},
abstract = {Attack graph construction seeks to convert textual cyber threat intelligence (CTI) reports into structured representations, portraying the evolutionary traces of cyber attacks. Even though previous research has proposed various methods to construct attack graphs, they generally suffer from limited generalization capability to diverse knowledge types as well as requirement of expertise in model design and tuning. Addressing these limitations, we seek to utilize Large Language Models (LLMs), which have achieved enormous success in a broad range of tasks given exceptional capabilities in both language understanding and zero-shot task fulfillment. Thus, we propose a fully automatic LLM-based framework to construct attack graphs named: AttacKG+. Our framework consists of four consecutive modules: rewriter, parser, identifier, and summarizer, each of which is implemented by instruction prompting and in-context learning empowered by LLMs. Furthermore, we upgrade the existing attack knowledge schema and propose a comprehensive version. We represent a cyber attack as a temporally unfolding event, each temporal step of which encapsulates three layers of representation, including behavior graph, MITRE TTP labels, and state summary. Extensive evaluation demonstrates that: (1) our formulation seamlessly satisfies the information needs in threat event analysis, (2) our construction framework is effective in faithfully and accurately extracting the information defined by AttacKG+. and (3) our attack graph directly benefits downstream security practices such as attack reconstruction. All the code and datasets will be released upon acceptance.}
}
@article{BABAIHA2024100095,
title = {Rationalism in the face of GPT hypes: Benchmarking the output of large language models against human expert-curated biomedical knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {5},
pages = {100095},
year = {2024},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2024.100095},
url = {https://www.sciencedirect.com/science/article/pii/S2667318524000023},
author = {Negin Sadat Babaiha and Sathvik Guru Rao and Jürgen Klein and Bruce Schultz and Marc Jacobs and Martin Hofmann-Apitius},
keywords = {Large language models (LLMs), Natural language processing (NLP), Biomedical text mining, Biomedical knowledge graphs, Biological expression language (BEL)},
abstract = {Biomedical knowledge graphs (KGs) hold valuable information regarding biomedical entities such as genes, diseases, biological processes, and drugs. KGs have been successfully employed in challenging biomedical areas such as the identification of pathophysiology mechanisms or drug repurposing. The creation of high-quality KGs typically requires labor-intensive multi-database integration or substantial human expert curation, both of which take time and contribute to the workload of data processing and annotation. Therefore, the use of automatic systems for KG building and maintenance is a prerequisite for the wide uptake and utilization of KGs. Technologies supporting the automated generation and updating of KGs typically make use of Natural Language Processing (NLP), which is optimized for extracting implicit triples described in relevant biomedical text sources. At the core of this challenge is how to improve the accuracy and coverage of the information extraction module by utilizing different models and tools. The emergence of pre-trained large language models (LLMs), such as ChatGPT which has grown in popularity dramatically, has revolutionized the field of NLP, making them a potential candidate to be used in text-based graph creation as well. So far, no previous work has investigated the power of LLMs on the generation of cause-and-effect networks and KGs encoded in Biological Expression Language (BEL). In this paper, we present initial studies towards one-shot BEL relation extraction using two different versions of the Generative Pre-trained Transformer (GPT) models and evaluate its performance by comparing the extracted results to a highly accurate, manually curated BEL KG curated by domain experts.}
}
@article{HOLLAND2024100,
title = {Large language model based agent for process planning of fiber composite structures},
journal = {Manufacturing Letters},
volume = {40},
pages = {100-103},
year = {2024},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2024.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S2213846324000221},
author = {Maximilian Holland and Kunal Chaudhari},
keywords = {Large language model, Generative AI, Planning agent, Process planning, Fiber composites, LangChain, OpenAI},
abstract = {Process planning is a crucial activity, connecting product development and manufacturing of fiber composite structures. Recently published Large Language Models (LLM) promise more flexible and autonomous workflows compared to state of the art automation methods. An autonomous agent for process planning of fiber composite structures is implemented with the LangChain framework, based on OpenAI’s GPT-4 language model. The agent is equipped with deterministic tools which encode a-priori process planning knowledge. It can handle different process planning problems, such as cycle time estimation and resource allocation. Combinations thereof are solved through executing a multi-step solution path.}
}
@article{MUSTAPHA2025103066,
title = {A survey of emerging applications of large language models for problems in mechanics, product design, and manufacturing},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103066},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103066},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007171},
author = {K.B. Mustapha},
keywords = {Pre-trained language models, Large language models, Generative AI, Generative pre-trained transformer, Mechanical engineering, Engineering design, Manufacturing, Mechanics, Intelligent digital twins, Intelligent maintenance, Creativity},
abstract = {In the span of three years, the application of large language models (LLMs) has accelerated across a multitude of professional sectors. Amid this development, a new collection of studies has manifested around leveraging LLMs for segments of the mechanical engineering (ME) field. Concurrently, it has become clear that general-purpose LLMs faced hurdles when deployed in this domain, partly due to their training on discipline-agnostic data. Accordingly, there is a recent uptick of derivative ME-specific LLMs being reported. As the research community shifts towards these new LLM-centric solutions for ME-related problems, the shift compels a deeper look at the diffusion of LLMs in this emerging landscape. Consequently, this review consolidates the diversity of ME-tailored LLMs use cases and identifies the supportive technical stacks associated with these implementations. Broadly, the review demonstrates how various categories of LLMs are re-shaping concrete aspects of engineering design, manufacturing and applied mechanics. At a more specific level, it uncovered emerging LLMs’ role in boosting the intelligence of digital twins, enriching bidirectional communication within the human-cyber-physical infrastructure, advancing the development of intelligent process planning in manufacturing and facilitating inverse mechanics. It further spotlights the coupling of LLMs with other generative models for promoting efficient computer-aided conceptual design, prototyping, knowledge discovery and creativity. Finally, it revealed training modalities/infrastructures necessary for developing ME-specific language models, discussed LLMs' features that are incongruent with typical engineering workflows, and concluded with prescriptive approaches to mitigate impediments to the progressive adoption of LLMs as part of advanced intelligent solutions.}
}
@article{KIM20242190,
title = {Assessing the utility of large language models for phenotype-driven gene prioritization in the diagnosis of rare genetic disease},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {10},
pages = {2190-2202},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2024.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0002929724002969},
author = {Junyoung Kim and Kai Wang and Chunhua Weng and Cong Liu},
keywords = {large language model, rare disease diagnosis, gene prioritization, precision medicine, artificial intelligence, generative pre-trained transformers, phenotypes},
abstract = {Summary
Phenotype-driven gene prioritization is fundamental to diagnosing rare genetic disorders. While traditional approaches rely on curated knowledge graphs with phenotype-gene relations, recent advancements in large language models (LLMs) promise a streamlined text-to-gene solution. In this study, we evaluated five LLMs, including two generative pre-trained transformers (GPT) series and three Llama2 series, assessing their performance across task completeness, gene prediction accuracy, and adherence to required output structures. We conducted experiments, exploring various combinations of models, prompts, phenotypic input types, and task difficulty levels. Our findings revealed that the best-performed LLM, GPT-4, achieved an average accuracy of 17.0% in identifying diagnosed genes within the top 50 predictions, which still falls behind traditional tools. However, accuracy increased with the model size. Consistent results were observed over time, as shown in the dataset curated after 2023. Advanced techniques such as retrieval-augmented generation (RAG) and few-shot learning did not improve the accuracy. Sophisticated prompts were more likely to enhance task completeness, especially in smaller models. Conversely, complicated prompts tended to decrease output structure compliance rate. LLMs also achieved better-than-random prediction accuracy with free-text input, though performance was slightly lower than with standardized concept input. Bias analysis showed that highly cited genes, such as BRCA1, TP53, and PTEN, are more likely to be predicted. Our study provides valuable insights into integrating LLMs with genomic analysis, contributing to the ongoing discussion on their utilization in clinical workflows.}
}
@article{ZHENG2025115173,
title = {Mastering building management systems data points tagging with minimal examples: unveiling the power of large language models},
journal = {Energy and Buildings},
volume = {328},
pages = {115173},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115173},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824012891},
author = {Zhiyu Zheng and Sylvain Marié and Elham Farazdaghi and Esma Yahia and Khal Makhoul and Théo Lagarde and Rani El Meouche and Fakhreddine Ababsa},
keywords = {Large Language Models, Building Management Systems, Brick Ontology, Semantic Web Technologies, Metadata Tagging, Few-Shot Learning, Prompt Engineering},
abstract = {The heterogeneity of metadata within Building Management Systems (BMS) poses substantial challenges for advanced analytics, including cross-building analysis. Over the past decade, metadata standard schemas such as Brick have been developed to address this challenge. Nevertheless, mapping BMS metadata with such standards accurately and efficiently continues to be a demanding task across both new and existing buildings. This work explores the application of Large Language Models (LLMs) to tag BMS data points, thus facilitating metadata standardization efforts. Manual or rule-based methods are not only labor-intensive but also error-prone. Similarly, supervised learning approaches using Machine Learning (ML) and Natural Language Processing (NLP) demand extensive labeled datasets, often making them laborious and inflexible to new BMS metadata types and tasks. We propose a novel three-step framework that enhances the tagging process by integrating a LLM with few-shot prompting and an embedding model. This approach not only improves result interpretability but also effectively mitigates hallucinations. This framework is further supported by analyses of the LLM’s inherent capabilities, prompt-aided specific interpretation and output formatting, and evaluations of few-shot sizes. Tested across five different building datasets, our approach, leveraging few-shot examples, achieves performance comparable to state-of-the-art supervised learning methods that rely on large labeled datasets.}
}
@article{HOU2025112622,
title = {Low-resource knowledge graph completion based on knowledge distillation driven by large language models},
journal = {Applied Soft Computing},
volume = {169},
pages = {112622},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112622},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013966},
author = {Wenlong Hou and Weidong Zhao and Ning Jia and Xianhui Liu},
keywords = {Knowledge graph completion, Knowledge reasoning, Link prediction, Large language models},
abstract = {Knowledge graph completion (KGC) refines the existing knowledge graph (KG) by predicting missing entities or relations. Existing methods are mainly based on embeddings or texts but only perform better with abundant labeled data. Hence, KGC in resource-constrained settings is a significant problem, which faces challenges of data imbalance across relations and lack of relation label semantics. Considering that Large Language Models (LLMs) demonstrate powerful reasoning and generation capabilities, this work proposes an LLM-driven Knowledge Graph Completion Distillation (KGCD) model to address low-resource KGC. A two-stage framework is developed, involving teacher-student distillation by using LLM to improve reasoning, followed by fine-tuning on real-world low-resource datasets. To deal with data imbalance, a hybrid prompt design for LLM is proposed, which includes rethink and open prompts. Furthermore, a virtual relation label generation strategy enhances the model’s understanding of triples. Extensive experiments on three benchmarks have shown that KGCD’s effectiveness for low-resource KGC, achieving improvements in Mean Reciprocal Rank (MRR) by 11% and Hits@1 by 10% on the WN18, MRR by 10% and Hits@1 by 14% on the WN18RR, and MRR by 12% and Hits@1 by 11% on the YAGO3-10.}
}
@article{CIATTO2025112940,
title = {Large language models as oracles for instantiating ontologies with domain-specific knowledge},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112940},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112940},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124015740},
author = {Giovanni Ciatto and Andrea Agiollo and Matteo Magnini and Andrea Omicini},
keywords = {Ontology population, Large language models, Nutrition, Automation, Domain-specific knowledge},
abstract = {Background:
Endowing intelligent systems with semantic data commonly requires designing and instantiating ontologies with domain-specific knowledge. Especially in the early phases, those activities are typically performed manually by human experts possibly leveraging on their own experience. The resulting process is therefore time-consuming, error-prone, and often biased by the personal background of the ontology designer.
Objective:
To mitigate that issue, we propose a novel domain-independent approach to automatically instantiate ontologies with domain-specific knowledge, by leveraging on large language models (LLMs) as oracles.
Methods:
Starting from (i) an initial schema composed by inter-related classes and properties and (ii) a set of query templates, our method queries the LLM multiple times, and generates instances for both classes and properties from its replies. Thus, the ontology is automatically filled with domain-specific knowledge, compliant to the initial schema. As a result, the ontology is quickly and automatically enriched with manifold instances, which experts may consider to keep, adjust, discard, or complement according to their own needs and expertise.
Contribution:
We formalise our method in general way and instantiate it over various LLMs, as well as on a concrete case study. We report experiments rooted in the nutritional domain where an ontology of food meals and their ingredients is automatically instantiated from scratch, starting from a categorisation of meals and their relationships. There, we analyse the quality of the generated ontologies and compare ontologies attained by exploiting different LLMs. Experimentally, our approach achieves a quality metric that is up to five times higher than the state-of-the-art, while reducing erroneous entities and relations by up to ten times. Finally, we provide a SWOT analysis of the proposed method.}
}
@article{WALLER2024103940,
title = {Questionable devices: Applying a large language model to deliberate carbon removal},
journal = {Environmental Science & Policy},
volume = {162},
pages = {103940},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103940},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124002740},
author = {Dr. Laurie Waller and Dr. David Moats and Dr. Emily Cox and Dr. Rob Bellamy},
keywords = {Carbon removal, Deliberation, Devices, Publics, Experiments in participation, Large language models, Generative AI},
abstract = {This paper presents a device-centred approach to deliberation, developed in deliberative workshops appraising methods for removing carbon dioxide from the air. Our approach involved deploying the Large Language Model application ChatGPT (sometimes termed “generative AI”) to elicit questions and generate texts about carbon removal. We develop the notion of the “questionable” device to foreground the informational unruliness ChatGPT introduced into the deliberations. The analysis highlights occasions where the deliberative apparatus became a focus of collective critique, including over: issue definitions, expert-curated resources, lay identities and social classifications. However, in this set-up ChatGPT was all too often engaged unquestioningly as an instrument for informing discussion; its instrumental lure disguising the unruliness it introduced into the workshops. In concluding, we elaborate the notion of questionable devices and reflect on the way carbon removal has been “devised” as a field in want of informed deliberation.}
}
@article{ONG2025126648,
title = {Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126648},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126648},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002702},
author = {Ryan Ong and Jiahao Sun and Yi-Ke Guo and Ovidiu Serban},
keywords = {Dynamic knowledge graphs, Language models, Link prediction},
abstract = {Knowledge graphs (KGs) represent real-world facts through entities and relations. However, static KGs fail to capture continuously emerging entities and relations over time. Temporal knowledge graphs address this by incorporating time information or providing multiple sequential snapshots of a static knowledge graph. Most existing work focuses on static KGs with fixed sets of entities and relations, meaning existing methods still struggle to encode emerging entities and relations. Therefore, we propose a novel methodology of combining language models and graph structure to enable the encoding of unseen entities and relations for temporal KG completion. Specifically, we encode relations with RoBERTa and entities using neighbouring relations alongside the entity’s relation type to provide contextual information. We evaluate our methodology on three datasets with emerging entities and relations over temporal snapshots: LKGE-Hybrid, FB-MBE, and the mergers and acquisitions domain TKGQA dataset. Our experiments show that our model achieves new state-of-the-art results on FB-MBE and LKGE-Hybrid while providing strong benchmark results for the TKGQA dataset. Our ablation studies show us that graph structure information is only beneficial if there is sufficient connectivity with the knowledge graph since sparser knowledge graphs can lead to noisy signals. We also explore the performance of Llama v2 on temporal link prediction, and the results show that current LLMs struggle with domain-specific temporal link prediction. Overall, our work provides an essential advance around effectively encoding continuously emerging entities and relations for temporal link prediction across evolving knowledge graphs over time.}
}
@article{WANG2025103342,
title = {Multi large language model collaboration framework for few-shot link prediction in evolutionary fault diagnosis event graphs},
journal = {Journal of Process Control},
volume = {145},
pages = {103342},
year = {2025},
issn = {0959-1524},
doi = {https://doi.org/10.1016/j.jprocont.2024.103342},
url = {https://www.sciencedirect.com/science/article/pii/S0959152424001823},
author = {Tian Wang and Ping Wang and Feng Yang and Shuai Wang and Qiang Fang and Meng Chi},
keywords = {Fault diagnosis, Evolutionary event graph, Link prediction, Large language model},
abstract = {Fault-tolerant control is crucial for ensuring flight safety in aircraft. However, existing methods for fault diagnosis in nonlinear systems face challenges such as data sparsity, limited generalization, and lack of explainability. To address these challenges, this paper proposes a multi-large language model (LLM) collaboration framework for few-shot link prediction in evolutionary fault diagnosis event graphs. The framework consists of two modules: the Clustering Language Model (LMc) and the Prediction Language Model (LMP). LMc utilizes the semantic understanding capabilities of LLMs to cluster entities and decompose large-scale graph data into smaller subgraphs, mitigating the impact of data sparsity on link prediction. LMP leverages the reasoning capabilities of LLMs to perform link prediction within each subgraph and fuses the prediction results to enhance accuracy and generalization. The completion of the link serves as a means to an end, which is to conduct fault diagnosis reasoning on a more detailed knowledge graph, thereby significantly improving the accuracy of fault diagnosis. Experimental results demonstrate that the proposed framework outperforms traditional embedding models and existing meta-learning methods on multiple datasets, particularly for sparse and background-rich datasets. This approach offers a novel solution for fault diagnosis in nonlinear systems, with significant theoretical and practical value.}
}
@article{SEQUEDA2025100858,
title = {Knowledge Graphs as a source of trust for LLM-powered enterprise question answering},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100858},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100858},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000441},
author = {Juan Sequeda and Dean Allemang and Bryon Jacob},
keywords = {Knowledge Graph, LLM, Large Language Model, Generative AI, Question answering, Knowledge engineering, SPARQL, SQL, OWL, R2RML},
abstract = {Generative AI provides an innovative and exciting way to manage knowledge and data at any scale; for small projects, at the enterprise level, and even at a world wide web scale. It is tempting to think that Generative AI has made other knowledge-based technologies obsolete; that anything we wanted to do with knowledge-based systems, Knowledge Graphs or even expert systems can instead be done with Generative AI. Our position is counter to that conclusion. Our practical experience on implementing enterprise question answering systems using Generative AI has shown that Knowledge Graphs support this infrastructure in multiple ways: they provide a formal framework to evaluate the validity of a query generated by an LLM, serve as a foundation for explaining results, and offer access to governed and trusted data. In this position paper, we share our experience, present industry needs, and outline the opportunities for future research contributions.}
}
@article{BRAHMACHARY2025129272,
title = {Large language model-based evolutionary optimizer: Reasoning with elitism},
journal = {Neurocomputing},
volume = {622},
pages = {129272},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129272},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224020435},
author = {Shuvayan Brahmachary and Subodh M. Joshi and Aniruddha Panda and Kaushik Koneripalli and Arun Kumar Sagotra and Harshil Patel and Ankush Sharma and Ameya D. Jagtap and Kaushic Kalyanaraman},
keywords = {Large language models, Evolutionary Optimizers, Multi-objective optimization, Aerodynamic Design},
abstract = {Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, prompting interest in their application as black-box optimizers. This paper asserts that LLMs possess the capability for zero-shot optimization across diverse scenarios, including multi-objective and high-dimensional problems. We introduce a novel population-based method for numerical optimization using LLMs called Large Language-Model-Based Evolutionary Optimizer (LEO). Our hypothesis is supported through numerical examples, spanning benchmark and industrial engineering problems such as supersonic nozzle shape optimization, heat transfer, and windfarm layout optimization. We compare our method to several gradient-based and gradient-free optimization approaches. While LLMs yield comparable results to state-of-the-art methods, their imaginative nature and propensity to hallucinate demand careful handling. We provide practical guidelines for obtaining reliable answers from LLMs and discuss method limitations and potential research directions.}
}
@article{XIAO2025102888,
title = {A comprehensive survey of large language models and multimodal large language models in medicine},
journal = {Information Fusion},
volume = {117},
pages = {102888},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102888},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524006663},
author = {Hanguang Xiao and Feizhong Zhou and Xingyue Liu and Tianqi Liu and Zhipeng Li and Xin Liu and Xiaoxuan Huang},
keywords = {Large language model, Multimodal large language model, Medicine, Healthcare, Clinical application},
abstract = {Since the release of ChatGPT and GPT-4, large language models (LLMs) and multimodal large language models (MLLMs) have attracted widespread attention for their exceptional capabilities in understanding, reasoning, and generation, introducing transformative paradigms for integrating artificial intelligence into medicine. This survey provides a comprehensive overview of the development, principles, application scenarios, challenges, and future directions of LLMs and MLLMs in medicine. Specifically, it begins by examining the paradigm shift, tracing the transition from traditional models to LLMs and MLLMs, and highlighting the unique advantages of these LLMs and MLLMs in medical applications. Next, the survey reviews existing medical LLMs and MLLMs, providing detailed guidance on their construction and evaluation in a clear and systematic manner. Subsequently, to underscore the substantial value of LLMs and MLLMs in healthcare, the survey explores five promising applications in the field. Finally, the survey addresses the challenges confronting medical LLMs and MLLMs and proposes practical strategies and future directions for their integration into medicine. In summary, this survey offers a comprehensive analysis of the technical methodologies and practical clinical applications of medical LLMs and MLLMs, with the goal of bridging the gap between these advanced technologies and clinical practice, thereby fostering the evolution of the next generation of intelligent healthcare systems.}
}
@article{ZHANG2025125861,
title = {LLM-TSFD: An industrial time series human-in-the-loop fault diagnosis method based on a large language model},
journal = {Expert Systems with Applications},
volume = {264},
pages = {125861},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125861},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424027283},
author = {Qi Zhang and Chao Xu and Jie Li and Yicheng Sun and Jinsong Bao and Dan Zhang},
keywords = {Time series, Fault diagnosis 2.0, Task-driven, Large language model, Human-in-the-loop},
abstract = {Industrial time series data provides real-time information about the operational status of equipment and helps identify anomalies. Data-driven and knowledge-guided methods have become predominant in this field. However, these methods depend on industrial domain knowledge and high-quality industrial data which can lead to issues such as unclear diagnostic results and lengthy development cycles. This paper introduces a novel human-in-the-loop task-driven approach to reduce reliance on manually annotated data and improve the interpretability of diagnostic outcomes. This approach utilises a large language model for fault detection, fostering process autonomy and enhancing human–machine collaboration. Furthermore, this paper explores four key roles of the large language model: managing the data pipeline, correcting causality, controlling model management, and making decisions about diagnostic results. Additionally, it presents a prompt structure designed for fault diagnosis of time series data, enabling the large language model to realize task-driven. Finally, the paper validates the proposed framework through a case study in the context of steel metallurgy.}
}
@article{SEO2025,
title = {Performance Assessment of Large Language Models in Medical Consultation: Comparative Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/64318},
url = {https://www.sciencedirect.com/science/article/pii/S229196942500033X},
author = {Sujeong Seo and Kyuli Kim and Heyoung Yang},
keywords = {artificial intelligence, biomedical, large language model, depression, similarity measurement, text validity},
abstract = {Background
The recent introduction of generative artificial intelligence (AI) as an interactive consultant has sparked interest in evaluating its applicability in medical discussions and consultations, particularly within the domain of depression.
Objective
This study evaluates the capability of large language models (LLMs) in AI to generate responses to depression-related queries.
Methods
Using the PubMedQA and QuoraQA data sets, we compared various LLMs, including BioGPT, PMC-LLaMA, GPT-3.5, and Llama2, and measured the similarity between the generated and original answers.
Results
The latest general LLMs, GPT-3.5 and Llama2, exhibited superior performance, particularly in generating responses to medical inquiries from the PubMedQA data set.
Conclusions
Considering the rapid advancements in LLM development in recent years, it is hypothesized that version upgrades of general LLMs offer greater potential for enhancing their ability to generate “knowledge text” in the biomedical domain compared with fine-tuning for the biomedical field. These findings are expected to contribute significantly to the evolution of AI-based medical counseling systems.}
}
@article{YU2025104068,
title = {Amplifying commonsense knowledge via bi-directional relation integrated graph-based contrastive pre-training from large language models},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104068},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104068},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500010X},
author = {Liu Yu and Fenghui Tian and Ping Kuang and Fan Zhou},
keywords = {Commonsense knowledge, Large language models, Knowledge generation, LLMs for KG generation},
abstract = {Commonsense knowledge graph acquisition (CKGA) is vital in numerous knowledge-intensive applications such as question-answering and knowledge reasoning. Conventional CKGA methods rely on node-level and unidirectional relations, making them suffer from a shallow grasp of between entities and relations. Moreover, they also demand expensive, labor-intensive human annotations, and the yielding CK lacks diversity and quality. Existing commonsense knowledge bases such as ConceptNet or ATOMIC often struggle with significant scarcity and pose a major challenge in meeting the high demand for a vast amount of commonsense information. Given the recent momentum of large language models (LLMs), there is growing interest in leveraging them to overcome the above challenges. In this study, we propose a new paradigm to amplify commonsense knowledge via bi-directional relation integrated graph-based contrastive pre-training (BIRGHT) from the newest foundation models. BRIGHT is an integral and closed-loop framework composed of corpora construction, further contrastive pre-training, task-driven instruction tuning, filtering strategy, and an evaluation system. The key of BRIGHT is to leverage reverse relations to create a symmetric graph and transform the bi-directional relations into sentence-level ones. The reverse sentences are considered positive examples for forward sentences, and three types of negatives are introduced to ensure efficient contrastive learning, which mitigates the “reversal curse” issue as evidenced in experiments. Empirical results demonstrate that BRIGHT is able to generate novel knowledge (up to 397K) and that the GPT-4 acceptance rate is high quality, with up to 90.51% (ATOMIC) and 85.59% (ConceptNet) accuracy at top 1, which approaches human performance for these resources. Our BRIGHT is publicly available at https://github.com/GreyHuu/BRIGHT/tree/main.}
}
@article{LI2024112588,
title = {KnowBug: Enhancing Large language models with bug report knowledge for deep learning framework bug prediction},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112588},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112588},
url = {https://www.sciencedirect.com/science/article/pii/S095070512401222X},
author = {Chenglong Li and Zheng Zheng and Xiaoting Du and Xiangyue Ma and Zhengqi Wang and Xinheng Li},
keywords = {Bug report, Bug prediction, Deep learning framework, Large language model},
abstract = {Understanding and predicting the bug type is crucial for developers striving to enhance testing efficiency and reduce software release problems. Bug reports, although semi-structured, contain valuable semantic information, making their comprehension critical for accurate bug prediction. Recent advances in large language models (LLMs), especially generative LLMs, have demonstrated their power in natural language processing. Many studies have utilized these models to understand various forms of textual data. However, the capability of LLMs to fully understand bug reports remains uncertain. To tackle this challenge, we propose KnowBug, a framework designed to augment LLMs with knowledge from bug reports to improve their ability to predict bug types. In this framework, we utilize bug reports from open-source deep learning frameworks, design specialized prompts, and fine-tune LLMs to assess KnowBug’s proficiency in understanding bug reports and predicting different bug types.}
}
@article{YOU2025,
title = {Developing a Predictive Platform for Salmonella Antimicrobial Resistance Based on a Large Language Model and Quantum Computing},
journal = {Engineering},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S209580992500030X},
author = {Yujie You and Kan Tan and Zekun Jiang and Le Zhang},
keywords = { resistance prediction, Pan-genomics, Large language model, Quantum computing, Bioinformatics},
abstract = {As a common foodborne pathogen, Salmonella poses risks to public health safety, common given the emergence of antimicrobial-resistant strains. However, there is currently a lack of systematic platforms based on large language models (LLMs) for Salmonella resistance prediction, data presentation, and data sharing. To overcome this issue, we firstly propose a two-step feature-selection process based on the chi-square test and conditional mutual information maximization to find the key Salmonella resistance genes in a pan-genomics analysis and develop an LLM-based Salmonella antimicrobial-resistance predictive (SARPLLM) algorithm to achieve accurate antimicrobial-resistance prediction, based on Qwen2 LLM and low-rank adaptation. Secondly, we optimize the time complexity to compute the sample distance from the linear to logarithmic level by constructing a quantum data augmentation algorithm denoted as QSMOTEN. Thirdly, we build up a user-friendly Salmonella antimicrobial-resistance predictive online platform based on knowledge graphs, which not only facilitates online resistance prediction for users but also visualizes the pan-genomics analysis results of the Salmonella datasets.}
}
@article{JEON2025103076,
title = {Hybrid large language model approach for prompt and sensitive defect management: A comparative analysis of hybrid, non-hybrid, and GraphRAG approaches},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103076},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103076},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007274},
author = {Kahyun Jeon and Ghang Lee},
keywords = {Housing defect management, Large language model (LLM), Question–answering (QA), Fine-tuning, Graph-retrieval augmented generation (GraphRAG), Synthetic data generation},
abstract = {This study aims to propose a large language model (LLM)-enhanced defect question-answering (QA) method that can secure private and sensitive data while yielding high performance. Prompt responses to residents’ complaints are crucial for preventing recurring defects. However, traditional defect analysis and response methods rely on the expertise of a few skilled workers, making it difficult to ensure timely responses. The rapid advancement of LLMs offers a potential solution for improving defect QA tasks. However, many companies prohibit the use of closed-source LLM services, such as ChatGPT, due to concerns about potential data breaches. One possible solution is to use open-source LLMs like Llama and BERT, which can be locally installed and used. However, open-source LLMs typically perform worse than closed-source LLMs. Although the performance of open-source LLMs can be greatly improved through fine-tuning, the preparation of training datasets requires a significant amount of time and labor. To address these challenges, this study proposes a hybrid defect QA method that deploys an open-source LLM for defect management to secure sensitive information, and a closed-source LLM for generating a training dataset to reduce both the time and labor required. To validate the proposed method, we compare it to the state-of-the-art LLMs, GPT-4o and Llama 3, as well as graph retrieval-augmented generation (GraphRAG)-based QA systems, which have been extensively studied recently. Our results show that the hybrid LLM-based QA method achieved the highest ROUGE score of 81.6%. These findings demonstrate superior practical applicability, enabling cost-effective data generation and reliable domain adaptation within a secure data environment. This approach is beneficial for domain-specific tasks beyond defect management, where the accurate provision of specialized information and integration of historical knowledge are essential.}
}
@article{MISHRA2025104045,
title = {PageLLM: Incremental approach for updating a Security Knowledge Graph by using Page ranking and Large language model},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104045},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104045},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004047},
author = {Chinmaya Mishra and Himangshu Sarma and Saravanan M.},
keywords = {Security knowledge graph, Knowledge graph, Knowledge representation learning, Page ranking, Embedding, Generative AI, Large language models (LLMs), Static knowledge graph (SKG), Incremental knowledge graph (IKG), Full knowledge graph (FKG)},
abstract = {Due to increase in cyber crime and evolution of sophisticated tools and techniques, Threat Intelligence plays a critical role. It helps defenders to stay ahead of attackers by developing the right defense mechanism to invade those attacks. In this regards security knowledge graph plays a critical role which can be used to signify complex entities and their relationship in a graphical structure. Further projecting those entities and relationships in to the lower dimension using several embedding techniques such as TransE help in many down streaming task. The learned embedding can be used to predict new cyber threat which is very helpful for defenders to stay alert and develop necessary weapons to stay ahead of an attack. One of the major challenge security knowledge graph has its dynamic nature of changing intelligence. Active learning can be used to only update the substantial portion of embedding rather than retraining the knowledge graph from scratch which has higher time and space complexity. Also given the rise in generative AI and large language models which are super rich in context, there is a scope of utilizing those for building a robust and good quality security knowledge graph. We will discuss a novel methodology called PageLLM which utilizes page ranking and LLMs to enable active learning in an incremental way and will improve the quality of knowledge graph through enriched context.}
}
@article{JUNG2025103445,
title = {PersonaCraft: Leveraging language models for data-driven persona development},
journal = {International Journal of Human-Computer Studies},
volume = {197},
pages = {103445},
year = {2025},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2025.103445},
url = {https://www.sciencedirect.com/science/article/pii/S1071581925000023},
author = {Soon-Gyo Jung and Joni Salminen and Kholoud Khalil Aldous and Bernard J. Jansen},
keywords = {Personas, Persona generation, Survey research, Large language models, Generative AI},
abstract = {Generative AI, with its large language models (LLMs), provides various opportunities for the development of user-centric systems in human–computer interaction (HCI). Yet, use cases of LLMs in HCI are still scarce, calling for developing and evaluating real systems. We present PersonaCraft, a data-driven persona system using LLMs to address this need. The system analyzes a common source of user data – surveys – and generates personas, humanized representations of real segments in the data. By integrating LLMs with survey data analysis, PersonaCraft combines persona development and modern artificial intelligence methodologies to provide researchers and designers with user-centric insights from nearly any survey dataset about people. Various evaluations of the system, including with internal evaluators, general users (n = 127), and user experience professionals (n = 21), indicated that PersonaCraft personas scored high on all evaluation criteria of clarity, completeness, fluency, consistency, and credibility. The application of PersonaCraft can extend across a range of domains, including user research and population-level people research.}
}
@article{ZHANG2025102883,
title = {A survey on potentials, pathways and challenges of large language models in new-generation intelligent manufacturing},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {92},
pages = {102883},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102883},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524001704},
author = {Chao Zhang and Qingfeng Xu and Yongrui Yu and Guanghui Zhou and Keyan Zeng and Fengtian Chang and Kai Ding},
keywords = {Large language models, Intelligent manufacturing, New-generation artificial intelligence, LLM applications, Industry 5.0},
abstract = {Nowadays, Industry 5.0 starts to gain attention, which advocates that intelligent manufacturing should adequately consider the roles and needs of humans. In this context, how to enhance human capabilities or even liberate humans from the processes of perception, learning, decision-making, and execution has been one of the key issues to be addressed in intelligent manufacturing. Large language models (LLMs), as the breakthrough in new-generation artificial intelligence, could provide human-like interaction, reasoning, and replies suitable for various application scenarios, thus demonstrating significant potential to address the above issues by providing aid or becoming partners for humans in perception, learning, decision-making, and execution in intelligent manufacturing. The combination of LLMs and intelligent manufacturing has inherent advantages and is expected to become the next research hotspot. Hence, this paper primarily conducts a systematic literature review on the application of LLMs in intelligent manufacturing to identify the promising research topics with high potential for further investigations. Firstly, this paper reveals the concept, connotation, and foundational architecture of LLMs. Then, several typical and trending interdisciplinary LLM applications, such as healthcare, drug discovery, social & economic, education, and software development, are summarized, on which an LLM-enabled intelligent manufacturing architecture is designed to provide a reference for applying LLMs in intelligent manufacturing. Thirdly, the specific pathways for applying LLMs in intelligent manufacturing are explored from the perspectives of design, production, and service. Finally, this paper identifies the limitations, barriers, and challenges that will be encountered during the research and application of LLMs in intelligent manufacturing, while providing potential research directions to address these limitations, barriers, and challenges.}
}
@article{HANNAH2025100843,
title = {On the legal implications of Large Language Model answers: A prompt engineering approach and a view beyond by exploiting Knowledge Graphs},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100843},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100843},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000295},
author = {George Hannah and Rita T. Sousa and Ioannis Dasoulas and Claudia d’Amato},
keywords = {Knowledge Graph, Large Language Models, Prompt engineering, Legislative texts},
abstract = {With the recent surge in popularity of Large Language Models (LLMs), there is the rising risk of users blindly trusting the information in the response. Nevertheless, there are cases where the LLM recommends actions that have potential legal implications and this may put the user in danger. We provide an empirical analysis on multiple existing LLMs showing the urgency of the problem. Hence, we propose a first short-term solution, consisting in an approach for isolating these legal issues through prompt engineering. We prove that this solution is able to stem some risks related to legal implications, nonetheless we also highlight some limitations. Hence, we argue on the need for additional knowledge-intensive resources and specifically Knowledge Graphs for fully solving these limitations. For the purpose, we draw our proposal aiming at designing and developing a solution powered by a legal Knowledge Graph (KG) that, besides capturing and alerting the user on possible legal implications coming from the LLM answers, is also able to provide actual evidence for them by supplying citations of the interested laws. We conclude with a brief discussion on the issues that may be needed to solve for building a comprehensive legal Knowledge Graph}
}
@article{ZHOU2024102333,
title = {CausalKGPT: Industrial structure causal knowledge-enhanced large language model for cause analysis of quality problems in aerospace product manufacturing},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102333},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102333},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004615},
author = {Bin Zhou and Xinyu Li and Tianyuan Liu and Kaizhou Xu and Wei Liu and Jinsong Bao},
keywords = {Aerospace product, Causal quality-related knowledge graph, Large language model, Causal knowledge graph-guided prompt, Cause analysis of quality defects},
abstract = {The whole cycle for manufacturing aerospace thin-walled shells is a lengthy and sophisticated process. A large amount of quality-related data exists within and between processes, involving many types of quality defects and influencing factors. However, there are ambiguous causal associations among quality-related data affecting the shape-properties of the shell. Also, the coupling of long processes and multiple factors makes it hard to analyze the main factors that affect the quality defects in shell manufacturing. In this paper, taking into account the advantages of causal Scientology and the large language model (LLM), we propose an industrial structure causal knowledge-enhanced large language model for the cause analysis of quality defects in aerospace product manufacturing. To reinforce the causal associations among quality-related data deriving from manufacturing documents (product defect survey sheets, quality inspection, and maintenance reports), a structure causal graph-based sum-product network (SCG-SPN) model is designed to model machining quality-related knowledge and eliminate pseudo-association confounding factors by doing an intervention. Thus, a causal quality-related knowledge graph (CQKG) with high-quality causal associations is constructed. With this, to provide a trustworthy guarantee in responding to quality problem solving, we construct a quality-related prompt dataset with multi-round conversations based on CQKG. Then, a novel P-tuning that adapts to utilize external CQKG instructions is designed to fine-tune an open-source ChatGLM base model. Based on this, a causal knowledge graph-augmented LLM, named CausalKGPT, is developed to enable reasoning and responding to quality defects in both Chinese and English. It uses natural text descriptions related to quality defects as input and takes a quality-related causal knowledge graph as an additional corpus. Finally, the case study shows that the CausalKGPT performs with more expertise and reliability in responding to quality question solving of aerospace shell manufacturing than the classic commercial models like ChatGPT and GPT4. The results indicate that the proposed method may provide a trustworthy guide in assisting workers to analyze quality defects in aerospace products.}
}
@article{YAO2024100211,
title = {A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly},
journal = {High-Confidence Computing},
volume = {4},
number = {2},
pages = {100211},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
author = {Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang},
keywords = {Large Language Model (LLM), LLM security, LLM privacy, ChatGPT, LLM attacks, LLM vulnerabilities},
abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.}
}
@article{TRAPPEY2024102332,
title = {Patent litigation mining using a large language model—Taking unmanned aerial vehicle development as the case domain},
journal = {World Patent Information},
pages = {102332},
year = {2024},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2024.102332},
url = {https://www.sciencedirect.com/science/article/pii/S0172219024000723},
author = {Amy J.C. Trappey and Shao-Chien Chou and Gi-Kuen J. Li},
keywords = {Unmanned aerial vehicle (UAV), Drone, Patent analysis, Patent litigation mining, Technology function matrix, Dynamic topic modeling, Large language model},
abstract = {As unmanned aerial vehicle (UAV), also called “drone”, swiftly advances with innovative functions and applications, the surge in patent applications has profoundly reshaped the intellectual property (IP) landscape in the UAV industry, leading to a growing number of litigations. This study is structured in two phases, aiming to develop an intelligent approach to analyzing the trend and evolution of patent litigations. The first phase involves macro- and micro-patent analyses of the related technology domain. Macro patent analysis elucidates the fundamental patent information in the drone industry, while micro patent analysis leverages the technology function matrix (TFM) to identify R&D hotspots and potentials. The second phase involves litigation (judgement) mining based on large language model (LLM). Beginning with the construction of a knowledge ontology, the domain infringement landscape can be detected through TFMs. A comparative analysis of the two-phase TFMs (i.e., both TFMs of patent and infringement allocations) is then conducted to pinpoint the key legal actions and the relevant technology. To drill deeper in infringement mining, dynamic topic modeling (DTM) is applied to analyze trends and dynamics in drone controller technology over time. This study aims to strengthen IP protection by developing an intelligent litigation mining approach that adopts large language model (LLM) and uses UAV/drone litigation studies as examples to show how the approach being applied in the industry.}
}
@article{BENJIRA2025102405,
title = {Automated mapping between SDG indicators and open data: An LLM-augmented knowledge graph approach},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102405},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102405},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001290},
author = {Wissal Benjira and Faten Atigui and Bénédicte Bucher and Malika Grim-Yefsah and Nicolas Travers},
keywords = {Sustainable Development Goals (SDG), Large language model (LLM), Knowledge graph (KG), Open data, Schema mapping},
abstract = {Meeting the Sustainable Development Goals (SDGs) presents a large-scale challenge for all countries. SDGs established by the United Nations provide a comprehensive framework for addressing global issues. To monitor progress towards these goals, we need to develop key performance indicators and integrate and analyze heterogeneous datasets. The definition of these indicators requires the use of existing data and metadata. However, the diversity of data sources and formats raises major issues in terms of structuring and integration. Despite the abundance of open data and metadata, its exploitation remains limited, leaving untapped potential for guiding urban policies towards sustainability. Thus, this paper introduces a novel approach for SDG indicator computation, leveraging the capabilities of Large Language Models (LLMs) and Knowledge Graphs (KGs). We propose a method that combines rule-based filtering with LLM-powered schema mapping to establish semantic correspondences between diverse data sources and SDG indicators, including disaggregation. Our approach integrates these mappings into a KG, which enables indicator computation by querying graph’s topology. We evaluate our method through a case study focusing on the SDG Indicator 11.7.1 about accessibility of public open spaces. Our experimental results show significant improvements in accuracy, precision, recall, and F1-score compared to traditional schema mapping techniques.}
}
@article{HUSSIEN2025125914,
title = {RAG-based explainable prediction of road users behaviors for automated driving using knowledge graphs and large language models},
journal = {Expert Systems with Applications},
volume = {265},
pages = {125914},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125914},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424027817},
author = {Mohamed Manzour Hussien and Angie Nataly Melo and Augusto Luis Ballardini and Carlota Salinas Maldonado and Rubén Izquierdo and Miguel Ángel Sotelo},
keywords = {Road users’ behaviors, Explainable predictions, Pedestrian crossing actions, Lane change maneuvers, Autonomous driving},
abstract = {The prediction of road user behaviors in the context of autonomous driving has attracted considerable attention from the scientific community in recent years. Most works focus on predicting behaviors based on kinematic information alone, a simplification of reality since road users are humans, and as such they are highly influenced by their surrounding context. In addition, a large plethora of research works rely on powerful Deep Learning techniques, which exhibit high-performance metrics in prediction tasks but may lack the ability to fully understand and exploit the contextual semantic information contained in the road scene, not to mention their inability to provide explainable predictions that can be understood by humans. In this work, we propose an explainable road users’ behavior prediction system that integrates the reasoning abilities of Knowledge Graphs (KG) and the expressiveness capabilities of Large Language Models (LLM) by using Retrieval Augmented Generation (RAG) techniques. For that purpose, Knowledge Graph Embeddings (KGE) and Bayesian inference are combined to allow the deployment of a fully inductive reasoning system that enables the issuing of predictions that rely on legacy information contained in the graph, as well as on current evidence gathered in real-time by onboard sensors. Two use cases have been implemented following the proposed approach: 1) Prediction of pedestrians’ crossing actions; and 2) Prediction of lane change maneuvers. In both cases, the performance attained exceeds the current state-of-the-art in terms of anticipation and F1 score, showing a promising avenue for future research in this field.}
}
@article{CHEN2024104804,
title = {Enhancing emergency decision-making with knowledge graphs and large language models},
journal = {International Journal of Disaster Risk Reduction},
volume = {113},
pages = {104804},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104804},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924005661},
author = {Minze Chen and Zhenxiang Tao and Weitong Tang and Tingxin Qin and Rui Yang and Chunli Zhu},
keywords = {Emergency decision support, Large language model, Knowledge graph, Decision support system},
abstract = {Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals’ cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL demonstrates significant improvement over baseline models in various emergency response scenarios, as rated by emergency commanders and firefighters. This work introduces a novel approach to applying LLMs to enhance emergency decision-making.}
}
@article{XIAO2024104730,
title = {FuseLinker: Leveraging LLM’s pre-trained text embeddings and domain knowledge to enhance GNN-based link prediction on biomedical knowledge graphs},
journal = {Journal of Biomedical Informatics},
volume = {158},
pages = {104730},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104730},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001485},
author = {Yongkang Xiao and Sinian Zhang and Huixue Zhou and Mingchen Li and Han Yang and Rui Zhang},
keywords = {Link Prediction, Knowledge graph, Graph Neural Network, Large Language Model, Drug Repurposing},
abstract = {Objective
To develop the FuseLinker, a novel link prediction framework for biomedical knowledge graphs (BKGs), which fully exploits the graph’s structural, textual and domain knowledge information. We evaluated the utility of FuseLinker in the graph-based drug repurposing task through detailed case studies.
Methods
FuseLinker leverages fused pre-trained text embedding and domain knowledge embedding to enhance the graph neural network (GNN)-based link prediction model tailored for BKGs. This framework includes three parts: a) obtain text embeddings for BKGs using embedding-visible large language models (LLMs), b) learn the representations of medical ontology as domain knowledge information by employing the Poincaré graph embedding method, and c) fuse these embeddings and further learn the graph structure representations of BKGs by applying a GNN-based link prediction model. We evaluated FuseLinker against traditional knowledge graph embedding models and a conventional GNN-based link prediction model across four public BKG datasets. Additionally, we examined the impact of using different embedding-visible LLMs on FuseLinker’s performance. Finally, we investigated FuseLinker’s ability to generate medical hypotheses through two drug repurposing case studies for Sorafenib and Parkinson’s disease.
Results
By comparing FuseLinker with baseline models on four BKGs, our method demonstrates superior performance. The Mean Reciprocal Rank (MRR) and Area Under receiver operating characteristic Curve (AUROC) for KEGG50k, Hetionet, SuppKG and ADInt are 0.969 and 0.987, 0.548 and 0.903, 0.739 and 0.928, and 0.831 and 0.890, respectively.
Conclusion
Our study demonstrates that FuseLinker is an effective novel link prediction framework that integrates multiple graph information and shows significant potential for practical applications in biomedical and clinical tasks. Source code and data are available at https://github.com/YKXia0/FuseLinker.}
}
@article{QIANG2025129373,
title = {Enhancing few-shot KB-VQA with panoramic image captions guided by Large Language Models},
journal = {Neurocomputing},
volume = {623},
pages = {129373},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129373},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225000451},
author = {Pengpeng Qiang and Hongye Tan and Xiaoli Li and Dian Wang and Ru Li and Xinyi Sun and Hu Zhang and Jiye Liang},
keywords = {Knowledge-based visual question answering, Image caption, Large language model, In-context learning},
abstract = {Current state-of-the-art (SOTA) KB-VQA techniques involve transforming images into image captions as prompts to harness the potent reasoning capabilities of large language models (LLMs) for generating answers. However, generic image captions often fall short in capturing crucial visual details, essential for LLMs to deliver precise responses. To address this challenge, we propose an image captioning model that effectively utilizes a set of visual language models, such as BLIP2, GRiT, OCR, etc., to extract rich visual information from images. Subsequently, we employ the inferential and summarization capabilities of LLM to generate panoramic image descriptions enriched with intricate details. Simultaneously, we employ Contextual Constraint Examples and Constraint Instruction to mitigate the potential hallucination issues arising from LLM-generated image captions. Extensive experiments validate the superiority and scalability of our proposed method, achieving significant improvements over SOTA methods in challenging few-shot settings. For instance, on the challenging OK-VQA, our method outperforms PICa by 6.5%. On the VQAv2 dataset, our method surpasses the SOTA approach by 5.4%.}
}
@article{LEE2024105846,
title = {Performance comparison of retrieval-augmented generation and fine-tuned large language models for construction safety management knowledge retrieval},
journal = {Automation in Construction},
volume = {168},
pages = {105846},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105846},
url = {https://www.sciencedirect.com/science/article/pii/S092658052400582X},
author = {Jungwon Lee and Seungjun Ahn and Daeho Kim and Dongkyun Kim},
keywords = {Large Language Model (LLM), Retrieval-Augmented Generation (RAG), Fine-tuned LLM, Construction safety, Knowledge graph},
abstract = {Construction safety standards are in unstructured formats like text and images, complicating their effective use in daily tasks. This paper compares the performance of Retrieval-Augmented Generation (RAG) and fine-tuned Large Language Model (LLM) for the construction safety knowledge retrieval. The RAG model was created by integrating GPT-4 with a knowledge graph derived from construction safety guidelines, while the fine-tuned LLM was fine-tuned using a question-answering dataset derived from the same guidelines. These models' performance is tested through case studies, using accident synopses as a query to generate preventive measurements. The responses were assessed using metrics, including cosine similarity, Euclidean distance, BLEU, and ROUGE scores. It was found that both models outperformed GPT-4, with the RAG model improving by 21.5 % and the fine-tuned LLM by 26 %. The findings highlight the relative strengths and weaknesses of the RAG and fine-tuned LLM approaches in terms of applicability and reliability for safety management.}
}
@article{BENJDIRA2025107723,
title = {Prompting Robotic Modalities (PRM): A structured architecture for centralizing language models in complex systems},
journal = {Future Generation Computer Systems},
volume = {166},
pages = {107723},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2025.107723},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X25000184},
author = {Bilel Benjdira and Anis Koubaa and Anas M. Ali},
keywords = {Expert systems architectures, Robotics, Languages models in robotics, Prompting robotic modalities, Large language models, LLMs, Vision language models, VLMs, Robotic operating system, ROS, ROS2, Robotic prompt engineering, Visual prompt, LLM prompt},
abstract = {Despite significant advancements in robotics and AI, existing systems often struggle to integrate diverse modalities (e.g., image, sound, actuator data) into a unified framework, resulting in fragmented architectures that limit adaptability, scalability, and explainability. To address these gaps, this paper introduces Prompting Robotic Modalities (PRM), a novel architecture that centralizes language models for controlling and managing complex systems through natural language. In PRM, each system modality (e.g., image, sound, actuator) is handled independently by a Modality Language Model (MLM), while a central Task Modality, powered by a Large Language Model (LLM), orchestrates complex tasks using information from the MLMs. Each MLM is trained on datasets that pair modality-specific data with rich textual descriptions, enabling intuitive, language-based interaction. We validate PRM with two main contributions: (1) ROSGPT_Vision, a new open-source ROS 2 package (available at https://github.com/bilel-bj/ROSGPT_Vision) for visual modality tasks, achieving up to 66% classification accuracy in driver-focus monitoring—surpassing other tested models in its category; and (2) CarMate, a driver-distraction detection application that significantly reduces development time and cost by allowing rapid adaptation to new monitoring tasks via simple prompt adjustments. In addition, we develop a Navigation Language Model (NLM) that converts free-form human language orders into detailed ROS commands, underscoring PRM’s modality-agnostic adaptability. Experimental results demonstrate that PRM simplifies system development, outperforms baseline vision-language approaches in specialized tasks (e.g., driver monitoring), reduces complexity through prompt engineering rather than extensive coding, and enhances explainability via natural-language-based diagnostics. Hence, PRM lays a promising foundation for next-generation complex and robotic systems by integrating advanced language model capabilities at their core, making them more adaptable to new environments, cost-effective, and user-friendly.}
}
@article{LIANG2024870,
title = {A survey of LLM-augmented knowledge graph construction and application in complex product design},
journal = {Procedia CIRP},
volume = {128},
pages = {870-875},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.07.069},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007911},
author = {Xinxin Liang and Zuoxu Wang and Mingrui Li and Zhijie Yan},
keywords = {Knowledge Graph, Large Language Model, Design: challenges & Innovation},
abstract = {In the field of complex product design, deploying knowledge graphs (KGs) has become a promising trend due to its strength on exploiting and applying the large-scale, complex, and specialized domain knowledge. In recent years, large language models (LLMs) have also attracted much attention due to their outstanding performance in natural language understanding and generation. However, in the research of complex product design dominated by domain knowledge, few studies involve LLMS and KG at the same time. To fill this gap, we survey 42 articles published in the last four years, focusing on three key questions. The combination of LLM and KG in specific applications in complex product design is deeply discussed. The analysis reveals how these techniques facilitate data collection, design concept formation and design process optimization and proposes a technical framework combining LLM and KG for complex product design domain. In addition, we identify key challenges and propose directions for future research. As an explorative survey paper, this paper provides insightful ideas for implementing more specialized domain knowledge graph in complex product design field.}
}
@article{HAKANSSON20245458,
title = {Generative AI and Large Language Models - Benefits, Drawbacks, Future and Recommendations},
journal = {Procedia Computer Science},
volume = {246},
pages = {5458-5468},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.689},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027492},
author = {Anne Håkansson and Gloria Phillips-Wren},
keywords = {Natural Language Processing, Generative AI, Large Language Models},
abstract = {Natural language processing, with parsing and generation, has a long tradition. Parsing has been easier to perform than a generation but with generative artificial intelligence (a.k.a Gen AI) and large language models (abbr. LLMs), this has changed. Generative artificial intelligence is a type of artificial intelligence that uses a large data set to create something in the genre of that data set. It can generate different outputs ranging from texts, audio, objects, pictures, and paintings to videos, but also synthetic data. LLMs use deep learning and deep neural networks to train on large text corpora for recognizing and generating texts. These models are based on massive data sets, collected from databases and the web. They use transformer models to detect how elements in sequences relate to each other. This provides context support. Two well-known large language models are the Generative Pre-trained Transformer, GPT, used in ChatGPT and Bidirectional Encoder Representations from Transformers, BERT. Although LLMs have advantages, they have problems. This paper presents generative artificial intelligence and LLMs with benefits and drawbacks. Results from applying these models have shown that they can work well for accuracy in specificity, user personalization and human-computer communication but they may not provide acceptable, reliable and truthful results. For example, ethics, hallucinations and incorrect information, or misjudgments, are some major problems. The paper ends with future directions, research questions on LLMs, and recommendations.}
}
@article{CAO2024,
title = {An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontology-Enhanced Large Language Models: Development Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/60665},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001881},
author = {Lang Cao and Jimeng Sun and Adam Cross},
keywords = {rare disease, clinical informatics, LLM, natural language processing, machine learning, artificial intelligence, large language models, data extraction, ontologies, knowledge graphs, text mining},
abstract = {Background
Rare diseases affect millions worldwide but sometimes face limited research focus individually due to low prevalence. Many rare diseases do not have specific International Classification of Diseases, Ninth Edition (ICD-9) and Tenth Edition (ICD-10), codes and therefore cannot be reliably extracted from granular fields like “Diagnosis” and “Problem List” entries, which complicates tasks that require identification of patients with these conditions, including clinical trial recruitment and research efforts. Recent advancements in large language models (LLMs) have shown promise in automating the extraction of medical information, offering the potential to improve medical research, diagnosis, and management. However, most LLMs lack professional medical knowledge, especially concerning specific rare diseases, and cannot effectively manage rare disease data in its various ontological forms, making it unsuitable for these tasks.
Objective
Our aim is to create an end-to-end system called automated rare disease mining (AutoRD), which automates the extraction of rare disease–related information from medical text, focusing on entities and their relations to other medical concepts, such as signs and symptoms. AutoRD integrates up-to-date ontologies with other structured knowledge and demonstrates superior performance in rare disease extraction tasks. We conducted various experiments to evaluate AutoRD’s performance, aiming to surpass common LLMs and traditional methods.
Methods
AutoRD is a pipeline system that involves data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implemented this system using GPT-4 and medical knowledge graphs developed from the open-source Human Phenotype and Orphanet ontologies, using techniques such as chain-of-thought reasoning and prompt engineering. We quantitatively evaluated our system’s performance in entity extraction, relation extraction, and knowledge graph construction. The experiment used the well-curated dataset RareDis2023, which contains medical literature focused on rare disease entities and their relations, making it an ideal dataset for training and testing our methodology.
Results
On the RareDis2023 dataset, AutoRD achieved an overall entity extraction F1-score of 56.1% and a relation extraction F1-score of 38.6%, marking a 14.4% improvement over the baseline LLM. Notably, the F1-score for rare disease entity extraction reached 83.5%, indicating high precision and recall in identifying rare disease mentions. These results demonstrate the effectiveness of integrating LLMs with medical ontologies in extracting complex rare disease information.
Conclusions
AutoRD is an automated end-to-end system for extracting rare disease information from text to build knowledge graphs, addressing critical limitations of existing LLMs by improving identification of these diseases and connecting them to related clinical features. This work underscores the significant potential of LLMs in transforming health care, particularly in the rare disease domain. By leveraging ontology-enhanced LLMs, AutoRD constructs a robust medical knowledge base that incorporates up-to-date rare disease information, facilitating improved identification of patients and resulting in more inclusive research and trial candidacy efforts.}
}
@article{CHANG2024,
title = {Use of SNOMED CT in Large Language Models: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/62924},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001364},
author = {Eunsuk Chang and Sumi Sung},
keywords = {SNOMED CT, ontology, knowledge graph, large language models, natural language processing, language models},
abstract = {Background
Large language models (LLMs) have substantially advanced natural language processing (NLP) capabilities but often struggle with knowledge-driven tasks in specialized domains such as biomedicine. Integrating biomedical knowledge sources such as SNOMED CT into LLMs may enhance their performance on biomedical tasks. However, the methodologies and effectiveness of incorporating SNOMED CT into LLMs have not been systematically reviewed.
Objective
This scoping review aims to examine how SNOMED CT is integrated into LLMs, focusing on (1) the types and components of LLMs being integrated with SNOMED CT, (2) which contents of SNOMED CT are being integrated, and (3) whether this integration improves LLM performance on NLP tasks.
Methods
Following the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines, we searched ACM Digital Library, ACL Anthology, IEEE Xplore, PubMed, and Embase for relevant studies published from 2018 to 2023. Studies were included if they incorporated SNOMED CT into LLM pipelines for natural language understanding or generation tasks. Data on LLM types, SNOMED CT integration methods, end tasks, and performance metrics were extracted and synthesized.
Results
The review included 37 studies. Bidirectional Encoder Representations from Transformers and its biomedical variants were the most commonly used LLMs. Three main approaches for integrating SNOMED CT were identified: (1) incorporating SNOMED CT into LLM inputs (28/37, 76%), primarily using concept descriptions to expand training corpora; (2) integrating SNOMED CT into additional fusion modules (5/37, 14%); and (3) using SNOMED CT as an external knowledge retriever during inference (5/37, 14%). The most frequent end task was medical concept normalization (15/37, 41%), followed by entity extraction or typing and classification. While most studies (17/19, 89%) reported performance improvements after SNOMED CT integration, only a small fraction (19/37, 51%) provided direct comparisons. The reported gains varied widely across different metrics and tasks, ranging from 0.87% to 131.66%. However, some studies showed either no improvement or a decline in certain performance metrics.
Conclusions
This review demonstrates diverse approaches for integrating SNOMED CT into LLMs, with a focus on using concept descriptions to enhance biomedical language understanding and generation. While the results suggest potential benefits of SNOMED CT integration, the lack of standardized evaluation methods and comprehensive performance reporting hinders definitive conclusions about its effectiveness. Future research should prioritize consistent reporting of performance comparisons and explore more sophisticated methods for incorporating SNOMED CT’s relational structure into LLMs. In addition, the biomedical NLP community should develop standardized evaluation frameworks to better assess the impact of ontology integration on LLM performance.}
}
@article{MA2024128490,
title = {A review of graph neural networks and pretrained language models for knowledge graph reasoning},
journal = {Neurocomputing},
volume = {609},
pages = {128490},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128490},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401261X},
author = {Jiangtao Ma and Bo Liu and Kunlin Li and Chenliang Li and Fan Zhang and Xiangyang Luo and Yaqiong Qiao},
keywords = {Knowledge graph reasoning, Graph neural networks, Pretrained language models, Logic rules},
abstract = {Knowledge Graph (KG) stores human knowledge facts in an intuitive graphical structure but faces challenges such as incomplete construction or inability to handle new knowledge. Knowledge Graph Reasoning (KGR) can make KGs more accurate, complete, and trustworthy to support various artificial intelligence applications better. Currently, the popular KGR methods are based on graph neural networks (GNNs). Recent studies have shown that hybrid logic rules and synergized pre-trained language models (PLMs) can enhance the GNN-based KGR methods. These methods mainly focus on data sparsity, insufficient knowledge evolution patterns, multi-modal fusion, and few-shot reasoning. Although many studies have been conducted, there are still few review papers that comprehensively summarize and explore KGR methods related to GNNs, logic rules, and PLMs. Therefore, this paper provides a comprehensive review of GNNs and PLMs for KGR based on a large number of high-quality papers. To present a clear overview of KGR, we propose a general framework. Specifically, we first introduce the KG preparation. Then we provide an overview of KGR methods, in which we categorize KGR methods into GNNs-based, logic rules-enhanced, and pre-trained language models-enhanced KGR methods. Furthermore, we also compare and analyze the GNN-based KGR methods in two scenarios. Moreover, we also present the application of KGR in different fields. Finally, we discuss the current challenges and future research directions for KGR.}
}
@article{VENKATASUBRAMANIAN2025108895,
title = {Quo Vadis ChatGPT? From large language models to Large Knowledge Models},
journal = {Computers & Chemical Engineering},
volume = {192},
pages = {108895},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108895},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424003132},
author = {Venkat Venkatasubramanian and Arijit Chakraborty},
keywords = {Artificial intelligence, Large language model, Knowledge graph, Domain-specific language processing, Machine learning, Hybrid AI},
abstract = {The startling success of ChatGPT and other large language models (LLMs) using transformer-based generative neural network architecture in applications such as natural language processing and image synthesis has many researchers excited about potential opportunities in process systems engineering (PSE). The almost human-like performance of LLMs in these areas is indeed very impressive, surprising, and a major breakthrough. Their capabilities are very useful in certain tasks, such as writing first drafts of documents, code writing assistance, text summarization, etc. However, their success is limited in highly scientific domains as they cannot yet reason, plan, or explain due to their lack of in-depth mechanistic domain knowledge. This is a problem in domains such as chemical engineering as they are governed by fundamental laws of physics and chemistry (and biology), constitutive relations, and highly technical knowledge about materials, processes, and systems. Although purely data-driven machine learning has its immediate uses, the long-term success of AI in scientific and engineering domains would depend on developing hybrid AI systems that combine first principles and technical knowledge effectively. We call these hybrid AI systems Large Knowledge Models (LKMs), as they will not be limited to only NLP-based techniques or NLP-like applications. In this paper, we discuss the challenges and opportunities in developing such systems in chemical engineering.}
}
@article{CHANDRASEKHAR2024100232,
title = {AMGPT: A large language model for contextual querying in additive manufacturing},
journal = {Additive Manufacturing Letters},
volume = {11},
pages = {100232},
year = {2024},
issn = {2772-3690},
doi = {https://doi.org/10.1016/j.addlet.2024.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2772369024000409},
author = {Achuth Chandrasekhar and Jonathan Chan and Francis Ogoke and Olabode Ajenifujah and Amir {Barati Farimani}},
keywords = {Large language models, Retrieval-augmented generation, Machine learning, Contextual querying, Laser powder bed fusion},
abstract = {Generalized large language models (LLMs) such as GPT-4 may not provide specific answers to queries formulated by materials science researchers. These models may produce a high-level outline but lack the capacity to return detailed instructions on manufacturing and material properties of novel alloys. We introduce “AMGPT”, a specialized LLM text generator designed for metal AM queries. The goal of AMGPT is to assist researchers and users in navigating a curated corpus of literature. Instead of training from scratch, we employ a pre-trained Llama2-7B model from Hugging Face in a Retrieval-Augmented Generation (RAG) setup, utilizing it to dynamically incorporate information from ∼50 AM papers and textbooks in PDF format. Mathpix is used to convert these PDF documents into TeX format, facilitating their integration into the RAG pipeline managed by LlamaIndex. A query retrieval function has also been added, enabling the system to fetch relevant literature from Elsevier journals based on the context of the query. Expert evaluations of this project highlight that specific embeddings from the RAG setup accelerate response times and maintain coherence in the generated text.}
}
@article{SAHBI2025102392,
title = {Semantic vs. LLM-based approach: A case study of KOnPoTe vs. Claude for ontology population from French advertisements},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102392},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102392},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001162},
author = {Aya Sahbi and Céline Alec and Pierre Beust},
keywords = {Ontology population, LLM, Textual descriptions},
abstract = {Automatic ontology population is the process of identifying, extracting, and integrating relevant information from diverse sources to instantiate the classes and properties specified in an ontology, thereby creating a Knowledge Graph (KG) for a particular domain. In this study, we evaluate two approaches for ontology population from text: KOnPoTe, a semantic technique that employs textual and domain knowledge analysis, and a generative AI method leveraging Claude, a Large Language Model (LLM). We conduct comparative experiments on three French advertisement domains: real estate, boats, and restaurants to assess the performance of these techniques. Our analysis highlights the respective strengths and limitations of the semantic approach and the LLM-based one in the context of the ontology population process.}
}
@article{FAN20241269,
title = {Unleashing the Potential of Large Language Models for Knowledge Augmentation: A Practical Experiment on Incremental Sheet Forming},
journal = {Procedia Computer Science},
volume = {232},
pages = {1269-1278},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.01.125},
url = {https://www.sciencedirect.com/science/article/pii/S187705092400125X},
author = {Haolin Fan and Jerry Fuh and Wen Feng Lu and A. Senthil Kumar and Bingbing Li},
keywords = {Incremental Sheet Forming, Large Language Models, Domain Knowledge Augmentation, Fine-tuning},
abstract = {As the influence of Incremental Sheet Forming (ISF) grows in manufacturing sectors, so does the demand for precise and updated knowledge construction in this domain. In this research, we evaluate the capability of Large Language Models (LLMs) to capture domain-specific knowledge, using ISF as a case study. Recognizing common LLMs’ limitations such as potential inaccuracies and outdated information reliance, we propose a comprehensive approach involving automated and adaptive knowledge extraction, enrichment, and integration into an ISF-specific dataset. We then fine-tune the LLMs for ISF-related text classification and prompt response tasks. Our results reveal a significant enhancement in LLMs’ performance within the ISF domain, with a domain knowledge acquisition rate exceeding that of GPT-3.5 by 10.4%, achieved by the fine-tuned Alpaca-33B model. Additionally, we introduce a novel conversational prototype designed to refine the accuracy and relevance of LLMs in the ISF domain. Our findings will guide future efforts in downstream tasks such as ISF-domain knowledge graph construction and quality prediction.}
}
@article{GUAN2025102359,
title = {Leveraging large language models for peptide antibiotic design},
journal = {Cell Reports Physical Science},
volume = {6},
number = {1},
pages = {102359},
year = {2025},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.102359},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424006738},
author = {Changge Guan and Fabiano C. Fernandes and Octavio L. Franco and Cesar {de la Fuente-Nunez}},
abstract = {Summary
Large language models (LLMs) have significantly impacted various domains of our society, including recent applications in complex fields such as biology and chemistry. These models, built on sophisticated neural network architectures and trained on extensive datasets, are powerful tools for designing, optimizing, and generating molecules. This review explores the role of LLMs in discovering and designing antibiotics, focusing on peptide molecules. We highlight advancements in drug design and outline the challenges of applying LLMs in these areas.}
}
@article{SUNIL2024102665,
title = {The gene function prediction challenge: Large language models and knowledge graphs to the rescue},
journal = {Current Opinion in Plant Biology},
volume = {82},
pages = {102665},
year = {2024},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2024.102665},
url = {https://www.sciencedirect.com/science/article/pii/S1369526624001560},
author = {Rohan Shawn Sunil and Shan Chun Lim and Manoj Itharajula and Marek Mutwil},
abstract = {Elucidating gene function is one of the ultimate goals of plant science. Despite this, only ∼15 % of all genes in the model plant Arabidopsis thaliana have comprehensively experimentally verified functions. While bioinformatical gene function prediction approaches can guide biologists in their experimental efforts, neither the performance of the gene function prediction methods nor the number of experimental characterization of genes has increased dramatically in recent years. In this review, we will discuss the status quo and the trajectory of gene function elucidation and outline the recent advances in gene function prediction approaches. We will then discuss how recent artificial intelligence advances in large language models and knowledge graphs can be leveraged to accelerate gene function predictions and keep us updated with scientific literature.}
}
@article{ZENG2025112837,
title = {KoSEL: Knowledge subgraph enhanced large language model for medical question answering},
journal = {Knowledge-Based Systems},
volume = {309},
pages = {112837},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112837},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124014710},
author = {Zefan Zeng and Qing Cheng and Xingchen Hu and Yan Zhuang and Xinwang Liu and Kunlun He and Zhong Liu},
keywords = {Domain-specific, Knowledge graph, Large language model, Medical question answering, Privacy, Retrieval},
abstract = {The integration of medical knowledge graphs (KGs) and large language models (LLMs) for medical question answering (Q&A) has attracted considerable interest in recent studies. However, current approaches that combine KGs and LLMs tend to either integrate KGs directly into the fine-tuning process of LLMs or use entire KGs as a contextual prompt base for LLMs to reason, raising concerns regarding potential data leakage and reasoning confusion. In this study, we propose KoSEL (Knowledge Subgraph Enhanced Large Language Model), a novel medical Q&A framework based on KG-enhanced LLMs. KoSEL comprises two modules: Knowledge Retrieval (KR) and Reasoning and Answering (RA). The KR module is LLM-independent and employs an entity-linking algorithm and a subgraph construction and fusion strategy to retrieve question-relevant knowledge. The RA module conveys prompts to the LLM for information extraction, knowledge fusion, reasoning, and answer generation. KoSEL, which is designed as a plug-and-play framework, effectively fuses structural and textual knowledge while ensuring efficiency and privacy. The construction of a precise and refined subgraph reduces knowledge noise and the number of input graph tokens, thus mitigating hallucination issues. Extensive experiments demonstrated that KoSEL outperformed advanced methods in terms of knowledge retrieval efficiency (20.27% reduction in retrieval time), knowledge utilization (15.16% increase in utilization rate), and data protection (113.50% reduction in data leakage rate), resulting in higher-quality answers for medical Q&A tasks (1.50% improvement in answer score).}
}
@article{LI2025104769,
title = {BiomedRAG: A retrieval augmented large language model for biomedicine},
journal = {Journal of Biomedical Informatics},
volume = {162},
pages = {104769},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104769},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001874},
author = {Mingchen Li and Halil Kilicoglu and Hua Xu and Rui Zhang},
keywords = {Retrieval-augmented generation, Large language model},
abstract = {Retrieval-augmented generation (RAG) involves a solution by retrieving knowledge from an established database to enhance the performance of large language models (LLM). , these models retrieve information at the sentence or paragraph level, potentially introducing noise and affecting the generation quality. To address these issues, we propose a novel BiomedRAG framework that directly feeds automatically retrieved chunk-based documents into the LLM. Our evaluation of BiomedRAG across four biomedical natural language processing tasks using eight datasets demonstrates that our proposed framework not only improves the performance by 9.95% on average, but also achieves state-of-the-art results, surpassing various baselines by 4.97%. BiomedRAG paves the way for more accurate and adaptable LLM applications in the biomedical domain.}
}
@article{LIU2024119280,
title = {MAKG: A maritime accident knowledge graph for intelligent accident analysis and management},
journal = {Ocean Engineering},
volume = {312},
pages = {119280},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119280},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824026180},
author = {Dongge Liu and Liang Cheng},
keywords = {Maritime accident, Knowledge graph, Named entity recognition, BERT, Prompt learning},
abstract = {With the increasing frequency of human activities at sea, maritime accidents are occurring more often. Analyzing and mining maritime accident cases can help uncover the causal mechanisms behind these incidents, thereby enhancing maritime safety. As an emerging technology for knowledge management and mining, knowledge graphs offer significant support for the storage, reasoning, and decision-making processes related to maritime accidents. In this study, we established a knowledge graph construction and application framework for maritime accidents to facilitates the extraction and management of maritime knowledge from unstructured texts. First, 581 accident reports released by the China Maritime Safety Administration over the past decade (2014–2023) were used as the data basis for analysis and construction of the maritime accident ontology structure using the seven-step method, which comprises 8 entity types, 8 relationship types, and 18 attribute entity types. Second, We proposed MBERT-BiLSTM-CRF-SF, a named entity recognition model based on domain pretraining and self-training, to reduce graph construction costs. This model achieved state-of-the-art performance in the maritime domain, with an F1 score of 0.910 ± 0.006, which is about 5% higher than the mainstream model. In addition, we proposed an entity alignment method based on font and semantics to refine knowledge further. On the basis of the proposed method, we constructed a large, high-quality maritime accident knowledge graph (MAKG) system that contains 16,099 entities and 20,809 relationship instances. Finally, we reduced the complexity of applying knowledge graphs by integrating the CRISPE prompt learning framework of the large language model, and experiments on graph traversal, pattern recognition, and aggregation analysis were conducted to assess the quality of MAKG. Results demonstrate that MAKG can effectively enhance the efficiency of querying and reasoning about maritime accident information, thus providing significant support for the prevention and management of maritime accidents.}
}
@article{SIEPMANN2025100378,
title = {An automated information extraction model for unstructured discharge letters using large language models and GPT-4},
journal = {Healthcare Analytics},
volume = {7},
pages = {100378},
year = {2025},
issn = {2772-4425},
doi = {https://doi.org/10.1016/j.health.2024.100378},
url = {https://www.sciencedirect.com/science/article/pii/S2772442524000807},
author = {Robert M. Siepmann and Giulia Baldini and Cynthia S. Schmidt and Daniel Truhn and Gustav Anton Müller-Franzes and Amin Dada and Jens Kleesiek and Felix Nensa and René Hosch},
keywords = {Large language models, Automated information extraction, Artificial intelligence, Generative pre-trained transformer (GPT), ChatGPT, Discharge letters},
abstract = {The administrative burden of manually extracting clinical information from discharge letters is a common challenge in healthcare. This study aims to explore the use of Large Language Models (LLMs), specifically Generative Pretrained Transformer 4 (GPT-4) by OpenAI, for automated extraction of diagnoses, medications, and allergies from discharge letters. Data for this study were sourced from two healthcare institutions in Germany, comprising discharge letters for ten patients from each institution. The first experiment is conducted using a standardized prompt for information extraction. However, challenges were encountered, and the prompt was fine-tuned in a second experiment to improve the results. We further tested whether open-source LLMs can achieve similar results. In the first experiment, primary diagnoses were identified with 85% accuracy and secondary diagnoses with 55.8%. Medications and allergies were extracted with 85.9% and 100% accuracy, respectively. The International Classification of Diseases, 10th revision (ICD-10) codes for the identified diagnoses achieved an accuracy of 85% for primary diagnoses and 60.7% for secondary diagnoses. Anatomical Therapeutic Chemical (ATC) codes were identified with an accuracy of 78.8%. On the other hand, open-source LLMs did not provide similar levels of accuracy and could not consistently fill the template. With prompt fine-tuning in the second experiment, the primary diagnoses, secondary diagnoses, and medications could be predicted with 95%, 88.9%, and 92.2% accuracy, respectively. GPT-4 shows excellent potential for automated extraction of crucial diagnostic and medication information from discharge letters, presumably lowering the administrative burden for healthcare professionals and improving patient outcomes.}
}
@article{SHIMIZU2025100862,
title = {Accelerating knowledge graph and ontology engineering with large language models},
journal = {Journal of Web Semantics},
pages = {100862},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2025.100862},
url = {https://www.sciencedirect.com/science/article/pii/S1570826825000022},
author = {Cogan Shimizu and Pascal Hitzler},
keywords = {Knowledge graph engineering, Ontology engineering, Large language models, Modular ontologies, Ontology modeling, Ontology population, Ontology alignment, Entity disambiguation},
abstract = {Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance.}
}
@article{KUANG2024100146,
title = {Harnessing multimodal large language models for traffic knowledge graph generation and decision-making},
journal = {Communications in Transportation Research},
volume = {4},
pages = {100146},
year = {2024},
issn = {2772-4247},
doi = {https://doi.org/10.1016/j.commtr.2024.100146},
url = {https://www.sciencedirect.com/science/article/pii/S2772424724000295},
author = {Senyun Kuang and Yang Liu and Xin Wang and Xinhua Wu and Yintao Wei}
}
@article{XU2025104047,
title = {Historical facts learning from Long-Short Terms with Language Model for Temporal Knowledge Graph Reasoning},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104047},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104047},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004060},
author = {Wenjie Xu and Ben Liu and Miao Peng and Zihao Jiang and Xu Jia and Kai Liu and Lei Liu and Min Peng},
keywords = {Knowledge graph, Temporal knowledge graph reasoning, Pre-trained language model, Large-scale language model},
abstract = {Temporal Knowledge Graph Reasoning (TKGR) aims to reason the missing parts in TKGs based on historical facts from different time periods. Traditional GCN-based TKGR models depend on structured relations between entities. To utilize the rich linguistic information in TKGs, some models have focused on applying pre-trained language models (PLMs) to TKGR. However, previous PLM-based models still face some issues: (1) they did not mine the associations in relations; (2) they did not differentiate the impact of historical facts from different time periods. (3) they introduced external knowledge to enhance the performance without fully utilizing the inherent reasoning capabilities of PLMs. To deal with these issues, we propose HFL: Historical Facts Learning from Long-Short Terms with Language Model for TKGR. Firstly, we construct time tokens for different types of time intervals to use timestamps and input the historical facts relevant to the query into the PLMs to learn the associations in relations. Secondly, we take a multi-perspective sampling strategy to learn from different time periods and use the original text information in TKGs or even no text information to learn reasoning abilities without any external knowledge. Finally, we perform HFL on four TKGR benchmarks, and the experiment results demonstrate that HFL has great competitiveness compared to both graph-based and PLM-based models. Additionally, we design a variant that applies HFL to LLMs and evaluate the performance of different LLMs.}
}
@article{WANG2024102820,
title = {Knowledge graph of agricultural engineering technology based on large language model},
journal = {Displays},
volume = {85},
pages = {102820},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102820},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224001847},
author = {Haowen Wang and Ruixue Zhao},
keywords = {LLM, Knowledge graph},
abstract = {Agriculture is an industry that has evolved alongside human evolution and has faithfully fulfilled its core mission of food supply. With the reduction of rural labor, the progress of artificial intelligence and the development of Internet of Things technology, it is hoped that the efficiency and productivity of the agricultural industry can be improved. Recently, with the development of information and intelligent technology, agricultural production and management have been significantly enhanced. However, there is still a considerable challenge in effectively integrating the vast amount of fragmented information for downstream applications. An agricultural knowledge graph (AGKG) will serve as the foundation for achieving these goals. Knowledge graphs can be general or domain-specific, and are the basis for many applications, such as search engines, online question-and-answer services, and knowledge inference. Therefore, there are many knowledge graphs, including Wikidata and DBpedia, for accessing structured knowledge. Although some general knowledge graphs contain some entities and relationships related to agriculture, there are no domain-specific knowledge graphs specifically for agricultural applications. Therefore, this paper proposes an agricultural knowledge graph (AGKG) for automatically integrating large amounts of agricultural data from the Internet. By applying natural language processing and deep learning technologies, AGKG can automatically identify agricultural entities from unstructured text and connect them to form a knowledge graph. In addition, we have described the typical scenarios of our AGKG and validated it through real-world applications such as agricultural entity retrieval and agricultural question-answering.}
}
@article{HU2024103999,
title = {LLM-TIKG: Threat intelligence knowledge graph construction utilizing large language model},
journal = {Computers & Security},
volume = {145},
pages = {103999},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103999},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824003043},
author = {Yuelin Hu and Futai Zou and Jiajia Han and Xin Sun and Yilei Wang},
keywords = {Threat intelligence, Large language model, Knowledge graph, TTP classification},
abstract = {Open-source threat intelligence is often unstructured and cannot be directly applied to the next detection and defense. By constructing a knowledge graph through open-source threat intelligence, we can better apply this information to intrusion detection. However, the current methods for constructing knowledge graphs face limitations due to the domain-specific attributes of entities and the analysis of lengthy texts, and they require large amounts of labeled data. Furthermore, there is a lack of authoritative open-source annotated threat intelligence datasets, which require significant manual effort. Moreover, it is noteworthy that current research often neglects the textual descriptions of attack behaviors, resulting in the loss of vital information to understand intricate cyber threats. To address these issues, we propose LLM-TIKG that applies the large language model to construct a knowledge graph from unstructured open-source threat intelligence. The few-shot learning capability of GPT is leveraged to achieve data annotation and augmentation, thereby creating the datasets for fine-tuning a smaller language model (7B). Using the fine-tuned model, we perform topic classification on the collected reports, extract entities and relationships, and extract TTPs from the attack description. This process results in the construction of a threat intelligence knowledge graph, enabling automated and universal analysis of textualized threat intelligence. The experimental results demonstrate improved performance in both named entity recognition and TTP classification, achieving the precision of 87.88% and 96.53%, respectively.}
}
@article{GAO2024105634,
title = {Exploring bridge maintenance knowledge graph by leveraging GrapshSAGE and text encoding},
journal = {Automation in Construction},
volume = {166},
pages = {105634},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105634},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003704},
author = {Yan Gao and Guanyu Xiong and Haijiang Li and Jarrod Richards},
keywords = {Bridge maintenance knowledge graph, Text encoding, Graph neural networks, Node classification, Link prediction},
abstract = {Knowledge graphs (KGs) are crucial in documenting bridge maintenance expertise. However, existing KG schemas lack integration of bridge design and practical inspection insights. Meanwhile, traditional methods for node feature initialization, relying on meticulous manual encoding or word embeddings, are inadequate for real-world maintenance textual data. To address these challenges, this paper introduces a bridge maintenance-oriented KG (BMKG) schema and approaches for graph data mining, including node-layer classification and link prediction. These methods leverage large language model (LLM)-based text encoding combined with GraphSAGE, demonstrating excellent performance in semantic enrichment and KG completion on deficient BMKGs. Additionally, ablation studies reveal the superiority of the pre-trained BERT text encoder and the L2 distance pairwise scoring calculator. Furthermore, a practical implementation framework integrating these approaches is developed for routine bridge maintenance, which can facilitate various practical applications, such as maintenance planning, and has the potential to enhance the efficiency of engineers' documentation work.}
}
@article{LECU2024443,
title = {Using LLMs and ontologies to extract causal relationships from medical abstracts},
journal = {Procedia Computer Science},
volume = {244},
pages = {443-452},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.219},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030205},
author = {Alexandru Lecu and Adrian Groza and Lezan Hawizy},
keywords = {Causal Relation Extraction, Knowledge Graphs, Large Language Models, Age-Related Macular Degeneration},
abstract = {The substantiation of the causal relationships behind its development is very important in identifying possible interventions and early treatment. Knowledge Graphs (KG) play a crucial role in the medical research domain by organizing data into interconnected structures that represent relationships between entities such as disease, treatments, and progressions. This paper shows a complete workflow that demonstrates the extraction of causal relationships from medical abstracts using a fine-tuned GPT-based model and the integration of these relationships into a KG.}
}
@article{LI2025113052,
title = {CoLE: A collaborative legal expert prompting framework for large language models in law},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113052},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113052},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000991},
author = {Bo Li and Shuang Fan and Shaolin Zhu and Lijie Wen},
keywords = {Artificial intelligence, Natural language processing, Legal large language models},
abstract = {Large Language Models (LLMs) have achieved remarkable outcomes in various natural language processing tasks. However, their application to the highly specialized field of law presents unique challenges. Legal language, characterized by complex syntax, domain-specific terminology, and nuanced logical relationships, poses significant hurdles for existing NLP models in accurately understanding and processing legal queries. Furthermore, the sheer volume of legal documents complicates information retrieval and knowledge extraction, making it difficult for models to pinpoint relevant legal articles and cases. Moreover, existing legal LLMs often struggle to effectively handle colloquial user queries and lack efficient mechanisms for selecting the most relevant demonstrations in In-Context Learning (ICL), hindering their ability to provide accurate and comprehensive legal advice. In order to address these issues, we propose a novel prompting framework named “Collaborative Legal Experts” (CoLE). This framework draws inspiration from teamwork paradigms in real-world legal case processing. First, we design an intent identification module to analyze user queries for identifying potential intents and law domains. Then, through two subsequent processes, potential background information and the best demonstration are generated. Finally, we design a prompt generator to assemble prompts generated from the previous steps. It combines with the LLMs to generate the final answer. Notably, we find that the self-generated information by LLMs has a smaller gap when fused with LLMs. We evaluate performance by integrating it with 7 general-purpose Chinese LLMs and comparing its performance against 8 specialized legal LLMs across 10 datasets, including Single-Choice, Multiple-Choice, and Question&Answer. The results indicate that integrating with CoLE’s LLMs has the potential to significantly enhance performance in the law field, particularly without the need for annotated datasets or model parameter updates. Moreover, our proposed model outperforms all state-of-the-art LLMs in law. The code is available at https://github.com/liboaccn/cole.}
}
@article{FAN2025113644,
title = {AutoMEX: Streamlining material extrusion with AI agents powered by large language models and knowledge graphs},
journal = {Materials & Design},
volume = {251},
pages = {113644},
year = {2025},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2025.113644},
url = {https://www.sciencedirect.com/science/article/pii/S0264127525000644},
author = {Haolin Fan and Junlin Huang and Jilong Xu and Yifei Zhou and Jerry Ying Hsi Fuh and Wen Feng Lu and Bingbing Li},
keywords = {Material extrusion, Large language models, Artificial intelligence agents, Knowledge graphs, Autonomous additive manufacturing},
abstract = {Additive manufacturing (AM), particularly material extrusion (MEX), has become a versatile and widely adopted technology with significant applications in the consumer goods and healthcare industries. Despite its affordability, adaptability, and user-friendliness advantages, MEX faces challenges in scaling for mass production due to limited process automation and fragmented domain knowledge, leaving gaps in end-to-end workflow integration. We propose AutoMEX, an innovative framework that integrates large language models (LLMs) as artificial intelligence (AI) agents to automate the MEX process to address these limitations. AutoMEX utilizes a knowledge graph (KG) derived from the scientific literature to enable LLMs to provide expert recommendations on material selection, process parameters, and design considerations, thereby improving accessibility and efficiency. With minimal human intervention, the framework encompasses a complete workflow, including CAD model generation, printing parameter recommendation, slicing, and machine operation. Experimental validation demonstrated a query acceptance rate of 94.6% for the recommendation system and up to a 9.6% improvement in print strength when employing the recommended parameters. These results highlight enhanced quality, autonomy, and customization of AM outputs, making AutoMEX suitable for batch production and streamlined manufacturing processes. While the framework shows promising potential, challenges such as the computational demands of advanced LLMs and the need for continual updates to the KG remain areas for future work. Overall, AutoMEX offers a pathway toward broader adoption and scalability of MEX technology, advancing the field of AM through enhanced automation and efficiency.}
}
@article{XIA2024102728,
title = {Leveraging error-assisted fine-tuning large language models for manufacturing excellence},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {88},
pages = {102728},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102728},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524000140},
author = {Liqiao Xia and Chengxi Li and Canbin Zhang and Shimin Liu and Pai Zheng},
keywords = {Large language model, Smart manufacturing, Industry 4.0, Knowledge management, Generative AI},
abstract = {The emergence of large language models (LLM), like GPT, is revolutionizing the field of information retrieval, finding applications across a wide range of domains. However, the intricate domain knowledge and the unique software paradigms inherent to the manufacturing sector have posed significant barriers to the effective utilization of LLM. To address this divide, an error-assisted fine-tuning approach is proposed to adapt LLM specifically for the manufacturing domain. Initially, the LLM is fine-tuned using a manufacturing-domain corpus, allowing it to learn and adapt to the nuances of the manufacturing field. Additionally, the injection of a labeled dataset into a pre-configured LLM enhances its ability to identify key elements within the domain. To ensure the generation of syntactically valid programs in domain-specific languages, and to accommodate environmental constraints, an error-assisted iterative prompting procedure is introduced, which facilitates the generation of reliable and expected code. Experimental results demonstrate the model’s proficiency in accurately responding to manufacturing-related queries and its effectiveness in generating reliable code, where the accuracy of judgment querying can experience an improvement of approximately 4.1%. By expanding the applicability of LLM to the manufacturing industry, it is hoped that this research will pave the way for a broad array of new LLM-based applications within manufacturing.}
}
@article{BASHIR2025102406,
title = {Logic-infused knowledge graph QA: Enhancing large language models for specialized domains through Prolog integration},
journal = {Data & Knowledge Engineering},
volume = {157},
pages = {102406},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102406},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000011},
author = {Aneesa Bashir and Rong Peng and Yongchang Ding},
keywords = {Knowledge Graph Question Answering (KGQA), Large language models (LLMs), Logical programming (Prolog), Named entity recognition (NER), Multi-hop reasoning, Transformer, BERT},
abstract = {Efficiently answering questions over complex, domain-specific knowledge graphs remain a substantial challenge, as large language models (LLMs) often lack the logical reasoning abilities and particular knowledge required for such tasks. This paper presents a novel framework integrating LLMs with logical programming languages like Prolog for Logic-Infused Knowledge Graph Question Answering (KGQA) in specialized domains. The proposed methodology uses a transformer-based encoder–decoder architecture. An encoder reads the question, and a named entity recognition (NER) module connects entities to the knowledge graph. The extracted entities are fed into a grammar-guided decoder, producing a logical form (Prolog query) that captures the semantic constraints and relationships. The Prolog query is executed over the knowledge graph to perform symbolic reasoning and retrieve relevant answer entities. Comprehensive experiments on the MetaQA benchmark dataset demonstrate the superior performance of this logic-infused method in accurately identifying correct answer entities from the knowledge graph. Even when trained on a limited subset of annotated data, it outperforms state-of-the-art baselines, achieving 89.60 % and F1-scores of up to 89.61 %, showcasing its effectiveness in enhancing large language models with symbolic reasoning capabilities for specialized question-answering tasks. The seamless integration of LLMs and logical programming enables the proposed framework to reason effectively over complex, domain-specific knowledge graphs, overcoming a key limitation of existing KGQA systems. In specialized domains, the interpretability provided by representing questions such as Prologue queries is a valuable asset.}
}
@article{YANG2025102868,
title = {GS-KGC: A generative subgraph-based framework for knowledge graph completion with large language models},
journal = {Information Fusion},
volume = {117},
pages = {102868},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102868},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524006468},
author = {Rui Yang and Jiahao Zhu and Jianping Man and Hongze Liu and Li Fang and Yi Zhou},
keywords = {Knowledge graph, Knowledge graph completion, Large language models, Question answer},
abstract = {Knowledge graph completion (KGC) focuses on identifying missing triples in a knowledge graph (KG) , which is crucial for many downstream applications. Given the rapid development of large language models (LLMs), some LLM-based methods are proposed for KGC task. However, most of them focus on prompt engineering while overlooking the fact that finer-grained subgraph information can aid LLMs in generating more accurate answers. In this paper, we propose a novel completion framework called Generative Subgraph-based KGC (GS-KGC), which utilizes subgraph information as contextual reasoning and employs a QA approach to achieve the KGC task. This framework primarily includes a subgraph partitioning algorithm designed to generate negatives and neighbors. Specifically, negatives can encourage LLMs to generate a broader range of answers, while neighbors provide additional contextual insights for LLM reasoning. Furthermore, we found that GS-KGC can discover potential triples within the KGs and new facts beyond the KGs. Experiments conducted on four common KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it shows a 5.6% increase in Hits@3 compared to the LLM-based model CP-KGC on the FB15k-237N, and a 9.3% increase over the LLM-based model TECHS on the ICEWS14.}
}
@article{UHM2025105926,
title = {Effectiveness of retrieval augmented generation-based large language models for generating construction safety information},
journal = {Automation in Construction},
volume = {170},
pages = {105926},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105926},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006629},
author = {Miyoung Uhm and Jaehee Kim and Seungjun Ahn and Hoyoung Jeong and Hongjo Kim},
keywords = {LLMs (large language models), RAG (retrieval-augmented generation), Personalized safety, Construction safety information generation},
abstract = {While Generative Pre-Trained Transformers (GPT)-based models offer high potential for context-specific information generation, inaccurate numerical responses, a lack of detailed information, and hallucination problems remain as the main challenges for their use in assisting safety engineering and management tasks. To address the challenges, this paper systematically evaluates the effectiveness of the Retrieval-Augmented Generation-based GPT (RAG-GPT) model for generating detailed and specific construction safety information. The RAG-GPT model was compared with four other GPT models, evaluating the models' responses from three different groups––2 researchers, 10 construction safety experts, and 30 construction workers. Quantitative analysis demonstrated that the RAG-GPT model showed superior performance compared to the other models. Experts rated the RAG-GPT model as providing more contextually relevant answers, with high marks for accuracy and essential information inclusion. The findings indicate that the RAG strategy, which uses vector data to enhance information retrieval, significantly improves the accuracy of construction safety information.}
}
@article{CELINO2025100850,
title = {Procedural knowledge management in Industry 5.0: Challenges and opportunities for knowledge graphs},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100850},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100850},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000362},
author = {Irene Celino and Valentina Anita Carriero and Antonia Azzini and Ilaria Baroni and Mario Scrocca},
keywords = {Procedural knowledge, Industry 5.0, Knowledge graphs, Artificial Intelligence},
abstract = {With digital transformation, industrial companies today are facing the challenges to change and innovate their business, by leveraging digital technologies and tools to support their processes and their operations. One of their main challenges is the management of the company knowledge, especially when tacit and owned by industry workers. In this paper, we illustrate how knowledge graphs can be the turning point to allow industry workers digitize and exploit the knowledge about the “what”, the “how” and the “why” of their everyday activities. In particular, we focus on the “how” by illustrating the challenges related to procedural knowledge management, i.e., the knowledge about processes and workflows that employees need to follow, and comply with, to correctly execute their tasks, in order to improve efficiency and effectiveness, to reduce risks and human errors and to optimize operations. We also explain the relationship in this context between knowledge graphs and sub-symbolic AI approaches.}
}
@article{XU2025126585,
title = {Towards normalized clinical information extraction in Chinese radiology report with large language models},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126585},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126585},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002076},
author = {Qinwei Xu and Xingkun Xu and Chenyi Zhou and Zuozhu Liu and Feiyue Huang and Shaoxin Li and Lifeng Zhu and Zhian Bai and Yuchen Xu and Weiguo Hu},
keywords = {Clinical information extraction, Large language models, Instruction tuning, Data-efficient learning, Chinese radiology reports},
abstract = {Radiology reports serve as a fundamental component within electronic medical records. Converting unstructured free-text reports into structured formats holds paramount importance for the management and utilization of radiology reports. In this paper, we propose a novel information extraction paradigm named normalized clinical information extraction (NCIE) for Chinese radiology reports. Specifically, NCIE operates in an end-to-end fashion to extract normalized and structured clinical information without decomposing the information extraction process into multiple intermediate tasks. Motivated by recent progress in Large Language Models (LLMs), we address the NCIE problem based on the instruction tuning of LLMs. The proposed approach, termed Radiological Information End-to-end Extraction with LLM (RIEEL), excels at extracting structural information comprising radiological observations alongside their corresponding anatomical locations and status. To ensure the model in learning the normalized medical concepts correctly, we establish a radiology knowledge base with expert knowledge and further curate a high-quality instruction tuning dataset. Moreover, we incorporate two data-efficient learning strategies based on data augmentation and self-training to enhance the model’s NCIE capabilities during instruction tuning. Through extensive experiments, we demonstrate that the proposed RIEEL achieves superior performances with different state-of-the-arts backbone LLMs, including Qwen1.5, Baichuan2 and LLaMA3. Remarkably, the best version of RIEEL surpasses GPT-4 in NCIE by a substantial margin of 30.61% in terms of the F1 score.}
}
@article{DAQUIN2025100854,
title = {On the role of knowledge graphs in AI-based scientific discovery},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100854},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100854},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000404},
author = {Mathieu d’Aquin},
keywords = {Scientific discovery, Knowledge graphs, Machine learning, Interpretability},
abstract = {Research and the scientific activity are widely seen as an area where the current trends in AI, namely the development of deep learning models (including large language models), are having an increasing impact. Indeed, the ability of such models to extrapolate from data, seemingly finding unknown patterns relating implicit features of the objects under study to their properties can, at the very least, help accelerate and scale up those studies as demonstrated in fields such as molecular biology and chemistry. Knowledge graphs, on the other hand, have more traditionally been used to organize information around the scientific activity, keeping track of existing knowledge, of conducted experiments, of interactions within the research community, etc. However, for machine learning models to be truly used as a tool for scientific advancement, we have to find ways for the knowledge implicitly gained by these models from their training to be integrated with the explicitly represented knowledge captured through knowledge graphs. Based on our experience in ongoing projects in the domain of material science, in this position paper, we discuss the role that knowledge graphs can play in new methodologies for scientific discovery. These methodologies are based on the creation of large and opaque neural models. We therefore focus on the research challenges we need to address to support aligning such neural models to knowledge graphs for them to become a knowledge-level interface to those neural models.}
}
@article{WANG2024361,
title = {Ontology-integrated tuning of large language model for intelligent maintenance},
journal = {CIRP Annals},
volume = {73},
number = {1},
pages = {361-364},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S000785062400026X},
author = {Peng Wang and John Karigiannis and Robert X. Gao},
keywords = {Maintenance, Machine learning, Large language models},
abstract = {As new AI technologies such as Large Language Models (LLM) quickly evolve, the need for enhancing general-purpose LLMs with physical knowledge to better serve the manufacturing community has been increasingly recognized. This paper presents a method that tailors GPT-3.5 with domain-specific knowledge for intelligent aircraft maintenance. Specifically, aircraft ontology is investigated to curate maintenance logs with encoded component hierarchical structure to fine-tune GPT-3.5. Experimental results demonstrate the effectiveness of the developed method in accurately identifying defective components and providing consistent maintenance action recommendations, outperforming general-purpose GPT-3.5 and GPT-4.0. The method can be adapted to other domains in manufacturing and beyond.}
}
@article{KABAL20242617,
title = {Enhancing Domain-Independent Knowledge Graph Construction through OpenIE Cleaning and LLMs Validation},
journal = {Procedia Computer Science},
volume = {246},
pages = {2617-2626},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.436},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024761},
author = {Othmane Kabal and Mounira Harzallah and Fabrice Guillet and Ryutaro Ichise},
keywords = {Knowledge graph, From raw text, Domain-independent building, Knowledge graph construction pipeline, LLMs based validation, Information Extraction, LLMs for KG},
abstract = {In the challenging context of Knowledge Graph (KG) construction from text, traditional approaches often rely on Open Information Extraction (OpenIE) pipelines. However, they are prone to generating many incorrect triplets. While domain specific Named Entity Recognition (NER) is commonly used to enhance the results, it compromises the domain independence and misses crucial triplets. To address these limitations, we introduce G-T2KG, a novel pipeline for KG construction that aims to preserve the domain independence while reducing incorrect triplets, thus offering a cost-effective solution without the need for domain-specific adaptations. Our pipeline utilizes state-of-the-art OpenIE combined with both a noun phrase-based cleaning and a LLMs based validation. It is evaluated using gold standards in two distinct domains (i.e., computer science and music) that we have constructed in the context of this study. On computer science corpus, the experimental results demonstrate a higher recall as compared to state-of-the-art approaches, and a higher precision notably increased by the integration of LLMs. Experiments on the music corpus show good performance, underscoring the versatility and effectiveness of G-T2KG in domain-independent KG construction.}
}
@article{ANISUZZAMAN2025100184,
title = {Fine-Tuning Large Language Models for Specialized Use Cases},
journal = {Mayo Clinic Proceedings: Digital Health},
volume = {3},
number = {1},
pages = {100184},
year = {2025},
issn = {2949-7612},
doi = {https://doi.org/10.1016/j.mcpdig.2024.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2949761224001147},
author = {D.M. Anisuzzaman and Jeffrey G. Malins and Paul A. Friedman and Zachi I. Attia},
abstract = {Large language models (LLMs) are a type of artificial intelligence, which operate by predicting and assembling sequences of words that are statistically likely to follow from a given text input. With this basic ability, LLMs are able to answer complex questions and follow extremely complex instructions. Products created using LLMs such as ChatGPT by OpenAI and Claude by Anthropic have created a huge amount of traction and user engagements and revolutionized the way we interact with technology, bringing a new dimension to human-computer interaction. Fine-tuning is a process in which a pretrained model, such as an LLM, is further trained on a custom data set to adapt it for specialized tasks or domains. In this review, we outline some of the major methodologic approaches and techniques that can be used to fine-tune LLMs for specialized use cases and enumerate the general steps required for carrying out LLM fine-tuning. We then illustrate a few of these methodologic approaches by describing several specific use cases of fine-tuning LLMs across medical subspecialties. Finally, we close with a consideration of some of the benefits and limitations associated with fine-tuning LLMs for specialized use cases, with an emphasis on specific concerns in the field of medicine.}
}
@article{CHENG2024109361,
title = {A link prediction method for Chinese financial event knowledge graph based on graph attention networks and convolutional neural networks},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109361},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109361},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624015197},
author = {Haitao Cheng and Ke Wang and Xiaoying Tan},
keywords = {Chinese financial event knowledge graph, Link prediction, Graph attention network, Convolutional neural network, Large language model},
abstract = {Finance is a knowledge-intensive domain in nature, with its data containing a significant amount of interconnected information. Constructing a financial knowledge graph is an important application for transforming financial text/web content into machine-readable data. However, the complexity of Chinese financial knowledge and the dynamic and evolving nature of Chinese financial data often lead to incomplete knowledge graphs. To address this challenge, we propose a novel link prediction method for Chinese financial event knowledge graph based on Graph Attention Networks and Convolutional Neural Networks. Our method begins with the construction of the foundational Chinese financial event knowledge graph using a relational triple extraction module integrated with a large language model framework, along with a Prompting with Iterative Verification (PiVe) module for validation. To enhance the completeness of the knowledge graph, we introduce an encoder-decoder framework, where a graph attention network with joint embeddings of financial event entities and relations acts as the encoder, while a Convolutional Knowledge Base embedding model (ConvKB) serves as the decoder. This framework effectively aggregates crucial neighbor information and captures global relationships among entity and relation embeddings. Extensive comparative experiments demonstrate the utility and accuracy of this method, ultimately enabling the effective completion of Chinese financial event knowledge graphs.}
}
@article{MICHEL2025104175,
title = {Seeing economic development like a large language model. A methodological approach to the exploration of geographical imaginaries in generative AI},
journal = {Geoforum},
volume = {158},
pages = {104175},
year = {2025},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2024.104175},
url = {https://www.sciencedirect.com/science/article/pii/S0016718524002367},
author = {Boris Michel and Yannick Ecker},
abstract = {The recent hype surrounding the disruptive potential of AI technologies in the form of large language models or text to image generators also raises questions for geographical research and practice. These questions include the power relations and inequalities inscribed in these systems, their significance for work and labor relations, their ecological and economic impact, but also the geographical and spatial imaginaries they reproduce. This article focuses on the latter and formulates a series of theoretical and methodological considerations for dealing with the output of these systems. As we assume that outputs generated by large language models will play an increasing role in the future, both in public and media discourses as well as in the discourses and practices of spatial planning and economic policy making, we consider it important to gain a critical understanding of these socio-technical systems. The empirical object of investigation of this paper is generated output that deals with questions of regional development and economic challenges in three European regions that are currently particularly affected by the transition to a climate-neutral economy and are designated by the European Union as Just Transition Fund Territories. We are particularly interested in how geographical imaginaries about these regions are formulated, how economic and social problems of these regions are presented and how this is translated into planning advice and development plans.}
}
@article{HE2025102963,
title = {A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics},
journal = {Information Fusion},
volume = {118},
pages = {102963},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102963},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000363},
author = {Kai He and Rui Mao and Qika Lin and Yucheng Ruan and Xiang Lan and Mengling Feng and Erik Cambria},
keywords = {Healthcare application, Large language model, Medicine, Pretrained language model},
abstract = {The utilization of large language models (LLMs) for Healthcare has generated both excitement and concern due to their ability to effectively respond to free-text queries with certain professional knowledge. This survey outlines the capabilities of the currently developed Healthcare LLMs and explicates their development process, to provide an overview of the development road map from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, and summarize related Healthcare training data, learning methods, and usage. Finally, the unique concerns associated with deploying LLMs are investigated, particularly regarding fairness, accountability, transparency, and ethics. Besides, we support researchers by compiling a collection of open-source resources11https://github.com/KaiHe-CatOwner/LLM-for-Healthcare.. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a move from model-centered methodologies to data-centered methodologies. We determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics.}
}
@article{YANG2024112155,
title = {Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement},
journal = {Knowledge-Based Systems},
volume = {300},
pages = {112155},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112155},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124007895},
author = {Rui Yang and Jiahao Zhu and Jianping Man and Li Fang and Yi Zhou},
keywords = {Knowledge graph, Knowledge graph completion, Large language models, Semantic enhancement},
abstract = {The design and development of text-based knowledge graph completion (KGC) methods leveraging textual entity descriptions are at the forefront of research. These methods involve advanced optimization techniques such as soft prompts and contrastive learning to enhance KGC models. The effectiveness of text-based methods largely hinges on the quality and richness of the training data. Large language models (LLMs) can utilize straightforward prompts to alter text data, thereby enabling data augmentation for KGC. Nevertheless, LLMs typically demand substantial computational resources. To address these issues, we introduce a framework termed constrained prompts for KGC (CP-KGC). This CP-KGC framework designs prompts that adapt to different datasets to enhance semantic richness. Additionally, CP-KGC employs a context constraint strategy to effectively identify polysemous entities within KGC datasets. Through extensive experimentation, we have verified the effectiveness of this framework. Even after quantization, the LLM (Qwen-7B-Chat-int4) still enhances the performance of text-based KGC methods.11Code and datasets are available at https://github.com/sjlmg/CP-KGC. This study extends the performance limits of existing models and promotes further integration of KGC with LLMs.}
}
@article{FERRAG2025,
title = {Generative AI in Cybersecurity: A Comprehensive Review of LLM Applications and Vulnerabilities},
journal = {Internet of Things and Cyber-Physical Systems},
year = {2025},
issn = {2667-3452},
doi = {https://doi.org/10.1016/j.iotcps.2025.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667345225000082},
author = {Mohamed Amine Ferrag and Fatima Alwahedi and Ammar Battah and Bilel Cherif and Abdechakour Mechri and Norbert Tihanyi and Tamas Bisztray and Merouane Debbah},
keywords = {Generative AI, LLM, Transformer, Security, Cyber Security},
abstract = {This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.}
}
@article{LIAO2025103134,
title = {Large language model assisted fine-grained knowledge graph construction for robotic fault diagnosis},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103134},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103134},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000278},
author = {Xingming Liao and Chong Chen and Zhuowei Wang and Ying Liu and Tao Wang and Lianglun Cheng},
keywords = {Knowledge Graph, Large Language Model, Fault Diagnosis, Industrial Robots},
abstract = {With the rapid deployment of industrial robots in manufacturing, the demand for advanced maintenance techniques to sustain operational efficiency has become crucial. Fault diagnosis Knowledge Graph (KG) is essential as it interlinks multi-source data related to industrial robot faults, capturing multi-level semantic associations among different fault events. However, the construction and application of fine-grained fault diagnosis KG face significant challenges due to the inherent complexity of nested entities in maintenance texts and the severe scarcity of annotated industrial data. In this study, we propose a Large Language Model (LLM) assisted data augmentation approach, which handles the complex nested entities in maintenance corpora and constructs a more fine-grained fault diagnosis KG. Firstly, the fine-grained ontology is constructed via LLM Assistance in Industrial Nested Named Entity Recognition (assInNNER). Then, an Industrial Nested Label Classification Template (INCT) is designed, enabling the use of nested entities in Attention-map aware keyword selection for the Industrial Nested Language Model (ANLM) data augmentation methods. ANLM can effectively improve the model’s performance in nested entity extraction when corpora are scarce. Subsequently, a Confidence Filtering Mechanism (CFM) is introduced to evaluate and select the generated data for enhancement, and assInNNER is further deployed to recall the negative samples corpus again to further improve performance. Experimental studies based on multi-source corpora demonstrate that compared to existing algorithms, our method achieves an average F1 increase of 8.25 %, 3.31 %, and 1.96 % in 5%, 10 %, and 25 % in few-shot settings, respectively.}
}
@article{KUNZE2025547,
title = {Large Language Models Applied to Health Care Tasks May Improve Clinical Efficiency, Value of Care Rendered, Research, and Medical Education},
journal = {Arthroscopy: The Journal of Arthroscopic & Related Surgery},
volume = {41},
number = {3},
pages = {547-556},
year = {2025},
issn = {0749-8063},
doi = {https://doi.org/10.1016/j.arthro.2024.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0749806324010478},
author = {Kyle N. Kunze and Benedict U. Nwachukwu and Mark P. Cote and Prem N. Ramkumar},
abstract = {Large language models (LLMs) are generative artificial intelligence models that create content on the basis of the data on which it was trained. Processing capabilities have evolved from text only to being multimodal including text, images, audio, and video features. In health care settings, LLMs are being applied to several clinically important areas, including patient care and workflow efficiency, communications, hospital operations and data management, medical education, practice management, and health care research. Under the umbrella of patient care, several core use cases of LLMs include simplifying documentation tasks, enhancing patient communication (interactive language and written), conveying medical knowledge, and performing medical triage and diagnosis. However, LLMs warrant scrutiny when applied to health care tasks, as errors may have negative implications for health care outcomes, specifically in the context of perpetuating bias, ethical considerations, and cost-effectiveness. Customized LLMs developed for more narrow purposes may help overcome certain performance limitations, transparency challenges, and biases present in contemporary generalized LLMs by curating training data. Methods of customizing LLMs broadly fall under 4 categories: prompt engineering, retrieval augmented generation, fine-tuning, and agentic augmentation, with each approach conferring different information-retrieval properties for the LLM.
Level of Evidence
Level V, expert opinion.}
}
@article{FU2025105347,
title = {Generative AI in the context of assistive technologies: Trends, limitations and future directions},
journal = {Image and Vision Computing},
volume = {154},
pages = {105347},
year = {2025},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105347},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624004529},
author = {Biying Fu and Abdenour Hadid and Naser Damer},
keywords = {Assistive AI, Generative AI, Generative models, Assistive systems, Assistive technologies and services},
abstract = {With the tremendous successes of Large Language Models (LLMs) like ChatGPT for text generation and Dall-E for high-quality image generation, generative Artificial Intelligence (AI) models have shown a hype in our society. Generative AI seamlessly delved into different aspects of society ranging from economy, education, legislation, computer science, finance, and even healthcare. This article provides a comprehensive survey on the increased and promising use of generative AI in assistive technologies benefiting different parties, ranging from the assistive system developers, medical practitioners, care workforce, to the people who need the care and the comfort. Ethical concerns, biases, lack of transparency, insufficient explainability, and limited trustworthiness are major challenges when using generative AI in assistive technologies, particularly in systems that impact people directly. Key future research directions to address these issues include creating standardized rules, establishing commonly accepted evaluation metrics and benchmarks for explainability and reasoning processes, and making further advancements in understanding and reducing bias and its potential harms. Beyond showing the current trends of applying generative AI in the scope of assistive technologies in four identified key domains, which include care sectors, medical sectors, helping people in need, and co-working, the survey also discusses the current limitations and provides promising future research directions to foster better integration of generative AI in assistive technologies.}
}
@article{XU202449,
title = {GeoPredict-LLM: Intelligent tunnel advanced geological prediction by reprogramming large language models},
journal = {Intelligent Geoengineering},
volume = {1},
number = {1},
pages = {49-57},
year = {2024},
issn = {3050-6190},
doi = {https://doi.org/10.1016/j.ige.2024.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S3050619024000053},
author = {Zhenhao Xu and Zhaoyang Wang and Shucai Li and Xiao Zhang and Peng Lin},
keywords = {Advanced geological prediction, Large language model, Data diffusion, Multisource data, Multimodal data, Knowledge graph},
abstract = {With the improvement of multisource information sensing and data acquisition capabilities inside tunnels, the availability of multimodal data in tunnel engineering has significantly increased. However, due to structural differences in multimodal data, traditional intelligent advanced geological prediction models have limited capacity for data fusion. Furthermore, the lack of pre-trained models makes it difficult for neural networks trained from scratch to deeply explore the features of multimodal data. To address these challenges, we utilize the fusion capability of knowledge graph for multimodal data and the pre-trained knowledge of large language models (LLMs) to establish an intelligent advanced geological prediction model (GeoPredict-LLM). First, we develop an advanced geological prediction ontology model, forming a knowledge graph database. Using knowledge graph embeddings, multisource and multimodal data are transformed into low-dimensional vectors with a unified structure. Secondly, pre-trained LLMs, through reprogramming, reconstruct these low-dimensional vectors, imparting linguistic characteristics to the data. This transformation effectively reframes the complex task of advanced geological prediction as a "language-based" problem, enabling the model to approach the task from a linguistic perspective. Moreover, we propose the prompt-as-prefix method, which enables output generation, while freezing the core of the LLM, thereby significantly reduces the number of training parameters. Finally, evaluations show that compared to neural network models without pre-trained models, GeoPredict-LLM significantly improves prediction accuracy. It is worth noting that as long as a knowledge graph database can be established, GeoPredict-LLM can be adapted to multimodal data mining tasks with minimal modifications.}
}
@article{SHI2024678,
title = {Interoperable information modelling leveraging asset administration shell and large language model for quality control toward zero defect manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {77},
pages = {678-696},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002395},
author = {Dachuan Shi and Philipp Liedl and Thomas Bauernhansl},
keywords = {Interoperability, Large language model, Ontology, Asset administration shell, Zero defect manufacturing, Information modelling, Quality control, Industry 4.0},
abstract = {In the era of Industry 4.0, Zero Defect Manufacturing (ZDM) has emerged as a prominent strategy for quality improvement, emphasizing data-driven approaches for defect prediction, prevention, and mitigation. The success of ZDM heavily depends on the availability and quality of data typically collected from diverse and heterogeneous sources during production and quality control, presenting challenges in data interoperability. Addressing this, we introduce a novel approach leveraging Asset Administration Shell (AAS) and Large Language Models (LLMs) for creating interoperable information models that incorporate semantic contextual information to enhance the interoperability of data integration in the quality control process. AAS, initiated by German industry stakeholders, shows a significant advancement in information modeling, blending ontology and digital twin concepts for the virtual representation of assets. In this work, we develop a systematic, use-case-driven methodology for AAS-based information modeling. This methodology guides the design and implementation of AAS models, ensuring model properties are presented in a unified structure and reference external standardized vocabularies to maintain consistency across different systems. To automate this referencing process, we propose a novel LLM-based algorithm to semantically search model properties within a standardized vocabulary repository. This algorithm significantly reduces manual intervention in model development. A case study in the injection molding domain demonstrates the practical application of our approach, showcasing the integration and linking of product quality and machine process data with the help of the developed AAS models. Statistical evaluation of our LLM-based semantic search algorithm confirms its efficacy in enhancing data interoperability. This methodology offers a scalable and adaptable solution for various industrial use cases, promoting widespread data interoperability in the context of Industry 4.0.}
}
@article{LI2025125920,
title = {Taming large language models to implement diagnosis and evaluating the generation of LLMs at the semantic similarity level in acupuncture and moxibustion},
journal = {Expert Systems with Applications},
volume = {264},
pages = {125920},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125920},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424027878},
author = {Shusheng Li and Wenjun Tan and Changshuai Zhang and Jiale Li and Haiyan Ren and Yanliang Guo and Jing Jia and Yangyang Liu and Xingfang Pan and Jing Guo and Wei Meng and Zhaoshui He},
keywords = {Artificial intelligence, Large language model, Traditional chinese medicine, Acupuncture and moxibustion, Prompting, Evaluation},
abstract = {With the rapid advancement of artificial intelligence and deep learning technologies, large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in comprehending and responding to human instructions. Acupuncture and moxibustion, therapeutic modalities in Traditional Chinese Medicine (TCM), possess extensive knowledge beneficial for patient treatment. Currently, acupuncture diagnosis relies on the experience and skills of individual acupuncturists, emphasizing the need for research to improve diagnostic accuracy through objective methods. Therefore, the integration of LLMs into the field of acupuncture can facilitate the recommendation of personalized acupuncture treatment programs. However, the application of general LLMs to the field of acupuncture diagnosis often yields suboptimal results. In addition, most LLM evaluation metrics depend solely on literal overlap and fail to capture semantic similarity. To address these challenges, this paper introduces AcupunctureGPT, a specialized large language model for acupuncture diagnosis, aimed at exploring the potential application of LLMs in this field. Patient Diagnostic Acupuncture Data is constructed to enhance the diagnostic capabilities of AcupunctureGPT in acupuncture. The Generated Knowledge Filter Prompting approach is proposed to improve the accuracy of LLMs in identifying similar diseases through the development and filtering of knowledge statements. The Sentence Similarity Evaluation Module (SSEM) is employed to assess the generation quality of LLMs at the semantic level. The Sentence Adaptive Enhancement Fusion Module (SAEFM), proposed within SSEM, enhances the adaptive fusion of output features at various levels. Experimental results demonstrate that AcupunctureGPT outperforms other large language models in diagnosing diseases and devising reasonable treatment plans. Furthermore, the evaluation metrics proposed in this paper have been validated for effectiveness.}
}@article{ZHENG2024104738,
title = {PLRTE: Progressive learning for biomedical relation triplet extraction using large language models},
journal = {Journal of Biomedical Informatics},
volume = {159},
pages = {104738},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104738},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001564},
author = {Yi-Kai Zheng and Bi Zeng and Yi-Chun Feng and Lu Zhou and Yi-Xue Li},
keywords = {Large language model, Relation triplet extraction, Biomedical text mining, Supervised fine-tuning, Natural language processing, Named entity recognition},
abstract = {Document-level relation triplet extraction is crucial in biomedical text mining, aiding in drug discovery and the construction of biomedical knowledge graphs. Current language models face challenges in generalizing to unseen datasets and relation types in biomedical relation triplet extraction, which limits their effectiveness in these crucial tasks. To address this challenge, our study optimizes models from two critical dimensions: data-task relevance and granularity of relations, aiming to enhance their generalization capabilities significantly. We introduce a novel progressive learning strategy to obtain the PLRTE model. This strategy not only enhances the model’s capability to comprehend diverse relation types in the biomedical domain but also implements a structured four-level progressive learning process through semantic relation augmentation, compositional instruction, and dual-axis level learning. Our experiments on the DDI and BC5CDR document-level biomedical relation triplet datasets demonstrate a significant performance improvement of 5% to 20% over the current state-of-the-art baselines. Furthermore, our model exhibits exceptional generalization capabilities on the unseen Chemprot and GDA datasets, further validating the effectiveness of optimizing data-task association and relation granularity for enhancing model generalizability.}
}
@article{GUAN2024100070,
title = {Drug discovery and development in the era of artificial intelligence: From machine learning to large language models},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {1},
pages = {100070},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000289},
author = {Shenghui Guan and Guanyu Wang},
keywords = {Machine learning, Drug Discovery, Bioinformatics},
abstract = {Drug Research and Development (R&D) is a complex and difficult process, and current drug R&D faces the challenges of long time span, high investment, and high failure rate. Machine learning, with its powerful learning ability to characterize big data and complex networks, is increasingly effective to improve the efficiency and success rate of drug R&D. Here we review some recent examples of the application of machine learning methods in six areas: disease gene prediction, virtual screening, drug molecule generation, molecular attribute prediction, and prediction of drug combination synergism. We also discuss the advantages of integrative learning in multi-attribute prediction. Integrative models based on base learners constructed from data of different dimensions on the one hand fully utilize the information contained in these data, and on the other hand improve the average prediction performance. Finally, we envision a new paradigm for drug discovery and development: a large language model acts as a central hub to organize public resources into a knowledge base, validating the knowledge with computational software and smaller predictive models, as well as high-throughput automated screening platforms based on organoidal technologies, to speed up development and reduce the differences in efficacy between disease models and humans to improve the success rate of a drug.}
}
@article{RIEMER2024102824,
title = {Conceptualizing generative AI as style engines: Application archetypes and implications},
journal = {International Journal of Information Management},
volume = {79},
pages = {102824},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2024.102824},
url = {https://www.sciencedirect.com/science/article/pii/S0268401224000720},
author = {Kai Riemer and Sandra Peter},
keywords = {Generative AI, Large language models, Style engines, AI assistants, AI agents},
abstract = {The rise of generative AI has brought with it a surprising paradox: systems that excel at tasks once thought to be uniquely human, like fluent conversation or persuasive writing, while simultaneously failing to meet traditional expectations of computing, in terms of reliability, accuracy, and veracity (e.g., given the various issues with so-called ‘hallucinations’). We argue that, when generative AI is seen through a traditional computing lens, its development focuses on optimizing for traditional computing traits that remain in principle unattainable. This risks backgrounding what is most novel and defining about it. As probabilistic technologies, generative AIs do not store, in any traditional sense, any data or content. Rather, essential features of training data become encoded in deep neural networks as patterns, that become practically available as styles. We discuss what happens when the distinction between objects and their appearance dissolves and all aspects of images or text become understood as styles, accessible for exploration and creative combination and generation. For example, defining visual qualities of entities like ‘chair’ or ‘cat’ become available as ‘chair-ness’ or ‘cat-ness’ for creative image generation. We argue that, when understood as style engines, unique generative AI capabilities become conceptualized as complementing traditional computing ones. This will aid both computing practitioners and information systems researchers in reconciling and integrating generative AI into the traditional IS landscape. Our conceptualization leads us to propose four archetypes of generative AI application and use, and to highlight future avenues for information systems research made visible by this conceptualization, as well as implications for practice and policymaking.}
}
@article{LI20242481,
title = {Enhancing Relational Triple Extraction in Specific Domains: Semantic Enhancement and Synergy of Large Language Models and Small Pre-Trained Language Models},
journal = {Computers, Materials and Continua},
volume = {79},
number = {2},
pages = {2481-2503},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.050005},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824002248},
author = {Jiakai Li and Jianpeng Hu and Geng Zhang},
keywords = {Relational triple extraction, semantic interaction, large language models, data augmentation, specific domains},
abstract = {In the process of constructing domain-specific knowledge graphs, the task of relational triple extraction plays a critical role in transforming unstructured text into structured information. Existing relational triple extraction models face multiple challenges when processing domain-specific data, including insufficient utilization of semantic interaction information between entities and relations, difficulties in handling challenging samples, and the scarcity of domain-specific datasets. To address these issues, our study introduces three innovative components: Relation semantic enhancement, data augmentation, and a voting strategy, all designed to significantly improve the model’s performance in tackling domain-specific relational triple extraction tasks. We first propose an innovative attention interaction module. This method significantly enhances the semantic interaction capabilities between entities and relations by integrating semantic information from relation labels. Second, we propose a voting strategy that effectively combines the strengths of large language models (LLMs) and fine-tuned small pre-trained language models (SLMs) to reevaluate challenging samples, thereby improving the model’s adaptability in specific domains. Additionally, we explore the use of LLMs for data augmentation, aiming to generate domain-specific datasets to alleviate the scarcity of domain data. Experiments conducted on three domain-specific datasets demonstrate that our model outperforms existing comparative models in several aspects, with F1 scores exceeding the State of the Art models by 2%, 1.6%, and 0.6%, respectively, validating the effectiveness and generalizability of our approach.}
}
@article{SHENG2025104060,
title = {Confusing negative commonsense knowledge generation with hierarchy modeling and LLM-enhanced filtering},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104060},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104060},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000020},
author = {Yaqing Sheng and Weixin Zeng and Jiuyang Tang and Lihua Liu and Xiang Zhao},
keywords = {Knowledge graph, Large language model, Negative statement},
abstract = {While most of the world’s knowledge exists in a positive and affirmative form, negative knowledge also plays a significant role by showing what is not true or what not to think, and has yet been largely overlooked. Existing negative commonsense knowledge generation methods adopt the generation-filtering paradigm, while the produced negative statements are easy to detect and fail to contribute to both human perception and task-specific algorithms that require negative samples for training. In response, we put forward CONEG, a negative commonsense knowledge generation framework that generates confusing statements, featuring hierarchy modeling in candidate generation and LLM-enhanced two-stage filtering. Specifically, in the candidate generation stage, we identify congeners for entity phrases in the commonsense knowledge base using box embeddings, which can effectively capture the hierarchical correlations among entity phrases and produce confusing candidates. In the candidate filtering stage, we design a two-stage filtering strategy, consisting of intrinsic triple confidence measuring and extrinsic refinement through large language models with group-based instructions, which can effectively filter out true facts and low-quality negative candidates. We empirically evaluate our proposal on both intrinsic assessment and downstream tasks, and the results demonstrate that CONEG and its components are effective in terms of producing confusing negative knowledge, surpassing the state-of-the-art methods.}
}
@article{LI2024100612,
title = {Building a knowledge graph to enrich ChatGPT responses in manufacturing service discovery},
journal = {Journal of Industrial Information Integration},
volume = {40},
pages = {100612},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100612},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000566},
author = {Yunqing Li and Binil Starly},
keywords = {Digital supply chain, Knowledge graph, ChatGPT, Manufacturing service discovery},
abstract = {Sourcing and identification of new manufacturing partners is crucial for manufacturing system integrators to enhance agility and reduce risk through supply chain diversification in the global economy. The advent of advanced large language models has captured significant interest, due to their ability to generate comprehensive and articulate responses across a wide range of knowledge domains. However, the system often falls short in accuracy and completeness when responding to domain-specific inquiries, particularly in areas like manufacturing service discovery. This research explores the potential of leveraging Knowledge Graphs in conjunction with ChatGPT to streamline the process for prospective clients in identifying small manufacturing enterprises. In this study, we propose a method that integrates bottom-up ontology with advanced machine learning models to develop a Manufacturing Service Knowledge Graph from an array of structured and unstructured data sources, including the digital footprints of small-scale manufacturers throughout North America. The Knowledge Graph and the learned graph embedding vectors are leveraged to tackle intricate queries within the digital supply chain network, responding with enhanced reliability and greater interpretability. The approach highlighted is scalable to millions of entities that can be distributed to form a global Manufacturing Service Knowledge Network Graph that can potentially interconnect multiple types of Knowledge Graphs that span industry sectors, geopolitical boundaries, and business domains. The dataset developed for this study, now publicly accessible, encompasses more than 13,000 manufacturers’ weblinks, manufacturing services, certifications, and location entity types.}
}
@article{CHEN2024111165,
title = {Systems engineering issues for industry applications of large language model},
journal = {Applied Soft Computing},
volume = {151},
pages = {111165},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.111165},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623011833},
author = {Wang Chen and Liu Yan-yi and Guo Tie-zheng and Li Da-peng and He Tao and Li Zhi and Yang Qing-wen and Wang Hui-han and Wen Ying-you},
keywords = {LLM, AIGC, Systems engineering, CDSS, Industry application},
abstract = {Large language model (LLM) is an important direction in the development of AGI, but its technology is still in rapid change, and its capabilities still have obvious deficiencies and imbalances, with persistent problems such as hallucination, value non-alignment, weak specialization, and black-box effect. In this case, how to apply LLM to different professional fields and develop high-quality AIGC industry applications has become a great challenge for ISVs. Building AIGC industry applications based on LLM is not simply a matter of functional realization. Although researchers and open-source communities have proposed numerous application development frameworks or tool components, there is a lack of overall architecture design for systems engineering and a lack of discussion on theories and methods of LLM application development in large-scale industry domains, such as healthcare, government affairs, finance, and media. This paper analyzes the basic ideas of LLM industry applications development, defines the functional requirements and feature requirements of LLM industry applications, puts forward the concept of Large Language Model Systems Engineering (LLM-SE), and develops an AI assisted clinical risk prediction system for amyloidosis disease based on the architecture of LLM-SE, which adopt knowledge engineering, quality engineering, etc., and verifies the LLM-SE development architecture and methodology.}
}
@article{NAJI20243694,
title = {Towards an LLM based approach for medical e-consent},
journal = {Procedia Computer Science},
volume = {246},
pages = {3694-3701},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.187},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924021987},
author = {Mouncef Naji and Maroua Masmoudi and Hajer Baazaoui Zghal},
keywords = {E-consent, Large Language Model, knowledge graph, Healthcare Information Systems},
abstract = {The question of informed and voluntary consent emerges as a matter of significance in healthcare. Obtaining informed consent, encounters many obstacles coupled with systemic, clinician-related, and patient-related factors, demanding interventions at different levels. This paper introduces a novel approach to present personalized consent based on Large Language Models (LLMs). The personalization of information is displayed through the combination of the LLM with a knowledge graph. We focus in our approach on how the knowledge graph enhances and personalize content generation, allowing therefore the acquisition of informed consent. The paper focuses as well on aspects related to hyper-parameters of information retrieval that help giving better prompt to the LLM. Experiments have showcased intresting results in terms of personalization and information retrieval using metrics of Rouge, Faithfulness and Relevance.}
}
@article{ADHIKARY2024,
title = {Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study},
journal = {JMIR Mental Health},
volume = {11},
year = {2024},
issn = {2368-7959},
doi = {https://doi.org/10.2196/57306},
url = {https://www.sciencedirect.com/science/article/pii/S2368795924000775},
author = {Prottay Kumar Adhikary and Aseem Srivastava and Shivani Kumar and Salam Michael Singh and Puneet Manuja and Jini K Gopinath and Vijay Krishnan and Swati Kedia Gupta and Koushik Sinha Deb and Tanmoy Chakraborty},
keywords = {mental health, counseling summarization, large language models, digital health, artificial intelligence, AI},
abstract = {Background
Comprehensive session summaries enable effective continuity in mental health counseling, facilitating informed therapy planning. However, manual summarization presents a significant challenge, diverting experts’ attention from the core counseling process. Leveraging advances in automatic summarization to streamline the summarization process addresses this issue because this enables mental health professionals to access concise summaries of lengthy therapy sessions, thereby increasing their efficiency. However, existing approaches often overlook the nuanced intricacies inherent in counseling interactions.
Objective
This study evaluates the effectiveness of state-of-the-art large language models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance.
Methods
We first created Mental Health Counseling-Component–Guided Dialogue Summaries, a benchmarking data set that consists of 191 counseling sessions with summaries focused on 3 distinct counseling components (also known as counseling aspects). Next, we assessed the capabilities of 11 state-of-the-art LLMs in addressing the task of counseling-component–guided summarization. The generated summaries were evaluated quantitatively using standard summarization metrics and verified qualitatively by mental health professionals.
Results
Our findings demonstrated the superior performance of task-specific LLMs such as MentalLlama, Mistral, and MentalBART evaluated using standard quantitative metrics such as Recall-Oriented Understudy for Gisting Evaluation (ROUGE)-1, ROUGE-2, ROUGE-L, and Bidirectional Encoder Representations from Transformers Score across all aspects of the counseling components. Furthermore, expert evaluation revealed that Mistral superseded both MentalLlama and MentalBART across 6 parameters: affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. However, these models exhibit a common weakness in terms of room for improvement in the opportunity costs and perceived effectiveness metrics.
Conclusions
While LLMs fine-tuned specifically on mental health domain data display better performance based on automatic evaluation scores, expert assessments indicate that these models are not yet reliable for clinical application. Further refinement and validation are necessary before their implementation in practice.}
}
@article{SHI2025102837,
title = {Dual data mapping with fine-tuned large language models and asset administration shells toward interoperable knowledge representation},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {91},
pages = {102837},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102837},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524001248},
author = {Dachuan Shi and Olga Meyer and Michael Oberle and Thomas Bauernhansl},
keywords = {Interoperability, Large language model, Asset administration shell, Digital twin, Entity matching, Knowledge representation},
abstract = {In the context of Industry 4.0, ensuring the compatibility of digital twins (DTs) with existing software systems in the manufacturing sector presents a significant challenge. The Asset Administration Shell (AAS), conceptualized as the standardized DT for an asset, offers a powerful framework that connects the DT with the established software infrastructure through interoperable knowledge representation. Although the IEC 63278 series specifies the AAS metamodel, it lacks a matching strategy for automating the mapping between proprietary data from existing software and AAS information models. Addressing this gap, we introduce a novel dual data mapping system (DDMS) that utilizes a fine-tuned open-source large language model (LLM) for entity matching. This system facilitates not only the mapping between existing software and AAS models but also between AAS models and standardized vocabulary dictionaries, thereby enhancing the model's semantic interoperability. A case study within the injection molding domain illustrates the practical application of DDMS for the automated creation of AAS instances, seamlessly integrating the manufacturer's existing data. Furthermore, we extensively investigate the potential of fine-tuning decode-only LLMs as generative classifiers and encoding-based classifiers for the entity matching task. To this end, we establish two AAS-specific datasets by collecting and compiling AAS-related resources. In addition, supplementary experiments are performed on general entity-matching benchmark datasets to ensure that our empirical conclusions and insights are generally applicable. The experiment results indicate that the fine-tuned generative LLM classifier achieves slightly better results, while the encoding-based classifier enables much faster inference. Furthermore, the fine-tuned LLM surpasses all state-of-the-art approaches for entity matching, including GPT-4 enhanced with in-context learning and chain of thoughts. This evidence highlights the effectiveness of the proposed DDMS in bridging the interoperability gap within DT applications, offering a scalable solution for the manufacturing industry.}
}
@article{REMADI2024102313,
title = {To prompt or not to prompt: Navigating the use of Large Language Models for integrating and modeling heterogeneous data},
journal = {Data & Knowledge Engineering},
volume = {152},
pages = {102313},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102313},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000375},
author = {Adel Remadi and Karim {El Hage} and Yasmina Hobeika and Francesca Bugiotti},
keywords = {Data engineering, Large language models, Conceptual schema modeling, Entity resolution, Data integration, Property graph models},
abstract = {Manually integrating data of diverse formats and languages is vital to many artificial intelligence applications. However, the task itself remains challenging and time-consuming. This paper highlights the potential of Large Language Models (LLMs) to streamline data extraction and resolution processes. Our approach aims to address the ongoing challenge of integrating heterogeneous data sources, encouraging advancements in the field of data engineering. Applied on the specific use case of learning disorders in higher education, our research demonstrates LLMs’ capability to effectively extract data from unstructured sources. It is then further highlighted that LLMs can enhance data integration by providing the ability to resolve entities originating from multiple data sources. Crucially, the paper underscores the necessity of preliminary data modeling decisions to ensure the success of such technological applications. By merging human expertise with LLM-driven automation, this study advocates for the further exploration of semi-autonomous data engineering pipelines.}
}
@article{AMOORE2024103134,
title = {A world model: On the political logics of generative AI},
journal = {Political Geography},
volume = {113},
pages = {103134},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2024.103134},
url = {https://www.sciencedirect.com/science/article/pii/S0962629824000830},
author = {Louise Amoore and Alexander Campolo and Benjamin Jacobsen and Ludovico Rella},
abstract = {The computational logics of large language models (LLMs) or generative AI – from the early models of CLIP and BERT to the explosion of text and image generation via ChatGPT and DALL-E − are increasingly penetrating the social and political world. Not merely in the direct sense that generative AI models are being deployed to govern difficult problems, whether decisions on the battlefield or responses to pandemic, but also because generative AI is shaping and delimiting the political parameters of what can be known and actioned in the world. Contra the promise of a generalizable “world model” in computer science, the article addresses how and why generative AI gives rise to a model of the world, and with it a set of political logics and governing rationalities that have profound and enduring effects on how we live today. The article traces the genealogies of generative AI models, how they have come into being, and why some concepts and techniques that animate these models become durable forms of knowledge that actively shape the world, even long after a specific material commercial GPT model has moved on to a new iteration. Though generative AI retains significant traces of former scientific and computational regimes – in statistical practices, probabilistic knowledge, and so on – it is also dislocating epistemological arrangements and opening them to novel ways of perceiving, characterising, classifying, and knowing the world. Four defining aspects of the political logic of generative AI are elaborated: i) generativity as something more than the capacity to generate image or text outputs, so that a generative logic acts upon the world understood as estimates of “underlying distributions” in data; ii) latency as a political logic of compression in which (by contrast with claims to reduction or distortion) the thing that is hidden, unknown or latent becomes surfaced and amenable to being governed; iii) broken and parallelized sequences as the ordering device of the political logic of generative AI, where attention frameworks radically change the possibilities for governing non-linear problems; iv) pre-training and fine-tuning as a computational logic of generative AI that simultaneously shapes a “zero shot politics” oriented towards unencountered data and new tasks. Across each of the four aspects, the article maps the emerging contemporary political logic of generative AI.}
}
@article{YE202443,
title = {Generative AI for visualization: State of the art and future directions},
journal = {Visual Informatics},
volume = {8},
number = {2},
pages = {43-66},
year = {2024},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2024.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X24000160},
author = {Yilin Ye and Jianing Hao and Yihan Hou and Zhan Wang and Shishi Xiao and Yuyu Luo and Wei Zeng},
keywords = {Visualization, Generative AI},
abstract = {Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design. Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations. Concurrently, recent major breakthroughs in GenAI like diffusion models and large language models have also drastically increased the potential of GenAI4VIS. From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research. Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction. For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI methods and visualizations. By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research.}
}
@article{XU2024,
title = {Data Set and Benchmark (MedGPTEval) to Evaluate Responses From Large Language Models in Medicine: Evaluation Development and Validation},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/57674},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000760},
author = {Jie Xu and Lu Lu and Xinwei Peng and Jiali Pang and Jinru Ding and Lingrui Yang and Huan Song and Kang Li and Xin Sun and Shaoting Zhang},
keywords = {ChatGPT, LLM, assessment, data set, benchmark, medicine},
abstract = {Background
Large language models (LLMs) have achieved great progress in natural language processing tasks and demonstrated the potential for use in clinical applications. Despite their capabilities, LLMs in the medical domain are prone to generating hallucinations (not fully reliable responses). Hallucinations in LLMs’ responses create substantial risks, potentially threatening patients’ physical safety. Thus, to perceive and prevent this safety risk, it is essential to evaluate LLMs in the medical domain and build a systematic evaluation.
Objective
We developed a comprehensive evaluation system, MedGPTEval, composed of criteria, medical data sets in Chinese, and publicly available benchmarks.
Methods
First, a set of evaluation criteria was designed based on a comprehensive literature review. Second, existing candidate criteria were optimized by using a Delphi method with 5 experts in medicine and engineering. Third, 3 clinical experts designed medical data sets to interact with LLMs. Finally, benchmarking experiments were conducted on the data sets. The responses generated by chatbots based on LLMs were recorded for blind evaluations by 5 licensed medical experts. The evaluation criteria that were obtained covered medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with 16 detailed indicators. The medical data sets include 27 medical dialogues and 7 case reports in Chinese. Three chatbots were evaluated: ChatGPT by OpenAI; ERNIE Bot by Baidu, Inc; and Doctor PuJiang (Dr PJ) by Shanghai Artificial Intelligence Laboratory.
Results
Dr PJ outperformed ChatGPT and ERNIE Bot in the multiple-turn medical dialogues and case report scenarios. Dr PJ also outperformed ChatGPT in the semantic consistency rate and complete error rate category, indicating better robustness. However, Dr PJ had slightly lower scores in medical professional capabilities compared with ChatGPT in the multiple-turn dialogue scenario.
Conclusions
MedGPTEval provides comprehensive criteria to evaluate chatbots by LLMs in the medical domain, open-source data sets, and benchmarks assessing 3 LLMs. Experimental results demonstrate that Dr PJ outperforms ChatGPT and ERNIE Bot in social and professional contexts. Therefore, such an assessment system can be easily adopted by researchers in this community to augment an open-source data set.}
}
@article{XIAO2023417,
title = {Knowledge graph-based manufacturing process planning: A state-of-the-art review},
journal = {Journal of Manufacturing Systems},
volume = {70},
pages = {417-435},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2023.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612523001577},
author = {Youzi Xiao and Shuai Zheng and Jiancheng Shi and Xiaodong Du and Jun Hong},
keywords = {Knowledge graph, Process planning, Process knowledge graph, CAPP},
abstract = {Computer-aided process planning is the bridge between computer-aided design and computer-aided manufacturing. With the advent of the intelligent manufacturing era, process knowledge is important for process planning. Knowledge graph is a semantic representation method of knowledge that has attracted extensive attention from the industry and academia. Process planning using the process knowledge graph has become an important development direction for computer-aided process planning. From the analysis of the published reviews, there have been many computer-aided process planning reviews with different focuses. We focus on the techniques and applications of knowledge graph in manufacturing process planning. Therefore, this paper comprehensively reviews knowledge graphs in manufacturing process planning. We analyze the key technologies of process knowledge graph, including process knowledge representation, process knowledge extraction, process knowledge graph construction, process knowledge graph refinement, process knowledge graph validation, and process generation. We also explore the combination of process knowledge graphs and large language models. Finally, potential future research directions are proposed.}
}
@article{JALALI2024109801,
title = {Large language models in electronic laboratory notebooks: Transforming materials science research workflows},
journal = {Materials Today Communications},
volume = {40},
pages = {109801},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109801},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824017823},
author = {Mehrdad Jalali and Yi Luo and Lachlan Caulfield and Eric Sauter and Alexei Nefedov and Christof Wöll},
keywords = {Materials science research, Natural language processing (NLP), Electronic laboratory notebooks (ELNs), Large language models (LLMs), Knowledge extraction, Scientific data management},
abstract = {In recent years, there has been a surge in research efforts dedicated to harnessing the capabilities of Large Language Models (LLMs) in various domains, particularly in material science. This paper delves into the transformative role of LLMs within Electronic Laboratory Notebooks (ELNs) for scientific research. ELNs represent a pivotal technological advancement, providing a digital platform for researchers to record and manage their experiments, data, and findings. This study explores the potential of LLMs to revolutionize fundamental aspects of science, including experimental methodologies, data analysis, and knowledge extraction within the ELN framework. We present a demonstrative showcase of LLM applications in ELN environments and, furthermore, we conduct a series of empirical evaluations to critically assess the practical impact of LLMs in enhancing research processes within the dynamic field of materials science. Our findings illustrate how LLMs can significantly elevate the quality and efficiency of research outcomes in ELNs, thereby advancing knowledge and innovation in materials science research and beyond.}
}
@article{SONNENBURG2024153933,
title = {Artificial intelligence-based data extraction for next generation risk assessment: Is fine-tuning of a large language model worth the effort?},
journal = {Toxicology},
volume = {508},
pages = {153933},
year = {2024},
issn = {0300-483X},
doi = {https://doi.org/10.1016/j.tox.2024.153933},
url = {https://www.sciencedirect.com/science/article/pii/S0300483X24002142},
author = {Anna Sonnenburg and Benthe {van der Lugt} and Johannes Rehn and Paul Wittkowski and Karsten Bech and Florian Padberg and Dimitra Eleftheriadou and Todor Dobrikov and Hans Bouwmeester and Carla Mereu and Ferdinand Graf and Carsten Kneuer and Nynke I. Kramer and Tilmann Blümmel},
keywords = {Artificial intelligence, Risk Assessment, Systematic literature review, Automated data extraction, Large Language models, Fine-tuning},
abstract = {To underpin scientific evaluations of chemical risks, agencies such as the European Food Safety Authority (EFSA) heavily rely on the outcome of systematic reviews, which currently require extensive manual effort. One specific challenge constitutes the meaningful use of vast amounts of valuable data from new approach methodologies (NAMs) which are mostly reported in an unstructured way in the scientific literature. In the EFSA-initiated project ‘AI4NAMS’, the potential of large language models (LLMs) was explored. Models from the GPT family, where GPT refers to Generative Pre-trained Transformer, were used for searching, extracting, and integrating data from scientific publications for NAM-based risk assessment. A case study on bisphenol A (BPA), a substance of very high concern due to its adverse effects on human health, focused on the structured extraction of information on test systems measuring biologic activities of BPA. Fine-tuning of a GPT-3 model (Curie base model) for extraction tasks was tested and the performance of the fine-tuned model was compared to the performance of a ready-to-use model (text-davinci-002). To update findings from the AI4NAMS project and to check for technical progress, the fine-tuning exercise was repeated and a newer ready-to-use model (text-davinci-003) served as comparison. In both cases, the fine-tuned Curie model was found to be superior to the ready-to-use model. Performance improvement was also obvious between text-davinci-002 and the newer text-davinci-003. Our findings demonstrate how fine-tuning and the swift general technical development improve model performance and contribute to the growing number of investigations on the use of AI in scientific and regulatory tasks.}
}
@article{LOPEZ2025100845,
title = {Enhancing foundation models for scientific discovery via multimodal knowledge graph representations},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100845},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100845},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000313},
author = {Vanessa Lopez and Lam Hoang and Marcos Martinez-Galindo and Raúl Fernández-Díaz and Marco Luca Sbodio and Rodrigo Ordonez-Hurtado and Mykhaylo Zayats and Natasha Mulligan and Joao Bettencourt-Silva},
keywords = {Multimodal graph learning, Multimodal knowledge graphs, Knowledge-enhanced drug discovery},
abstract = {Foundation Models (FMs) hold transformative potential to accelerate scientific discovery, yet reaching their full capacity in complex, highly multimodal domains such as genomics, drug discovery, and materials science requires a deeper consideration of the contextual nature of the scientific knowledge. We revisit the synergy between FMs and Multimodal Knowledge Graph (MKG) representation and learning, exploring their potential to enhance predictive and generative tasks in biomedical contexts like drug discovery. We seek to exploit MKGs to improve generative AI models’ ability to capture intricate domain-specific relations and facilitate multimodal fusion. This integration promises to accelerate discovery workflows by providing more meaningful multimodal knowledge-enhanced representations and contextual evidence. Despite this potential, challenges and opportunities remain, including fusing multiple sequential, structural and knowledge modalities and models leveraging the strengths of each; developing scalable architectures for multi-task multi-dataset learning; creating end-to-end workflows to enhance the trustworthiness of biomedical FMs using knowledge from heterogeneous datasets and scientific literature; the domain data bottleneck and the lack of a unified representation between natural language and chemical representations; and benchmarking, specifically the transfer learning to tasks with limited data (e.g., unseen molecules and proteins, rear diseases). Finally, fostering openness and collaboration is key to accelerate scientific breakthroughs.}
}
@article{CARTA20242235,
title = {A Zero-Shot Strategy for Knowledge Graph Engineering Using GPT-3.5},
journal = {Procedia Computer Science},
volume = {246},
pages = {2235-2243},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.573},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026231},
author = {Salvatore Carta and Alessandro Giuliani and Marco Manolo Manca and Leonardo Piano and Alessandro Sebastian Podda and Livio Pompianu and Sandro Gabriele Tiddia},
keywords = {Knowledge Engineering, Knowledge Graphs, Large Language Models},
abstract = {In the recent digitization era, capturing, representing, and understanding knowledge is essential in countless real-world scenarios. Knowledge graphs emerged as a powerful tool for representing information through an adequately interconnected and interpretable structure in such a context. Nevertheless, generating proper knowledge graphs usually requires significant manual effort and domain expertise, resulting in graphs often affected by human subjectivity, limited scalability, or inability to capture implicit knowledge or handle heterogeneity. This paper proposes an innovative zero-shot strategy tailored to uncover reliable knowledge from text leveraging the recent highly effective generative large language models, with a particular focus on the GPT-3.5 model. Our proposal aims to create a suitable knowledge graph or improve existing ones by discovering missing qualitative triples. To assess the effectiveness of our methodology, we performed experiments on domain-specific datasets, confirming its potential for scalable and versatile knowledge discovery.}
}
@article{JOSE2024124603,
title = {Advancing multimodal diagnostics: Integrating industrial textual data and domain knowledge with large language models},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124603},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124603},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424014702},
author = {Sagar Jose and Khanh T.P Nguyen and Kamal Medjaher and Ryad Zemouri and Mélanie Lévesque and Antoine Tahan},
keywords = {Fault detection and diagnostics, Large language model, Expert knowledge, Technical language processing, Multimodal data},
abstract = {The rapid advancement and application of large language models (LLMs) in various domains prompt an investigation into their potential in the field of prognostics and health management (PHM), particularly for enhancing data-driven model capabilities. This study explores the integration of domain knowledge accumulated in unstructured text data such as technical documents and maintenance logs into diagnostics models using LLMs. The study demonstrates the new possibilities to exploit data that are traditionally underutilized due to their complexity and the presence of domain-specific jargon. By leveraging LLMs for contextual understanding and information extraction from such texts, this study proposes a novel approach that combines textual data with existing condition monitoring systems to improve the accuracy of diagnostics models. A case study on hydrogenerators illustrates the feasibility and value of integrating LLMs into PHM systems. The findings suggest that the incorporation of LLMs can lead to more informed, accurate diagnostics, ultimately enhancing operational efficiency and safety within industrial environments.}
}
@article{RAMRAKHIYANI2025103892,
title = {Gauging, enriching and applying geography knowledge in Pre-trained Language Models},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103892},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103892},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002516},
author = {Nitin Ramrakhiyani and Vasudeva Varma and Girish Keshav Palshikar and Sachin Pawar},
keywords = {Pre-trained language models, Geography knowledge, Language model probing},
abstract = {To employ Pre-trained Language Models (PLMs) as knowledge containers in niche domains it is important to gauge the knowledge of these PLMs about facts in these domains. It is also an important pre-requisite to know how much enrichment effort is required to make them better. As part of this work, we aim to gauge and enrich small PLMs for knowledge of world geography. Firstly, we develop a moderately sized dataset of masked sentences covering 24 different fact types about world geography to estimate knowledge of PLMs on these facts. We hypothesize that for this niche domain, smaller PLMs may not be well equipped. Secondly, we enrich PLMs with this knowledge through fine-tuning and check if the knowledge in the dataset is infused sufficiently. We further hypothesize that linguistic variability in the manual templates used to embed the knowledge in masked sentences does not affect the knowledge infusion. Finally, we demonstrate the application of PLMs to tourism blog search and Wikidata KB augmentation. In both applications, we aim at showing the effectiveness of using PLMs to achieve competitive performance.}
}
@article{PLONKA2025126711,
title = {A comparative evaluation of the effectiveness of document splitters for large language models in legal contexts},
journal = {Expert Systems with Applications},
volume = {272},
pages = {126711},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126711},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425003331},
author = {Mateusz Płonka and Krzysztof Kocot and Kacper Hołda and Krzysztof Daniec and Aleksander Nawrat},
keywords = {Large language models, Retrieval-augmented generation, Legal documents, Context splitters, Semantic splitting},
abstract = {The study explores the development and application of an advanced artificial intelligence-based system aimed at improving the efficiency and accuracy of legal document processing. Due to the specific nature and complexity of legal texts, traditional document management techniques often prove inadequate and error-prone, creating significant challenges for legal practitioners. The proposed method leverages natural language processing and machine learning algorithms to automate key processes such as analysis, search, and classification. By utilizing vector embedding techniques, the system enables precise information retrieval from large legal document collections, while advanced splitting methods generate concise and relevant chunks of extensive texts. The study employs a Retrieval-Augmented Generation approach, combining Large Language Models (LLMs) with external knowledge bases to enhance the accuracy and contextual relevance of generated responses, addressing common issues such as hallucinations and outdated information in traditional LLMs. The research provides an in-depth analysis of the application of various text-splitting algorithms in the context of legal document databases. The findings highlight the characteristics of appropriate algorithms and offer recommendations on the conditions under which specific mechanisms should be employed.}
}
@article{KASPRZIK2024160,
title = {The Automation of Subject Indexing at ZBW and the Role of Metadata in Times of Large Language Models},
journal = {Procedia Computer Science},
volume = {249},
pages = {160-166},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032708},
author = {Anna Kasprzik},
keywords = {subject indexing, automation, machine learning, artificial intelligence, IT infrastructure, metadata, large language models},
abstract = {Subject indexing is one of the core activities of libraries. Due to the proliferation of digital documents it is no longer possible to annotate every single document intellectually, which is why we need to explore the potentials of automation. At ZBW the efforts to automate the subject indexing process started as early as 2000 with experiments involving external partners and commercial software. The conclusion from that first period was that supposedly shelf-ready solutions would not cover the requirements of the library. In 2014 the decision was made to start doing the necessary applied research in-house by establishing a corresponding PhD position. However, the prototypical machine learning solutions they developed were yet to be integrated into productive operations at the library. Therefore in 2020 an additional position for a software engineer was established and a 4-year pilot phase was initiated with the goal to build a software architecture that allows for real-time subject indexing with our trained models and the integration thereof into the other metadata workflows at ZBW. This paper gives an account of how we tackled the task of transferring results from applied research into a productive subject indexing service (the “AutoSE service”), including the milestones we have reached, the challenges we were facing on a strategic level, and the measures and resources (computing power, software, personnel) that were needed in order to be able to effect the transfer and get a first version going, which went live in 2021. The models used by AutoSE until now were models from classical machine learning. We therefore also touch on the question if and how the recent advent of large language models (LLMs) has changed our outlook on the task of automating subject indexing and on the role of metadata in information management and retrieval in general, and the ways in which it impacts our research and development roadmap going forward.}
}
@article{JEMAL2025125648,
title = {A new approach for competency frameworks mapping using large language models},
journal = {Expert Systems with Applications},
volume = {263},
pages = {125648},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125648},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424025156},
author = {Imene Jemal and Naoussi Sijou Wilfried Armand and Belkacem Chikhaoui},
keywords = {Competency framework, Framework competency mapping, Large language models, Natural language processing, Project management, PMBOK},
abstract = {Competency frameworks are essential for organizations to align their workforce with strategic goals and for individuals to assess and develop their skills. However, the absence of a universal or unified competency framework presents a challenge, as each framework is subject to distinct guidelines and standards. Moreover, within a single framework, continuous updates to align with evolving standards can lead to equivalent competencies being expressed in different ways. Despite the fundamental similarity of the competencies, this divergence across frameworks can impede interoperability and complicate the aggregation of data from multiple frameworks. This paper addresses this issue by proposing an approach that leverages large language models (LLMs) for mapping competency frameworks to enhance interoperability among frameworks. We investigated various pre-trained LLMs to encode competency names from each framework. Subsequently, we employed cosine similarity to measure semantic similarity scores, which facilitated the identification of equivalent or closely related competencies across different frameworks. We evaluated our approach using three competency frameworks for project management, each aligned with different editions of the Project Management Body of Knowledge (PMBOK) standards. The experimental results demonstrate the effectiveness of the proposed approach in ameliorating frameworks interoperability.}
}
@article{CHARTIER2025102383,
title = {HiBenchLLM: Historical Inquiry Benchmarking for Large Language Models},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102383},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102383},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001071},
author = {Mathieu Chartier and Nabil Dakkoune and Guillaume Bourgeois and Stéphane Jean},
keywords = {Large Language Models, Historical Inquiry, Benchmarking},
abstract = {Large Language Models (LLMs) such as ChatGPT or Bard have significantly transformed information retrieval and captured the public’s attention with their ability to generate customized responses across various topics. In this paper, we analyze the capabilities of different LLMs to generate responses related to historical facts in French. Our objective is to evaluate their reliability, comprehensiveness, and relevance for direct usability or extraction. To accomplish this, we propose a benchmark consisting of numerous historical questions covering various types, themes, and difficulty levels. Our evaluation of responses provided by 14 selected LLMs reveals several limitations in both content and structure. In addition to an overall insufficient precision rate, we observe uneven treatment of the French language, along with issues related to verbosity and inconsistency in the responses generated by LLMs.}
}
@article{ZOU2025129589,
title = {A novel large language model enhanced joint learning framework for fine-grained sentiment analysis on drug reviews},
journal = {Neurocomputing},
volume = {626},
pages = {129589},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129589},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225002619},
author = {Haochen Zou and Yongli Wang},
keywords = {Fine-grained sentiment analysis, Aspect-based sentiment analysis, Large language model, Health informatics},
abstract = {Patient feedback on drug reviews extracted from social media platforms and online forums provides genuine sentiment information regarding post-medication usage. The insights are invaluable for prospective patients seeking appropriate medical guidance and stakeholders within the biomedical industry aiming to improve products. This paper presents a novel joint learning framework for fine-grained aspect-based sentiment analysis on drug reviews. By leveraging prior biomedical knowledge from the domain-specific pre-trained large language model, we address the challenge of fine-grained aspect-based sentiment analysis by collaboratively integrating both coarse and fine-grained contextual features within the text content, capturing precise biomedical aspect terms and the corresponding sentiment polarities. To the best of our knowledge, this work pioneers the initial introduction of a joint learning framework based on the fine-tuned biomedical large language model for fine-grained aspect-based sentiment analysis within drug reviews. By conducting extensive experiments on publicly available drug review datasets and comparing the constructed architecture with state-of-the-art techniques, the joint learning framework outperforms baseline competitors across evaluation metrics.}
}
@article{YU2024100076,
title = {Large-language models: The game-changers for materials science research},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100076},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100076},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000344},
author = {Songlin Yu and Nian Ran and Jianjun Liu},
keywords = {Large language models, Agent, LLM for materials, Artificial intelligence, Materials science research, Intelligent laboratory},
abstract = {Large Language Models (LLMs), such as GPT-4, are precipitating a new "industrial revolution" by significantly enhancing productivity across various domains. These models encode an extensive corpus of scientific knowledge from vast textual datasets, functioning as near-universal generalists with the ability to engage in natural language communication and exhibit advanced reasoning capabilities. Notably, agents derived from LLMs can comprehend user intent and autonomously design, plan, and utilize tools to execute intricate tasks. These attributes are particularly advantageous for materials science research, an interdisciplinary field characterized by numerous complex and time-intensive activities. The integration of LLMs into materials science research holds the potential to fundamentally transform the research paradigm in this field.}
}
@article{YIN2025107388,
title = {A novel approach to unlocking the synergy of large language models and chemical knowledge in biomedical signal applications},
journal = {Biomedical Signal Processing and Control},
volume = {103},
pages = {107388},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.107388},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424014460},
author = {Zilong Yin and Haoyu Wang and Bin Chen and Hangling Sun and Anji Li and Chenyu Zhou},
keywords = {Biomedical signal processing, Supervised chemical knowledge, Large language models, Molecular property prediction},
abstract = {This work explores the potential of using the pre-trained large language model Llama2 to address challenges in biomedical signal processing and control (BSPC), particularly in predicting the electronic and functional properties of organic molecules, an area of growing importance in fields such as drug discovery and materials science. Current approaches in BSPC often rely on specialized graph neural network models, which can be limited in their ability to capture the complex relationships inherent in molecular structures. To address this, we demonstrate that a fine-tuned Llama2 model can accurately predict the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) energies of organic semiconductor molecules, with performance comparable to state-of-the-art specialized models. To further enhance the model’s robustness and generalization, we propose several key innovations, including optimized simplified molecular input line entry system (SMILES) tokenization, incorporation of chemical knowledge as auxiliary supervised tasks, and a low-rank adaptation (LORA) based fine-tuning strategy. These techniques enable the language model to simultaneously learn SMILES prediction and acquire relevant chemical knowledge, while also improving its handling of incomplete structural information and ability to generalize to ”unseen” molecular classes. The work also discusses the limitations of using large language models for molecular property prediction, such as the lack of interpretability and the need for improved handling of non-standard SMILES representations, highlighting the potential of this approach in BSPC while identifying areas for further improvement.}
}
@article{GU2025112694,
title = {CECA: An intelligent large-language-model-enabled method for accounting embodied carbon in buildings},
journal = {Building and Environment},
volume = {272},
pages = {112694},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.112694},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325001763},
author = {Xierong Gu and Cheng Chen and Yuan Fang and Ron Mahabir and Lei Fan},
keywords = {Embodied carbon, Large language model, Machine learning, Artificial intelligence, Life cycle assessment, Building},
abstract = {The construction sector's contribution to global carbon emissions is significant, with embodied carbon accounting for around 40 % of the sector's total emissions and even more than 50 % in the case of net-zero energy buildings. Traditional life cycle assessment methods for embodied carbon accounting are time-consuming and labor-intensive, hindering rapid evaluation of carbon footprint in buildings. This study presents a novel construction embodied carbon assessment (CECA) method. It uses large language models for intelligent semantic parsing and automatically matches material and equipment information with corresponding carbon emission factors. Based on our experiments, CECA outperforms traditional machine learning algorithms, achieving a matching accuracy of 0.8412. Validation across 18 real-world building cases demonstrates that the CECA method with Claude-3.5 achieves comparable carbon accounting accuracy to traditional life cycle assessment methods, with an average mean absolute percentage error of 12.5 %. Additionally, CECA significantly enhances computational efficiency, achieving a 216-fold improvement—approximately 60 ss for CECA compared to 3.6 h for LCA in our experiment where adopted BIM models are already available. Furthermore, the CECA method can identify carbon emission contributions of various materials and components, facilitating carbon optimization during the building design process. By advancing the application of large language models in evaluating embodied carbon in buildings, the CECA method offers a promising solution for rapid, accurate, and automated embodied carbon accounting, addressing the growing demand for efficient carbon evaluation and management in construction projects.}
}
@article{DING2025112663,
title = {Large language models for cyber resilience: A comprehensive review, challenges, and future perspectives},
journal = {Applied Soft Computing},
volume = {170},
pages = {112663},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112663},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624014376},
author = {Weiping Ding and Mohamed Abdel-Basset and Ahmed M. Ali and Nour Moustafa},
keywords = {Large Language Model, Cyber Resilience, Cyber Security, Data Privacy and Protection, Network and Endpoint Security},
abstract = {Interconnect cyber system is used by various users and organizations worldwide to perform different activities. These activities are combined with digital information and systems around the organizations to obtain higher accuracy and performance. However, these combinations of activities have faced cyber threats and attacks by single or multiple attackers. So, protecting and saving users' and organizations' sensitive data is a big challenge. So, the cyber resilience concept refers to the ability to prepare, absorb, recover, and adapt against cyberattacks and threats. It is used to mitigate cyberattacks and risks by the ability of the system to recover from threats. Artificial intelligence models enhance cyber resilience using machine learning and deep learning models. One of the most common components of artificial intelligence is large language models (LLM). It is used to understand language from text data and extract features to predict future words or missing in text datasets. LLM can enhance cyber resilience by providing various benefits for users and organizations. We divide the cyber resilience strategies into five parts. We review the LLM in each part, including security posture, data privacy and protection, security awareness, network security, and security automation. The fundamentals of LLMs are introduced as pre-trained models, transformers, encoders, and decoders. Then, we review the challenges of LLM in cyber resilience and cyber defense methods to overcome these challenges. We applied the LLM into three case studies including two for email spam text classifications and one for cyber threat detection. We obtained higher accuracy including 96.67 %, 90.70 %, and 89.94 % from three case studies respectively. Then we compared our LLM with other traditional machine learning models. The results show the LLM has higher accuracy, precision, recall, and f1 score compared with other models. Finally, the future directions of LLM in cyber resilience are provided.}
}
@article{AGARONNIK2025243,
title = {Large Language Models to Identify Advance Care Planning in Patients With Advanced Cancer},
journal = {Journal of Pain and Symptom Management},
volume = {69},
number = {3},
pages = {243-250.e1},
year = {2025},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2024.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S088539242401128X},
author = {Nicole D. Agaronnik and Joshua Davis and Christopher R. Manz and James A. Tulsky and Charlotta Lindvall},
keywords = {Large language models, Advance care planning, Goals of care, Artificial intelligence},
abstract = {Context
Efficiently tracking Advance Care Planning (ACP) documentation in electronic heath records (EHRs) is essential for quality improvement and research efforts. The use of large language models (LLMs) offers a novel approach to this task.
Objectives
To evaluate the ability of LLMs to identify ACP in EHRs for patients with advanced cancer and compare performance to gold-standard manual chart review and natural language processing (NLP).
Methods
EHRs from patients with advanced cancer followed at seven Dana Farber Cancer Center (DFCI) clinics in June 2024. We utilized GPT-4o-2024-05-13 within DFCI's HIPAA-secure digital infrastructure. We designed LLM prompts to identify ACP domains: goals of care, limitation of life-sustaining treatment, hospice, and palliative care. We developed a novel hallucination index to measure production of factually-incorrect evidence by the LLM. Performance was compared to gold-standard manual chart review and NLP.
Results
60 unique patients associated with 528 notes were used to construct the gold-standard data set. LLM prompts had sensitivity ranging from 0.85 to 1.0, specificity ranging from 0.80 to 0.91, and accuracy ranging from 0.81 to 0.91 across domains. The LLM had better sensitivity than NLP for identifying complex topics such as goals of care. Average hallucination index for notes identified by LLM was less than 0.5, indicating a low probability of hallucination. Despite lower precision compared to NLP, false positive documentation identified by LLMs was clinically-relevant and useful for guiding management.
Conclusion
LLMs can capture ACP domains from EHRs, with sensitivity exceeding NLP methods for complex domains such as goals of care. Future studies should explore approaches for scaling this methodology.}
}
@article{FLAHARTY20241819,
title = {Evaluating large language models on medical, lay-language, and self-reported descriptions of genetic conditions},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {9},
pages = {1819-1833},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2024.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0002929724002556},
author = {Kendall A. Flaharty and Ping Hu and Suzanna Ledgister Hanchard and Molly E. Ripper and Dat Duong and Rebekah L. Waikel and Benjamin D. Solomon},
keywords = {large language model, deep learning, in-context prompting, artificial intelligence, machine learning, medical genetics, medical genomics},
abstract = {Summary
Large language models (LLMs) are generating interest in medical settings. For example, LLMs can respond coherently to medical queries by providing plausible differential diagnoses based on clinical notes. However, there are many questions to explore, such as evaluating differences between open- and closed-source LLMs as well as LLM performance on queries from both medical and non-medical users. In this study, we assessed multiple LLMs, including Llama-2-chat, Vicuna, Medllama2, Bard/Gemini, Claude, ChatGPT3.5, and ChatGPT-4, as well as non-LLM approaches (Google search and Phenomizer) regarding their ability to identify genetic conditions from textbook-like clinician questions and their corresponding layperson translations related to 63 genetic conditions. For open-source LLMs, larger models were more accurate than smaller LLMs: 7b, 13b, and larger than 33b parameter models obtained accuracy ranges from 21%–49%, 41%–51%, and 54%–68%, respectively. Closed-source LLMs outperformed open-source LLMs, with ChatGPT-4 performing best (89%–90%). Three of 11 LLMs and Google search had significant performance gaps between clinician and layperson prompts. We also evaluated how in-context prompting and keyword removal affected open-source LLM performance. Models were provided with 2 types of in-context prompts: list-type prompts, which improved LLM performance, and definition-type prompts, which did not. We further analyzed removal of rare terms from descriptions, which decreased accuracy for 5 of 7 evaluated LLMs. Finally, we observed much lower performance with real individuals’ descriptions; LLMs answered these questions with a maximum 21% accuracy.}
}
@article{LI2025102995,
title = {Vision-Language Models in medical image analysis: From simple fusion to general large models},
journal = {Information Fusion},
volume = {118},
pages = {102995},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102995},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000685},
author = {Xiang Li and Like Li and Yuchen Jiang and Hao Wang and Xinyu Qiao and Ting Feng and Hao Luo and Yong Zhao},
keywords = {Vision-language model, Modality fusion, Medical image analysis, Large model},
abstract = {Vision-Language Model (VLM) is a kind of multi-modality deep learning model that aims to fuse visual information with language information to enhance the understanding and analysis of visual content. VLM was originally used to integrate multi-modality information and improve task accuracy. Then, VLM was further developed in combination with zero-shot and few-shot learning to solve the problem of insufficient medical labels. At present, it is the technical basis of the popular medical general large model. Its role is no longer limited to simple information fusion. This paper makes a comprehensive review for the development and application of VLM-based medical image analysis technology. Specifically, this paper first introduces the basic principle and explains the pre-training and fine-tuning framework. Then, the research progress of medical image classification, segmentation, report generation, question answering, image generation, large model and other application scenarios is introduced. This paper also summarizes seven main characteristics of medical image VLM, and analyzes the specific embodiment of these characteristics in each task. Finally, the challenges, potential solutions and future directions in this field are discussed. VLM is still in a rapid development in the field of medical image analysis, and a continuously updated repository of papers and code has been built, it is available at https://github.com/XiangQA-Q/VLM-in-MIA.}
}
@article{DAGA2025100846,
title = {Process Knowledge Graphs (PKG): Towards unpacking and repacking AI applications},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100846},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100846},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000325},
author = {Enrico Daga},
keywords = {Knowledge graphs, Prompt engineering, Data science pipelines, Data pipelines documentation, Data pipelines design},
abstract = {In the past years, a new generation of systems has emerged, which apply recent advances in generative Artificial Intelligence (AI) in combination with traditional technologies. Specifically, generative AI is being delegated tasks in natural language or vision understanding within complex hybrid architectures that also include databases, procedural code, and interfaces. Process Knowledge Graphs (PKG) have a long-standing tradition within symbolic AI research. On the one hand, PKGs can play an important role in describing complex, hybrid applications, thus opening the way for addressing fundamental challenges such as explaining and documenting such systems (unpacking). On the other hand, by organising complex processes in simpler building blocks, PKGs can potentially increase accuracy and control over such systems (repacking). In this position paper, we discuss opportunities and challenges of PGRs and their potential role towards a more robust and principled design of AI applications.}
}
@article{DAI2024107530,
title = {TCMChat: A generative large language model for traditional Chinese medicine},
journal = {Pharmacological Research},
volume = {210},
pages = {107530},
year = {2024},
issn = {1043-6618},
doi = {https://doi.org/10.1016/j.phrs.2024.107530},
url = {https://www.sciencedirect.com/science/article/pii/S1043661824004754},
author = {Yizheng Dai and Xin Shao and Jinlu Zhang and Yulong Chen and Qian Chen and Jie Liao and Fei Chi and Junhua Zhang and Xiaohui Fan},
keywords = {Traditional Chinese medicine, Large language model, Dialogue system, Pre-training, Supervised fine-tuning},
abstract = {The utilization of ground-breaking large language models (LLMs) accompanied with dialogue system has been progressively prevalent in the medical domain. Nevertheless, the expertise of LLMs in Traditional Chinese Medicine (TCM) remains restricted despite several TCM LLMs proposed recently. Herein, we introduced TCMChat (https://xomics.com.cn/tcmchat), a generative LLM with pre-training (PT) and supervised fine-tuning (SFT) on large-scale curated TCM text knowledge and Chinese Question-Answering (QA) datasets. In detail, we first compiled a customized collection of six scenarios of Chinese medicine as the training set by text mining and manual verification, involving TCM knowledgebase, choice question, reading comprehension, entity extraction, medical case diagnosis, and herb or formula recommendation. Next, we subjected the model to PT and SFT, using the Baichuan2–7B-Chat as the foundation model. The benchmarking datasets and case studies further demonstrate the superior performance of TCMChat in comparison to existing models. Our code, data and model are publicly released on GitHub (https://github.com/ZJUFanLab/TCMChat) and HuggingFace (https://huggingface.co/ZJUFanLab), providing high-quality knowledgebase for the research of TCM modernization with a user-friendly dialogue web tool.}
}
@article{OZEN2025100679,
title = {Extracting chemical food safety hazards from the scientific literature automatically using large language models},
journal = {Applied Food Research},
volume = {5},
number = {1},
pages = {100679},
year = {2025},
issn = {2772-5022},
doi = {https://doi.org/10.1016/j.afres.2024.100679},
url = {https://www.sciencedirect.com/science/article/pii/S2772502224002890},
author = {Neris Özen and Wenjuan Mu and Esther D. {van Asselt} and Leonieke M. {van den Bulk}},
keywords = {Chemical contamination, Food safety, Information extraction, Prompt engineering, Natural language processing, Artificial intelligence},
abstract = {The number of scientific articles published in the domain of food safety has consistently been increasing over the last few decades. It has therefore become unfeasible for food safety experts to read all relevant literature related to food safety and the occurrence of hazards in the food chain. However, it is important that food safety experts are aware of the newest findings and can access this information in an easy and concise way. In this study, an approach is presented to automate the extraction of chemical hazards from the scientific literature through large language models. The large language model was used out-of-the-box and applied on scientific abstracts; no extra training of the models or a large computing cluster was required. Three different styles of prompting the model were tested to assess which was the most optimal for the task at hand. The prompts were optimized with two validation foods (leafy greens and shellfish) and the final performance of the best prompt was evaluated using three test foods (dairy, maize and salmon). The specific wording of the prompt was found to have a considerable effect on the results. A prompt breaking the task down into smaller steps performed best overall. This prompt reached an average accuracy of 93 % and contained many chemical contaminants already included in food monitoring programs, validating the successful retrieval of relevant hazards for the food safety domain. The results showcase how valuable large language models can be for the task of automatic information extraction from the scientific literature.}
}
@article{PANAGOULIAS20241181,
title = {Knowledge Space reduction via Sequential Language Model Integration},
journal = {Procedia Computer Science},
volume = {246},
pages = {1181-1190},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.544},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025894},
author = {Dimitrios P. Panagoulias and Maria Virvou and George A. Tsihrintzis},
keywords = {AI-empowered software engineering, Large Language Models, GPT, BERT},
abstract = {In Large Language Models (LLMs), such as GPT, BERT, Mistral or others, “reducing the domain space” for text generation involves limiting the range of content that can be utilized for generating responses. This approach aims at enhancing the relevance and precision of the text produced. While various strategies exist to achieve this, this work explores Sequential Language Model Integration (SLMI), which mirrors the organization and distribution of knowledge across different fields of expertise. More specifically, SLMI is the technique of linking multiple LLMs (LLM-Chains) in a systematic manner. In this paper, we refer to a process of choosing, linking and connecting LLMs with other services (often to complete a generative task, invoke external functions and machine learning services, or tackle problems) as “Large Language Models as a Service”. We outline the development and evaluation process of an SLMI methodology to refine response accuracy. Focusing on the medical field, we also establish a framework for knowledge reduction based on “knowledge paths”, analogous to the distinct specializations within medicine. We apply this framework to a dermatology case study and utilize our evaluation pipeline to assess the results. Reducing the knowledge domain from medicine in general down to dermatology, we tested our methodology and found gains regarding accuracy and diagnostic improvement, as well as a reduction in costs regarding total tokens generated.}
}
@article{RUAN2025105746,
title = {CPRS: a clinical protocol recommendation system based on LLMs},
journal = {International Journal of Medical Informatics},
volume = {195},
pages = {105746},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105746},
url = {https://www.sciencedirect.com/science/article/pii/S138650562400409X},
author = {Jingkai Ruan and Qianmin Su and Zihang Chen and Jihan Huang and Ying Li},
keywords = {Recommendation system, LLMs, Knowledge graph, Clinical trial},
abstract = {Background: As fundamental documents in clinical trials, clinical trial protocols are intended to ensure that trials are conducted according to the objectives set by researchers. The advent of large models with superior semantic performance compared to traditional models provides fresh perspectives for research recommendations in clinical trial protocols. Method: A clinical trial protocol recommendation system based on Large Language Models (LLMs) is proposed in this paper, combining GPT-4 and knowledge graph to assist in clinical trial protocol recommendations. Using knowledge graphs as an auxiliary tool, a finite set of clinical trial projects with similar features is identified. Subsequently, through the semantic capabilities of GPT-4, targeted recommendations are made to patients. Results: Experiments were conducted to compare GPT-4 and multiple models from the SBERT family that handle semantic similarity. The results indicate that GPT-4 is capable of better sorting clinical trial protocols based on similarity criteria and offering targeted recommendations to patients. Consequently, this capability meets the matching requirements between projects and patients and enhances the automation of clinical trial protocol recommendations. Additionally, in the future, personal factors of patients will be fully considered during the recommendation process to provide more accurate and personalized protocol recommendations. Conclusion: By integrating knowledge graphs and LLMs, a better understanding and processing of clinical trial protocol information can be achieved, enabling the recommendation of appropriate protocols for patients and enhancing both matching efficiency and accuracy. Furthermore, the application of this system contributes to the automation of clinical trial protocol recommendations, playing a crucial role in medical research institutions such as clinical trial research institutes and public health management departments. Additionally, it significantly aids in advancing the development of clinical trials and the medical field at large.}
}
@article{RAMOS20252514,
title = {A review of large language models and autonomous agents in chemistry},
journal = {Chemical Science},
volume = {16},
number = {6},
pages = {2514-2572},
year = {2025},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc03921a},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024020650},
author = {Mayk Caldas Ramos and Christopher J. Collison and Andrew D. White},
abstract = {Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation. We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains. This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry. Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods. Due to the quick pace of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.}
}
@article{KALYAN2024100048,
title = {A survey of GPT-3 family large language models including ChatGPT and GPT-4},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100048},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2949719123000456},
author = {Katikapalli Subramanyam Kalyan},
keywords = {Large language models, LLMs, GPT-3, ChatGPT, GPT-4, Transformers, LLM survey},
abstract = {Large language models (LLMs) are a special class of pretrained language models (PLMs) obtained by scaling model size, pretraining corpus and computation. LLMs, because of their large size and pretraining on large volumes of text data, exhibit special abilities which allow them to achieve remarkable performances without any task-specific training in many of the natural language processing tasks. The era of LLMs started with OpenAI’s GPT-3 model, and the popularity of LLMs has increased exponentially after the introduction of models like ChatGPT and GPT4. We refer to GPT-3 and its successor OpenAI models, including ChatGPT and GPT4, as GPT-3 family large language models (GLLMs). With the ever-rising popularity of GLLMs, especially in the research community, there is a strong need for a comprehensive survey which summarizes the recent research progress in multiple dimensions and can guide the research community with insightful future research directions. We start the survey paper with foundation concepts like transformers, transfer learning, self-supervised learning, pretrained language models and large language models. We then present a brief overview of GLLMs and discuss the performances of GLLMs in various downstream tasks, specific domains and multiple languages. We also discuss the data labelling and data augmentation abilities of GLLMs, the robustness of GLLMs, the effectiveness of GLLMs as evaluators, and finally, conclude with multiple insightful future research directions. To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GLLMs.}
}
@article{MERONOPENUELA2025100847,
title = {KG.GOV: Knowledge graphs as the backbone of data governance in AI},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100847},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100847},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000337},
author = {Albert Meroño-Peñuela and Elena Simperl and Anelia Kurteva and Ioannis Reklos},
keywords = {Knowledge graphs, AI, Governance},
abstract = {As (generative) Artificial Intelligence continues to evolve, so do the challenges associated with governing the data that powers it. Ensuring data quality, privacy, security, and ethical use become more and more challenging due to the increasing volume and variety of the data, the complexity of AI models, and the rapid pace of technological advancement. Knowledge graphs have the potential to play a significant role in enabling data governance in AI, as we move beyond their traditional use as data organisational systems. To address this, we present KG.gov, a framework that positions KGs at a higher abstraction level within AI workflows, and enables them as a backbone of AI data governance. We illustrate the three dimensions of KG.gov: modelling data, alternative representations, and describing behaviour; and describe the insights and challenges of three use cases implementing them: Croissant, a vocabulary to model and document ML datasets; WikiPrompts, a collaborative KG of prompts and prompt workflows to study their behaviour at scale; and Multimodal transformations, an approach for multimodal KGs harmonisation and completion aiming at broadening access to knowledge.}
}
@article{GU2024102822,
title = {Automatic quantitative stroke severity assessment based on Chinese clinical named entity recognition with domain-adaptive pre-trained large language model},
journal = {Artificial Intelligence in Medicine},
volume = {150},
pages = {102822},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102822},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000642},
author = {Zhanzhong Gu and Xiangjian He and Ping Yu and Wenjing Jia and Xiguang Yang and Gang Peng and Penghui Hu and Shiyan Chen and Hongjie Chen and Yiguang Lin},
keywords = {Automatic stroke severity assessment, Chinese electronic health records, Clinical named entity recognition, Domain-adaptive pre-training, Large language model},
abstract = {Background:
Stroke is a prevalent disease with a significant global impact. Effective assessment of stroke severity is vital for an accurate diagnosis, appropriate treatment, and optimal clinical outcomes. The National Institutes of Health Stroke Scale (NIHSS) is a widely used scale for quantitatively assessing stroke severity. However, the current manual scoring of NIHSS is labor-intensive, time-consuming, and sometimes unreliable. Applying artificial intelligence (AI) techniques to automate the quantitative assessment of stroke on vast amounts of electronic health records (EHRs) has attracted much interest.
Objective:
This study aims to develop an automatic, quantitative stroke severity assessment framework through automating the entire NIHSS scoring process on Chinese clinical EHRs.
Methods:
Our approach consists of two major parts: Chinese clinical named entity recognition (CNER) with a domain-adaptive pre-trained large language model (LLM) and automated NIHSS scoring. To build a high-performing CNER model, we first construct a stroke-specific, densely annotated dataset “Chinese Stroke Clinical Records” (CSCR) from EHRs provided by our partner hospital, based on a stroke ontology that defines semantically related entities for stroke assessment. We then pre-train a Chinese clinical LLM coined “CliRoberta” through domain-adaptive transfer learning and construct a deep learning-based CNER model that can accurately extract entities directly from Chinese EHRs. Finally, an automated, end-to-end NIHSS scoring pipeline is proposed by mapping the extracted entities to relevant NIHSS items and values, to quantitatively assess the stroke severity.
Results:
Results obtained on a benchmark dataset CCKS2019 and our newly created CSCR dataset demonstrate the superior performance of our domain-adaptive pre-trained LLM and the CNER model, compared with the existing benchmark LLMs and CNER models. The high F1 score of 0.990 ensures the reliability of our model in accurately extracting the entities for the subsequent automatic NIHSS scoring. Subsequently, our automated, end-to-end NIHSS scoring approach achieved excellent inter-rater agreement (0.823) and intraclass consistency (0.986) with the ground truth and significantly reduced the processing time from minutes to a few seconds.
Conclusion:
Our proposed automatic and quantitative framework for assessing stroke severity demonstrates exceptional performance and reliability through directly scoring the NIHSS from diagnostic notes in Chinese clinical EHRs. Moreover, this study also contributes a new clinical dataset, a pre-trained clinical LLM, and an effective deep learning-based CNER model. The deployment of these advanced algorithms can improve the accuracy and efficiency of clinical assessment, and help improve the quality, affordability and productivity of healthcare services.}
}
@article{YI2025128684,
title = {Fine-grained detoxification framework via instance-level prefixes for large language models},
journal = {Neurocomputing},
volume = {611},
pages = {128684},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128684},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014553},
author = {Xin Yi and Linlin Wang and Xiaoling Wang and Liang He},
keywords = {Large language model, Detoxification framework, Safety and security},
abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks. However, their practical usability is often compromised by a propensity to generate toxic content, such as insults, threats, and profanity, particularly in response to adversarial prompts. Several fine-tuning and decoding approaches have been employed to address this challenge to mitigate toxicity. Nevertheless, these methods typically necessitate additional resources, such as high-quality training data or auxiliary models, thereby incurring extra costs. In this paper, we propose a novel detoxification framework, Fine-Grained Detoxification via Instance-Level Prefixes (FGDILP), which effectively mitigates the generation of toxic text without incurring additional training costs. Specifically, FGDILP leverages contextualized representations in attention space by contrasting a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This methodology facilitates the construction of fine-grained subtoxicity vectors, which are subsequently fused to adjust the original generation pathway when responding to raw prompts. Our results demonstrate that FGDILP enables controlled text generation concerning detoxification at both the utterance and context levels. While our method slightly impacts generation fluency and diversity, it significantly outperforms prompt-based baselines regarding detoxification effectiveness. Our code is available at https://github.com/xinykou/FGDILP.}
}
@article{JAHAN2024108189,
title = {A comprehensive evaluation of large Language models on benchmark biomedical text processing tasks},
journal = {Computers in Biology and Medicine},
volume = {171},
pages = {108189},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108189},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524002737},
author = {Israt Jahan and Md Tahmid Rahman Laskar and Chun Peng and Jimmy Xiangji Huang},
keywords = {Large language models, ChatGPT, PaLM, LLaMA, Claude, Transformer, Natural language processing, LLM evaluation},
abstract = {Recently, Large Language Models (LLMs) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets has been conducted. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art models when they were fine-tuned only on the training set of these datasets. This suggests that pre-training on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task. While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.}
}
@article{LI2025104789,
title = {Improving entity recognition using ensembles of deep learning and fine-tuned large language models: A case study on adverse event extraction from VAERS and social media},
journal = {Journal of Biomedical Informatics},
pages = {104789},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104789},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000188},
author = {Yiming Li and Deepthi Viswaroopan and William He and Jianfu Li and Xu Zuo and Hua Xu and Cui Tao},
keywords = {Named-entity recognition, VAERS, Generative pre-trained transformer (GPT), Large language model (LLM), Social media, Deep learning},
abstract = {Objective
Adverse event (AE) extraction following COVID-19 vaccines from text data is crucial for monitoring and analyzing the safety profiles of immunizations, identifying potential risks and ensuring the safe use of these products. Traditional deep learning models are adept at learning intricate feature representations and dependencies in sequential data, but often require extensive labeled data. In contrast, large language models (LLMs) excel in understanding contextual information, but exhibit unstable performance on named entity recognition (NER) tasks, possibly due to their broad but unspecific training. This study aims to evaluate the effectiveness of LLMs and traditional deep learning models in AE extraction, and to assess the impact of ensembling these models on performance.
Methods
In this study, we utilized reports and posts from the Vaccine Adverse Event Reporting System (VAERS) (n = 230), Twitter (n = 3,383), and Reddit (n = 49) as our corpora. Our goal was to extract three types of entities: vaccine, shot, and adverse event (ae). We explored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5, GPT-4, Llama-2 7b, and Llama-2 13b, as well as traditional deep learning models like Recurrent neural network (RNN) and Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT). To enhance performance, we created ensembles of the three models with the best performance. For evaluation, we used strict and relaxed F1 scores to evaluate the performance for each entity type, and micro-average F1 was used to assess the overall performance.
Results
The ensemble demonstrated the best performance in identifying the entities “vaccine,” “shot,” and “ae,” achieving strict F1-scores of 0.878, 0.930, and 0.925, respectively, and a micro-average score of 0.903. These results underscore the significance of fine-tuning models for specific tasks and demonstrate the effectiveness of ensemble methods in enhancing performance.
Conclusion
In conclusion, this study demonstrates the effectiveness and robustness of ensembling fine-tuned traditional deep learning models and LLMs, for extracting AE-related information following COVID-19 vaccination. This study contributes to the advancement of natural language processing in the biomedical domain, providing valuable insights into improving AE extraction from text data for pharmacovigilance and public health surveillance}
}
@article{CONG2025101700,
title = {Demystifying large language models in second language development research},
journal = {Computer Speech & Language},
volume = {89},
pages = {101700},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101700},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000834},
author = {Yan Cong},
keywords = {Large language models, Natural language processing, Automatic essay scoring, L2 writing development, L2 interlanguage, Bilingualism},
abstract = {Evaluating students' textual response is a common and critical task in language research and education practice. However, manual assessment can be tedious and may lack consistency, posing challenges for both scientific discovery and frontline teaching. Leveraging state-of-the-art large language models (LLMs), we aim to define and operationalize LLM-Surprisal, a numeric representation of the interplay between lexical diversity and syntactic complexity, and to empirically and theoretically demonstrate its relevance for automatic writing assessment and Chinese L2 (second language) learners’ English writing development. We developed an LLM-based natural language processing pipeline that can automatically compute text Surprisal scores. By comparing Surprisal metrics with the widely used classic indices in L2 studies, we extended the usage of computational metrics in Chinese learners’ L2 English writing. Our analyses suggested that LLM-Surprisals can distinguish L2 from L1 (first language) writing, index L2 development stages, and predict scores provided by human professionals. This indicated that the Surprisal dimension may manifest itself as critical aspects in L2 development. The relative advantages and disadvantages of these approaches were discussed in depth. We concluded that LLMs are promising tools that can enhance L2 research. Our showcase paves the way for more nuanced approaches to computationally assessing and understanding L2 development. Our pipelines and findings will inspire language teachers, learners, and researchers to operationalize LLMs in an innovative and accessible manner.}
}
@article{KONSTANTINOU2024621,
title = {Leveraging Generative AI Prompt Programming for Human-Robot Collaborative Assembly},
journal = {Procedia CIRP},
volume = {128},
pages = {621-626},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.03.040},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007510},
author = {Christos Konstantinou and Dimitris Antonarakos and Panagiotis Angelakis and Christos Gkournelos and George Michalos and Sotiris Makris},
keywords = {Generative AI, Human Robot collaboration, Design informatics},
abstract = {In manufacturing, traditional robotic programming methodologies have often been focused on independent operation, offering limited capabilities for seamless human-robot collaboration. This paper introduces a paradigm shift in collaborative production systems by leveraging generative artificial intelligence (AI), specifically large language models (LLMs). Contrary to traditional methods that rely on pre-defined assembly instructions, this paper introduces a novel framework employing primitive knowledge of the production process, including product design and required assembly steps. By integrating LLMs and a behavior tree-based system control, this approach enables programmers to rapidly deploy collaborative assembly procedures by expediting the programming of robotic operations. The system also incorporates Natural Language Processing (NLP) technologies, which facilitate real-time alterations in assembly steps, leading to reduced overall production time. The framework’s behavior tree-based control architecture allows for dynamic adaptability, offering optimized solutions across a range of assembly scenarios. The results of the framework’s deployment suggest that this innovative programming paradigm significantly enhances both the adaptability and efficiency of collaborative manufacturing settings.}
}
@article{YANG2024100085,
title = {Understanding natural language: Potential application of large language models to ophthalmology},
journal = {Asia-Pacific Journal of Ophthalmology},
volume = {13},
number = {4},
pages = {100085},
year = {2024},
issn = {2162-0989},
doi = {https://doi.org/10.1016/j.apjo.2024.100085},
url = {https://www.sciencedirect.com/science/article/pii/S2162098924000860},
author = {Zefeng Yang and Deming Wang and Fengqi Zhou and Diping Song and Yinhang Zhang and Jiaxuan Jiang and Kangjie Kong and Xiaoyi Liu and Yu Qiao and Robert T. Chang and Ying Han and Fei Li and Clement C. Tham and Xiulan Zhang},
keywords = {Large language model, Ophthalmology, Artificial intelligence, ChatGPT},
abstract = {Large language models (LLMs), a natural language processing technology based on deep learning, are currently in the spotlight. These models closely mimic natural language comprehension and generation. Their evolution has undergone several waves of innovation similar to convolutional neural networks. The transformer architecture advancement in generative artificial intelligence marks a monumental leap beyond early-stage pattern recognition via supervised learning. With the expansion of parameters and training data (terabytes), LLMs unveil remarkable human interactivity, encompassing capabilities such as memory retention and comprehension. These advances make LLMs particularly well-suited for roles in healthcare communication between medical practitioners and patients. In this comprehensive review, we discuss the trajectory of LLMs and their potential implications for clinicians and patients. For clinicians, LLMs can be used for automated medical documentation, and given better inputs and extensive validation, LLMs may be able to autonomously diagnose and treat in the future. For patient care, LLMs can be used for triage suggestions, summarization of medical documents, explanation of a patient’s condition, and customizing patient education materials tailored to their comprehension level. The limitations of LLMs and possible solutions for real-world use are also presented. Given the rapid advancements in this area, this review attempts to briefly cover many roles that LLMs may play in the ophthalmic space, with a focus on improving the quality of healthcare delivery.}
}
@article{LEI20241257,
title = {Materials science in the era of large language models: a perspective††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00074a},
journal = {Digital Discovery},
volume = {3},
number = {7},
pages = {1257-1272},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00074a},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001190},
author = {Ge Lei and Ronan Docherty and Samuel J. Cooper},
abstract = {Large Language Models (LLMs) have garnered considerable interest due to their impressive natural language capabilities, which in conjunction with various emergent properties make them versatile tools in workflows ranging from complex code generation to heuristic finding for combinatorial problems. In this paper we offer a perspective on their applicability to materials science research, arguing their ability to handle ambiguous requirements across a range of tasks and disciplines means they could be a powerful tool to aid researchers. We qualitatively examine basic LLM theory, connecting it to relevant properties and techniques in the literature before providing two case studies that demonstrate their use in task automation and knowledge extraction at-scale. At their current stage of development, we argue LLMs should be viewed less as oracles of novel insight, and more as tireless workers that can accelerate and unify exploration across domains. It is our hope that this paper can familiarise materials science researchers with the concepts needed to leverage these tools in their own research.}
}
@article{DAI2025106956,
title = {Large Language Model Enhanced Logic Tensor Network for Stance Detection},
journal = {Neural Networks},
volume = {183},
pages = {106956},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106956},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024008852},
author = {Genan Dai and Jiayu Liao and Sicheng Zhao and Xianghua Fu and Xiaojiang Peng and Hu Huang and Bowen Zhang},
keywords = {Logic tensor network, Stance detection, Chain-of-thought},
abstract = {Social media platforms, rich in user-generated content, offer a unique perspective on public opinion, making stance detection an essential task in opinion mining. However, traditional deep neural networks for stance detection often suffer from limitations, including the requirement for large amounts of labeled data, uninterpretability of prediction results, and difficulty in incorporating human intentions and domain knowledge. This paper introduces the First-Order Logic Aggregated Reasoning framework (FOLAR), an innovative approach that integrates first-order logic (FOL) with large language models (LLMs) to enhance the interpretability and efficacy of stance detection. FOLAR comprises three key components: a Knowledge Elicitation module that generates FOL rules using a chain-of-thought prompting method, a Logic Tensor Network (LTN) that encodes these rules for stance detection, and a Multi-Decision Fusion mechanism that aggregates LTNs’ outputs to minimize biases and improve robustness. Our experiments on standard benchmarks demonstrate the effectiveness of FOLAR, showing it as a promising solution for explainable and accurate stance detection. The source code will be made publicly available to foster further research.}
}
@article{LIU2025103920,
title = {EvoPath: Evolutionary meta-path discovery with large language models for complex heterogeneous information networks},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103920},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103920},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002796},
author = {Shixuan Liu and Haoxiang Cheng and Yunfei Wang and Yue He and Changjun Fan and Zhong Liu},
keywords = {Meta-path discovery, Large language models, Heterogeneous information networks},
abstract = {Heterogeneous Information Networks (HINs) encapsulate diverse entity and relation types, with meta-paths providing essential meta-level semantics for knowledge reasoning, although their utility is constrained by discovery challenges. While Large Language Models (LLMs) offer new prospects for meta-path discovery due to their extensive knowledge encoding and efficiency, their adaptation faces challenges such as corpora bias, lexical discrepancies, and hallucination. This paper pioneers the mitigation of these challenges by presenting EvoPath, an innovative framework that leverages LLMs to efficiently identify high-quality meta-paths. EvoPath is carefully designed, with each component aimed at addressing issues that could lead to potential knowledge conflicts. With a minimal subset of HIN facts, EvoPath iteratively generates and evolves meta-paths by dynamically replaying meta-paths in the buffer with prioritization based on their scores. Comprehensive experiments on three large, complex HINs with hundreds of relations demonstrate that our framework, EvoPath, enables LLMs to generate high-quality meta-paths through effective prompting, confirming its superior performance in HIN reasoning tasks. Further ablation studies validate the effectiveness of each module within the framework.}
}
@article{WONG2024104082,
title = {Construction contract risk identification based on knowledge-augmented language models},
journal = {Computers in Industry},
volume = {157-158},
pages = {104082},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104082},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000101},
author = {Saika Wong and Chunmo Zheng and Xing Su and Yinqiu Tang},
keywords = {Large language models, Construction contract risk, Knowledge augmentation, Knowledge database},
abstract = {Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. Although large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how LLMs employ logical thinking during the task and provided insights and recommendations for future research.}
}
@article{BULLA2025100609,
title = {Large Language Models meet moral values: A comprehensive assessment of moral abilities},
journal = {Computers in Human Behavior Reports},
pages = {100609},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100609},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000247},
author = {Luana Bulla and Stefano {De Giorgis} and Misael Mongiovì and Aldo Gangemi},
keywords = {Large Language Models, Value detection, Natural language inference, Natural Language Processing},
abstract = {Automatic moral classification in textual data is crucial for various fields including Natural Language Processing (NLP), social sciences, and ethical AI development. Despite advancements in supervised models, their performance often suffers when faced with real-world scenarios due to overfitting to specific data distributions. To address these limitations, we propose leveraging state-of-the-art Large Language Models (LLMs) trained on extensive common-sense data for unsupervised moral classification. We introduce an innovative evaluation framework that directly compares model outputs with human annotations, ensuring an assessment of model performance. Our methodology explores the effectiveness of different LLM sizes and prompt designs in moral value detection tasks, considering both multi-label and binary classification scenarios. We present experimental results using the Moral Foundation Reddit Corpus (MFRC) and discuss implications for future research in ethical AI development and human–computer interaction. Experimental results demonstrate that GPT-4 achieves superior performance, followed by GPT-3.5, Llama-70B, Mixtral-8x7B, Mistral-7B and Llama-7B. Additionally, the study reveals significant variations in model performance across different moral domains, particularly between everyday morality and political contexts. Our work provides meaningful insights into the use of zero-shot and few-shot models for moral value detection and discusses the potential and limitations of current technology in this task.}
}
@article{GOPFERT2024100383,
title = {Opportunities for large language models and discourse in engineering design},
journal = {Energy and AI},
volume = {17},
pages = {100383},
year = {2024},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2024.100383},
url = {https://www.sciencedirect.com/science/article/pii/S2666546824000491},
author = {Jan Göpfert and Jann M. Weinand and Patrick Kuckertz and Detlef Stolten},
keywords = {Product development process, Conceptual design, Design methodology, Design generation, Natural language processing, Foundation models, Multi-modal models},
abstract = {In recent years, large language models have achieved breakthroughs on a wide range of benchmarks in natural language processing and continue to increase in performance. Recently, the advances of large language models have raised interest outside the natural language processing community and could have a large impact on daily life. In this paper, we pose the question: How will large language models and other foundation models shape the future product development process? We provide the reader with an overview of the subject by summarizing both recent advances in natural language processing and the use of information technology in the engineering design process. We argue that discourse should be regarded as the core of engineering design processes, and therefore should be represented in a digital artifact. On this basis, we describe how foundation models such as large language models could contribute to the design discourse by automating parts thereof that involve creativity and reasoning, and were previously reserved for humans. We describe how simulations, experiments, topology optimizations, and other process steps can be integrated into a machine-actionable, discourse-centric design process. As an example, we present a design discourse on the optimization of wind turbine blades. Finally, we outline the future research that will be necessary for the implementation of the conceptualized framework.}
}
@article{LAI2024181,
title = {Large language models in law: A survey},
journal = {AI Open},
volume = {5},
pages = {181-196},
year = {2024},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2024.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651024000172},
author = {Jinqi Lai and Wensheng Gan and Jiayang Wu and Zhenlian Qi and Philip S. Yu},
keywords = {Artificial intelligence, LLMs, Justice, Legal model},
abstract = {The advent of artificial intelligence (AI) has significantly impacted the traditional judicial industry. Moreover, recently, with the development of AI-generated content (AIGC), AI and law have found applications in various domains, including image recognition, automatic text generation, and interactive chat. With the rapid emergence and growing popularity of large models, it is evident that AI will drive transformation in the traditional judicial industry. However, the application of legal large language models (LLMs) is still in its nascent stage. Several challenges need to be addressed. In this paper, we aim to provide a comprehensive survey of legal LLMs. We not only conduct an extensive survey of LLMs but also expose their applications in the judicial system. We first provide an overview of AI technologies in the legal field and showcase the recent research in LLMs. Then, we discuss the practical implementations presented by legal LLMs, such as providing legal advice to users and assisting judges during trials. In addition, we explore the limitations of legal LLMs, including data, algorithms, and judicial practice. Finally, we summarize practical recommendations and propose future development directions to address these challenges.}
}
@article{FENG2024117540,
title = {Large language models for biomolecular analysis: From methods to applications},
journal = {TrAC Trends in Analytical Chemistry},
volume = {171},
pages = {117540},
year = {2024},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.117540},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624000220},
author = {Ruijun Feng and Chi Zhang and Yang Zhang},
keywords = {Large language model, Biomolecular analysis, Fine-tuning, Prompt engineering, Parameter-efficient fine-tuning, In-context learning, Protein structure analysis, Protein sequence generation, Gene sequence analysis, Molecular representation learning},
abstract = {Large language models (LLMs) are proving to be very useful in many fields, especially chemistry and biology, because of their amazing capabilities. Biomolecular data is often represented sequentially, much like textual data used to train LLMs. However, developing LLMs from scratch requires a substantial amount of data and computational resources, which may not be feasible for most researchers. A more workable solution to this problem is to change the inputs or parameters so that the previously trained general LLMs can pick up the specific knowledge needed for biomolecular analysis. These adaption strategies lower the amount of data and hardware needed, providing a more affordable option. This review provides the introduction of two popular LLM adaptation techniques: fine-tuning and prompt engineering, along with their uses in the analysis of molecules, proteins, and genes. A thorough overview of current common datasets and pre-trained models is also provided. This review outlines the possible advantages and difficulties of LLMs for biomolecular analysis, opening the door for chemists and biologists to effectively utilize LLMs in their future studies.}
}
@article{MOCANU2023100036,
title = {Knowledge representation and acquisition in the era of large language models: Reflections on learning to reason via PAC-Semantics},
journal = {Natural Language Processing Journal},
volume = {5},
pages = {100036},
year = {2023},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2023.100036},
url = {https://www.sciencedirect.com/science/article/pii/S294971912300033X},
author = {Ionela G. Mocanu and Vaishak Belle},
keywords = {Pac-semantics, Logical knowledge bases, Knowledge acquisition},
abstract = {Human beings are known for their remarkable ability to comprehend, analyse, and interpret common sense knowledge. This ability is critical for exhibiting intelligent behaviour, often defined as a mapping from beliefs to actions, which has led to attempts to formalize and capture explicit representations in the form of databases, knowledge bases, and ontologies in AI agents. But in the era of large language models (LLMs), this emphasis might seem unnecessary. After all, these models already capture the extent of human knowledge and can infer appropriate things from it (presumably) as per some innate logical rules. The question then is whether they can also be trained to perform mathematical computations. Although the consensus on the reliability of such models is still being studied, early results do seem to suggest they do not offer logically and mathematically consistent results. In this short summary article, we articulate the motivations for still caring about logical/symbolic artefacts and representations, and report on recent progress in learning to reason via the so-called probably approximately correct (PAC)-semantics.}
}
@article{WU2024429,
title = {ProcessCarbonAgent: A large language models-empowered autonomous agent for decision-making in manufacturing carbon emission management},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {429-442},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001729},
author = {Tao Wu and Jie Li and Jinsong Bao and Qiang Liu},
keywords = {Knowledge-intensive production, Large language models, Autonomous agents, Assisted decision-making, Process carbon agent, Carbon emission management},
abstract = {Knowledge-intensive production represents a primary trend in industrial manufacturing, which heavily relies on the production logs of large-scale, historically similar orders for enhancing production efficiency and process quality. These logs are essential for predicting resource allocation and identifying bottlenecks in throughput. As a result, root cause analysis of the production process state is crucial for supporting decision-making in these settings. However, current methodologies heavily depend on expert knowledge, making the analysis time-consuming and inefficient for large-scale, multivariable processes. Although the development of large language models and autonomous agents presents a potential solution, these models are limited in their direct interaction with event logs due to inadequate data representation, token constraints, and insufficient accuracy. Therefore, enabling the interactive capabilities of large language models to overcome these specific limitations in process event data and industrial domain illusions poses a significant challenge. To address these issues, this paper introduces the ProcessCarbonAgent framework, an autonomous agent empowered by large language models, designed to enhance decision-making within industrial processes. Initially, a process data agent combines predefined semantic text representation methods with process template prompting strategies to improve interaction capabilities. Subsequently, an intention agent utilizing self-information and large language models is developed to address context length limitations by identifying and eliminating redundancies. Finally, a two-stage confidence estimation method is implemented to refine the precision of decision-making assistance, thereby improving the accuracy of decisions supported by large language models. Experiments with textile industry carbon emission data reveal that the assisted decision-making scores employing a compression ratio of 0.5, closely align with scores from manually labeled evaluations, with a 98% overlap across scoring intervals. Moreover, in contrast to relying solely on the original evaluation method, the two-stage confidence estimation method has led to a 20% increase in accuracy performance. The ProcessCarbonAgent achieved scores of 16.64, 55.13, 26.32, and 34.17 on METEOR, BERTScore, NUBIA, and BLEURT, respectively. The results demonstrate that the ProcessCarbonAgent framework significantly enhances the decision-making process for high-carbon emission states in industrial production, providing technical support for the low-carbon transformation and intelligent upgrading of these processes.}
}
@article{RAY2024174,
title = {Large language models in laparoscopic surgery: A transformative opportunity},
journal = {Laparoscopic, Endoscopic and Robotic Surgery},
volume = {7},
number = {4},
pages = {174-180},
year = {2024},
issn = {2468-9009},
doi = {https://doi.org/10.1016/j.lers.2024.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2468900924000483},
author = {Partha Pratim Ray},
keywords = {Large language model, Artificial intelligence, Generative artificial intelligence, Laparoscopy, Surgery},
abstract = {This opinion paper explores the transformative potential of large language models (LLMs) in laparoscopic surgery and argues for their integration to enhance surgical education, decision support, reporting, and patient care. LLMs can revolutionize surgical education by providing personalized learning experiences and accelerating skill acquisition. Intelligent decision support systems powered by LLMs can assist surgeons in making complex decisions, optimizing surgical workflows, and improving patient outcomes. Moreover, LLMs can automate surgical reporting and generate personalized patient education materials, streamlining documentation and improving patient engagement. However, challenges such as data scarcity, surgical semantic capture, real-time inference, and integration with existing systems need to be addressed for successful LLM integration. The future of laparoscopic surgery lies in the seamless integration of LLMs, enabling autonomous robotic surgery, predictive surgical planning, intraoperative decision support, virtual surgical assistants, and continuous learning. By harnessing the power of LLMs, laparoscopic surgery can be transformed, empowering surgeons and ultimately benefiting patients.}
}
@article{HUANG2024109100,
title = {Knowledge graph based reasoning in medical image analysis: A scoping review},
journal = {Computers in Biology and Medicine},
volume = {182},
pages = {109100},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109100},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524011855},
author = {Qinghua Huang and Guanghui Li},
keywords = {Medical diagnosis, Medical expert systems, Knowledge graph, Medical image analysis},
abstract = {Automated computer-aided diagnosis (CAD) is becoming more significant in the field of medicine due to advancements in computer hardware performance and the progress of artificial intelligence. The knowledge graph is a structure for visually representing knowledge facts. In the last decade, a large body of work based on knowledge graphs has effectively improved the organization and interpretability of large-scale complex knowledge. Introducing knowledge graph inference into CAD is a research direction with significant potential. In this review, we briefly review the basic principles and application methods of knowledge graphs firstly. Then, we systematically organize and analyze the research and application of knowledge graphs in medical imaging-assisted diagnosis. We also summarize the shortcomings of the current research, such as medical data barriers and deficiencies, low utilization of multimodal information, and weak interpretability. Finally, we propose future research directions with possibilities and potentials to address the shortcomings of current approaches.}
}
@article{SUN2025103135,
title = {Enhancing multimodal-input object goal navigation by leveraging large language models for inferring room–object relationship knowledge},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103135},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103135},
url = {https://www.sciencedirect.com/science/article/pii/S147403462500028X},
author = {Leyuan Sun and Asako Kanezaki and Guillaume Caron and Yusuke Yoshiyasu},
keywords = {Object-goal navigation, Large language model, Multimodal fusion, Multitask learning, Room segmentation, Sim2real transfer},
abstract = {Object-goal navigation is a task in embodied AI where an agent is navigated to a specified object within unfamiliar indoor scenarios. This task is crucial for engineering activities such as training agents in 3D simulated environments and deploying these models in actual mobile robots. Extensive research has been conducted to develop various navigation methods, including end-to-end reinforcement learning and modular map-based approaches. However, fully enabling an agent to perceive and understand the environment, and to navigate towards a target object as efficiently as humans, remains a considerable challenge. In this study, we introduce a data-driven and modular map-based approach, trained on a dataset incorporated with common-sense knowledge of object-to-room relationships extracted from a Large Language Model (LLM), aiming to enhance the efficiency of object-goal navigation. This approach enables the agent to seek the target object in rooms where it is commonly found (e.g., a bed in a bedroom, a couch in a living room), according to LLM-based common-sense knowledge. Additionally, we employ the multi-channel Swin-Unet architecture for multi-task learning, integrating multimodal sensory inputs to effectively extract meaningful features for spatial comprehension and navigation. Results from the Habitat simulator show that our framework surpasses the baseline by an average of 10.6% in the Success-weighted by Path Length (SPL) efficiency metric. Real-world demonstrations confirm that our method can effectively navigate multiple rooms in the object-goal navigation task. For further details and real-world demonstrations, please visit our project webpage (https://sunleyuan.github.io/ObjectNav).}
}
@article{LEE2024,
title = {Unlocking the Secrets Behind Advanced Artificial Intelligence Language Models in Deidentifying Chinese-English Mixed Clinical Text: Development and Validation Study},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/48443},
url = {https://www.sciencedirect.com/science/article/pii/S143888712400030X},
author = {You-Qian Lee and Ching-Tai Chen and Chien-Chang Chen and Chung-Hong Lee and Peitsz Chen and Chi-Shin Wu and Hong-Jie Dai},
keywords = {code mixing, electronic health record, deidentification, pretrained language model, large language model, ChatGPT},
abstract = {Background
The widespread use of electronic health records in the clinical and biomedical fields makes the removal of protected health information (PHI) essential to maintain privacy. However, a significant portion of information is recorded in unstructured textual forms, posing a challenge for deidentification. In multilingual countries, medical records could be written in a mixture of more than one language, referred to as code mixing. Most current clinical natural language processing techniques are designed for monolingual text, and there is a need to address the deidentification of code-mixed text.
Objective
The aim of this study was to investigate the effectiveness and underlying mechanism of fine-tuned pretrained language models (PLMs) in identifying PHI in the code-mixed context. Additionally, we aimed to evaluate the potential of prompting large language models (LLMs) for recognizing PHI in a zero-shot manner.
Methods
We compiled the first clinical code-mixed deidentification data set consisting of text written in Chinese and English. We explored the effectiveness of fine-tuned PLMs for recognizing PHI in code-mixed content, with a focus on whether PLMs exploit naming regularity and mention coverage to achieve superior performance, by probing the developed models’ outputs to examine their decision-making process. Furthermore, we investigated the potential of prompt-based in-context learning of LLMs for recognizing PHI in code-mixed text.
Results
The developed methods were evaluated on a code-mixed deidentification corpus of 1700 discharge summaries. We observed that different PHI types had preferences in their occurrences within the different types of language-mixed sentences, and PLMs could effectively recognize PHI by exploiting the learned name regularity. However, the models may exhibit suboptimal results when regularity is weak or mentions contain unknown words that the representations cannot generate well. We also found that the availability of code-mixed training instances is essential for the model’s performance. Furthermore, the LLM-based deidentification method was a feasible and appealing approach that can be controlled and enhanced through natural language prompts.
Conclusions
The study contributes to understanding the underlying mechanism of PLMs in addressing the deidentification process in the code-mixed context and highlights the significance of incorporating code-mixed training instances into the model training phase. To support the advancement of research, we created a manipulated subset of the resynthesized data set available for research purposes. Based on the compiled data set, we found that the LLM-based deidentification method is a feasible approach, but carefully crafted prompts are essential to avoid unwanted output. However, the use of such methods in the hospital setting requires careful consideration of data security and privacy concerns. Further research could explore the augmentation of PLMs and LLMs with external knowledge to improve their strength in recognizing rare PHI.}
}
@article{ZHANG2025115001,
title = {Data-driven building load prediction and large language models: Comprehensive overview},
journal = {Energy and Buildings},
volume = {326},
pages = {115001},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115001},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011174},
author = {Yake Zhang and Dijun Wang and Guansong Wang and Peng Xu and Yihao Zhu},
keywords = {Data-driven approach, Building load prediction, Machine learning, Large language models, Feature engineering, Data preparation, Room-scale load prediction, Retrieval augmented generation},
abstract = {Building load forecasting is essential for optimizing the architectural design and managing energy efficiently, enhancing the performance of Heating, Ventilation, and Air Conditioning systems, and enhancing occupant comfort. With advancements in data science and machine learning, the focus on predicting building loads through data analysis has significantly intensified as a research domain. However, previous studies have typically faced challenges such as data scarcity, improper feature extraction methods, and weak model generalization capabilities. To gain a deeper understanding of these issues, a comprehensive review of data processing, feature selection, and model selection methods in previous research is conducted from the perspective of the entire load forecasting process. The aim is to identify the most suitable methods for each step of load forecasting to enhance prediction accuracy. This review surveys the research progress of statistical learning methods, traditional machine learning methods, deep learning methods, and hybrid methods in different application scenarios of building load prediction. Then, it emphasized the critical role of data preprocessing and focused on techniques like data fusion and transfer learning to overcome data shortages and bolster the models’ ability to generalize. Moreover, the obtainment of significant features from building characteristics, weather data, and operational statistics to boost prediction accuracy is explored. A notable contribution of this review is the proposed technical framework for EnergyPlus model generation using LLM-based Retrieval Augmented Generation (RAG) technology and room- level load prediction with Spatio-Temporal Graph Neural Networks. This framework utilize architectural design drawings to achieve an “end-to-end” prediction process, aiming to reduce the professional threshold of load prediction and provide technical support for fine-grained regulation of building operation. Exploratory experiment is conducted using a single-zone building model to verify the feasibility of LLM-generated EnergyPlus models, with IDF simulation file generation taking only 196 s. Room-level load forecasting with LLMs remains to be explored further. It is reasonable to believe that the methods proposed in this review hold promise for advancing data-driven building load forecasting technologies.}
}
@incollection{KARKERA2024,
title = {Large Language Models for Pathway Curation: A Preliminary Investigation},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00254-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002542},
author = {Nikitha Karkera and Nikshita Karkera and Mahanash Kumar and Vishnuvardhan P. Srinivasulu and Samik Ghosh and Sucheendra K. Palaniappan},
keywords = {Generative AI, Gpt3.5, Gpt4, LLM, Pathway curation},
abstract = {The pathway curation task involves analyzing scientific literature to identify and represent cellular processes as pathways. This process, often time-consuming and labor-intensive, requires significant curation efforts amidst the rapidly growing biomedical literature. Natural Language Processing (NLP) offers a promising method to automatically extract these interactions from scientific texts. Despite immense progress, there remains room for improvement in these systems. The emergence of Large Language Models (LLMs) provides a promising solution for this challenge. Our study conducts a preliminary investigation into leveraging LLMs for the pathway curation task. This paper first presents a review of the current state-of-the-art algorithms for the pathway curation task. Our objective is to check the feasibility and formulate strategies of using these LLMs to improve the accuracy of pathway curation task. Our experiments demonstrate that our GPT-3.5 based fine-tuned models outperforms existing state-of-the-art methods. Specifically, our model achieved a 10 basis point improvement in overall recall and F1 score compared to the best existing algorithms. These findings highlight the potential of LLMs in pathway curation tasks, warranting further research and substantial efforts in this direction.}
}
@article{SU202412200,
title = {Automation and machine learning augmented by large language models in a catalysis study},
journal = {Chemical Science},
volume = {15},
number = {31},
pages = {12200-12233},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d3sc07012c},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024010794},
author = {Yuming Su and Xue Wang and Yuanxiang Ye and Yibo Xie and Yujing Xu and Yibin Jiang and Cheng Wang},
abstract = {Recent advancements in artificial intelligence and automation are transforming catalyst discovery and design from traditional trial-and-error manual mode into intelligent, high-throughput digital methodologies. This transformation is driven by four key components, including high-throughput information extraction, automated robotic experimentation, real-time feedback for iterative optimization, and interpretable machine learning for generating new knowledge. These innovations have given rise to the development of self-driving labs and significantly accelerated materials research. Over the past two years, the emergence of large language models (LLMs) has added a new dimension to this field, providing unprecedented flexibility in information integration, decision-making, and interacting with human researchers. This review explores how LLMs are reshaping catalyst design, heralding a revolutionary change in the fields.}
}
@article{ZHOU2024103705,
title = {ProMvSD: Towards unsupervised knowledge graph anomaly detection via prior knowledge integration and multi-view semantic-driven estimation},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103705},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103705},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000657},
author = {Yunfeng Zhou and Cui Zhu and Wenjun Zhu},
keywords = {Knowledge graph, Anomaly detection, Pre-trained language models, Semantics, Unsupervised learning},
abstract = {Knowledge graphs (KGs) have found extensive applications within intelligent systems, such as information retrieval. Much of the research has predominantly focused on completing missing knowledge, with little consideration given to examining errors. Unfortunately, during customizing KGs, diverse unpredictable errors are virtually unavoidable to be introduced, and these anomalies significantly impact the performance of applications. Detecting erroneous knowledge presents a formidable challenge due to the costly acquisition of ground-truth labels. In this work, we develop an unsupervised anomaly detection framework named ProMvSD, aiming to adapt KGs of varying scales via serialization components. To overcome the insufficient contextual information provided by the topological structure, we introduce the large language model as a reasoner to extract prior knowledge from extensive pre-trained textual data, thereby enhancing the understanding of KGs. Anomalous triple may result in a larger semantic gap between the head and tail neighborhoods. To uncover latent anomalies effectively, we propose a multi-view semantic-driven model (MvSD) based on the assumptions of self-consistency and information stability. MvSD jointly estimates the suspiciousness of triples from three hyperviews: node-view semantic contradiction, triple-view semantic gap, and pathway-view semantic gap. Extensive experiments on three English benchmark KGs and a Chinese medical KG demonstrate that, for the top 1% of the most suspicious triples, we can detect real anomalies with at most 99.9% accuracy. Furthermore, ProMvSD significantly outperforms state-of-the-art representation learning baselines, achieving a 29.2% improvement in detecting all anomalies.}
}
@article{WANG2024,
title = {An Entity Extraction Pipeline for Medical Text Records Using Large Language Models: Analytical Study},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54580},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124001493},
author = {Lei Wang and Yinyao Ma and Wenshuai Bi and Hanlin Lv and Yuxiang Li},
keywords = {clinical data extraction, large language models, feature hallucination, modular approach, unstructured data processing},
abstract = {Background
The study of disease progression relies on clinical data, including text data, and extracting valuable features from text data has been a research hot spot. With the rise of large language models (LLMs), semantic-based extraction pipelines are gaining acceptance in clinical research. However, the security and feature hallucination issues of LLMs require further attention.
Objective
This study aimed to introduce a novel modular LLM pipeline, which could semantically extract features from textual patient admission records.
Methods
The pipeline was designed to process a systematic succession of concept extraction, aggregation, question generation, corpus extraction, and question-and-answer scale extraction, which was tested via 2 low-parameter LLMs: Qwen-14B-Chat (QWEN) and Baichuan2-13B-Chat (BAICHUAN). A data set of 25,709 pregnancy cases from the People’s Hospital of Guangxi Zhuang Autonomous Region, China, was used for evaluation with the help of a local expert’s annotation. The pipeline was evaluated with the metrics of accuracy and precision, null ratio, and time consumption. Additionally, we evaluated its performance via a quantified version of Qwen-14B-Chat on a consumer-grade GPU.
Results
The pipeline demonstrates a high level of precision in feature extraction, as evidenced by the accuracy and precision results of Qwen-14B-Chat (95.52% and 92.93%, respectively) and Baichuan2-13B-Chat (95.86% and 90.08%, respectively). Furthermore, the pipeline exhibited low null ratios and variable time consumption. The INT4-quantified version of QWEN delivered an enhanced performance with 97.28% accuracy and a 0% null ratio.
Conclusions
The pipeline exhibited consistent performance across different LLMs and efficiently extracted clinical features from textual data. It also showed reliable performance on consumer-grade hardware. This approach offers a viable and effective solution for mining clinical research data from textual records.}
}
@article{CHEN2024105873,
title = {Knowledge graph for safety management standards of water conservancy construction engineering},
journal = {Automation in Construction},
volume = {168},
pages = {105873},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105873},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006095},
author = {Yun Chen and Gengyang Lu and Ke Wang and Shu Chen and Chenfei Duan},
keywords = {Water conservancy construction engineering, Knowledge graph, Safety management standards, ALBERT-BiLSTM-CRF, Association rules},
abstract = {With the increasing demand for water conservancy engineering (WCE), the number of safety accidents during construction has continued to rise, requiring an urgent improvement in construction safety. The existing safety management regulations for water conservancy construction engineering (WCCE) comprise a considerable amount of text, with cross-references between different standards severely reducing their use efficiency. To address this issue, this paper proposes an ALBERT-BiLSTM-CRF model based on textual data from WCCE safety management standards. ALBERT, a lightweight pretrained language model, is integrated with the BiLSTM-CRF to construct an intelligent text entity recognition method. Association rules are used to extract entity relationships, and a knowledge graph representing the WCCE safety management standards is established. The results show that the ALBERT-BiLSTM-CRF algorithm improves the precision, with a recognition accuracy exceeding 85 %. Case studies validate that the constructed knowledge graph can quickly query safety standard knowledge, aiding in the generation of safety measures.}
}
@article{YANG2024100887,
title = {Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT},
journal = {Patterns},
volume = {5},
number = {1},
pages = {100887},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100887},
url = {https://www.sciencedirect.com/science/article/pii/S266638992300288X},
author = {Jingye Yang and Cong Liu and Wendy Deng and Da Wu and Chunhua Weng and Yunyun Zhou and Kai Wang},
keywords = {Human Phenotype Ontology, named entity recognition, transformer, clinical notes, electronic health records, BERT, GPT},
abstract = {Summary
To enhance phenotype recognition in clinical notes of genetic diseases, we developed two models—PhenoBCBERT and PhenoGPT—for expanding the vocabularies of Human Phenotype Ontology (HPO) terms. While HPO offers a standardized vocabulary for phenotypes, existing tools often fail to capture the full scope of phenotypes due to limitations from traditional heuristic or rule-based approaches. Our models leverage large language models to automate the detection of phenotype terms, including those not in the current HPO. We compare these models with PhenoTagger, another HPO recognition tool, and found that our models identify a wider range of phenotype concepts, including previously uncharacterized ones. Our models also show strong performance in case studies on biomedical literature. We evaluate the strengths and weaknesses of BERT- and GPT-based models in aspects such as architecture and accuracy. Overall, our models enhance automated phenotype detection from clinical texts, improving downstream analyses on human diseases.}
}
@article{CHEN2024104016,
title = {A survey of large language models for cyber threat detection},
journal = {Computers & Security},
volume = {145},
pages = {104016},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104016},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824003213},
author = {Yiren Chen and Mengjiao Cui and Ding Wang and Yiyang Cao and Peian Yang and Bo Jiang and Zhigang Lu and Baoxu Liu},
keywords = {Large language models, Cyber security, Threat detection, Literature review},
abstract = {With the increasing complexity of cyber threats and the expanding scope of cyberspace, there exist progressively more challenges in cyber threat detection. It is proven that most previous threat detection models may become inadequate due to the escalation of hacker attacks. However, recent research has shown that some of these problems can be effectively addressed by Large Language Models (LLMs) directly or indirectly. Nowadays, a growing number of security researchers are adopting LLMs for analyzing various cyber threats. According to the investigation, we found that while there are numerous emerging reviews on the utilization of LLMs in some fields of cyber security, there is currently a lack of a comprehensive review on the application of LLMs in the threat detection stage. Through retrieving and collating existing works in recent years, we examined various threat detection and monitoring tasks for which LLMs may be well-suited, including cyber threat intelligence, phishing email detection, threat prediction, logs analysis, and so on. Additionally, the review explored the specific stages of different detection tasks in which LLMs are involved, evaluating the points at which LLMs are optimized. For instance, LLMs have been found to enhance the interpretability of log analysis in real-time anomaly event discovery. Additionally, we discussed some tasks where LLMs may not be suitable and explored future directions and challenges in this field. By providing a detailed status update and comprehensive insights, this review aims to assist security researchers in leveraging LLMs to enhance existing detection frameworks or develop domain-specific LLMs.}
}
@article{LOPEZUBEDA2024105443,
title = {Evaluation of large language models performance against humans for summarizing MRI knee radiology reports: A feasibility study},
journal = {International Journal of Medical Informatics},
volume = {187},
pages = {105443},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105443},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624001060},
author = {Pilar López-Úbeda and Teodoro Martín-Noguerol and Carolina Díaz-Angulo and Antonio Luna},
keywords = {Radiology report summarization, Natural Language Processing, Large Language Model, Knee MRI reports, Human expert evaluation},
abstract = {Objectives
This study addresses the critical need for accurate summarization in radiology by comparing various Large Language Model (LLM)-based approaches for automatic summary generation. With the increasing volume of patient information, accurately and concisely conveying radiological findings becomes crucial for effective clinical decision-making. Minor inaccuracies in summaries can lead to significant consequences, highlighting the need for reliable automated summarization tools.
Methods
We employed two language models — Text-to-Text Transfer Transformer (T5) and Bidirectional and Auto-Regressive Transformers (BART) — in both fine-tuned and zero-shot learning scenarios and compared them with a Recurrent Neural Network (RNN). Additionally, we conducted a comparative analysis of 100 MRI report summaries, using expert human judgment and criteria such as coherence, relevance, fluency, and consistency, to evaluate the models against the original radiologist summaries. To facilitate this, we compiled a dataset of 15,508 retrospective knee Magnetic Resonance Imaging (MRI) reports from our Radiology Information System (RIS), focusing on the findings section to predict the radiologist's summary.
Results
The fine-tuned models outperform the neural network and show superior performance in the zero-shot variant. Specifically, the T5 model achieved a Rouge-L score of 0.638. Based on the radiologist readers' study, the summaries produced by this model were found to be very similar to those produced by a radiologist, with about 70% similarity in fluency and consistency between the T5-generated summaries and the original ones.
Conclusions
Technological advances, especially in NLP and LLM, hold great promise for improving and streamlining the summarization of radiological findings, thus providing valuable assistance to radiologists in their work.}
}
@article{WYSOCKA2024104724,
title = {Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation},
journal = {Journal of Biomedical Informatics},
volume = {158},
pages = {104724},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104724},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001424},
author = {Magdalena Wysocka and Oskar Wysocki and Maxime Delmas and Vincent Mutel and André Freitas},
keywords = {Factual knowledge, Large language models, Antibiotic discovery, Retrieval-augmented generation},
abstract = {Objective:
The paper introduces a framework for the evaluation of the encoding of factual scientific knowledge, designed to streamline the manual evaluation process typically conducted by domain experts. Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially define a step change in biomedical discovery, reducing the barriers for accessing and integrating existing medical evidence. This work explores the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery.
Methods:
The framework involves three evaluation steps, each assessing different aspects sequentially: fluency, prompt alignment, semantic coherence, factual knowledge, and specificity of the generated responses. By splitting these tasks between non-experts and experts, the framework reduces the effort required from the latter. The work provides a systematic assessment on the ability of eleven state-of-the-art LLMs, including ChatGPT, GPT-4 and Llama 2, in two prompting-based tasks: chemical compound definition generation and chemical compound–fungus relation determination.
Results:
Although recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities. The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted.
Conclusion:
While LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases in a zero-shot setting, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale up in size and level of human feedback.}
}
@article{ALGHERAIRY2025101697,
title = {Prompting large language models for user simulation in task-oriented dialogue systems},
journal = {Computer Speech & Language},
volume = {89},
pages = {101697},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101697},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000809},
author = {Atheer Algherairy and Moataz Ahmed},
keywords = {Task Oriented Dialogue, User simulator, Large language models, Prompting, Agenda-based simulator},
abstract = {Large Language Models (LLMs) have gained widespread popularity due to their instruction-following abilities. In this study, we evaluate their ability in simulating user interactions for task-oriented dialogue (TOD) systems. Our findings demonstrate that prompting LLMs reveals their promising capabilities for training and testing dialogue policies, eliminating the need for domain expertise in crafting complex rules or relying on large annotated data, as required by traditional simulators. The results show that the dialogue system trained with the ChatGPT simulator achieves a success rate of 59%, comparable to a 62% success rate of the dialogue system trained with the manual-rules, agenda-based user simulator (ABUS). Furthermore, the dialogue system trained with the ChatGPT simulator demonstrates better generalization ability compared to the dialogue system trained with the ABUS. Its success rate outperforms that of the dialogue system trained with the ABUS by 4% on GenTUS, 5% on the ChatGPT Simulator, and 3% on the Llama simulator. Nevertheless, LLM-based user simulators provide challenging environment, lexically rich, diverse, and random responses. Llama simulator outperforms the human reference in all lexical diversity metrics with a margin of 0.66 in SE, 0.39 in CE, 0.01 in MSTTR, 0.04 in HDD, and 0.55 in MTLD, while the ChatGPT simulator achieves comparable results. This ultimately contributes to enhancing the system’s ability to generalize more effectively.}
}
@article{TAN2024108290,
title = {MedChatZH: A tuning LLM for traditional Chinese medicine consultations},
journal = {Computers in Biology and Medicine},
volume = {172},
pages = {108290},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108290},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524003743},
author = {Yang Tan and Zhixing Zhang and Mingchen Li and Fei Pan and Hao Duan and Zijie Huang and Hua Deng and Zhuohang Yu and Chen Yang and Guoyang Shen and Peng Qi and Chengyuan Yue and Yuxian Liu and Liang Hong and Huiqun Yu and Guisheng Fan and Yun Tang},
keywords = {Generative large language models (LLMs), Question-answering (QA), Dialogue model, Traditional Chinese medical QA, Fine-tuning},
abstract = {Generative Large Language Models (LLMs) have achieved significant success in various natural language processing tasks, including Question-Answering (QA) and dialogue systems. However, most models are trained on English data and lack strong generalization in providing answers in Chinese. This limitation is especially evident in specialized domains like traditional Chinese medical QA, where performance suffers due to the absence of fine-tuning and high-quality datasets. To address this, we introduce MedChatZH, a dialogue model optimized for Chinese medical QA based on transformer decoder with LLaMA architecture. Continued pre-training on a curated corpus of Chinese medical books is followed by fine-tuning with a carefully selected medical instruction dataset, resulting in MedChatZH outperforming several Chinese dialogue baselines on a real-world medical dialogue dataset. Our model, code, and dataset are publicly available on GitHub (https://github.com/tyang816/MedChatZH) to encourage further research in traditional Chinese medicine and LLMs.}
}
@article{LIU2024127505,
title = {ZVQAF: Zero-shot visual question answering with feedback from large language models},
journal = {Neurocomputing},
volume = {580},
pages = {127505},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127505},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224002765},
author = {Cheng Liu and Chao Wang and Yan Peng and Zhixu Li},
keywords = {Feedback, Large language model, Visual question answering},
abstract = {Due to the prominent zero-shot generalization in new language tasks shown by large language models (LLMs), applying LLMs for zero-shot visual question answering (VQA) has been a new trend. However, most prior approaches directly use off-the-shelf captioning models to generate captions that compose in-context examples for LLMs, and such generated captions may be uninformative, thus leading the LLMs to give false predictions. To address this, we propose zero-shot VQA with feedback from LLMs (ZVQAF), a framework that applies LLMs to discriminate the quality of generated captions and leverages this feedback to train the captioning model. ZVQAF consists of two stages: the first stage is the training with feedback, which enables the captioning model to recognize the task objective and information requirements from the LLM, and the second stage is utilizing the optimized captioning model and LLM for inference. Extensive experiments show that ZVQAF achieves zero-shot VQA performance that is comparable or even superior to those previous zero-shot, few-shot, and end-to-end training approaches. For example, on VQAv2 test dataset, ZVQAF outperforms Flamingo (Alayrac et al., 2022) which employs end-to-end training by a large margin of 8.0%. In addition, on A-OKVQA dataset, ZVQAF outperforms zero-shot method Img2LLM (Guo et al., 2023) by 3.8% when employing LLMs with similar scales.}
}
@article{GORIDKOV2024964,
title = {What's in this LCA Report? A Case Study on Harnessing Large Language Models to Support Designers in Understanding Life Cycle Reports},
journal = {Procedia CIRP},
volume = {122},
pages = {964-969},
year = {2024},
note = {31st CIRP Conference on Life Cycle Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.01.131},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124001756},
author = {Nicole Goridkov and Ye Wang and Kosa Goucher-Lambert},
keywords = {sustainable design, life cycle reports, document understanding, knowledge management, large language models},
abstract = {Life cycle assessment (LCA) is a well-established approach and benchmark for design for sustainability efforts, in which detailed reports are produced that can serve as decision-making guides for developing new products. However, LCA reports are typically dense and technically complex, making it difficult for many engineering design project stakeholders to appropriately leverage the information found within them. Our work seeks to understand and improve the transfer of knowledge from LCA reports during the early stages of the design process, specifically leveraging the natural language capabilities of large language models (LLMs). In this paper, we investigate how four LCA-and sustainability-centric prompting frameworks can extract relevant design knowledge from LCA reports, demonstrated through a case study where an LLM (ChatGPT) is prompted on a provided electric toothbrush LCA report. Key findings illustrate the prompting frameworks can establish high-level summaries and identify life-cycle specific information, but the development of specific and design-focused sub-prompts will allow for richer understanding. We envision designers can use these proposed frameworks to query an LLM to gain context and insights from relevant LCA reports. The proposed techniques serve as a basis for automatic knowledge extraction from life cycle documents, creating accessible information in a user-friendly manner for designers who look to develop life-cycle-informed products.}
}
@article{NERELLA2024102900,
title = {Transformers and large language models in healthcare: A review},
journal = {Artificial Intelligence in Medicine},
volume = {154},
pages = {102900},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102900},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724001428},
author = {Subhash Nerella and Sabyasachi Bandyopadhyay and Jiaqing Zhang and Miguel Contreras and Scott Siegel and Aysegul Bumin and Brandon Silva and Jessica Sena and Benjamin Shickel and Azra Bihorac and Kia Khezeli and Parisa Rashidi},
keywords = {Transformers, Healthcare, Electronic Health Records, Large Language Models, Medical Imaging, Natural Language Processing},
abstract = {With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of healthcare data, including clinical NLP, medical imaging, structured Electronic Health Records (EHR), social media, bio-physiological signals, biomolecular sequences. Furthermore, which have also include the articles that used the transformer architecture for generating surgical instructions and predicting adverse outcomes after surgeries under the umbrella of critical care. Under diverse settings, these models have been used for clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. Finally, we also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.}
}
@article{TAYLOR2024103009,
title = {Developing healthcare language model embedding spaces},
journal = {Artificial Intelligence in Medicine},
volume = {158},
pages = {103009},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103009},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002513},
author = {Niall Taylor and Dan Schofield and Andrey Kormilitzin and Dan W. Joyce and Alejo Nevado-Holgado},
keywords = {LLMs, Contrastive loss, Embeddings, Healthcare, Classification},
abstract = {Pre-trained Large Language Models (LLMs) have revolutionised Natural Language Processing (NLP) tasks, but often struggle when applied to specialised domains such as healthcare. The traditional approach of pre-training on large datasets followed by task-specific fine-tuning is resource-intensive and poorly aligned with the constraints of many healthcare settings. This presents a significant challenge for deploying LLM-based NLP solutions in medical contexts, where data privacy, computational resources, and domain-specific language pose unique obstacles. This study aims to develop and evaluate efficient methods for adapting smaller LLMs to healthcare-specific datasets and tasks. We seek to identify pre-training approaches that can effectively instil healthcare competency in compact LLMs under tight computational budgets, a crucial capability for responsible and sustainable deployment in local healthcare settings. We explore three specialised pre-training methods to adapt smaller LLMs to different healthcare datasets: traditional Masked Language modelling (MLM), Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel approach utilising metadata categories from healthcare settings. These methods are assessed across multiple healthcare datasets, with a focus on downstream document classification tasks. We evaluate the performance of the resulting LLMs through classification accuracy and analysis of the derived embedding spaces. Contrastively trained models consistently outperform other approaches on classification tasks, delivering strong performance with limited labelled data and fewer model parameter updates. While our novel metadata-based pre-training does not further improve classifications across datasets, it yields interesting embedding cluster separability. Importantly, all domain-adapted LLMs outperform their publicly available, general-purpose base models, validating the importance of domain specialisation. This research demonstrates the efficacy of specialised pre-training methods in adapting compact LLMs to healthcare tasks, even under resource constraints. We provide guidelines for pre-training specialised healthcare LLMs and motivate continued inquiry into contrastive objectives. Our findings underscore the potential of these approaches for aligning small LLMs with privacy-sensitive medical tasks, offering a path toward more efficient and responsible NLP deployment in healthcare settings. This work contributes to the broader goal of making advanced NLP techniques accessible and effective in specialised domains, particularly where resource limitations and data sensitivity are significant concerns.}
}
@article{BENEDETTO2025100353,
title = {Assessing how accurately large language models encode and apply the common European framework of reference for languages},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100353},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100353},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001565},
author = {Luca Benedetto and Gabrielle Gaudeau and Andrew Caines and Paula Buttery},
keywords = {Large language models, Language learning, Common European Framework of Reference for Languages},
abstract = {Large Language Models (LLMs) can have a transformative effect on a variety of domains, including education, and it is therefore pressing to understand whether these models have knowledge of – or, in other words, how they have encoded – the specific pedagogical requirements of different educational domains, and whether they use this when performing educational tasks. In this work, we propose an approach to evaluate the knowledge – or encoding – that the LLMs have of the Common European Framework of Reference for Languages (CEFR), and use it to evaluate five modern LLMs. Our study shows that the suite of tasks we propose is quite challenging for all the LLMs, and they often provide results which are not satisfactory and would be unusable in educational applications, suggesting that – even if they encode some information about the CEFR – this knowledge is not really leveraged when performing downstream tasks.}
}
@article{DECARDINELSON2024108723,
title = {Generative AI and process systems engineering: The next frontier},
journal = {Computers & Chemical Engineering},
volume = {187},
pages = {108723},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108723},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424001418},
author = {Benjamin Decardi-Nelson and Abdulelah S. Alshehri and Akshay Ajagekar and Fengqi You},
keywords = {Generative AI, Process systems engineering, Large language models, Multiscale},
abstract = {This review article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could potentially advance PSE methodologies, providing insights and prospects for each area. Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and benchmarks, and trust and safety, thereby deepening the discourse on effective GenAI integration into systems analysis, design, optimization, operations, monitoring, and control. This paper provides a guide for future research focused on the applications of emerging GenAI in PSE.}
}
@article{CHEN2025104213,
title = {AECR: Automatic attack technique intelligence extraction based on fine-tuned large language model},
journal = {Computers & Security},
volume = {150},
pages = {104213},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104213},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005194},
author = {Minghao Chen and Kaijie Zhu and Bin Lu and Ding Li and Qingjun Yuan and Yuefei Zhu},
keywords = {Cyber threat intelligence (CTI), Attack technique extraction, Prompt engineering, Large language model (LLM), Advanced persistent threat (APT)},
abstract = {Cyber Threat Intelligence (CTI) reports contain resourceful intelligence on cyber-attack campaigns, which provides great help for security analysts to infer attack trends and enhance their defenses. However, due to the diversity of report content and writing styles, current intelligence extraction is mostly based on time-consuming manual efforts. Moreover, existing automatic methods generally neglect the importance of background knowledge and produce inexact extraction results. These problems prevent the effective utilization and sharing of intelligence from CTI reports. In this paper, we primarily focus on the automatic extraction of attack technique (AT) intelligence, which reveals patterns of attack behaviors and hardly changes over time. We propose a novel automatic AT extraction pipeline for CTI reports (AECR). AECR explores the feasibility of extracting AT intelligence based on a fined-tuned large language model (LLM). Particularly, we endow the selected LLM with enhanced domain-specific knowledge to improve its comprehension of AT-relevant content and alleviate the hallucination problem. Experimental results demonstrate that AECR outperforms state-of-the-art methods by a wide margin with a reasonable time cost. Specifically, we improve the accuracy, precision, recall, and F1-score by 108%, 37.2%, 22.4%, and 67.5% respectively. To the best of our knowledge, AECR is the first to perform AT extraction based on fine-tuned LLM.}
}
@article{LE2024100052,
title = {The performance of large language models on fictional consult queries indicates favorable potential for AI-assisted vascular surgery consult handling},
journal = {JVS-Vascular Insights},
volume = {2},
pages = {100052},
year = {2024},
issn = {2949-9127},
doi = {https://doi.org/10.1016/j.jvsvi.2023.100052},
url = {https://www.sciencedirect.com/science/article/pii/S2949912723000491},
author = {Quang Le and Kedar S. Lavingia and Michael Amendola},
keywords = {Artificial intelligence, Consult, Delivery of care, Large language model, Vascular emergencies},
abstract = {Objective
Recently, the use of large language models (LLMs) in medicine has become a prominent topic of discussion due to the rapid improvement of these tools in understanding and responding to natural language. Several models are widely available to the public, both proprietary and open-sourced. We aim to evaluate the possible use of such LLMs in vascular surgery by understanding their abilities to process common consult requests.
Methods
The senior author created 25 fictional vascular surgery consultation queries based on common consultation requests. Five attending surgeons and four LLMs (GPT 3.5, GPT 4, Bard, and Falcon 40B) were asked to answer whether each consult was an emergency that needed immediate attention within an hour. Responders were also asked whether the next best step was an examination, additional imaging, or an urgent operation. GPT 3.5 and 4 also provided free-response answers on the next best step, graded by attending surgeons based on scientific accuracy, possible harm, and content completeness.
Results
The rates of accurate emergency identification were 88%, 100%, 76%, and 88% for GPT 3.5, GPT 4, Falcon 40B, and Bard, respectively. Although they have similar overall accuracy, GPT 3.5 has a high sensitivity at 100%, whereas Bard has a high specificity at 90%. GPT 4.0 had 100% sensitivity and specificity. LLMs agreed with the majority surgeon opinion on the next best step in 64% (GPT 3.5), 32% (GPT 4), 68% (Falcon 40B), and 36% (Bard) of cases. GPT 3.5 and 4 had a collective ratio of 89.5% of answers adhering to the scientific consensus. Only 5% of responses were highly likely to cause clinically significant harm. Although only 4% included incorrect content, 17.5% of answers missed important content. There was no significant difference between GPT 3.5 and 4 regarding the free-response grade.
Conclusions
Existing, widely available LLMs exhibited a solid ability to identify vascular emergencies, with GPT 4.0 agreeing with surgeon attendings in 100% of cases. However, these models continue to have identifiable deficiencies in treatment recommendations, a higher-level task. Future models might help triage incoming consults and provide preliminary management suggestions. The utility of such tools in clinical practice remains to be explored.}
}
@article{KAUR2025100315,
title = {Harnessing the power of language models in cybersecurity: A comprehensive review},
journal = {International Journal of Information Management Data Insights},
volume = {5},
number = {1},
pages = {100315},
year = {2025},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100315},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824001046},
author = {Ramanpreet Kaur and Tomaž Klobučar and Dušan Gabrijelčič},
keywords = {Cybersecurity, Large language model, Fine-tuning, BERT},
abstract = {Language models are transforming cybersecurity by addressing critical challenges such as the growing skills gap, the need for expertise augmentation, and knowledge retention. These models offer scalable, adaptable, and round-the-clock defenses against evolving cyber threats. By generating human-like text, processing data efficiently, and providing actionable responses, language models bridge the gap between automated systems and human expertise for different cybersecurity applications. However, the application and adaptation of language models for cyber security is still in its infancy. This review explores the use of general models, such as BERT, and larger models in cybersecurity research. It provides a structured framework for developing customized language models tailored to applications including content analysis, software and systems analysis, threat intelligence and monitoring, and cyber vetting. The study critically examines challenges, such as data confidentiality, infrastructure requirements, integration complexity and the evolving threat landscape. Moreover, it underscores the need for transparency, responsible use, and bias mitigation to ensure reliable and secure deployment of these models. In addition, this work critically examines the socio-technical dimensions of language model integration, focusing on their impact on organizational workflows, decision making and human-machine collaboration. By considering both technical and socio-technical considerations, this review provides a roadmap for future research and development. It highlights the potential of language models to improve organizational resilience, ensure secure implementation, and support informed decision-making in cybersecurity practice.}
}
@article{AFSHAR2024104707,
title = {On the role of the UMLS in supporting diagnosis generation proposed by Large Language Models},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104707},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104707},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001254},
author = {Majid Afshar and Yanjun Gao and Deepak Gupta and Emma Croxford and Dina Demner-Fushman},
keywords = {Artificial intelligence, Knowledge representation (computer), Natural language processing, Unified medical language system, Evaluation methodology, Differential diagnoses},
abstract = {Objective:
Traditional knowledge-based and machine learning diagnostic decision support systems have benefited from integrating the medical domain knowledge encoded in the Unified Medical Language System (UMLS). The emergence of Large Language Models (LLMs) to supplant traditional systems poses questions of the quality and extent of the medical knowledge in the models’ internal knowledge representations and the need for external knowledge sources. The objective of this study is three-fold: to probe the diagnosis-related medical knowledge of popular LLMs, to examine the benefit of providing the UMLS knowledge to LLMs (grounding the diagnosis predictions), and to evaluate the correlations between human judgments and the UMLS-based metrics for generations by LLMs.
Methods:
We evaluated diagnoses generated by LLMs from consumer health questions and daily care notes in the electronic health records using the ConsumerQA and Problem Summarization datasets. Probing LLMs for the UMLS knowledge was performed by prompting the LLM to complete the diagnosis-related UMLS knowledge paths. Grounding the predictions was examined in an approach that integrated the UMLS graph paths and clinical notes in prompting the LLMs. The results were compared to prompting without the UMLS paths. The final experiments examined the alignment of different evaluation metrics, UMLS-based and non-UMLS, with human expert evaluation.
Results:
In probing the UMLS knowledge, GPT-3.5 significantly outperformed Llama2 and a simple baseline yielding an F1 score of 10.9% in completing one-hop UMLS paths for a given concept. Grounding diagnosis predictions with the UMLS paths improved the results for both models on both tasks, with the highest improvement (4%) in SapBERT score. There was a weak correlation between the widely used evaluation metrics (ROUGE and SapBERT) and human judgments.
Conclusion:
We found that while popular LLMs contain some medical knowledge in their internal representations, augmentation with the UMLS knowledge provides performance gains around diagnosis generation. The UMLS needs to be tailored for the task to improve the LLMs predictions. Finding evaluation metrics that are aligned with human judgments better than the traditional ROUGE and BERT-based scores remains an open research question.}
}
@article{ABDELAZIM202466,
title = {Multi-Hop Arabic LLM Reasoning in Complex QA},
journal = {Procedia Computer Science},
volume = {244},
pages = {66-75},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.179},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029806},
author = {Hazem Abdelazim and Tony Begemy and Ahmed Galal and Hala Sedki and Ali Mohamed},
keywords = {Large Language Models, Retrieval Augmented Generation, Arabic NLP},
abstract = {The introduction of Large Language Models (LLMs), and generative AI has significantly transformed the field of natural language processing. These models have exhibited profound reasoning capabilities, marking considerable progress across diverse general knowledge reasoning tasks. Consequently, the deployment of LLMs in domain-specific contexts has become a prime objective for governments and corporations eager to leverage the generative AI revolution. However, the Arabic language has notably lagged in attention and development compared to other languages in this arena. This research endeavors to delve into various facets of Arabic closed-domain question and answering systems that emulate the reasoning requirements of private enterprise data. Our study focuses on the practical deployment of Arabic LLMs in targeted applications, specifically utilizing the ACQAD (Arabic Complex Question Answering Dataset), which exhibits multi-hop reasoning. Different strategies are experimented using Long Context Window (LCW) and Retrieval Augmented Generation (RAG). Results showed that decomposing complex questions using Chain-of-Thought reasoning considerably improved the performance from 75% to 92% using LCW, but at much higher token cost compared to RAG. Trade-of between cost and performance showed that 80% accuracy can be attained using only 30% of the cost using RAG Sentence - level embeddings. Microsoft E5 embedding model is used and OpenAI GPT4-turbo LLM which proved superior reasoning performance compared to other Arabic LLMs}
}
@article{CHEN2025102982,
title = {Integrating large language model and digital twins in the context of industry 5.0: Framework, challenges and opportunities},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {94},
pages = {102982},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.102982},
url = {https://www.sciencedirect.com/science/article/pii/S0736584525000365},
author = {Chong Chen and Kuanhong Zhao and Jiewu Leng and Chao Liu and Junming Fan and Pai Zheng},
keywords = {Large language models, Digital twins, Industry 5.0, Intelligent manufacturing},
abstract = {In Industry 5.0, where human ingenuity is combined with cutting-edge technologies such as artificial intelligence (AI) and robotics to revolutionize manufacturing with a focus on sustainability and human well-being, Digital Twins (DT) have become essential to real-time optimization. However, the complexity of managing DT for large-scale systems poses challenges in terms of data transmission, analytics, and advanced applications, which can be potentially addressed by Large Language Model (LLM). This research firstly performs a literature review to study the roles and functions of LLM in DT in the context of Industry 5.0. Subsequently, we propose a framework named Interactive-DT for LLM-DT integration that reveals the technical pathway for how LLM can be effectively integrated and function within DT environments. Within this framework, the roles and functionalities of LLM at the edge layer, DT layer, and service layer are elaborated upon. Finally, the identified research gaps and prospects for the integration of LLM and DT are outlined and discussed. The research outcomes of this paper highlight the potential of LLM to augment DT capabilities through improved construction and operation, enhanced cloud-edge collaboration, and sophisticated data analytics, ultimately promoting industrial practices that are both efficient and aligned with human-centric and sustainability principles in Industry 5.0.}
}
@article{VISALLI2025105456,
title = {Can natural language processing or large language models replace human operators for pre-processing word and sentence-based free comments sensory evaluation data?},
journal = {Food Quality and Preference},
volume = {127},
pages = {105456},
year = {2025},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2025.105456},
url = {https://www.sciencedirect.com/science/article/pii/S095032932500031X},
author = {Michel Visalli and Ronan Symoneaux and Cécile Mursic and Margaux Touret and Flore Lourtioux and Kipédène Coulibaly and Benjamin Mahieu},
keywords = {Open ended questions, ChatGPT, Textual data, Consumer test, Method comparison, Drivers of liking},
abstract = {The free comment (FC) method enables the collection of insights on products based on consumers' natural language. The primary drawback is the need for extensive data pre-processing. This study compared the results of three data pre-processing techniques applied to FC data related to the perception of six madeleines by two panels of 100 consumers: manual pre-processing by four human experts, automated pre-processing by an expert system, and automated pre-processing by the large language model ChatGPT. Two modes of data collection were used: responses only with words or short expressions (“FC words”), or responses based on complete sentences (“FC sentences”). Various indicators (number of words extracted, number of concepts retained, pre-processing time, level of repeatability/discrimination/stability of findings) were computed and compared between data collection modes and pre-processing techniques. It was shown that the automated systems performed correctly with FC words; however, they were less effective at extracting relevant words from FC sentences. The findings from statistical analyses following automated pre-processing were less repeatable and discriminative compared to those from the most proficient human operators. It was also demonstrated that, beyond the overall differences between products, the pre-processing of FC data can be a major source of non-reproducibility in findings, depending on the operators and the level of detail they consider when extracting information. Finally, the advantages and disadvantages of each pre-processing technique were summarized, along with several recommendations for pre-processing and analysing FC data at the appropriate level of granularity to draw robust conclusions.}
}
@article{SIVARAJKUMAR2024,
title = {An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/55318},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000383},
author = {Sonish Sivarajkumar and Mark Kelley and Alyssa Samolyk-Mazzanti and Shyam Visweswaran and Yanshan Wang},
keywords = {large language model, LLM, LLMs, natural language processing, NLP, in-context learning, prompt engineering, evaluation, zero-shot, few shot, prompting, GPT, language model, language, models, machine learning, clinical data, clinical information, extraction, BARD, Gemini, LLaMA-2, heuristic, prompt, prompts, ensemble},
abstract = {Background
Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches.
Objective
The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types—heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models.
Methods
This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches.
Results
The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types.
Conclusions
This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.}
}
@article{KOSONOCKY20241150,
title = {Mining patents with large language models elucidates the chemical function landscape††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4dd00011k},
journal = {Digital Discovery},
volume = {3},
number = {6},
pages = {1150-1159},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00011k},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000925},
author = {Clayton W. Kosonocky and Claus O. Wilke and Edward M. Marcotte and Andrew D. Ellington},
abstract = {The fundamental goal of small molecule discovery is to generate chemicals with target functionality. While this often proceeds through structure-based methods, we set out to investigate the practicality of methods that leverage the extensive corpus of chemical literature. We hypothesize that a sufficiently large text-derived chemical function dataset would mirror the actual landscape of chemical functionality. Such a landscape would implicitly capture complex physical and biological interactions given that chemical function arises from both a molecule's structure and its interacting partners. To evaluate this hypothesis, we built a Chemical Function (CheF) dataset of patent-derived functional labels. This dataset, comprising 631 K molecule–function pairs, was created using an LLM- and embedding-based method to obtain 1.5 K unique functional labels for approximately 100 K randomly selected molecules from their corresponding 188 K unique patents. We carry out a series of analyses demonstrating that the CheF dataset contains a semantically coherent textual representation of the functional landscape congruent with chemical structural relationships, thus approximating the actual chemical function landscape. We then demonstrate through several examples that this text-based functional landscape can be leveraged to identify drugs with target functionality using a model able to predict functional profiles from structure alone. We believe that functional label-guided molecular discovery may serve as an alternative approach to traditional structure-based methods in the pursuit of designing novel functional molecules.}
}
@article{ZHANG2025126651,
title = {SecLMNER: A framework for enhanced named entity recognition in multi-source cybersecurity data using large language models},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126651},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126651},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002738},
author = {Yunlong Zhang and Jingju Liu and Xiaofeng Zhong and Lei Wu},
keywords = {Cybersecurity, Named entity recognition, Large language models, Open-source intelligence},
abstract = {In the realm of cybersecurity, Named Entity Recognition (NER) has predominantly centered on cyber threat intelligence. However, cybersecurity-related information often resides in open-source intelligence and unprocessed tool outputs. With structured, semi-structured, and unstructured text data coexisting, traditional BERT-based NER methods encounter challenges in handling diverse data formats and technical terminology richness. To tackle these obstacles, this paper presents a framework that integrates large language models for NER in multi-source cybersecurity data. Leveraging decoder-based large language models’ generative capabilities, the framework intelligently crafts natural language segments containing data information for enhanced adaptability. Subsequently, the SecureBERT model, excelling as the premier open-source solution for NER in the network security domain, leverages a pre-trained encoder tailored for cybersecurity text to proficiently detect diverse entities and associated information embedded within natural language segments. The experimental findings demonstrate that our proposed method outperforms traditional BERT and its variants in fine-tuning across five distinct cybersecurity text data sources. The results reveal that by integrating three generative language models, each with parameters under 10 billion, we achieve an improvement in Recall ranging from 8.1% to 21.81% over the state-of-the-art open-source SecureBERT, and an increase in F1 score from 6.19% to 16.7%. Moreover, with the enhancements to our framework, the NER performance can match that of open-source large-parameter language models.}
}
@article{PANDEY2025104244,
title = {Generating product reviews from aspect-based ratings using large language models},
journal = {Journal of Retailing and Consumer Services},
volume = {84},
pages = {104244},
year = {2025},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2025.104244},
url = {https://www.sciencedirect.com/science/article/pii/S0969698925000232},
author = {Prince Pandey and Jyoti Prakash Singh},
keywords = {E-commerce, GPT, LLM, Review, Feedback},
abstract = {The rapid growth of e-commerce has made textual reviews and product ratings crucial for consumer purchase decisions. However, the overall Likert scale rating of the product does not convey any information about major aspects of a product. In contrast, many textual reviews often lack detailing of various aspects of the product, leading to incomplete feedback. This paper proposes a framework that generates detailed textual reviews from user-provided ratings on various aspects of a product using large language models (LLMs). Our approach enhances the online product review system by integrating specific feedback from structured ratings, resulting in more detailed and reliable product reviews. Our results show that AI-generated reviews exhibit high readability, coherence, relevance, and informativeness, rivaling human-written reviews to the extent that distinguishing between the two proves challenging, even for human evaluators. This research contributes to develop more accurate and comprehensive review systems, enhancing the overall quality and usefulness of e-commerce reviews and empowering consumers to make informed purchasing decisions. The proposed framework offers a valuable tool for businesses and e-commerce platforms to improve product reviews, enhance customer satisfaction, and increase sales.}
}
@article{KOSTOPOLUS2025102894,
title = {Student use of generative AI as a composing process supplement: Concerns for intellectual property and academic honesty},
journal = {Computers and Composition},
volume = {75},
pages = {102894},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102894},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000707},
author = {Emma Kostopolus},
keywords = {Artificial intelligence, Intellectual property, Academic honesty, Multimodality},
abstract = {This article discusses the nuanced challenges of using Generative Artificial Intelligence in multimodal compositions while maintaining an ethical adherence to ideas of academic honesty and intellectual property. Through examining hypothetical scenarios, we can see that multimodality complicates the concept of “fair use” in academic contexts, since image or audio generation via AI functions differently than text generated by a Large Language Model. In thinking through the case studies, the article presents an argument for how educators can still use Generative AI in their multimodal composition assignments, through teaching students to us it as a process supplement and to always be critically aware of their citational responsibilities. This understanding of Generative AI use is placed in conversation with our understanding of intellectual property law as relates to both the classroom and broader digital composing environments, to better prepare students to create texts in their future careers.}
}
@article{LUO2024100488,
title = {Large language model-based code generation for the control of construction assembly robots: A hierarchical generation approach},
journal = {Developments in the Built Environment},
volume = {19},
pages = {100488},
year = {2024},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2024.100488},
url = {https://www.sciencedirect.com/science/article/pii/S2666165924001698},
author = {Hanbin Luo and Jianxin Wu and Jiajing Liu and Maxwell Fordjour Antwi-Afari},
keywords = {Construction assembly robot, Large language model, Code generation, , Human–robot collaboration},
abstract = {Offline programming (OLP) is a mainstream approach for controlling assembly robots at construction sites. However, existing methods are tailored to specific assembly tasks and workflows, and thus lack flexibility. Additionally, the emerging large language model (LLM)-based OLP cannot effectively handle the code logic of robot programming. Thus, this paper addresses the question: How can robot control programs be generated effectively and accurately for diverse construction assembly tasks using LLM techniques? This paper describes a closed user-on-the-loop control framework for construction assembly robots based on LLM techniques. A hierarchical strategy to generate robot control programs is proposed to logically integrate code generation at high and low levels. Additionally, customized application programming interfaces and a chain of action are combined to enhance the LLM's understanding of assembly action logic. An assembly task set was designed to evaluate the feasibility and reliability of the proposed approach. The results show that the proposed approach (1) is widely applicable to diverse assembly tasks, and (2) can improve the quality of the generated code by decreasing the number of errors. Our approach facilitates the automation of construction assembly tasks by simplifying the robot control process.}
}
@article{KUMAR2024100308,
title = {AOPWIKI-EXPLORER: An interactive graph-based query engine leveraging large language models},
journal = {Computational Toxicology},
volume = {30},
pages = {100308},
year = {2024},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2024.100308},
url = {https://www.sciencedirect.com/science/article/pii/S2468111324000100},
author = {Saurav Kumar and Deepika Deepika and Karin Slater and Vikas Kumar},
keywords = {Adverse outcome pathway, Large language model, Graph database, Risk assessment, Artificial intelligence, Data integration, Information retrieval, Information extraction},
abstract = {Adverse Outcome Pathways (AOPs) provide a basis for non-animal testing, by outlining the cascade of molecular and cellular events initiated upon stressor exposure, leading to adverse effects. In recent years, the scientific community has shown interest in developing AOPs through crowdsourcing, with the results archived in the AOP-Wiki: a centralized repository coordinated by the OECD, hosting nearly 512 AOPs (April, 2023). However, the AOP-Wiki platform currently lacks a versatile querying system, which hinders developers' exploration of the AOP network and impedes its practical use in risk assessment. This work proposes to unleash the full potential of the AOP-Wiki archive by adapting its data into a Labelled Property Graph (LPG) schema. Additionally, the tool offers a visual network query interface for both database-specific and natural language queries, facilitating the retrieval and analysis of graph data. The multi-query interface allows non-technical users to construct flexible queries, thereby enhancing the potential for AOP exploration. By reducing the time and technical requirements, the present query engine enhances the practical utilization of the valuable data within AOP-Wiki. To evaluate the platform, a case study is presented with three levels of use-case scenarios (simple, moderate, and complex queries). AOPWIKI-EXPLORER is freely available on GitHub (https://github.com/Crispae/AOPWiki_Explorer) for wider community reach and further enhancement.}
}
@article{CHIZHIKOVA2025108515,
title = {Automatic TNM staging of colorectal cancer radiology reports using pre-trained language models},
journal = {Computer Methods and Programs in Biomedicine},
volume = {259},
pages = {108515},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108515},
url = {https://www.sciencedirect.com/science/article/pii/S016926072400508X},
author = {Mariia Chizhikova and Pilar López-Úbeda and Teodoro Martín-Noguerol and Manuel C. Díaz-Galiano and L. Alfonso Ureña-López and Antonio Luna and M. Teresa Martín-Valdivia},
keywords = {Colorectal cancer staging, NLP, TNM, Text classification, Transformer, Language models},
abstract = {Background and Objective:
Colorectal cancer is one of the major causes of cancer death worldwide. Essential for prognosis and treatment planning, TNM staging offers critical insights into the advancement of colorectal cancer. However, manual TNM staging from colon magnetic resonance imaging (MRI) is a laborious and error prone process. This study introduces an automated text classification system for TNM staging of colon MRI images in Spanish.
Methods:
A dataset of 1319 Spanish colon MRI reports was collected and manually labeled with TNM staging. In order to automate the task of TNM staging, a multimodal system was proposed. The system is based on RoBERTa language model pre-trained on a combination of biomedical and clinical Spanish language corpora and uses Natural Language Processing (NLP) techniques to extract relevant categorical and numerical features from MRI reports.
Results:
The performance of the system was evaluated using different metrics and the results obtained are very promising: the best performance among the proposed systems reached 0.7464, 0.8792 and 0.6776 of macro F1-score for T, N and M respectively.
Conclusions:
This study demonstrates the feasibility of using a language model for automatic TNM staging based on Spanish clinical reports of colorectal cancer patients. The proposed system can be a useful tool to improve the efficiency and accuracy of colorectal cancer diagnosis.}
}
@article{CADEDDU2024108166,
title = {A comparative analysis of knowledge injection strategies for large language models in the scholarly domain},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108166},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108166},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003245},
author = {Andrea Cadeddu and Alessandro Chessa and Vincenzo {De Leo} and Gianni Fenu and Enrico Motta and Francesco Osborne and Diego {Reforgiato Recupero} and Angelo Salatino and Luca Secchi},
keywords = {Knowledge injection, Knowledge graphs, Large language models, Transformers, BERT, Classification, Natural language processing},
abstract = {In recent years, transformer-based models have emerged as powerful tools for natural language processing tasks, demonstrating remarkable performance in several domains. However, they still present significant limitations. These shortcomings become more noticeable when dealing with highly specific and complex concepts, particularly within the scientific domain. For example, transformer models have particular difficulties when processing scientific articles due to the domain-specific terminologies and sophisticated ideas often encountered in scientific literature. To overcome these challenges and further enhance the effectiveness of transformers in specific fields, researchers have turned their attention to the concept of knowledge injection. Knowledge injection is the process of incorporating outside knowledge into transformer models to improve their performance on certain tasks. In this paper, we present a comprehensive study of knowledge injection strategies for transformers within the scientific domain. Specifically, we provide a detailed overview and comparative assessment of four primary methodologies, evaluating their efficacy in the task of classifying scientific articles. For this purpose, we constructed a new benchmark including both 24K labelled papers and a knowledge graph of 9.2K triples describing pertinent research topics. We also developed a full codebase to easily re-implement all knowledge injection strategies in different domains. A formal evaluation indicates that the majority of the proposed knowledge injection methodologies significantly outperform the baseline established by Bidirectional Encoder Representations from Transformers.}
}
@article{BZDOK2024698,
title = {Data science opportunities of large language models for neuroscience and biomedicine},
journal = {Neuron},
volume = {112},
number = {5},
pages = {698-717},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324000424},
author = {Danilo Bzdok and Andrew Thieme and Oleksiy Levkovskyy and Paul Wren and Thomas Ray and Siva Reddy},
abstract = {Large language models (LLMs) are a new asset class in the machine-learning landscape. Here we offer a primer on defining properties of these modeling techniques. We then reflect on new modes of investigation in which LLMs can be used to reframe classic neuroscience questions to deliver fresh answers. We reason that LLMs have the potential to (1) enrich neuroscience datasets by adding valuable meta-information, such as advanced text sentiment, (2) summarize vast information sources to overcome divides between siloed neuroscience communities, (3) enable previously unthinkable fusion of disparate information sources relevant to the brain, (4) help deconvolve which cognitive concepts most usefully grasp phenomena in the brain, and much more.}
}
@article{GUO2024,
title = {Large Language Models for Mental Health Applications: Systematic Review},
journal = {JMIR Mental Health},
volume = {11},
year = {2024},
issn = {2368-7959},
doi = {https://doi.org/10.2196/57400},
url = {https://www.sciencedirect.com/science/article/pii/S2368795924001173},
author = {Zhijun Guo and Alvina Lai and Johan H Thygesen and Joseph Farrington and Thomas Keen and Kezhi Li},
keywords = {large language models, mental health, digital health care, ChatGPT, Bidirectional Encoder Representations from Transformers, BERT},
abstract = {Background
Large language models (LLMs) are advanced artificial neural networks trained on extensive datasets to accurately understand and generate natural language. While they have received much attention and demonstrated potential in digital health, their application in mental health, particularly in clinical settings, has generated considerable debate.
Objective
This systematic review aims to critically assess the use of LLMs in mental health, specifically focusing on their applicability and efficacy in early screening, digital interventions, and clinical settings. By systematically collating and assessing the evidence from current studies, our work analyzes models, methodologies, data sources, and outcomes, thereby highlighting the potential of LLMs in mental health, the challenges they present, and the prospects for their clinical use.
Methods
Adhering to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines, this review searched 5 open-access databases: MEDLINE (accessed by PubMed), IEEE Xplore, Scopus, JMIR, and ACM Digital Library. Keywords used were (mental health OR mental illness OR mental disorder OR psychiatry) AND (large language models). This study included articles published between January 1, 2017, and April 30, 2024, and excluded articles published in languages other than English.
Results
In total, 40 articles were evaluated, including 15 (38%) articles on mental health conditions and suicidal ideation detection through text analysis, 7 (18%) on the use of LLMs as mental health conversational agents, and 18 (45%) on other applications and evaluations of LLMs in mental health. LLMs show good effectiveness in detecting mental health issues and providing accessible, destigmatized eHealth services. However, assessments also indicate that the current risks associated with clinical use might surpass their benefits. These risks include inconsistencies in generated text; the production of hallucinations; and the absence of a comprehensive, benchmarked ethical framework.
Conclusions
This systematic review examines the clinical applications of LLMs in mental health, highlighting their potential and inherent risks. The study identifies several issues: the lack of multilingual datasets annotated by experts, concerns regarding the accuracy and reliability of generated content, challenges in interpretability due to the “black box” nature of LLMs, and ongoing ethical dilemmas. These ethical concerns include the absence of a clear, benchmarked ethical framework; data privacy issues; and the potential for overreliance on LLMs by both physicians and patients, which could compromise traditional medical practices. As a result, LLMs should not be considered substitutes for professional mental health services. However, the rapid development of LLMs underscores their potential as valuable clinical aids, emphasizing the need for continued research and development in this area.
Trial Registration
PROSPERO CRD42024508617; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=508617}
}
@article{ZHANG2024100549,
title = {Automatic bridge inspection database construction through hybrid information extraction and large language models},
journal = {Developments in the Built Environment},
volume = {20},
pages = {100549},
year = {2024},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2024.100549},
url = {https://www.sciencedirect.com/science/article/pii/S2666165924002308},
author = {Chenhong Zhang and Xiaoming Lei and Ye Xia and Limin Sun},
keywords = {Bridge inspection data, Natural language processing, Information extraction, Large languge model, Pseudo label},
abstract = {Regular bridge inspections generate extensive reports that, while critical for maintenance, often remain underutilized due to their unstructured format. Traditional information extraction methods depend on intricate labeling systems that commonly require time-consuming and labor-intensive labeling. This paper presents a novel bridge inspection database construction method leveraging LLM-assisted information extraction. First, we introduce the pseudo-labelling method using a closed-source LLM to generate high-quality data. Then we propose the hybrid extraction pipeline to extract relevant information segments and process them by a generation-based IE model, fine-tuned on pseudo-labeled data. Finally, the extracted data is used to construct the bridge inspection database. The proposed method, validated with real-world data, not only demonstrates higher extraction precision than the closed-source LLM used for pseudo-labeling but also outperforms traditional methods in both data preparation time and extraction accuracy. This approach provides a scalable solution for more proactive and data-driven bridge maintenance strategies.}
}
@article{MICHELET2024301683,
title = {ChatGPT, Llama, can you write my report? An experiment on assisted digital forensics reports written using (local) large language models},
journal = {Forensic Science International: Digital Investigation},
volume = {48},
pages = {301683},
year = {2024},
note = {DFRWS EU 2024 - Selected Papers from the 11th Annual Digital Forensics Research Conference Europe},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2023.301683},
url = {https://www.sciencedirect.com/science/article/pii/S2666281723002020},
author = {Gaëtan Michelet and Frank Breitinger},
keywords = {Digital forensics investigation, Local large language models, ChatGPT, Report automation, Assisted report writing},
abstract = {Generative AIs, especially Large Language Models (LLMs) such as ChatGPT or Llama, have advanced significantly, positioning them as valuable tools for digital forensics. While initial studies have explored the potential of ChatGPT in the context of investigations, the question of to what extent LLMs can assist the forensic report writing process remains unresolved. To answer the question, this article first examines forensic reports with the goal of generalization (e.g., finding the ‘average structure’ of a report). We then evaluate the strengths and limitations of LLMs for generating the different parts of the forensic report using a case study. This work thus provides valuable insights into the automation of report writing, a critical facet of digital forensics investigations. We conclude that combined with thorough proofreading and corrections, LLMs may assist practitioners during the report writing process but at this point cannot replace them.}
}@article{ZHANG2025100767,
title = {The impact of generative AI on management innovation},
journal = {Journal of Industrial Information Integration},
volume = {44},
pages = {100767},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100767},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24002103},
author = {Caiming Zhang and Hui Zhang},
keywords = {Generative artificial intelligence, Management decision-making, Management Algorithms, Information Integration},
abstract = {Generative Artificial Intelligence (GAI) demonstrates significant potential in the application of management and organizational innovation. This paper systematically investigates the multifaceted impacts of GAI on management decision-making, management algorithms, information integration, and various specific domains. GAI significantly enhances the accuracy of management decisions through its robust data analysis and predictive capabilities. By effectively integrating internal and external information, it reduces information asymmetry and improves both information transparency and the quality of decisions. In terms of specific application areas, GAI shows broad prospects in multiple fields, including business, education, healthcare, content creation, and game development. As GAI technology continues to advance, it will become more intelligent and adaptive. However, further research and the establishment of relevant ethical guidelines and legal frameworks are necessary to ensure its safety and reliability.}
}
@article{EKE2023100060,
title = {ChatGPT and the rise of generative AI: Threat to academic integrity?},
journal = {Journal of Responsible Technology},
volume = {13},
pages = {100060},
year = {2023},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2023.100060},
url = {https://www.sciencedirect.com/science/article/pii/S2666659623000033},
author = {Damian Okaibedi Eke},
keywords = {ChatGPT, Large language models, OpenAI, Academic integrity, Generative AI},
abstract = {The emergence of OpenAI's ChatGPT has put intense spotlight on Generative AI (Gen-AI) systems and their possible impacts on Academic integrity. This paper provides an overview of the current arguments around ChatGPT and Academic integrity and concludes that although these technologies are capable of revolutionising academia, the way ChatGPT and other generative AI systems are used could surely undermine academic integrity. However, to ensure that the risks to academic integrity are mitigated for greater maximisation, institutional and multi-stakeholder efforts are required.}
}
@article{CONG2025103131,
title = {Enhancing novel product iteration: An integrated framework for heuristic ideation via interpretable conceptual design knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103131},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103131},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000242},
author = {Yangfan Cong and Suihuai Yu and Jianjie Chu and Yuexin Huang and Ning Ding and Cong Fang and Stephen Jia Wang},
keywords = {Novel product iteration, Conceptual product design, Design knowledge, Interpretable knowledge graph, Heuristic product ideation},
abstract = {Novel products emerge over time to survive the competitive landscape as no existing product can perpetually satisfy all evolving customer expectations. These products are often characterized by groundbreaking solutions previously unavailable on the market. However, the swift imitation of successful novel products by competitors underscores the need for sustained iteration and continuous improvement. Designers increasingly face challenges in keeping up to date with the growing volume and fragmented nature of design information from diverse sources. While knowledge graphs show promise in structuring and organizing complex design information, their effective application in the ideation process remains limited due to difficulties in automatic knowledge extraction and the lack of interpretability aligned well with designers’ cognitive processes. This study proposes an integrated method to construct an interpretable conceptual design knowledge graph (I-CDKG) that features both inherent and acquired interpretability for heuristic product ideation. First, the schema layer models product design knowledge and governs the semantic connection of design information reinforced by design cognition principles to create a reasonable organizational framework to foster intuitive knowledge exploration. Second, the data layer mainly fulfills automatic and smooth design knowledge extraction for I-CDKG construction through the deep learning ERNIE-BiGRU-CRF model combined with BIESO labeling mode and triple-extracting algorithm. Third, the application layer empowers designers to visually delve into interpretable design knowledge to locate inspiration from cluster, relation, and nest levels and enable constant I-CDKG expansion as design schemes proliferate. A case study on the smart cat litter box demonstrates the feasibility of the proposed methodology. The evaluation results confirm the I-CDKG’s advantages as a productive design tool for inspiring creative, practical, and cost-effective product ideations, thereby empowering the iterative development of competitive novel products.}
}
@article{LI20243825,
title = {Survey and Prospect for Applying Knowledge Graph in Enterprise Risk Management},
journal = {Computers, Materials and Continua},
volume = {78},
number = {3},
pages = {3825-3865},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.046851},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824003618},
author = {Pengjun Li and Qixin Zhao and Yingmin Liu and Chao Zhong and Jinlong Wang and Zhihan Lyu},
keywords = {Knowledge graph, enterprise risk, risk identification, risk management, review},
abstract = {Enterprise risk management holds significant importance in fostering sustainable growth of businesses and in serving as a critical element for regulatory bodies to uphold market order. Amidst the challenges posed by intricate and unpredictable risk factors, knowledge graph technology is effectively driving risk management, leveraging its ability to associate and infer knowledge from diverse sources. This review aims to comprehensively summarize the construction techniques of enterprise risk knowledge graphs and their prominent applications across various business scenarios. Firstly, employing bibliometric methods, the aim is to uncover the developmental trends and current research hotspots within the domain of enterprise risk knowledge graphs. In the succeeding section, systematically delineate the technical methods for knowledge extraction and fusion in the standardized construction process of enterprise risk knowledge graphs. Objectively comparing and summarizing the strengths and weaknesses of each method, we provide recommendations for addressing the existing challenges in the construction process. Subsequently, categorizing the applied research of enterprise risk knowledge graphs based on research hotspots and risk category standards, and furnishing a detailed exposition on the applicability of technical routes and methods. Finally, the future research directions that still need to be explored in enterprise risk knowledge graphs were discussed, and relevant improvement suggestions were proposed. Practitioners and researchers can gain insights into the construction of technical theories and practical guidance of enterprise risk knowledge graphs based on this foundation.}
}
@article{LIANG2024103805,
title = {Candidate-Heuristic In-Context Learning: A new framework for enhancing medical visual question answering with LLMs},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103805},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103805},
url = {https://www.sciencedirect.com/science/article/pii/S030645732400164X},
author = {Xiao Liang and Di Wang and Haodi Zhong and Quan Wang and Ronghan Li and Rui Jia and Bo Wan},
keywords = {Medical Visual Question Answering, In-Context Learning, Large language models, Knowledge-based visual question answering, Multi-modal learning},
abstract = {Medical Visual Question Answering (MedVQA) is designed to answer natural language questions related to medical images. Existing methods largely adopting the cross-modal pre-training and fine-tuning paradigm, face limitations in accuracy due to data scarcity and insufficient incorporation of extensive medical knowledge. Drawing inspiration from the Knowledge-Based Visual Question Answering (KB-VQA) domain, which leverages Large Language Models (LLMs) and external knowledge bases, we introduce the Candidate-Heuristic In-Context Learning (CH-ICL) framework, a novel approach that leverages LLMs augmented with external knowledge to directly enhance existing MedVQA models. Specifically, we collect a pathology terminology dictionary from a public digital pathology library as an external knowledge base and use it to train a knowledge scope discriminator, which helps identify the knowledge scope required to answer a question. Then, we employ existing MedVQA models to provide reliable answer candidates along with their confidence scores. Finally, the knowledge scope and candidates, combined with retrieved in-context exemplars, are aggregated into prompts for heuristically guiding LLMs in answer generation. Experimental results on the PathVQA, VQA-RAD, and SLAKE public benchmarks show state-of-the-art performance, with improvements of 1.91%, 1.88%, and 2.17% respectively over the baseline. Code and dataset are available at https://github.com/ecoxial2007/CH-ICL.}
}
@article{LIMA2025103263,
title = {ULKB Logic: A HOL-based framework for reasoning over knowledge graphs},
journal = {Science of Computer Programming},
volume = {242},
pages = {103263},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2025.103263},
url = {https://www.sciencedirect.com/science/article/pii/S0167642325000024},
author = {Guilherme Lima and Alexandre Rademaker and Rosario Uceda-Sosa},
keywords = {HOL, Python, Wikidata, SPARQL, MRS, NLP},
abstract = {ULKB Logic is an open-source framework written in Python for reasoning over knowledge graphs. It provides an interactive theorem prover-like environment equipped with a higher-order language similar to the one used by HOL Light. The main goal of ULKB Logic is to ease the construction of applications that combine state-of-the-art computational logic tools with the knowledge available in knowledge graphs, such as Wikidata. To this end, the framework provides APIs for fetching statements from SPARQL endpoints and operating over the constructed theories using automated theorem provers and SMT solvers (such as the E prover and Z3). In this paper, we describe the design and implementation of ULKB Logic, present its interfaces for querying knowledge graphs and for calling external provers, and discuss a use case of commonsense reasoning in which ULKB Logic is used as the target logic for representing the semantics of English sentences.}
}
@article{PHAM2024109517,
title = {How rationals boost textual entailment modeling: Insights from large language models},
journal = {Computers and Electrical Engineering},
volume = {119},
pages = {109517},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109517},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624004440},
author = {Duc-Huy Pham and Tung Le and Huy Tien Nguyen},
keywords = {Rationale, Distillation, Large language model, Textual entailment},
abstract = {This study introduces an innovative methodology for rationale-based distillation in textual entailment. Central to our methodology is the use of diverse and deep rationale types generated by large language models, eliminating the need for explicit feature engineering between text-hypothesis pairs. Through extensive experimentation, we demonstrate the effectiveness of our rationale-enhanced distillation process, underpinned by a comprehensive study of rationales. Remarkably, our model, which utilizes the most impactful rationales and operates with 795 times fewer parameters, exhibits competitive performance, especially in contexts limited by resource availability. Specifically, our model significantly surpasses RoBERTa, FLAN-T5 Large, and GPT-3.5 in performance. Our findings underscore the potential of rationale-based approaches, which improve textual entailment modeling and pave the way for future research. These techniques could be applied to other areas of NLP and beyond. This study’s contributions are poised to benefit researchers and practitioners seeking to leverage the power of large language models in augmenting reasoning capabilities with rationales. The novelty of our approach lies in its ability to deliver high performance with significantly fewer computational resources, making it a valuable advancement in the field.}
}
@article{VIDAL2025100856,
title = {Integrating Knowledge Graphs with Symbolic AI: The Path to Interpretable Hybrid AI Systems in Medicine},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100856},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100856},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000428},
author = {Maria-Esther Vidal and Yashrajsinh Chudasama and Hao Huang and Disha Purohit and Maria Torrente},
keywords = {Knowledge Graphs, Neuro-symbolic systems, Semantic Data Management, Valid link prediction, Counterfactual prediction, KG-based applications},
abstract = {Knowledge Graphs (KGs) are graph-based structures that integrate heterogeneous data, capture domain knowledge, and enable explainable AI through symbolic reasoning. This position paper examines the challenges and research opportunities in integrating KGs with neuro-symbolic AI, highlighting their potential to enhance explainability, scalability, and context-aware reasoning in hybrid AI systems. Using a lung cancer use case, we illustrate how hybrid approaches address tasks such as link prediction—uncovering hidden relationships in medical data—and counterfactual reasoning—analyzing alternative scenarios to understand causal factors. The discussion is framed around TrustKG, which demonstrates how constraint validation, causal reasoning, and user-centric communication can support transparent and reliable decision-making. Additionally, we identify current limitations of KGs, including gaps in knowledge coverage, evolving data integration challenges, and the need for improved usability and impact assessment. These insights are not limited to healthcare but extend to other domains like energy, manufacturing, and mobility, showcasing the broad applicability of KGs. Finally, we propose research directions to unlock their full potential in building robust, transparent, and widely adopted real-world applications.}
}
@article{FENWICK2023105892,
title = {Originality and the future of copyright in an age of generative AI},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105892},
year = {2023},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2023.105892},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923001024},
author = {Mark Fenwick and Paulius Jurcys},
keywords = {AI, Generative AI, Copyright, Creativity, Originality, Artificial intelligence, Data, Feist, David Guetta, ChatGPT, Input, Output, Machine Learning, Authorship, Intellectual Property},
abstract = {This paper takes the occasion of French DJ David Guetta's use of generative AI tools to create lyrics and a voice in the style of Eminem, which he then used in one of his concerts, as the basis for an exploration of the shifting meaning of creativity and originality in the age of generative AI. Our main contention is that the Guetta form of creativity with generative AI tools differs in certain important respects from what has come before. The paper describes an iterative, dynamic process of conception, prompting, generation, refining, and deployment to characterise creativity in this context. Nevertheless, we contend that copyright – specifically the concept of originality as articulated in US federal law – is a sufficiently durable legal mechanism that can manage these new cultural forms, and that the two basic requirements of modern copyright law (a tangible medium of expression and a modest degree of creativity) remain relevant in identifying the scope of legal protection. The paper argues that the David Guetta story reveals something more general about creativity in a digital age, namely that while hybrid-networked (i.e., human – corporate – machine) creators have always created hybrid-networked cultural forms (i.e., creations that blend human and technology-constituted elements), such hybridity becomes increasingly visible and complex in the context of a new world of generative AI. At the very least, earlier – and influential – models of creativity as human-driven involving creation ex nihilo become harder to sustain in a new age of generative AI. But this does not mean copyright or notions of originality are redundant or that copyright law cannot accommodate Guetta and other cases. Such an account seems important as it challenges the hegemonic and reductive view that AI “generates” artistic works autonomously and avoids reducing the copyright issues raised by such creative works to the related but distinct question of whether learning models rely on copyrighted data. As such, copyright law should remain an important mechanism to facilitate genuine creators who are using AI systems in innovative and unique ways to push the boundaries of their creativity.}
}
@article{MA2025125366,
title = {Historical Trends and Normalizing Flow for One-shot Temporal Knowledge Graph Reasoning},
journal = {Expert Systems with Applications},
volume = {260},
pages = {125366},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125366},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424022334},
author = {Ruixin Ma and Longfei Wang and Huinan Wu and Buyun Gao and Xiaoru Wang and Liang Zhao},
keywords = {One-shot, Temporal knowledge graphs, Normalizing flow, Historical trends},
abstract = {Temporal Knowledge Graphs (TKGs) have garnered significant scholarly interest for their ability to dynamically represent the evolution of real-world events. However, the presence of the long tail effect in knowledge graphs necessitates the exploration of one-shot temporal knowledge graph reasoning. Concurrently, the issues of out-of-distribution and overfitting heavily impact the performance of models. Empirical evidence from real-world cases has demonstrated the significance of historical trend information in accurately predicting future events, and normalizing flows effectively address out-of-distribution and overfitting challenges. Thus, this paper proposes a novel approach named Historical Trends and Normalizing Flow for One-shot Temporal Knowledge Graph Reasoning (HNOT). Specifically, HNOT combine normalizing flows with a historical information aggregator to obtain more complex distributions, thereby resolving out-of-distribution and overfitting problems and capturing global representations of TKGs. In contrast to existing models that focus solely on entity and relationship modeling, we have introduced a novel historical trend information aggregator. This aggregator is designed to extract trend information from temporal knowledge subgraphs. By employing trend information as the primary modeling object, it facilitates the exploration of trend variations at different timestamps. This approach serves as an auxiliary source of global information. Subsequently, we integrate these two distinct types of information to achieve predictions of future events. Experimental results demonstrate the superiority of the proposed method over state-of-the-art baselines on three benchmark datasets.}
}
@article{LIU2025102900,
title = {Knowledge extraction for additive manufacturing process via named entity recognition with LLMs},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {93},
pages = {102900},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102900},
url = {https://www.sciencedirect.com/science/article/pii/S073658452400187X},
author = {Xuan Liu and John Ahmet Erkoyuncu and Jerry Ying Hsi Fuh and Wen Feng Lu and Bingbing Li},
keywords = {Domain knowledge management, Named entity recognition, Retrieval augmented generation, Large Language Models, Additive manufacturing, Fused deposition modeling},
abstract = {This paper proposes a novel NER framework, leveraging the advanced capabilities of Large Language Models (LLMs), to address the limitations of manually defined taxonomy. Our framework integrates the expert knowledge internalized in both academic materials and LLMs through retrieval-augmented generation (RAG) to automatically customize taxonomies for specific manufacturing processes and adopts two distinct strategies of using LLMs — In-Context Learning (ICL) and fine-tuning to complete manufacturing NER tasks with minimal training data. We demonstrate the framework efficiency through its superior ability to define precise taxonomies, identify and classify process-level entities related to the most popular additive manufacturing process fused deposition modeling (FDM) as case study, achieving a high F1 score of 0.9192.}
}
@article{ZHENG202464,
title = {Construction of microgravity biological knowledge graph and its applications in anti-osteoporosis drug prediction},
journal = {Life Sciences in Space Research},
volume = {41},
pages = {64-73},
year = {2024},
issn = {2214-5524},
doi = {https://doi.org/10.1016/j.lssr.2024.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S2214552424000142},
author = {Yu-Han Zheng and Guan-Jing Pan and Yuan Quan and Hong-Yu Zhang},
keywords = {Bone loss, Drug repurposing, Knowledge graph embedding, Chinese herbal medicine, Western medicine},
abstract = {Microgravity in the space environment can potentially have various negative effects on the human body, one of which is bone loss. Given the increasing frequency of human space activities, there is an urgent need to identify effective anti-osteoporosis drugs for the microgravity environment. Traditional microgravity experiments conducted in space suffer from limitations such as time-consuming procedures, high costs, and small sample sizes. In recent years, the in-silico drug discovery method has emerged as a promising strategy due to the advancements in bioinformatics and computer technology. In this study, we first collected a total of 184,915 literature articles related to microgravity and bone loss. We employed a combination of dependency path extraction and clustering techniques to extract data from the text. Afterwards, we conducted data cleaning and standardization to integrate data from several sources, including The Global Network of Biomedical Relationships (GNBR), Curated Drug–Drug Interactions Database (DDInter), Search Tool for Interacting Chemicals (STITCH), DrugBank, and Traditional Chinese Medicines Integrated Database (TCMID). Through this integration process, we constructed the Microgravity Biology Knowledge Graph (MBKG) consisting of 134,796 biological entities and 3,395,273 triplets. Subsequently, the TransE model was utilized to perform knowledge graph embedding. By calculating the distances between entities in the model space, the model successfully predicted potential drugs for treating osteoporosis and microgravity-induced bone loss. The results indicate that out of the top 10 ranked western medicines, 7 have been approved for the treatment of osteoporosis. Additionally, among the top 10 ranked traditional Chinese medicines, 5 have scientific literature supporting their effectiveness in treating bone loss. Among the top 20 predicted medicines for microgravity-induced bone loss, 15 have been studied in microgravity or simulated microgravity environments, while the remaining 5 are also applicable for treating osteoporosis. This research highlights the potential application of MBKG in the field of space drug discovery.}
}
@article{QI2025105873,
title = {Linking geo-models for geomorphological classification using knowledge graphs},
journal = {Computers & Geosciences},
volume = {196},
pages = {105873},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2025.105873},
url = {https://www.sciencedirect.com/science/article/pii/S0098300425000238},
author = {Yanmin Qi and Yunqiang Zhu and Shu Wang and Yutao Zhong and Stuart Marsh and Amin Farjudian and Heshan Du},
keywords = {Knowledge graph, Geographic computation, Geographic model, Geomorphological classification},
abstract = {Geographic computation is an important process in geographic information systems to detect, predict, and simulate geographic entities, events, and phenomena, which is performed through a series of geographic models over geographic data. However, selecting and sequencing appropriate models is challenging for users with limited knowledge. To automate the process of linking models into workflows, a knowledge graph-based approach is proposed. In this approach, the first part is to construct a knowledge graph that integrates knowledge from geographic models and domain experts. Then, an algorithm is designed to assist the constructed knowledge graph in automating model linking. This paper takes the geomorphological classification of the Hengduan Mountains in China as a case study, which geomorphological classification maps are generated by performing querying and computing through the geomorphological classification knowledge graph. Experimental results demonstrate that the proposed knowledge graph-based approach links the models into workflows automatically and generates reliable classification results.}
}
@article{IJEBU2025112551,
title = {Soft cosine and extended cosine adaptation for pre-trained language model semantic vector analysis},
journal = {Applied Soft Computing},
volume = {169},
pages = {112551},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112551},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013255},
author = {Funebi Francis Ijebu and Yuanchao Liu and Chengjie Sun and Patience Usoro Usip},
keywords = {Transformer-based model, Large language model, Vector space model, Transfer learning, Semantic textual similarity},
abstract = {Semantic textual analysis is a natural language processing task that has enjoyed several research contributions towards solving diverse real-life problems. Vector comparison is a core subtask in semantic textual similarity analysis. A plethora of solutions including recent state-of-the-art transformer-based pre-trained language models for transfer learning have focused on using only cosine similarity for embedding evaluation in downstream tasks and ignored other vector comparison methods. To investigate the relative performance of some such ignored measures, this work proposes novel adaptations for soft cosine and extended cosine vector measures. We investigate their performance against the conventional cosine measure, distance-weighted cosine, vector similarity measure, negative Manhattan, and Euclidean distances on downstream semantic textual similarity tasks, under same conditions, for the first time in literature. Adopting transformer-based Universal sentence encoder, SBERT, SRoBERTa, SimCSE, and ST5 for text encoding; the performances of the adapted measures are evaluated on diverse real world datasets using Pearson, Spearman, accuracy and F1 evaluation metrics. Results obtained show that the adapted measures significantly surpass previously reported state-of-the-art cosine similarity-based correlations in several test cases considered.}
}
@article{CHEN2024e29048,
title = {A hyper-knowledge graph system for research on AI ethics cases},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e29048},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29048},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024050795},
author = {Chuan Chen and Yu Feng and Mengyi Wei and Zihan Liu and Peng Luo and Shengkai Wang and Liqiu Meng},
keywords = {AI ethics, Hyper-knowledge graph system, Case-oriented ontological model, Knowledge-based system, Visual analytics},
abstract = {Current studies on the artificial intelligence (AI) ethics focus either on very broad guidelines or on a very special domain. Therefore, the research outcome can hardly be converted into actionable measures or transferred to other domains. Potential correlations between various cases of AI ethics at different granularity levels are unexplored. To overcome these deficiencies, the authors designed a case-oriented ontological model (COOM) and a hyper-knowledge graph system (HKGS) for the research of collected AI ethics cases. COOM describes criteria for modelling cases by attributes from three perspectives: event attributes, relational attributes, and positional attributes on the value chain. Based on it, HKGS stores the correlation between cases as knowledge and allows advanced visual analysis. The correlations between cases and their dynamic changes on value chain can be observed and explored. In HKGS's implementation part, one of the collected ethics cases is used as an example to demonstrate how to generate a hyper-knowledge graph and to visually analyze it. The authors also anticipated how different practitioners of AI ethics, can achieve the desired outputs from HKGS in their diverse scenarios.}
}
@article{GHOLAMIDASTGERDI2024103040,
title = {SSKG: Subject stream knowledge graph, a new approach for event detection from text},
journal = {Ain Shams Engineering Journal},
volume = {15},
number = {12},
pages = {103040},
year = {2024},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2024.103040},
url = {https://www.sciencedirect.com/science/article/pii/S2090447924004155},
author = {Pejman Gholami-Dastgerdi and Mohammad-Reza Feizi-Derakhshi and Pedram Salehpour},
keywords = {Text stream, Event detection, Stream graph, Subject stream, Knowledge graph},
abstract = {Events play a crucial role in shaping societal meaning and discourse. They are recorded in text data streams from social networks or online sources and analyzing them is vital for predicting and managing future occurrences. Methods for event detection from text data streams have been developed to enhance accuracy and efficiency in analyzing large datasets. This study focuses on events published as text data streams in Telegram. Two main perspectives are considered for better results: firstly, major events can overshadow the detection of smaller ones with fewer narrative instances. Secondly, due to diverse narratives surrounding each event, establishing meaningful connections between narrative sets is essential. In this research, two new concepts, “subject stream” and “stream graph,” are introduced. The subject stream aims to process topics to mitigate the impact of bursty events on detecting others. The stream graph models data stream and identifies various narratives associated with each event for better identification and categorization. Combining these approaches accurately represents events and their characteristics in the real world. Implementing the system in three versions demonstrates the effectiveness of using the “stream graph” alongside the “subject stream,” resulting in improved execution speed and accuracy. Evaluation results show a 6% enhancement in topic recall.}
}
@article{WANG2023100033,
title = {R2GenGPT: Radiology Report Generation with frozen LLMs},
journal = {Meta-Radiology},
volume = {1},
number = {3},
pages = {100033},
year = {2023},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2023.100033},
url = {https://www.sciencedirect.com/science/article/pii/S2950162823000334},
author = {Zhanyu Wang and Lingqiao Liu and Lei Wang and Luping Zhou},
keywords = {Radiology report generation, Large language models, LLAMA},
abstract = {Large Language Models (LLMs) have consistently showcased remarkable generalization capa-bilities when applied to various language tasks. Nonetheless, harnessing the full potential of LLMs for Radiology Report Generation (R2Gen) still presents a challenge, stemming from the inherent disparity in modality between LLMs and the R2Gen task. To bridge this gap effectively, we propose R2GenGPT, which is a novel solution that aligns visual features with the word embedding space of LLMs using an efficient visual alignment module. This innovative approach empowers the previously static LLM to seamlessly integrate and process image information, marking a step forward in optimizing R2Gen performance. R2GenGPT offers the following benefits. First, it attains state-of-the-art (SOTA) performance by training only the lightweight visual alignment module while freezing all the parameters of LLM. Second, it exhibits high training efficiency, as it requires the training of an exceptionally minimal number of parameters while achieving rapid convergence. By employing delta tuning, our model only trains 5 ​M parameters (which constitute just 0.07 ​% of the total parameter count) to achieve performance close to the SOTA levels. Our code is available at https://github.com/wang-zhanyu/R2GenGPT.}
}
@article{RONG2024111297,
title = {KGCDP-T: Interpreting knowledge graphs into text by content ordering and dynamic planning with three-level reconstruction},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111297},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111297},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123010456},
author = {Huan Rong and Shengjie Sun and Tinghuai Ma and Di Jin and Victor S. Sheng},
keywords = {Knowledge graph interpretation, KG-to-text, Natural language generation, Neural text generation},
abstract = {Knowledge graph-to-text (KG-to-text) interpretation is employed to interpret given KG into semantically coherent and logically reasonable text to enhance the applicability of KG in more natural language generation (NLG) scenarios, such as search engines and text-dialog systems. Unfortunately, existing relevant research suffers from the semantic gap between structural knowledge graphs and unstructured text. To overcome this challenge, in this paper, we propose a novel pipeline-based Knowledge Graph interpreting model constructed by Content Ordering and Dynamic Planning with Three-Level Reconstruction (KGCDP-T). Specifically, the first “pipe” Content Ordering converts the given KG into several triple-groups and then into an ordered triple-sequence to plan the ordering by which the “content” in the given knowledge graph should be interpreted. Next, an entity-sequence is derived from the above ordered triple-sequence, where the second “pipe” Dynamic Planning captures the context shifting in the entity-sequence via Memory Network. In this way, the entity-sequence-level and memory-level contexts are learned and fused to generate the interpreted-text with more context-adaptive tokens. Moreover, the Three-Level Reconstruction mechanism is incorporated to capture the critical features transferred among the triple-groups, the ordered triple-sequence, the generated text-sequence and the given KG. The experimental results indicate that our proposed KGCDP-T can achieve the overall best KG-interpretation performance on content integrity (reflected by a 7.69% average improvement in Coverage), sentence fluency (reflected by 5.98% and 6.00% average improvements in BLEU and ROUGE-L, respectively) and logic coherency (reflected by 6.10% and 9.18% average improvements in METEOR and Chrf++, respectively) when compared with state-of-art KG-to-text models.}
}
@article{CHEN2024123320,
title = {Optimizing automated compliance checking with ontology-enhanced natural language processing: Case in the fire safety domain},
journal = {Journal of Environmental Management},
volume = {371},
pages = {123320},
year = {2024},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2024.123320},
url = {https://www.sciencedirect.com/science/article/pii/S0301479724033061},
author = {Yian Chen and Huixian Jiang},
keywords = {Ontology automatic construction, Automated compliance checking (ACC), Named entity recognition, Relationship extraction, Pre-trained language model, Rule interpretation},
abstract = {The fire safety compliance checking (FSCC) plays a crucial role in ensuring the quality of fire engineering design and eliminating inherent fire hazards. It requires an objective and rational interpretation of fire regulations. However, the texts of fire regulations are filled with numerous rules related to spatial limitations, which pose a significant challenge in interpreting them. The current method of interpreting these rules mostly relies on manual translation, which is not efficient. To address this issue, this study proposes an innovative automated framework for interpreting rules by combining ontology technology with natural language processing (NLP). Through the utilization of pre-trained language models (PLMs), concepts and relationships are extracted from sentences, a domain-specific ontology is established, spatial knowledge is transformed into language-agnostic tree structures based on the ontology, and the semantic components of spatial relationships are extracted. The tree structure is then mapped to logical clauses based on semantic consistency, thereby improving the efficiency of interpretation. Experimental results demonstrate that the architecture achieves an F1 score of 86.27 for entity extraction and 81.81 for spatial relationship joint extraction tasks, with an accuracy of 96.26% in the formalization of logical rules, highlighting its proficiency in automatically interpreting fire spatial rules. This study offers technical support to enhance public understanding of fire safety management and fire prevention predictions, thereby promoting the intelligent management of the building safety environment.}
}
@article{ARSLAN20243781,
title = {A Survey on RAG with LLMs},
journal = {Procedia Computer Science},
volume = {246},
pages = {3781-3790},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924021860},
author = {Muhammad Arslan and Hussam Ghanem and Saba Munawar and Christophe Cruz},
keywords = {Large Language Models (LLMs), Natural Language Processing (NLP), Retrieval-Augmented Generation (RAG), Text generation, Digital transformation},
abstract = {In the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain competitive and foster growth. Large Language Models (LLMs) have emerged as game-changers across industries, revolutionizing various sectors by harnessing extensive text data to analyze and generate human-like text. Despite their impressive capabilities, LLMs often encounter challenges when dealing with domain-specific queries, potentially leading to inaccuracies in their outputs. In response, Retrieval-Augmented Generation (RAG) has emerged as a viable solution. By seamlessly integrating external data retrieval into text generation processes, RAG aims to enhance the accuracy and relevance of the generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG, overlooking a comprehensive exploration of its applications. This paper seeks to address this gap by providing a thorough review of RAG applications, encompassing both task-specific and discipline-specific studies, while also outlining potential avenues for future research. By shedding light on current RAG research and outlining future directions, this review aims to catalyze further exploration and development in this dynamic field, thereby contributing to ongoing digital transformation efforts.}
}
@article{CONSTANTINOU2025126120,
title = {Using GPT-4 to guide causal machine learning},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126120},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126120},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029877},
author = {Anthony C. Constantinou and Neville K. Kitson and Alessio Zanga},
keywords = {Bayesian networks, Causal discovery, ChatGPT, Directed acyclic graphs, Knowledge graphs, LLMs, Structure learning},
abstract = {Since its introduction to the public, ChatGPT has had an unprecedented impact. While some experts praised AI advancements and highlighted their potential risks, others have been critical about the accuracy and usefulness of Large Language Models (LLMs). In this paper, we are interested in the ability of LLMs to identify causal relationships. We focus on the well-established GPT-4 (Turbo) and evaluate its performance under the most restrictive conditions, by isolating its ability to infer causal relationships based solely on the variable labels without being given any other context by humans, demonstrating the minimum level of effectiveness one can expect when it is provided with label-only information. We show that questionnaire participants judge the GPT-4 graphs as the most accurate in the evaluated categories, closely followed by knowledge graphs constructed by domain experts, with causal Machine Learning (ML) far behind. We use these results to highlight the important limitation of causal ML, which often produces causal graphs that violate common sense, affecting trust in them. However, we show that pairing GPT-4 with causal ML overcomes this limitation, resulting in graphical structures learnt from real data that align more closely with those identified by domain experts, compared to structures learnt by causal ML alone. Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.}
}
@article{SONG2024114983,
title = {Ontology-assisted GPT-based building performance simulation and assessment: Implementation of multizone airflow simulation},
journal = {Energy and Buildings},
volume = {325},
pages = {114983},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114983},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824010995},
author = {Jihwan Song and Sungmin Yoon},
keywords = {GPT, ChatGPT, Large language model (LLM), Building performance simulation (BPS), Building performance, Digital twins, Artificial intelligence, CONTAM},
abstract = {Building performance simulation (BPS) is crucial for building performance assessments across its lifecycle. However, the complexity of buildings and the iterative nature of simulation poses challenges, leading to high costs and low values. Previous studies focused on simplification, but did not fully utilize advanced simulation engines. Despite recent advancements, there is a lack of research on leveraging artificial intelligence (AI), specifically generative pre-trained transformer (GPT), for BPS. Therefore, this study proposes a GPT-based BPS system, enhancing simulation efficiency and value by integrating simulation engines and advanced data analytics in the GPT environment. The ontology for GPT-based BPS is also developed to enable comprehensive, reliable, informative BPS environments. Based on this framework, case studies were conducted for GPT-based multizone airflow network simulation in a high-rise residential building using CONTAM software. They demonstrate GPT’s capabilities in retrieving simulation data, visualizing results with data mining, answering questions based on building knowledge, checking compliance with design guidelines, and proposing design alternatives. Finally, this study emphasizes expert interventions with ontological engineering informatics to utilize strictly structured BPS engines.}
}
@article{ABUSALIH2024e25383,
title = {A systematic literature review of knowledge graph construction and application in education},
journal = {Heliyon},
volume = {10},
number = {3},
pages = {e25383},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e25383},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024014142},
author = {Bilal Abu-Salih and Salihah Alotaibi},
keywords = {Knowledge graphs, Knowledge graph construction, Education, Learning, Systematic literature review, Survey},
abstract = {In the dynamic landscape of modern education, the search for improved pedagogical methods, enriched learning experiences, and empowered educators remains a perpetual pursuit. In recent years, a remarkable technological innovation has asserted its dominance in education: Knowledge Graphs (KGs). These structured representations of knowledge are increasingly proving to be indispensable tools, fostering advancements driven by the growing recognition of their essential role in enriching personalised learning, curriculum design, concept mapping, and educational content recommendation systems. In this paper, a systematic literature review (SLR) has been conducted to comprehensively examine KG construction methodologies and their applications across five key domains in education. In each examined study, we highlight the specific KG functionalities, knowledge extraction techniques, knowledge base characteristics, resource requirements, evaluation criteria, and limitations. This paper distinguishes itself by offering a broad overview of KGs in education, analyzing state-of-the-art methodologies, and identifying research gaps and limitations, paving the way for future advancements.}
}
@article{ROMANO2024,
title = {The Alzheimer’s Knowledge Base: A Knowledge Graph for Alzheimer Disease Research},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/46777},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124001857},
author = {Joseph D Romano and Van Truong and Rachit Kumar and Mythreye Venkatesan and Britney E Graham and Yun Hao and Nick Matsumoto and Xi Li and Zhiping Wang and Marylyn D Ritchie and Li Shen and Jason H Moore},
keywords = {Alzheimer disease, knowledge graph, knowledge base, artificial intelligence, drug repurposing, drug discovery, open source, Alzheimer, etiology, heterogeneous graph, therapeutic targets, machine learning, therapeutic discovery},
abstract = {Background
As global populations age and become susceptible to neurodegenerative illnesses, new therapies for Alzheimer disease (AD) are urgently needed. Existing data resources for drug discovery and repurposing fail to capture relationships central to the disease’s etiology and response to drugs.
Objective
We designed the Alzheimer’s Knowledge Base (AlzKB) to alleviate this need by providing a comprehensive knowledge representation of AD etiology and candidate therapeutics.
Methods
We designed the AlzKB as a large, heterogeneous graph knowledge base assembled using 22 diverse external data sources describing biological and pharmaceutical entities at different levels of organization (eg, chemicals, genes, anatomy, and diseases). AlzKB uses a Web Ontology Language 2 ontology to enforce semantic consistency and allow for ontological inference. We provide a public version of AlzKB and allow users to run and modify local versions of the knowledge base.
Results
AlzKB is freely available on the web and currently contains 118,902 entities with 1,309,527 relationships between those entities. To demonstrate its value, we used graph data science and machine learning to (1) propose new therapeutic targets based on similarities of AD to Parkinson disease and (2) repurpose existing drugs that may treat AD. For each use case, AlzKB recovers known therapeutic associations while proposing biologically plausible new ones.
Conclusions
AlzKB is a new, publicly available knowledge resource that enables researchers to discover complex translational associations for AD drug discovery. Through 2 use cases, we show that it is a valuable tool for proposing novel therapeutic hypotheses based on public biomedical knowledge.}
}
@article{QU20243583,
title = {A Review of Knowledge Graph in Traditional Chinese Medicine: Analysis, Construction, Application and Prospects},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {3583-3616},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.055671},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008294},
author = {Xiaolong Qu and Ziwei Tian and Jinman Cui and Ruowei Li and Dongmei Li and Xiaoping Zhang},
keywords = {Systematic review, traditional Chinese medicine, knowledge graph, deep learning, medical applications},
abstract = {As an advanced data science technology, the knowledge graph systematically integrates and displays the knowledge framework within the field of traditional Chinese medicine (TCM). This not only contributes to a deeper comprehension of traditional Chinese medical theories but also provides robust support for the intelligent decision systems and medical applications of TCM. Against this backdrop, this paper aims to systematically review the current status and development trends of TCM knowledge graphs, offering theoretical and technical foundations to facilitate the inheritance, innovation, and integrated development of TCM. Firstly, we introduce the relevant concepts and research status of TCM knowledge graphs. Secondly, we conduct an in-depth analysis of the challenges and trends faced by key technologies in TCM knowledge graph construction, such as knowledge representation, extraction, fusion, and reasoning, and classifies typical knowledge graphs in various subfields of TCM. Next, we comprehensively outline the current medical applications of TCM knowledge graphs in areas such as information retrieval, diagnosis, question answering, recommendation, and knowledge mining. Finally, the current research status and future directions of TCM knowledge graphs are concluded and discussed. We believe this paper contributes to a deeper understanding of the research dynamics in TCM knowledge graphs and provides essential references for scholars in related fields.}
}
@article{FABRE2024264,
title = {Knowledge Graphs – The Future of Integration in CRIS Systems for Uses of Assistance to Scientific Reasoning},
journal = {Procedia Computer Science},
volume = {249},
pages = {264-279},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032836},
author = {Renaud Fabre and Otmane Azeroual},
keywords = {Knowledge graphs, data integration, knowledge representation, CRIS systems, semantic data model, scientific uses, comprehensive information},
abstract = {Knowledge graphs (KGs) are gaining prominence for their efficacy in data integration and knowledge representation within Current Research Information Systems (CRIS) Systems. By employing a semantic data model to represent entities, attributes, and their relationships, they prove versatile across scientific applications. In the era of Science platformization, particularly within CRIS systems, KGs serve to amalgamate diverse data sources and formats, facilitating the creation of interconnected data models. This enables stakeholders to access comprehensive, consistent information pertinent to their endeavors. This paper examines the pivotal roles of KGs in current and future data integration within CRIS systems, emphasizing their contributions to scientific reasoning. While their benefits include flexible knowledge modeling, support for semantic queries, and interoperability with various data sources, they face systemic limitations, particularly in methodological and technological aspects, hindering classical scientific investigations. The paper underscores the necessity for novel approaches to address these limitations, offering insights, use cases, and best practices for implementing KGs in CRIS systems. This paves the way for research institutions and scientific organizations to enhance their data analytics capabilities and support scientific reasoning effectively.}
}
@article{JALDI2025100857,
title = {Education in the era of Neurosymbolic AI},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100857},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100857},
url = {https://www.sciencedirect.com/science/article/pii/S157082682400043X},
author = {Chris Davis Jaldi and Eleni Ilkou and Noah Schroeder and Cogan Shimizu},
keywords = {Education, Knowledge graphs, Large language models, Neurosymbolic AI, Agents},
abstract = {Education is poised for a transformative shift with the advent of neurosymbolic artificial intelligence (NAI), which will redefine how we support deeply adaptive and personalized learning experiences. The integration of Knowledge Graphs (KGs) with Large Language Models (LLMs), a significant and popular form of NAI, presents a promising avenue for advancing personalized instruction via neurosymbolic educational agents. By leveraging structured knowledge, these agents can provide individualized learning experiences that align with specific learner preferences and desired learning paths, while also mitigating biases inherent in traditional AI systems. NAI-powered education systems will be capable of interpreting complex human concepts and contexts while employing advanced problem-solving strategies, all grounded in established pedagogical frameworks. In this paper, we propose a system that leverages the unique affordances of KGs, LLMs, and pedagogical agents – embodied characters designed to enhance learning – as critical components of a hybrid NAI architecture. We discuss the rationale for our system design and the preliminary findings of our work. We conclude that education in the era of NAI will make learning more accessible, equitable, and aligned with real-world skills. This is an era that will explore a new depth of understanding in educational tools.}
}
@article{LU2024109395,
title = {Heterogeneous propagation graph convolution network for a recommendation system based on a knowledge graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109395},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109395},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624015537},
author = {Jiawei Lu and Jiapeng Li and Wenhui Li and Junfeng Song and Gang Xiao},
keywords = {Recommendation system, Knowledge graph, Heterogeneous propagation, Attention mechanism, Graph convolution network, Subgraph},
abstract = {Recently, there has been a surge of interest in recommendation systems that leverage knowledge graphs, primarily because of their effectiveness in addressing sparsity and cold-start challenges inherent in collaborative filtering approaches. In most previous studies, researchers have focused on the way knowledge associations are encoded in knowledge graphs, but have not sufficiently highlighted the signals of collaboration that are implicit in the interaction between users and items. As a result, the learned embeddings do not provide a complete representation of the semantic information. In this paper, we describe a new model called a heterogeneous propagation graph convolution network for a recommendation system combined with a knowledge graph (HP-GCN). It adopts a heterogeneous propagation to generate user embedding representations, thereby combining encoded collaborative signals and auxiliary knowledge in knowledge graphs. Furthermore, we incorporate an attention mechanism to differentiate the contributions made by diverse neighbors as opposed to those made by users. Since most graph convolutions tend to suffer from over-smoothing when the number of convolutional layers increases, leading to insufficient utilization of high-order information, this paper uses an improved graph convolution strategy to generate item embeddings. This strategy has two different aggregation mechanisms embedding into different subgraphs, which can more fully utilize high-order information and mitigate the over-smoothing problem. Thus, we are able to efficiently prevent negative information originating from higher-order neighbors into the process of embedding learning. In extensive experiments, we applied HP-GCN to four large-scale real datasets for music, books, movies, and restaurants. The experimental outcomes revealed that HP-GCN generally surpassed the baseline methods in both recommendation accuracy and diversity, showing superior recommendation performance overall.}
}
@article{SIDDHARTH2024112410,
title = {Retrieval augmented generation using engineering design knowledge},
journal = {Knowledge-Based Systems},
volume = {303},
pages = {112410},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112410},
url = {https://www.sciencedirect.com/science/article/pii/S095070512401044X},
author = {L. Siddharth and Jianxi Luo},
keywords = {Knowledge graphs, Retrieval-augmented generation, Large-language models, Engineering design knowledge, Patent documents, Graph neural networks},
abstract = {Aiming to support Retrieval Augmented Generation (RAG) in the design process, we present a method to identify explicit, engineering design facts – {head entity:: relationship:: tail entity} from patented artefact descriptions. Given a sentence with a pair of entities (selected from noun phrases) marked in a unique manner, our method extracts their relationship that is explicitly communicated in the sentence. For this task, we create a dataset of 375,084 examples and fine-tune language models for relation identification (token classification task) and relation elicitation (sequence-to-sequence task). The token classification approach achieves up to 99.7% accuracy. Upon applying the method to a domain of 4,870 fan system patents, we populate a knowledge base of over 2.93 million facts. Using this knowledge base, we demonstrate how Large Language Models (LLMs) are guided by explicit facts to synthesise knowledge and generate technical and cohesive responses when sought out for knowledge retrieval tasks in the design process.}
}
@article{DU2023110996,
title = {A contrastive framework for enhancing Knowledge Graph Question Answering: Alleviating exposure bias},
journal = {Knowledge-Based Systems},
volume = {280},
pages = {110996},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110996},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123007463},
author = {Huifang Du and Xixie Zhang and Meng Wang and Yunwen Chen and Daqi Ji and Jun Ma and Haofen Wang},
keywords = {Question answering, Knowledge graph, Semantic parsing, Contrastive learning, Sampling augmentation},
abstract = {Current encoder–decoders for Knowledge Graph Question Answering (KGQA) commonly utilize teacher-forcing training to accelerate convergence. However, this training approach limits the model’s exposure to ground truths, resulting in exposure bias that hampers generalization performance during autoregressive inference. To alleviate the issue, we propose a contrastive framework that enables the model to access a variety of positive and negative examples, thereby enhancing generalization. Firstly, we introduce a sampling augmentation strategy to construct contrastive samples, which can ensure explicit semantic consistency of positive pairs and inconsistency of negative pairs. Secondly, we augment the training process by incorporating “hard” negatives to enhance the contrastive objective, along with augmented positives to improve the generation objective. Finally, we also sample multiple logical forms for each question during the inference to reduce the bias potential and train a contrastive ranking model to obtain the target logical form. We achieve improvements of 1.95% and 1% over the previous state-of-the-art methods on the KQA Pro and OVERNIGHT benchmarks, respectively. Furthermore, our approach obtains competitive results on the WebQSP dataset. These findings validate the efficacy of our contrastive framework for advancing KGQA performance.}
}
@article{ZHAO2024114879,
title = {Generating Java code pairing with ChatGPT},
journal = {Theoretical Computer Science},
volume = {1021},
pages = {114879},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114879},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524004961},
author = {Zelong Zhao and Nan Zhang and Bin Yu and Zhenhua Duan},
keywords = {Code generation, Large language models, ChatGPT, Prompt engineering, Iterative prompting},
abstract = {The Large Language Models (LLMs) like ChatGPT 3.5 have created a new era of automatic code generation. However, the existing research primarily focuses on generating simple code based on datasets (such as HumanEval, etc.). Most of approaches pay less attention to complex and practical code generation. Therefore, in this paper, we propose a new approach called “Xd-CodeGen” which can be used to generate large scale Java code. This approach is composed of four phases: requirement analysis, modeling, code generation, and code verification. In the requirement analysis phase, ChatGPT 3.5 is utilized to decompose and restate user requirements. To do so, a knowledge graph is developed to describe entities and their relationship in detail. Further, Propositional Projection Temporal Logic (PPTL) formulas are employed to define the properties of requirements. In the modeling phase, we use knowledge graphs to enhance prompts and generate UML class and activity diagrams for each sub-requirement using ChatGPT 3.5. In the code generation phase, based on established UML models, we make use of prompt engineering and knowledge graph to generate Java code. In the code verification phase, a runtime verification at code level approach is employed to verify generated Java code. Finally, we apply the proposed approach to develop a practical Java web project.}
}
@article{BIAN2024104703,
title = {Call for papers: Special issue on biomedical multimodal large language models − novel approaches and applications},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104703},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104703},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001217},
author = {Jiang Bian and Yifan Peng and Eneida Mendonca and Imon Banerjee and Hua Xu and Hong Sun and Ye Ye and Casey {Overby Taylor} and Anália {Maria Garcia Lourenço} and Alejandro {Rodríguez González} and Elena Tutubalina}
}
@article{PAEK2024S18,
title = {CO14 Profiling Adverse Events in Multiple Myeloma: Insights from Clinical Trials Via Large Language Models},
journal = {Value in Health},
volume = {27},
number = {6, Supplement },
pages = {S18},
year = {2024},
note = {ISPOR Abstracts 2024},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2024.03.103},
url = {https://www.sciencedirect.com/science/article/pii/S1098301524002183},
author = {H. Paek and K. Lee and S. Datta and LC. Huang and J. Higashi and N. Ofoegbu and L. He and B. Lin and J. Wang and X. Wang}
}
@article{YANG2024100465,
title = {ChatDiet: Empowering personalized nutrition-oriented food recommender chatbots through an LLM-augmented framework},
journal = {Smart Health},
volume = {32},
pages = {100465},
year = {2024},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2024.100465},
url = {https://www.sciencedirect.com/science/article/pii/S2352648324000217},
author = {Zhongqi Yang and Elahe Khatibi and Nitish Nagesh and Mahyar Abbasian and Iman Azimi and Ramesh Jain and Amir M. Rahmani},
keywords = {Large Language Model, Personalization, Explainability, Interactivity, Chatbots, Recommender systems, Causal reasoning, Nutrition, Food},
abstract = {The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The personal model leverages causal discovery and inference techniques to assess personalized nutritional effects for a specific user, whereas the population model provides generalized information on food nutritional content. The orchestrator retrieves, synergizes and delivers the output of both models to the LLM, providing tailored food recommendations designed to support targeted health outcomes. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessments, including a food recommendation test showcasing a 92% effectiveness rate, coupled with illustrative dialogue examples, underscore ChatDiet’s strengths in explainability, personalization, and interactivity.}
}
@article{BABAIHA2023100078,
title = {A natural language processing system for the efficient updating of highly curated pathophysiology mechanism knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {4},
pages = {100078},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2667318523000223},
author = {Negin Sadat Babaiha and Hassan Elsayed and Bide Zhang and Abish Kaladharan and Priya Sethumadhavan and Bruce Schultz and Jürgen Klein and Bruno Freudensprung and Vanessa Lage-Rupprecht and Alpha Tom Kodamullil and Marc Jacobs and Stefan Geissler and Sumit Madan and Martin Hofmann-Apitius},
keywords = {Knowledge graphs, Relation extraction, Natural language processing, Biomedical text mining, Biological expression language (BEL), Human brain pharmacome (HBP)},
abstract = {Background
Biomedical knowledge graphs (KG) have become crucial for describing biological findings in a structured manner. To keep up with the constantly changing flow of knowledge, their embedded information must be regularly updated with the latest findings. Natural language processing (NLP) has created new possibilities for automating this upkeep by facilitating information extraction from free text. However, due to annotated and labeled biomedical data limitations, the development of completely autonomous information extraction systems remains a substantial scientific and technological hurdle. This study aims to explore methodologies best suited to support the automatic extraction of causal relationships from biomedical literature with the aim of regular and rapid updating of disease-specific pathophysiology mechanism KGs.
Methods
Our proposed approach first searches and retrieves PubMed abstracts using the desired terms and keywords. The extension corpora are then passed through the NLP pipeline for automatic information extraction. We then identify triples representing cause-and-effect relationships and encode this content using the Biological Expression Language (BEL). Finally, domain experts perform an analysis of the completeness, relevance, accuracy, and novelty of the extracted triples.
Results
In our test scenario, which is focused on the KG regarding the phosphorylation of the Tau protein, our pipeline successfully contributed novel data, which was then subsequently used to update the KG leading to the identification of six additional upstream regulators of Tau phosphorylation.
Conclusion
Here, it is demonstrated that the NLP-based workflow we created is capable of rapidly updating pathophysiology mechanism graphs. As a result, production-scale, semi-automated updating of pre-existing, curated mechanism graphs is enabled.}
}
@article{LIMA2024108464,
title = {The new chassis in the flask: Advances in Vibrio natriegens biotechnology research},
journal = {Biotechnology Advances},
volume = {77},
pages = {108464},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108464},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024001587},
author = {Matthew Lima and Charandatta Muddana and Zhengyang Xiao and Anindita Bandyopadhyay and Pramod P. Wangikar and Himadri B. Pakrasi and Yinjie J. Tang},
keywords = {Biomanufacturing, Synthetic biology, Systems biology, Genome scale model, Knowledge graph, Large language model},
abstract = {Biotechnology has been built on the foundation of a small handful of well characterized and well-engineered organisms. Recent years have seen a breakout performer gain attention as a new entrant into the bioengineering toolbox: Vibrio natriegens. This review covers recent research efforts into making V. natriegens a biotechnology platform, using a large language model (LLM) and knowledge graph to expedite the literature survey process. Scientists have made advancements in research pertaining to the fundamental metabolic characteristics of V. natriegens, development and characterization of synthetic biology tools, systems biology analysis and metabolic modeling, bioproduction and metabolic engineering, and microbial ecology. Each of these subcategories has relevance to the future of V. natriegens for bioengineering applications. In this review, we cover these recent advancements and offer context for the impact they may have on the field, highlighting benefits and drawbacks of using this organism. From examining the recent bioengineering research, it appears that V. natriegens is on the precipice of becoming a platform bacterium for the future of biotechnology.}
}
@article{HERNANDEZRAMIREZ2024414,
title = {The Future End of Design Work: A Critical Overview of Managerialism, Generative AI, and the Nature of Knowledge Work, and Why Craft Remains Relevant},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {10},
number = {4},
pages = {414-440},
year = {2024},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000960},
author = {Rodrigo Hernández-Ramírez and João Batalheiro Ferreira},
keywords = {creativity, design work, generative artificial intelligence (GenAI), knowledge work, managerialism},
abstract = {This article examines the transformation of design work under the influence of managerialism and the rise of Generative Artificial Intelligence (GenAI). Drawing on John Maynard Keynes’s projections of technological unemployment and the evolving nature of work, it argues that despite advancements in automation, work has not diminished but rather devalued. Design, understood as a type of knowledge work, faces an apparent existential crisis. GenAI grows adept at mimicking the output of creative processes. The article explores how the fear of the end of design work fueled by the rise of GenAI is rooted in a misunderstanding of design work. This misunderstanding is driven by managerialism—an ideology that prioritizes efficiency and quantifiable outcomes over the intrinsic value of work. Managerialism seeks to instrumentalize and automate design, turning it into a controllable procedure to generate quantifiable creative outputs. The article argues why design work cannot be turned into a procedure and automated using GenAI. Advocates of these systems claim they enhance productivity and open new opportunities. However, evidence so far shows that flawed GenAI models produce disappointing outcomes while operating at a significant environmental cost. The article concludes by arguing for a robust theory of design—one that acknowledges the unique ontological and epistemic boundaries of design work and underscores why design cannot be reduced to a procedural output.}
}
@article{YANG2023101200,
title = {API comparison knowledge extraction via prompt-tuned language model},
journal = {Journal of Computer Languages},
volume = {75},
pages = {101200},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101200},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000102},
author = {Yangrui Yang and Yaping Zhu and Sisi Chen and Pengpeng Jian},
keywords = {Knowledge extraction, API entity, Semantic relation, Joint extraction},
abstract = {Application Programming Interfaces (APIs) are frequent in software engineering domain texts, such as API references and Stack Overflow. These APIs and the comparison knowledge between them are not only important for solving programming issues (e.g., question answering), but they are also organized into structured knowledge to support many software engineering tasks (e.g., API misuse detection). As a result, extracting API comparison knowledge (API entities and semantic relations) from texts is essential. Existing rule-based and sequence labeling-based approaches must manually enumerate all linguistic patterns or label a large amount of data. Therefore, they involve a significant labor overhead and are exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, we formulates heterogeneous API extraction and API relation extraction tasks as a sequence-to-sequence generation task. It proposes APICKnow, an API entity-relation joint extraction model based on the large language model. To improve our model’s performance and quick learning ability, we adopt the prompt learning method to stimulate APICKnow to recognize API entities and relations. We systematically evaluate APICKnow on a set of sentences from Stack Overflow. The experimental results show that APICKnow can outperform the state-of-the-art baselines, and APICKnow has a quick learning ability and strong generalization ability.}
}
@article{ZHENG2025126313,
title = {A topic model-based knowledge graph to detect product defects from social media data},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126313},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126313},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031804},
author = {Lu Zheng and Zhen He and Shuguang He},
keywords = {Defect detection, Knowledge graph, Social media data, Topic model},
abstract = {With the help of topic models, social media data offers valuable insights for manufacturers to detect product defects in the after-sales stage. However, topic models struggle with texts mentioning multiple defects or discussing them at a coarse granularity level. Low topic discrimination further limits the application of topic models in defect discovery. To address these problems, we introduce a topic model-based Defect Knowledge Graph (DKG) for accurate defect detection. Firstly, to address the topic-indiscriminative problem, we utilize a topic model named Defect Latent Dirichlet Allocation and an improved Gibbs sampling to extract defect information from multi-source data and construct DKG. Secondly, we establish the Product Component Knowledge Graph (PCKG) to identify multiple defects discussed at coarse granularity levels. Thirdly, with DKG and PCKG, we unveil product defects and related defect information from social media data. Case studies of automobiles and laptops are used for validation. Experimental results show that our method outperforms the state-of-the-art in product defect discovery and provides more comprehensive defect information, which facilitates manufacturers to take prompt remedial actions.}
}
@article{ZHU2025106995,
title = {Knowledge graph based question-answering model with subgraph retrieval optimization},
journal = {Computers & Operations Research},
volume = {177},
pages = {106995},
year = {2025},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2025.106995},
url = {https://www.sciencedirect.com/science/article/pii/S0305054825000231},
author = {Rui Zhu and Bo Liu and Qiuyu Tian and Ruwen Zhang and Shengxiang Zhang and Yanna Hu and Jiuxin Cao},
keywords = {Subgraph retrieval, Entity disambiguation, Intelligent question-answering},
abstract = {Knowledge graph-based question answering (QA) is a critical domain within natural language processing, aimed at delivering precise and efficient responses to user queries. Current research predominantly focuses on minimizing subgraph sizes to enhance the efficiency and compactness of the search space. However, natural language queries often exhibit ambiguities, and merely reducing subgraph sizes may overlook relevant answer entities. Additionally, redundant relationships among entities in the knowledge graph can adversely affect QA model performance. To address these limitations, this paper introduces a novel QA model that optimizes subgraph retrieval. The proposed model enhances entity linking and subgraph retrieval by leveraging contextual features from both questions and entities. It disambiguates entities using relevant contextual features and refines the search process through entity relation merging and entity ranking strategies. This methodology improves entity recognition and linking, reduces subgraph dimensions, and broadens answer coverage, resulting in substantial improvements in QA performance. Experimental results on the CCKS2019-CKBQA dataset demonstrate the modelś effectiveness, showing an average F1 score improvement of 2.99% over the leading baseline model. Furthermore, the model’s application in the field of ocean engineering underscores its practical utility and significance.}
}
@article{ALBAYRAK2025100409,
title = {Enhancing human phenotype ontology term extraction through synthetic case reports and embedding-based retrieval: A novel approach for improved biomedical data annotation},
journal = {Journal of Pathology Informatics},
volume = {16},
pages = {100409},
year = {2025},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2024.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2153353924000488},
author = {Abdulkadir Albayrak and Yao Xiao and Piyush Mukherjee and Sarah S. Barnett and Cherisse A. Marcou and Steven N. Hart},
keywords = {Human phenotype ontology, PhenoTagger, Vector embeddings},
abstract = {With the increasing utilization of exome and genome sequencing in clinical and research genetics, accurate and automated extraction of human phenotype ontology (HPO) terms from clinical texts has become imperative. Traditional methods for HPO term extraction, such as PhenoTagger, often face limitations in coverage and precision. In this study, we propose a novel approach that leverages large language models (LLMs) to generate synthetic sentences with clinical context, which were semantically encoded into vector embeddings. These embeddings are linked to HPO terms, creating a robust knowledgebase that facilitates precise information retrieval. Our method circumvents the known issue of LLM hallucinations by storing and querying these embeddings within a true database, ensuring accurate context matching without the need for a predictive model. We evaluated the performance of three different embedding models, all of which demonstrated substantial improvements over PhenoTagger. Top recall (sensitivity), precision (positive-predictive value, PPV), and F1 are 0.64, 0.64, and 0.64, respectively, which were 31%, 10%, and 21% better than PhenoTagger. Furthermore, optimal performance was achieved when we combined the best performing embedding model with PhenoTagger (a.k.a. Fused model), resulting in recall (sensitivity), precision (PPV), and F1 values of 0.7, 0.7, and 0.7, respectively, which are 10%, 10%, and 10% better than the best embedding models. Our findings underscore the potential of this integrated approach to enhance the precision and reliability of HPO term extraction, offering a scalable and effective solution for biomedical data annotation.}
}
@article{LIU2025126562,
title = {A label knowledge graph powered multi-task framework for crowdsourcing and mobile crowd sensing tasks},
journal = {Expert Systems with Applications},
volume = {270},
pages = {126562},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126562},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001848},
author = {Yimeng Liu and Zhiwen Yu and Nuo Li and Bin Guo and Sumi Helal},
keywords = {Heterogeneous label knowledge graph, Multi-task Hybrid Reasoning, Crowdsourcing, Mobile crowd sensing tasks},
abstract = {Crowdsourcing and Mobile Crowd Sensing (MCS) platforms have revolutionized data collection, harnessing the collective intelligence of crowdsourced sensing. Accurately classifying and extracting core information such as heterogeneous sensors in MCS tasks plays a key role in the platform execution efficiency. However, existing methods struggle with extracting pivotal information from task descriptions that are open-domain, implicitly expressed, and linguistically diverse, ultimately hindering the efficiency of task assignment and execution. To overcome these challenges, we propose LKG-MF, a Label Knowledge Graph-powered Multi-task Framework, to achieve better core information mining performance in crowdsourcing and mobile crowd sensing tasks. Specifically, we first construct an MCS task dataset comprising over 10,000 real tasks from 7 platforms. Then we devise a label knowledge graph to capture heterogeneous semantics and relationships among labels and enhance label representation. Further, we present a multi-granularity feature extraction network to capture precise task-specific features. To optimize performance across disparate tasks, we incorporate a task-adaptive loss function that adeptly balances their optimization rates. Experimental results show that LKG-MF outperforms baselines average by 2.3%, significantly improving multi-task classification accuracy. Notably, when we integrate the LKG-MF model into MCS platforms, the task assignment efficiency is improved by 38.6% and the task completion time is reduced by 45.1%, which demonstrates the practical impact and effectiveness of our model in improving the performance of MCS platforms.}
}
@article{XU2025103976,
title = {Advancing rule learning in knowledge graphs with structure-aware graph transformer},
journal = {Information Processing & Management},
volume = {62},
number = {2},
pages = {103976},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103976},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003352},
author = {Kang Xu and Miqi Chen and Yifan Feng and Zhenjiang Dong},
keywords = {Rule learning, Knowledge graph reasoning, Graph neural networks},
abstract = {In knowledge graphs (KGs), logic rules offer interpretable explanations for predictions and are essential for reasoning on downstream tasks, such as question answering. However, a key challenge remains unresolved: how to effectively encode and utilize the structural features around the head entity to generate the most applicable rules. This paper proposes a structure-aware graph transformer for rule learning, namely Structure-Aware Rule Learning (SARL), which leverages both local and global structural information of the subgraph around the head entity to generate the most suitable rule path. SARL employs a generalized attention mechanism combined with replaceable feature extractors to aggregate local structural information of entities. It then incorporates global structural and relational information to further model the subgraph structure. Finally, a rule decoder utilizes the comprehensive subgraph representation to generate the most appropriate rules. Comprehensive experiments on four real-world knowledge graph datasets reveal that SARL significantly enhances performance and surpasses existing methods in the link prediction task on large-scale KGs, with Hits@1 improvements of 6.5% on UMLS and 4.5% on FB15K-237.}
}
@article{MANTLE2024125115,
title = {Querying large-scale knowledge graphs using Qualitative Spatial Reasoning},
journal = {Expert Systems with Applications},
volume = {258},
pages = {125115},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125115},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424019821},
author = {Matthew Mantle and Sotirios Batsakis and Grigoris Antoniou},
keywords = {Spatial reasoning, RCC, Knowledge graphs, Spatial queries, Distributed computing, GeoSPARQL},
abstract = {In this paper we consider how Qualitative Spatial Reasoning (QSR) can be used to answer queries over large-scale knowledge graphs such as YAGO and DBPedia. We describe the challenges associated with spatially querying knowledge graphs such as point based representations, sparsity of qualitative relations, and scale. We address these challenges and present a query engine, Parallel Qualitative Reasoner-Query Engine (ParQR-QE), that uses a novel distributed qualitative spatial reasoning algorithm to provide answers to GeoSPARQL queries. An experimental evaluation using a range of different query types and the YAGO knowledge graph shows the advantages of QSR techniques in comparison to purely quantitative approaches.}
}
@article{GANGEMI2025100859,
title = {Logic Augmented Generation},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100859},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100859},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000453},
author = {Aldo Gangemi and Andrea Giovanni Nuzzolese},
keywords = {Knowledge graphs, Large language models, Logic augmented generation},
abstract = {Semantic Knowledge Graphs (SKG) face challenges with scalability, flexibility, contextual understanding, and handling unstructured or ambiguous information. However, they offer formal and structured knowledge enabling highly interpretable and reliable results by means of reasoning and querying. Large Language Models (LLMs) may overcome those limitations, making them suitable in open-ended tasks and unstructured environments. Nevertheless, LLMs are hardly interpretable and often unreliable. To take the best out of LLMs and SKGs, we envision Logic Augmented Generation (LAG) to combine the benefits of the two worlds. LAG uses LLMs as Reactive Continuous Knowledge Graphs that can generate potentially infinite relations and tacit knowledge on-demand. LAG uses SKGs to inject a discrete heuristic dimension with clear logical and factual boundaries. We exemplify LAG in two tasks of collective intelligence, i.e., medical diagnostics and climate projections. Understanding the properties and limitations of LAG, which are still mostly unknown, is of utmost importance for enabling a variety of tasks involving tacit knowledge in order to provide interpretable and effective results.}
}
@article{ROUMELIOTIS2024100056,
title = {LLMs in e-commerce: A comparative analysis of GPT and LLaMA models in product review evaluation},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100056},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000049},
author = {Konstantinos I. Roumeliotis and Nikolaos D. Tselikas and Dimitrios K. Nasiopoulos},
keywords = {Sentiment analysis, LLMs, Instruction tuning, GPT model, LLaMA model, LLM fine-tuning},
abstract = {E-commerce has witnessed remarkable growth, especially following the easing of COVID-19 restrictions. Many people, who were initially hesitant about online shopping, have now embraced it, while existing online shoppers increasingly prefer the convenience of e-commerce. This surge in e-commerce has prompted the implementation of automated customer service processes, incorporating innovations such as chatbots and AI-driven sales. Despite this growth, customer satisfaction remains vital for E-commerce sustainability. Data scientists have made progress in utilizing machine learning to assess satisfaction levels but struggled to understand emotions within product reviews’ context. The recent AI revolution, marked by the release of powerful Large Language Models (LLMs) to the public, has brought us closer than ever before to understanding customer sentiment. This study aims to illustrate the effectiveness of LLMs by conducting a comparative analysis of two cutting-edge LLMs, GPT-3.5 and LLaMA-2, along with two additional Natural Language Process (NLP) models, BERT and RoBERTa. We evaluate the performance of these models before and after fine-tuning them specifically for product review sentiment analysis. The primary objective of this research is to determine if these specific LLMs, could contribute to understanding customer satisfaction within the context of an e-commerce environment. By comparing the effectiveness of these models, we aim to uncover insights into the potential impact of LLMs on customer satisfaction analysis and enhance our understanding of their capabilities in this particular context.}
}
@article{FUELLEN2025102617,
title = {Validation requirements for AI-based intervention-evaluation in aging and longevity research and practice},
journal = {Ageing Research Reviews},
volume = {104},
pages = {102617},
year = {2025},
issn = {1568-1637},
doi = {https://doi.org/10.1016/j.arr.2024.102617},
url = {https://www.sciencedirect.com/science/article/pii/S1568163724004355},
author = {Georg Fuellen and Anton Kulaga and Sebastian Lobentanzer and Maximilian Unfried and Roberto A. Avelar and Daniel Palmer and Brian K. Kennedy},
keywords = {Longevity, Preventive Medicine, Large Language Models},
abstract = {The field of aging and longevity research is overwhelmed by vast amounts of data, calling for the use of Artificial Intelligence (AI), including Large Language Models (LLMs), for the evaluation of geroprotective interventions. Such evaluations should be correct, useful, comprehensive, explainable, and they should consider causality, interdisciplinarity, adherence to standards, longitudinal data and known aging biology. In particular, comprehensive analyses should go beyond comparing data based on canonical biomedical databases, suggesting the use of AI to interpret changes in biomarkers and outcomes. Our requirements motivate the use of LLMs with Knowledge Graphs and dedicated workflows employing, e.g., Retrieval-Augmented Generation. While naive trust in the responses of AI tools can cause harm, adding our requirements to LLM queries can improve response quality, calling for benchmarking efforts and justifying the informed use of LLMs for advice on longevity interventions.}
}
@article{NAZI2025100124,
title = {Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100124},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000724},
author = {Zabir Al Nazi and Md. Rajib Hossain and Faisal Al Mamun},
keywords = {Large language models, Zero-shot, Few-shot, Chain-of-thought, GPT-4, Llama 3, Ablation studies, Prompting, LLM reasoning, Low-resource, Bangla},
abstract = {As the global deployment of Large Language Models (LLMs) increases, the demand for multilingual capabilities becomes more crucial. While many LLMs excel in real-time applications for high-resource languages, few are tailored specifically for low-resource languages. The limited availability of text corpora for low-resource languages, coupled with their minimal utilization during LLM training, hampers the models’ ability to perform effectively in real-time applications. Additionally, evaluations of LLMs are significantly less extensive for low-resource languages. This study offers a comprehensive evaluation of both open-source and closed-source multilingual LLMs focused on low-resource language like Bengali, a language that remains notably underrepresented in computational linguistics. Despite the limited number of pre-trained models exclusively on Bengali, we assess the performance of six prominent LLMs, i.e., three closed-source (GPT-3.5, GPT-4o, Gemini) and three open-source (Aya 101, BLOOM, LLaMA) across key natural language processing (NLP) tasks, including text classification, sentiment analysis, summarization, and question answering. These tasks were evaluated using three prompting techniques: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT). This study found that the default hyperparameters of these pre-trained models, such as temperature, maximum token limit, and the number of few-shot examples, did not yield optimal outcomes and led to hallucination issues in many instances. To address these challenges, ablation studies were conducted on key hyperparameters, particularly temperature and the number of shots, to optimize Few-Shot learning and enhance model performance. The focus of this research is on understanding how these LLMs adapt to low-resource downstream tasks, emphasizing their linguistic flexibility and contextual understanding. Experimental results demonstrated that the closed-source GPT-4o model, utilizing Few-Shot learning and Chain-of-Thought prompting, achieved the highest performance across multiple tasks: an F1 score of 84.54% for text classification, 99.00% for sentiment analysis, a F1bert score of 72.87% for summarization, and 58.22% for question answering. For transparency and reproducibility, all methodologies and code from this study are available on our GitHub repository: https://github.com/zabir-nabil/bangla-multilingual-llm-eval.}
}
@article{WU2024,
title = {Knowledge-Empowered, Collaborative, and Co-Evolving AI Models: The Post-LLM Roadmap},
journal = {Engineering},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924007239},
author = {Fei Wu and Tao Shen and Thomas Bäck and Jingyuan Chen and Gang Huang and Yaochu Jin and Kun Kuang and Mengze Li and Cewu Lu and Jiaxu Miao and Yongwei Wang and Ying Wei and Fan Wu and Junchi Yan and Hongxia Yang and Yi Yang and Shengyu Zhang and Zhou Zhao and Yueting Zhuang and Yunhe Pan},
keywords = {Artificial intelligence, Large language models, Knowledge empowerment, Model collaboration, Model co-evolution},
abstract = {Large language models (LLMs) have significantly advanced artificial intelligence (AI) by excelling in tasks such as understanding, generation, and reasoning across multiple modalities. Despite these achievements, LLMs have inherent limitations including outdated information, hallucinations, inefficiency, lack of interpretability, and challenges in domain-specific accuracy. To address these issues, this survey explores three promising directions in the post-LLM era: knowledge empowerment, model collaboration, and model co-evolution. First, we examine methods of integrating external knowledge into LLMs to enhance factual accuracy, reasoning capabilities, and interpretability, including incorporating knowledge into training objectives, instruction tuning, retrieval-augmented inference, and knowledge prompting. Second, we discuss model collaboration strategies that leverage the complementary strengths of LLMs and smaller models to improve efficiency and domain-specific performance through techniques such as model merging, functional model collaboration, and knowledge injection. Third, we delve into model co-evolution, in which multiple models collaboratively evolve by sharing knowledge, parameters, and learning strategies to adapt to dynamic environments and tasks, thereby enhancing their adaptability and continual learning. We illustrate how the integration of these techniques advances AI capabilities in science, engineering, and society—particularly in hypothesis development, problem formulation, problem-solving, and interpretability across various domains. We conclude by outlining future pathways for further advancement and applications.}
}
@article{HE2024,
title = {Physician Versus Large Language Model Chatbot Responses to Web-Based Questions From Autistic Patients in Chinese: Cross-Sectional Comparative Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54706},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124002164},
author = {Wenjie He and Wenyan Zhang and Ya Jin and Qiang Zhou and Huadan Zhang and Qing Xia},
keywords = {artificial intelligence, chatbot, ChatGPT, ERNIE Bot, autism},
abstract = {Background
There is a dearth of feasibility assessments regarding using large language models (LLMs) for responding to inquiries from autistic patients within a Chinese-language context. Despite Chinese being one of the most widely spoken languages globally, the predominant research focus on applying these models in the medical field has been on English-speaking populations.
Objective
This study aims to assess the effectiveness of LLM chatbots, specifically ChatGPT-4 (OpenAI) and ERNIE Bot (version 2.2.3; Baidu, Inc), one of the most advanced LLMs in China, in addressing inquiries from autistic individuals in a Chinese setting.
Methods
For this study, we gathered data from DXY—a widely acknowledged, web-based, medical consultation platform in China with a user base of over 100 million individuals. A total of 100 patient consultation samples were rigorously selected from January 2018 to August 2023, amounting to 239 questions extracted from publicly available autism-related documents on the platform. To maintain objectivity, both the original questions and responses were anonymized and randomized. An evaluation team of 3 chief physicians assessed the responses across 4 dimensions: relevance, accuracy, usefulness, and empathy. The team completed 717 evaluations. The team initially identified the best response and then used a Likert scale with 5 response categories to gauge the responses, each representing a distinct level of quality. Finally, we compared the responses collected from different sources.
Results
Among the 717 evaluations conducted, 46.86% (95% CI 43.21%-50.51%) of assessors displayed varying preferences for responses from physicians, with 34.87% (95% CI 31.38%-38.36%) of assessors favoring ChatGPT and 18.27% (95% CI 15.44%-21.10%) of assessors favoring ERNIE Bot. The average relevance scores for physicians, ChatGPT, and ERNIE Bot were 3.75 (95% CI 3.69-3.82), 3.69 (95% CI 3.63-3.74), and 3.41 (95% CI 3.35-3.46), respectively. Physicians (3.66, 95% CI 3.60-3.73) and ChatGPT (3.73, 95% CI 3.69-3.77) demonstrated higher accuracy ratings compared to ERNIE Bot (3.52, 95% CI 3.47-3.57). In terms of usefulness scores, physicians (3.54, 95% CI 3.47-3.62) received higher ratings than ChatGPT (3.40, 95% CI 3.34-3.47) and ERNIE Bot (3.05, 95% CI 2.99-3.12). Finally, concerning the empathy dimension, ChatGPT (3.64, 95% CI 3.57-3.71) outperformed physicians (3.13, 95% CI 3.04-3.21) and ERNIE Bot (3.11, 95% CI 3.04-3.18).
Conclusions
In this cross-sectional study, physicians’ responses exhibited superiority in the present Chinese-language context. Nonetheless, LLMs can provide valuable medical guidance to autistic patients and may even surpass physicians in demonstrating empathy. However, it is crucial to acknowledge that further optimization and research are imperative prerequisites before the effective integration of LLMs in clinical settings across diverse linguistic environments can be realized.
Trial Registration
Chinese Clinical Trial Registry ChiCTR2300074655; https://www.chictr.org.cn/bin/project/edit?pid=199432}
}
@article{HAQUE2025100356,
title = {Leveraging LLMs for optimised feature selection and embedding in structured data: A case study on graduate employment classification},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100356},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100356},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001590},
author = {Radiah Haque and Hui-Ngo Goh and Choo-Yee Ting and Albert Quek and M.D. Rakibul Hasan},
keywords = {Machine learning, Student employability prediction, Feature selection, Large language models, BERT, Tabular data},
abstract = {The application of Machine Learning (ML) for predicting graduate student employability is a growing area of research, driven by the need to align educational outcomes with job market requirements. In this context, this paper investigates the application of Large Language Models (LLMs) for tabular data transformation and embedding, specifically using Bidirectional Encoder Representations from Transformers (BERT), to enhance the performance of ML models in binary classification tasks for student employability prediction. The primary objective is to determine whether converting structured data into text format improves model accuracy. The study involves several ML models including Artificial Neural Networks (ANN), CatBoost, and BERT classifier. The focus is on predicting the employment status of graduate students based on demographic, academic, and graduate tracer study data, collected from over 4000 university graduates. Feature selection methods, including Boruta and Extra Tree Classifier (ETC) are employed to identify the optimal feature set, guided by a sliding window algorithm for automatic feature selection. The models are trained in four stages: 1) original dataset without feature selection or word embedding, 2) dataset with selected optimal features, 3) transformed data with word embedding, and 4) transformed data with feature selection applied both before and after word embedding. The baseline model (without feature selection and embedding) achieved the highest accuracy with the ANN model (79%). Subsequently, applying ETC for feature selection improved accuracy, with CatBoost achieving 83%. Further transformation with BERT-based embeddings raised the highest accuracy to 85% using the BERT classifier. Finally, the optimal accuracy of 88% was obtained by applying feature selection before and after embedding, with the BERT-Boruta model. The findings from this study demonstrate that using the dual-stage feature selection approach in combination with BERT embedding significantly increases the classification accuracy. This highlights the potential of LLMs in transforming tabular data for enhanced graduate employment prediction.}
}
@article{MA2025109117,
title = {f-KGQA: A fuzzy question answering system for knowledge graphs},
journal = {Fuzzy Sets and Systems},
volume = {498},
pages = {109117},
year = {2025},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109117},
url = {https://www.sciencedirect.com/science/article/pii/S016501142400263X},
author = {Ruizhe Ma and Yunxing Liu and Zongmin Ma},
keywords = {Knowledge graphs, Question answering, Fuzzy term, Fuzzy question answering system},
abstract = {The wide usage of large-scale knowledge graphs (KGs) motivates the development of user-friendly interfaces so that knowledge graphs become more readily accessible to a larger population. Natural language-based question answering (QA) systems are widely investigated and developed in the context of KGs, which can provide users with a natural means to retrieve the information they need from KGs without expecting them to know the query language. It is very common that natural language contains linguistic terms (fuzzy terms), and fuzzy (flexible) query has been widely investigated in the context of databases. This paper contributes a QA system with fuzzy terms over KGs called f-KGQA. f-KGQA can deal with different types of questions, including simple questions, complex questions, and questions with fuzzy terms. More importantly, users are provided with a channel to flexibly define their fuzzy terms based on their understanding. Our experimental results demonstrate the effectiveness and applicability of f-KGQA in handling questions with fuzzy terms.}
}
@article{PENG2025102729,
title = {Open knowledge graph completion with negative-aware representation learning and multi-source reliability inference},
journal = {Information Fusion},
volume = {115},
pages = {102729},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102729},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005074},
author = {Huang Peng and Weixin Zeng and Jiuyang Tang and Mao Wang and Hongbin Huang and Xiang Zhao},
keywords = {Knowledge graph completion, Representation learning, Truth inference, Source estimation},
abstract = {Multi-source data fusion is essential for building smart cities by providing a comprehensive and holistic understanding of urban environments. Specifically, smart city-oriented knowledge graphs (KGs) require supplementary information from other open sources to increase their completeness, thus better supporting downstream tasks for smart cities. Nevertheless, existing open knowledge graph completion (KGC) approaches often overlook source quality assessment and fail to fully utilize prior knowledge, which tend to yield less satisfying results. To fill in these gaps, in this work, we propose a new open KGC method with negative-aware representation learning and multi-source reliability inference, i.e., Nari, which can effectively integrate the multi-source data concerning sustainable cities, providing reliable knowledge for downstream tasks. Specifically, we first train a graph neural network based encoder with a novel negative sampling strategy to better characterize prior knowledge in KG, and then identify new facts based on the learned prior knowledge and source reliability. The experiments on both general benchmark and waterlogging benchmark pertaining to sustainable cities demonstrate the effectiveness and wide applicability of Nari.}
}
@article{SUN2024113506,
title = {Development of an intelligent design and simulation aid system for heat treatment processes based on LLM},
journal = {Materials & Design},
volume = {248},
pages = {113506},
year = {2024},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2024.113506},
url = {https://www.sciencedirect.com/science/article/pii/S0264127524008815},
author = {Yixiao Sun and Xusheng Li and Chao Liu and Xiaohu Deng and Wenyu Zhang and Jiangang Wang and Zeyu Zhang and Tengyang Wen and Tianyu Song and Dongying Ju},
keywords = {Metal material heat treatment, Expert knowledge system, Large language model, Knowledge embedding, Intelligent simulation system},
abstract = {Heat treatment of steel is a multi-physics coupled process. Designing programs that meet the desired results is challenging. The current design of processes relies on experience and experimentation, leading to high costs in developing processes and challenges in training practitioners. To reduce research and development costs in the industry and enable novices to reach expert levels, we propose an intelligent heat treatment process design and simulation assistant system based on large language models (LLMs), named Chat-IMSHT. Chat-IMSHT can impart knowledge and recommend processes. Additionally, Chat-IMSHT optimizes the interaction between humans and Computer Aided Engineering (CAE) software. To achieve knowledge impartation and process recommendation, a dialogue model based on Retrieval Augmented Generation (RAG) and LLMs was designed. It characterizes and compresses a massive amount of heat treatment knowledge and process data. The system designs a new CAE software interaction paradigm, using LLMs to map parameters from natural language into formatted text for the CAE software COSMAP. A Steel Heat Treatment Knowledge Understanding (SHTKU) evaluation method was designed. The improved model significantly increased the accuracy of knowledge responses, with a maximum accuracy of 94.54 %. Experimental results show that Chat-IMSHT effectively imparts knowledge and generates formatted text, completing the task of process recommendation.}
}
@article{YAN2024112684,
title = {Collaborate SLM and LLM with latent answers for event detection},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112684},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112684},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013182},
author = {Youcheng Yan and Jinshuo Liu and Donghong Ji and Jinguang Gu and Ahmed Abubakar Aliyu and Xinyan Wang and Jeff Z. Pan},
keywords = {Large language model, Small language model, Event detection, Latent answers},
abstract = {Event detection (ED) intends to identify events from text and classify them into predefined event types. One of the major issues for ED is the low-resource problem due to inadequate samples. Some studies address the low-resource issue with retrieving knowledge entries directly from knowledge bases while introducing a lot of irrelevant knowledge or failing the lookup. Moreover, recent work has attempted to employ large language models (LLMs, e.g., ChatGPT) that directly access event types in unstructured text under low-resource scenarios. Although LLM-based approaches have obtained promising results, we consider that the full potential of LLMs has not been activated due to insufficient prompt information. Our research proposes a two-stage event detection method that collaborates small language models (SLMs) and LLMs, namely LSLAED. Specifically, we first fine-tune the SLM to generate three types of latent answers: answer-aware examples, structure-aware examples, and corresponding answer candidates. Subsequently, all latent answers will form the prompt and enable the LLM to improve performance through in-context learning. We evaluate the proposed method using precision, recall, and F1-score as evaluation metrics. Experiments on the ACE2005 and ERE-EN datasets have demonstrated that LSLAED achieves significant improvement in both full-shot and few-shot scenarios.}
}
@article{WAN2024103,
title = {Making knowledge graphs work for smart manufacturing: Research topics, applications and prospects},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {103-132},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001572},
author = {Yuwei Wan and Ying Liu and Zheyuan Chen and Chong Chen and Xinyu Li and Fu Hu and Michael Packianather},
keywords = {Smart manufacturing, Industry 4.0, Knowledge graph, Semantic modelling under industry 4.0, Knowledge reasoning},
abstract = {Smart manufacturing (SM) confronts several challenges inherently suited to knowledge graphs (KGs) capabilities. The first key challenge lies in the synthesis of complex and varied data surrounding the manufacturing context, which demands advanced semantic analysis and inference capabilities. The second main limitation is the contextualization of manufacturing systems and the exploitation of manufacturing domain knowledge, which requires a dynamic and holistic representation of knowledge. The last major obstacle arises from the facilitation of intricate decision-making processes towards correlated manufacturing ecosystems, which benefit from interconnected data structures that KGs excel at organizing. However, the existing survey studies concentrated on distinct facets of SM and offered isolated insights into KG applications while overlooking the interconnections between various KG technologies and their application across multiple domains. What specific role KGs should play in SM towards the aforementioned challenges, how to effectively harness KGs for these challenges, and the essential topics and methodologies required to make KGs functional remain underexplored. To explore the potential of KGs in SM, this study adopts a systematic approach to investigate, evaluate, and analyse current research on KGs, identifying core advancements and their implications for future manufacturing practices. Firstly, cutting-edge developments in the challenge-driven roles of KGs and KG techniques are identified, from knowledge extraction and mining to techniques for KG construction and updates, further extending to KG embedding, fusion, and reasoning—central to driving SM ecosystems. Specifically, the KG technologies for SM are depicted holistically, emphasizing the interplay of diverse KG techniques with a comprehensive framework. Subsequently, this foundation outlines and discusses key application scenarios of KGs from engineering design to predictive maintenance, covering the main representative stages of the manufacturing life cycle. Lastly, this study explores the intricate interplay of the practical challenges and advantages of KGs in manufacturing systems, pointing to emerging research avenues.}
}
@article{ZENG2024121144,
title = {RuMER-RL: A hybrid framework for sparse knowledge graph explainable reasoning},
journal = {Information Sciences},
volume = {680},
pages = {121144},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121144},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524010582},
author = {Zefan Zeng and Qing Cheng and Yuehang Si and Zhong Liu},
keywords = {Knowledge graph reasoning, Rule mining, Embedding, Reinforcement learning, Interpretability},
abstract = {Knowledge Graph (KG) reasoning is a crucial technology for ensuring the accuracy and utility of KGs. However, robust and explainable reasoning on sparse KGs is challenging due to the lack of information and truncated paths. To address this issue, we introduce RuMER-RL, a hybrid reasoning framework comprising three modules: Rule Mining (RM), Embedding Representation (ER), and Reinforcement Learning (RL). The ER and RM modules collaborate to enhance the embedding models and rule quality, generating additional triples to mitigate the sparsity of the KG. The RL module models multi-hop KG reasoning as a Markov Decision Process (MDP), employing dynamic anticipation, action space expansion, and curiosity-driven strategies to enrich the reasoning process and mitigate sparsity. Additionally, we reshape the reward function by incorporating embedding representation, rule matching, and curiosity rewards to guide the training and optimization of the policy network. Extensive experiments on six sparse KG datasets demonstrate that RuMER-RL outperforms state-of-the-art models in terms of link prediction accuracy and interpretability.}
}
@article{NUNES2024,
title = {Health Care Language Models and Their Fine-Tuning for Information Extraction: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/60164},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001480},
author = {Miguel Nunes and Joao Bone and Joao C Ferreira and Luis B Elvas},
keywords = {language model, information extraction, healthcare, PRISMA-ScR, scoping literature review, transformers, natural language processing, European Portuguese},
abstract = {Background
In response to the intricate language, specialized terminology outside everyday life, and the frequent presence of abbreviations and acronyms inherent in health care text data, domain adaptation techniques have emerged as crucial to transformer-based models. This refinement in the knowledge of the language models (LMs) allows for a better understanding of the medical textual data, which results in an improvement in medical downstream tasks, such as information extraction (IE). We have identified a gap in the literature regarding health care LMs. Therefore, this study presents a scoping literature review investigating domain adaptation methods for transformers in health care, differentiating between English and non-English languages, focusing on Portuguese. Most specifically, we investigated the development of health care LMs, with the aim of comparing Portuguese with other more developed languages to guide the path of a non–English-language with fewer resources.
Objective
This study aimed to research health care IE models, regardless of language, to understand the efficacy of transformers and what are the medical entities most commonly extracted.
Methods
This scoping review was conducted using the PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews) methodology on Scopus and Web of Science Core Collection databases. Only studies that mentioned the creation of health care LMs or health care IE models were included, while large language models (LLMs) were excluded. The latest were not included since we wanted to research LMs and not LLMs, which are architecturally different and have distinct purposes.
Results
Our search query retrieved 137 studies, 60 of which met the inclusion criteria, and none of them were systematic literature reviews. English and Chinese are the languages with the most health care LMs developed. These languages already have disease-specific LMs, while others only have general–health care LMs. European Portuguese does not have any public health care LM and should take examples from other languages to develop, first, general-health care LMs and then, in an advanced phase, disease-specific LMs. Regarding IE models, transformers were the most commonly used method, and named entity recognition was the most popular topic, with only a few studies mentioning Assertion Status or addressing medical lexical problems. The most extracted entities were diagnosis, posology, and symptoms.
Conclusions
The findings indicate that domain adaptation is beneficial, achieving better results in downstream tasks. Our analysis allowed us to understand that the use of transformers is more developed for the English and Chinese languages. European Portuguese lacks relevant studies and should draw examples from other non-English languages to develop these models and drive progress in AI. Health care professionals could benefit from highlighting medically relevant information and optimizing the reading of the textual data, or this information could be used to create patient medical timelines, allowing for profiling.}
}
@article{LI2025107108,
title = {Temporal multi-modal knowledge graph generation for link prediction},
journal = {Neural Networks},
volume = {185},
pages = {107108},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107108},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024010372},
author = {Yuandi Li and Hui Ji and Fei Yu and Lechao Cheng and Nan Che},
keywords = {Multimodal knowledge graph, Temporal knowledge graphs, Knowledge graph generation, Link prediction},
abstract = {Temporal Multi-Modal Knowledge Graphs (TMMKGs) can be regarded as a synthesis of Temporal Knowledge Graphs (TKGs) and Multi-Modal Knowledge Graphs (MMKGs), combining the characteristics of both. TMMKGs can effectively model dynamic real-world phenomena, particularly in scenarios involving multiple heterogeneous information sources and time series characteristics, such as e-commerce websites, scene recording data, and intelligent transportation systems. We propose a Temporal Multi-Modal Knowledge Graph Generation (TMMKGG) method that can automatically construct TMMKGs, aiming to reduce construction costs. To support this, we construct a dynamic Visual-Audio-Language Multimodal (VALM) dataset, which is particularly suitable for extracting structured knowledge in response to temporal multimodal perception data. TMMKGG explores temporal dynamics and cross-modal integration, enabling multimodal data processing for dynamic knowledge graph generation and utilizing alignment strategies to enhance scene perception. To validate the effectiveness of TMMKGG, we compare it with state-of-the-art dynamic graph generation methods using the VALM dataset. Furthermore, TMMKG exhibits a significant disparity in the ratio of newly introduced entities to their associated newly introduced edges compared to TKGs. Based on this phenomenon, we introduce a Temporal Multi-Modal Link Prediction (TMMLP) method, which outperforms existing state-of-the-art techniques.}
}
@article{SINGH2025100848,
title = {Knowledge graph based entity selection framework for ad-hoc retrieval},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100848},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100848},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000349},
author = {Pankaj Singh and Plaban Kumar Bhowmick},
keywords = {Information retrieval, Entity-based retrieval, Query expansion, Knowledge graph, Pseudo-relevance feedback},
abstract = {Recent entity-based retrieval models utilizing knowledge bases have shown significant improvement in ad-hoc retrieval. However, a lack of coherence between candidate entities can lead to query intent drift at retrieval time. To address this issue, we present an entity selection algorithm that utilizes a graph clustering framework to discover the semantics between entities and encompass the query with highly coherent entities accumulated from different resources, including knowledge bases, and pseudo-relevance feedback documents. Through this work, we propose: (1) An entity acquisition strategy to systematically acquire coherent entities for query expansion. (2) We propose a graph representation of entities to capture the coherence between entities where nodes correspond to the entities and edges represent semantic relatedness between entities. (3) We propose two different entity ranking approaches to select candidate entities based on the coherence with query entities and other coherent entities. A set of experiments on five TREC collections: ClueWeb09B, ClueWeb12B, Robust04, GOV2, and MS-Marco dataset under document retrieval task were conducted to verify the proposed algorithm’s performance. The reported results indicated that the proposed methodology outperforms existing state-of-the-art retrieval approaches in terms of MAP, NDCG, and P@20. The code and relevant data are available in https://github.com/pankajkashyap65/KnowledgeGraph.}
}
@article{XIAO202560,
title = {Network for knowledge Organization (NEKO): An AI knowledge mining workflow for synthetic biology research},
journal = {Metabolic Engineering},
volume = {87},
pages = {60-67},
year = {2025},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2024.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1096717624001484},
author = {Zhengyang Xiao and Himadri B. Pakrasi and Yixin Chen and Yinjie J. Tang},
keywords = {Foundation model, Large language model, Qwen, Retrieval augmented generation, Knowledge graph},
abstract = {Large language models (LLMs) can complete general scientific question-and-answer, yet they are constrained by their pretraining cut-off dates and lack the ability to provide specific, cited scientific knowledge. Here, we introduce Network for Knowledge Organization (NEKO), a workflow that uses LLM Qwen to extract knowledge through scientific literature text mining. When user inputs a keyword of interest, NEKO can generate knowledge graphs to link bioinformation entities and produce comprehensive summaries from PubMed search. NEKO significantly enhance LLM ability and has immediate applications in daily academic tasks such as education of young scientists, literature review, paper writing, experiment planning/troubleshooting, and new ideas/hypothesis generation. We exemplified this workflow's applicability through several case studies on yeast fermentation and cyanobacterial biorefinery. NEKO's output is more informative, specific, and actionable than GPT-4's zero-shot Q&A. NEKO offers flexible, lightweight local deployment options. NEKO democratizes artificial intelligence (AI) tools, making scientific foundation model more accessible to researchers without excessive computational power.}
}
@article{LI2024205469,
title = {Artificial general intelligence for the upstream geoenergy industry: A review},
journal = {Gas Science and Engineering},
volume = {131},
pages = {205469},
year = {2024},
issn = {2949-9089},
doi = {https://doi.org/10.1016/j.jgsce.2024.205469},
url = {https://www.sciencedirect.com/science/article/pii/S2949908924002656},
author = {Jimmy Xuekai Li and Tiancheng Zhang and Yiran Zhu and Zhongwei Chen},
keywords = {Artificial general intelligence (AGI), ChatGPT, Large language models (LLMs), Generative AI, Multimodal, Upstream geoenergy industry},
abstract = {Artificial General Intelligence (AGI) is set to profoundly impact the traditional upstream geoenergy industry (i.e., geothermal energy, oil and gas industry) by introducing unprecedented efficiencies and innovations. This paper explores AGI's foundational principles and its transformative applications, particularly focusing on the advancements brought about by large language models (LLMs) and extensive computer vision systems in the upstream sectors of the industry. The integration of Artificial Intelligence (AI) has already begun reshaping the upstream geoenergy landscape, offering enhancements in production optimization, downtime reduction, safety improvements, and advancements in exploration and drilling techniques. These technologies streamline logistics, minimize maintenance costs, automate monotonous tasks, refine decision-making processes, foster team collaboration, and amplify profitability through error reduction and actionable insights extraction. Despite these advancements, the deployment of AI technologies faces challenges, including the necessity for skilled professionals for implementation and the limitations of model training on constrained datasets, which affects the models' adaptability across different contexts. The advent of generative AI, exemplified by innovations like ChatGPT and the Segment Anything Model (SAM), heralds a new era of high-density innovation. These developments highlight a shift towards natural language interfaces and domain-knowledge-driven AI, promising more accessible and tailored solutions for the upstream geoenergy industry. This review articulates the vast potential AGI holds for tackling complex operational challenges within the upstream geoenergy industry, requiring near-human levels of intelligence. We discussed the promising applications, the hurdles of large-scale AGI model deployment, and the necessity for domain-specific knowledge in maximizing the benefits of these technologies.}
}
@article{KLAASSEN2025100122,
title = {Advancing transdiagnostic data analytics using knowledge graphs},
journal = {Biomarkers in Neuropsychiatry},
volume = {12},
pages = {100122},
year = {2025},
issn = {2666-1446},
doi = {https://doi.org/10.1016/j.bionps.2025.100122},
url = {https://www.sciencedirect.com/science/article/pii/S2666144625000048},
author = {Fiona Klaassen and Emanuel Schwarz},
keywords = {Biomarker, Construct, Knowledge graph, RDoC, Transdiagnostic, Validity},
abstract = {Artificial intelligence approaches have tremendous potential to advance our understanding of biological and other processes contributing to mental illness risk. An important question is how such approaches can be tailored to support transdiagnostic investigations that are considered central for gaining deeper insight into etiological processes and psychopathology that may not align well with categorical illness delineations. Here, we present the so-called “knowledge graphs” that could be leveraged in analytic approaches to synthesize multimodal data of transdiagnostic relevance, identify important latent structures and biomarkers, and support the evaluation of existing transdiagnostic frameworks.}
}
@article{LU2025104091,
title = {LLM-infused bi-level semantic enhancement for corporate credit risk prediction},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104091},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104091},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000330},
author = {Sichong Lu and Yi Su and Xiaoming Zhang and Jiahui Chai and Lean Yu},
keywords = {Corporate credit risk prediction, Semantic enhancement, Large language model, Contrastive learning, Multitask learning},
abstract = {Corporate credit risk (CCR) prediction enables investors, governments, and companies to make informed financial decisions. Existing research primarily focuses solely on the tabular feature values, yet it often overlooks the rich inherent semantic information. In this paper, a novel bi-level semantic enhancement framework for CCR prediction is proposed. Firstly, at the data-level, a large language model (LLM) generates detailed textual descriptions of companies’ financial conditions, infusing raw tabular training data with semantic information and domain knowledge. Secondly, to enable semantic perception during inference when only tabular data is available, a contrastive multimodal multitask learning model (CMML) is proposed at the model level. CMML leverages the semantically enhanced data from the previous level to acquire semantic perception capabilities during the training phase, requiring only tabular data during prediction. It aligns the representations of tabular data with textual data, enabling extracting semantically rich features from tabular data. Furthermore, a semantic alignment classifier and an MLP classifier are integrated into a weighted ensemble learner within a multitask learning architecture to enhance robustness. Empirical verification on two datasets demonstrates that CMML surpasses benchmark models in key metrics, particularly in scenarios with limited samples and high proportions of unseen corporations, implying its effectiveness in CCR prediction through bi-level semantic enhancement.}
}
@article{ZHENG2025112547,
title = {A LLM-driven and motif-informed linearizing graph transformer for Web API recommendation},
journal = {Applied Soft Computing},
volume = {169},
pages = {112547},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112547},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624013218},
author = {Xin Zheng and Guiling Wang and Guiyue Xu and Jianye Yang and Boyang Han and Jian Yu},
keywords = {Graph transformer, Web API recommendation, Motif, Higher-order connectivity, Semantic information},
abstract = {The rapid growth in the number of Web APIs makes it difficult for developers to find the appropriate ones. To tackle this issue, researchers have created various powerful automatic approaches for recommendations. Recently, a range of graph neural networks, drawing inspiration from Transformers, have incorporated global attention to enhance recommendations. However, these approaches still have limitations in terms of processing information between nodes and utilizing other valid information, which reduces their effectiveness in large-scale Web API recommendations. To tackle these problems, this paper introduces a novel positional and semantic encoding method and motif-based linearizing graph Transformer for automatic Web API recommendation. We integrate the semantic information of the nodes into the positional information by using a fine-tuned large language model and graph attention mechanism. Furthermore, we leverage motif information to alter the computational sequence of the Vanilla Transformer, achieving linear time complexity. Experimental results on the two real-world datasets demonstrate the suitability of our model for Web API recommendation, surpassing existing state-of-the-art methods. In summary, our proposed technique exhibits promising results for Web API recommendation and underscores the potential of global attention in this field.}
}
@article{CHRISTINO2024104092,
title = {ChatKG: Visualizing time-series patterns aided by intelligent agents and a knowledge graph},
journal = {Computers & Graphics},
volume = {124},
pages = {104092},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.104092},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324002279},
author = {Leonardo Christino and Fernando V. Paulovich},
keywords = {Knowledge graphs, Intelligent agents, Visual analytics},
abstract = {Line-chart visualizations of temporal data enable users to identify interesting patterns for the user to inquire about. Using Intelligent Agents (IA), Visual Analytic tools can automatically uncover explicit knowledge related information to said patterns. Yet, visualizing the association of data, patterns, and knowledge is not straightforward. In this paper, we present ChatKG, a novel visual analytics strategy that allows exploratory data analysis of a Knowledge Graph that associates temporal sequences, the patterns found in each sequence, the temporal overlap between patterns, the related knowledge of each given pattern gathered from a multi-agent IA, and the IA’s suggestions of related datasets for further analysis visualized as annotations. We exemplify and informally evaluate ChatKG by analyzing the world’s life expectancy. For this, we implement an oracle that automatically extracts relevant or interesting patterns, populates the Knowledge Graph to be visualized, and, during user interaction, inquires the multi-agent IA for related information and suggests related datasets to be displayed as visual annotations. Our tests and an interview conducted showed that ChatKG is well suited for temporal analysis of temporal patterns and their related knowledge when applied to history studies.}
}
@article{YANG2024107497,
title = {Application of question answering systems for intelligent agriculture production and sustainable management: A review},
journal = {Resources, Conservation and Recycling},
volume = {204},
pages = {107497},
year = {2024},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2024.107497},
url = {https://www.sciencedirect.com/science/article/pii/S0921344924000910},
author = {Tian Yang and Yupeng Mei and Ling Xu and Huihui Yu and Yingyi Chen},
keywords = {Question answering system, Intelligent agriculture, Knowledge graphs, Large language models},
abstract = {The increasing application of artificial intelligence in agriculture production and management has generated a large amount of data, leading to a demand for processing this data. This review focuses on the knowledge storage approaches in agricultural question answering systems, namely corpora, knowledge graphs, and large language models. These systems are built on massive amounts of data and aim to process and retrieve information effectively in the context of sustainable agriculture. Corpora refer to large collections of diverse documents that serve as foundational resources for training and fine-tuning question answering systems. Knowledge graphs capture structured and interconnected knowledge by representing entities, relationships, and attributes, enabling efficient organization and querying of information. Large language models, such as GPT-4, enhance the capacity of question answering systems to provide accurate and relevant responses. By exploring these three prominent knowledge storage approaches, this review analyses the methodology and impact of agricultural question answering systems, highlighting their applications in the production process. The findings provide important implications for future research in agriculture, and potential directions for further exploration.}
}
@article{CHEN2025126226,
title = {Temporal knowledge graph extrapolation with subgraph information bottleneck},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126226},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126226},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030938},
author = {Kai Chen and Han Yu and Ye Wang and Xin Song and Xiaojuan Zhao and Yalong Xie and Liqun Gao and Aiping Li},
keywords = {Temporal knowledge graph extrapolation, Knowledge representation and reasoning, Subgraph information bottleneck},
abstract = {In the realm of temporal knowledge graph (TKG) extrapolation, subgraph-based reasoning methods offer clearer insights than those based on individual paths, effectively capturing local evidence. However, these subgraph-based methods face particular challenges, such as obtaining subgraph-level annotations and maintaining a balance between keeping enough information for accurate predictions and discarding unnecessary details. To overcome these obstacles, we introduce a novel reasoning method known as Subgraph Information Bottleneck based Reasoning (SIBR) for TKG extrapolation. Based on information bottleneck theory, SIBR aims to find subgraphs that are full of predictive value yet concise, leading to an efficient learning and inference process. SIBR is designed to capture the key temporal dynamics and evolution within TKGs by identifying subgraphs that are just large enough to represent the TKG’s structure and temporal changes. It skillfully handles the computational complexities related to mutual information using variational techniques, offering a practical optimization strategy for TKG analysis. Our method’s effectiveness is substantiated by comprehensive experiments on six public datasets, which demonstrate its superiority from multiple perspectives: it showcases improvement in predictive accuracy, robustness against data sparsity and data noise, and sensitivity to parameters, highlighting its strength in the sophisticated domain of TKG extrapolation.}
}
@article{XUE2025105525,
title = {How to realize the knowledge reuse and sharing from accident reports? A knowledge-driven modeling method combining ontology and deep learning},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {94},
pages = {105525},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2024.105525},
url = {https://www.sciencedirect.com/science/article/pii/S0950423024002833},
author = {Nannan Xue and Wei Zhang and Huayu Zhong and Wenbin Liao and Tingsheng Zhao},
keywords = {Process safety, Knowledge graph, Ontology design, Joint extraction model, Knowledge application},
abstract = {The exploration and understanding of past accidents are of great significance in enhancing the process safety. However, manually reading and analyzing a large number of accident reports is a time-consuming and inefficient task. In this study, a novel modeling method is developed to build the knowledge graph of process safety accidents, aiming to overcome the problem of knowledge reuse and sharing. Firstly, the dataset consists of 409 process safety accident reports selected from the official website of the Ministry of Emergency Management of China. Secondly, the ontology design schema is defined based on the seven-step method, including 34 ontology classes and 11 relations. Then, a new joint extraction model for the process domain is proposed based on the CasRel framework, which achieves 95.85% in precision, 61.54% in recall, and 74.95% in F1-score. Finally, the knowledge graph containing 9192 nodes and 11,257 edges is constructed in the Neo4j graph database, followed by the discussion of various related applications such as query, statistics, and analysis. The results indicate that the proposed method is a useful tool for obtaining valuable knowledge from accident reports, contributing to analysis and prevention of accidents.}
}
@article{ZHAO2023225,
title = {A Survey of Knowledge Graph Construction Using Machine Learning},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {139},
number = {1},
pages = {225-257},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.031513},
url = {https://www.sciencedirect.com/science/article/pii/S152614922300098X},
author = {Zhigang Zhao and Xiong Luo and Maojian Chen and Ling Ma},
keywords = {Knowledge graph (KG), semantic network, relation extraction, entity linking, knowledge reasoning},
abstract = {Knowledge graph (KG) serves as a specialized semantic network that encapsulates intricate relationships among real-world entities within a structured framework. This framework facilitates a transformation in information retrieval, transitioning it from mere string matching to far more sophisticated entity matching. In this transformative process, the advancement of artificial intelligence and intelligent information services is invigorated. Meanwhile, the role of machine learning method in the construction of KG is important, and these techniques have already achieved initial success. This article embarks on a comprehensive journey through the last strides in the field of KG via machine learning. With a profound amalgamation of cutting-edge research in machine learning, this article undertakes a systematical exploration of KG construction methods in three distinct phases: entity learning, ontology learning, and knowledge reasoning. Especially, a meticulous dissection of machine learning-driven algorithms is conducted, spotlighting their contributions to critical facets such as entity extraction, relation extraction, entity linking, and link prediction. Moreover, this article also provides an analysis of the unresolved challenges and emerging trajectories that beckon within the expansive application of machine learning-fueled, large-scale KG construction.}
}
@article{PRAMANIK2024100833,
title = {Uniqorn: Unified question answering over RDF knowledge graphs and natural language text},
journal = {Journal of Web Semantics},
volume = {83},
pages = {100833},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100833},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000192},
author = {Soumajit Pramanik and Jesujoba Alabi and Rishiraj Saha Roy and Gerhard Weikum},
keywords = {Complex question answering, Heterogeneous sources, Group Steiner Trees},
abstract = {Question answering over RDF data like knowledge graphs has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents a method for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called Uniqorn, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph typically contains all question-relevant evidences but also a lot of noise. Uniqorn copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that Uniqorn significantly outperforms state-of-the-art methods for heterogeneous QA – in a full training mode, as well as in zero-shot settings. The graph-based methodology provides user-interpretable evidence for the complete answering process.}
}
@article{DORON2025104272,
title = {Generative AI: driving productivity and scientific breakthroughs in pharmaceutical R&D},
journal = {Drug Discovery Today},
volume = {30},
number = {1},
pages = {104272},
year = {2025},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.104272},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624003970},
author = {Guy Doron and Sam Genway and Mark Roberts and Sai Jasti},
abstract = {The rapid advancement of generative artificial intelligence (AI) is reshaping pharmaceutical research and development (R&D), offering opportunities across drug discovery and development. Generative AI (GenAI) enhances productivity by enabling virtual assistants, which help automate routine tasks. It advances novel small-molecule drug design and drives new machine learning (ML) applications through synthetic data generation. Further impact is anticipated in drug development from improving operational efficiencies to novel digital innovations. Converging technologies enable rich data set capture, and next-generation AI will enable rapid, automated hypothesis generation and testing. Here, we assess the current and future applications, and the mid-term and long-term transformative potential, of GenAI in pharmaceutical R&D.}
}
@article{WALTERSDORFER2025100849,
title = {Leveraging Knowledge Graphs for AI System Auditing and Transparency},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100849},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100849},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000350},
author = {Laura Waltersdorfer and Marta Sabou},
keywords = {AI auditing, Knowledge Graphs, AI transparency},
abstract = {Auditing complex Artificial Intelligence (AI) systems is gaining importance in light of new regulations and is particularly challenging in terms of system complexity, knowledge integration, and differing transparency needs. Current AI auditing tools however, lack semantic context, resulting in difficulties for auditors in effectively collecting and integrating, but also for analysing and querying audit data. In this position paper, we explore how Knowledge Graphs (KGs) can address these challenges by offering a structured and integrative approach to collecting and transforming audit traces. This work discusses the current limitations in both AI auditing processes and tools. Furthermore, we examine how KGs can play a transformative role in overcoming these obstacles to achieve improved auditability and transparency of AI systems.}
}
@article{CHEN2024111968,
title = {A fine-grained self-adapting prompt learning approach for few-shot learning with pre-trained language models},
journal = {Knowledge-Based Systems},
volume = {299},
pages = {111968},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111968},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124006026},
author = {Xiaojun Chen and Ting Liu and Philippe Fournier-Viger and Bowen Zhang and Guodong Long and Qin Zhang},
keywords = {Natural language processing, Few-shot learning, Prompt learning},
abstract = {Pre-trained language models have demonstrated remarkable performance in few-shot learning through the emergence of “prompt-based learning” methods, where the performance of these tasks highly rely on the quality of prompts. Existing prompt learning methods typically customize a single prompt to each few-shot learning task and all the examples in the task share the universal prompt. However, a fine-grained prompt design can enhance the performance of few-shot learning task by leveraging more diverse information hidden in the set of examples. In light of this motivation, this paper introduce an example-specific prompt learning method to embody fine-grained self-adapting prompts for few-shot learning with pre-trained models. Specifically, we introduce the concept of the “weak consistency assumption”, to trade-off the task-specific consistent and example-specific diversity. Based on this assumption, a novel method called Self-adapting Continuous Prompt Learning (SP-learning) to learn example-specific prompts is proposed. It employs a cross-attention prompt generator that considers the characteristics of input samples and utilizes a diversity calibration technique to adjust the prompt generator accordingly. By personalizing prompts for each example, SP-learning aims to improve few-shot learning performance. We perform a systematic evaluation on 10 public benchmark tasks and our method outperforms 8 of those tasks. Our research sheds light on the importance of personalized prompts and opens up new possibilities for improving few-shot learning tasks.}
}
@article{WANG2025126308,
title = {Multimodal fusion framework based on knowledge graph for personalized recommendation},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126308},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126308},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031750},
author = {Jingjing Wang and Haoran Xie and Siyu Zhang and S. Joe Qin and Xiaohui Tao and Fu Lee Wang and Xiaoliang Xu},
keywords = {Knowledge graphs, Multimodal fusion framework, Recommender system},
abstract = {Knowledge Graphs (KGs), which contain a wealth of knowledge, have been commonly employed in recommendation systems as a valuable knowledge-driven tool for supporting high-quality representations. To further enhance the model’s ability to understand the real world, Multimodal Knowledge Graphs (MKGs) are proposed to extract rich knowledge and facts among objects from text and visual content. However, existing MKG-based methods primarily focus on the reasoning relationships between entities by utilizing multimodal information as auxiliary data in the KG while overlooking the interactions between modalities. In this paper, we propose a Multimodal fusion framework based on Knowledge Graph for personalized Recommendation (Multi-KG4Rec) to address these limitations. Specifically, we systematically analyze the shortcomings of existing multimodal graph construction methods. To this end, we propose a modal fusion module to extract the user modal preference at a fine-grained level. Furthermore, we conduct extensive experiments on two real-world datasets from different domains to evaluate the performance of our model, and the results demonstrate the efficiency of the Multi-KG4Rec.}
}
@article{LIU2025110889,
title = {A blockchain-based LLM-driven energy-efficient scheduling system towards distributed multi-agent manufacturing scenario of new energy vehicles within the circular economy},
journal = {Computers & Industrial Engineering},
volume = {201},
pages = {110889},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.110889},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225000348},
author = {Changchun Liu and Qingwei Nie},
keywords = {Large language model, Energy-efficient scheduling, Blockchain, Distributed manufacturing, Multi-agent, Circular economy},
abstract = {As processing technology is becoming increasingly complex, a single enterprise is no longer able to satisfy all the customization needs, which also requires extra energy consumption and vast time to seek cooperation from other enterprises for processing, especially in the field of new energy vehicle manufacturing. To coincide with the circular economy principle, a blockchain-based Large Language Model (LLM)-driven energy-efficient scheduling solution is proposed towards distributed multi-agent manufacturing scenario of new energy vehicles in this work. Firstly, distributed manufacturing system has the potential to efficiently organize distributed manufacturing resources by abstracting various machines from different factory nodes into agents with corresponding processing capabilities. Additionally, energy-efficient scheduling in line with the circular economy principles helps to optimize production cycle and reduce energy consumption and delay time, thereby lowering production costs and enhancing competitiveness. Compared with the traditional methods that suffer from long training time and local optimization, LLMs offer innovative solutions by learning a wealth of experiential knowledge in advance from vast amounts of data to further support self-adaptive and real-time energy-efficient scheduling. It is also worth noting that untrusted production data in factories may mislead the learning process of LLM, which may generate incorrect decision results. Therefore, a credit evaluation-based consensus mechanism is proposed to provide a trustworthy data access in distributed manufacturing, which can improve the transparency and traceability of the whole production process. Finally, the proposed approach is validated in the distributed manufacturing scenario for new energy vehicles. Compared with common methods, experimental results demonstrate the superiority of the proposed method on the distributed multi-agent manufacturing scenario for new energy vehicles, highlighting its potential to enhance production efficiency and circular economy.}
}
@article{CHO2024,
title = {Task-Specific Transformer-Based Language Models in Health Care: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/49724},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001650},
author = {Ha Na Cho and Tae Joon Jun and Young-Hak Kim and Heejun Kang and Imjin Ahn and Hansle Gwon and Yunha Kim and Jiahn Seo and Heejung Choi and Minkyoung Kim and Jiye Han and Gaeun Kee and Seohyun Park and Soyoung Ko},
keywords = {transformer-based language models, medicine, health care, medical language model},
abstract = {Background
Transformer-based language models have shown great potential to revolutionize health care by advancing clinical decision support, patient interaction, and disease prediction. However, despite their rapid development, the implementation of transformer-based language models in health care settings remains limited. This is partly due to the lack of a comprehensive review, which hinders a systematic understanding of their applications and limitations. Without clear guidelines and consolidated information, both researchers and physicians face difficulties in using these models effectively, resulting in inefficient research efforts and slow integration into clinical workflows.
Objective
This scoping review addresses this gap by examining studies on medical transformer-based language models and categorizing them into 6 tasks: dialogue generation, question answering, summarization, text classification, sentiment analysis, and named entity recognition.
Methods
We conducted a scoping review following the Cochrane scoping review protocol. A comprehensive literature search was performed across databases, including Google Scholar and PubMed, covering publications from January 2017 to September 2024. Studies involving transformer-derived models in medical tasks were included. Data were categorized into 6 key tasks.
Results
Our key findings revealed both advancements and critical challenges in applying transformer-based models to health care tasks. For example, models like MedPIR involving dialogue generation show promise but face privacy and ethical concerns, while question-answering models like BioBERT improve accuracy but struggle with the complexity of medical terminology. The BioBERTSum summarization model aids clinicians by condensing medical texts but needs better handling of long sequences.
Conclusions
This review attempted to provide a consolidated understanding of the role of transformer-based language models in health care and to guide future research directions. By addressing current challenges and exploring the potential for real-world applications, we envision significant improvements in health care informatics. Addressing the identified challenges and implementing proposed solutions can enable transformer-based language models to significantly improve health care delivery and patient outcomes. Our review provides valuable insights for future research and practical applications, setting the stage for transformative advancements in medical informatics.}
}
@article{ZHANG2024112454,
title = {A survey on temporal knowledge graph embedding: Models and applications},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112454},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112454},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010888},
author = {Yuchao Zhang and Xiangjie Kong and Zhehui Shen and Jianxin Li and Qiuhua Yi and Guojiang Shen and Bo Dong},
keywords = {Temporal knowledge graph embedding, Time information, Extension of static knowledge graph embedding model, Evolutionary model, Downstream task},
abstract = {Knowledge graph embedding (KGE), as a pivotal technology in artificial intelligence, plays a significant role in enhancing the logical reasoning and management efficiency of downstream tasks in knowledge graphs (KGs). It maps the intricate structure of a KG to a continuous vector space. Conventional KGE techniques primarily focus on representing static data within a KG. However, in the real world, facts frequently change over time, as exemplified by evolving social relationships and news events. The effective utilization of embedding technologies to represent KGs that integrate temporal data has gained significant scholarly interest. This paper comprehensively reviews the existing methods for learning KG representations that incorporate temporal data. It offers a highly intuitive perspective by categorizing temporal KGE (TKGE) methods into seven main classes based on dynamic evolution models and extensions of static KGE. The review covers various aspects of TKGE, including the background, problem definition, symbolic representation, training process, commonly used datasets, evaluation schemes, and relevant research. Furthermore, detailed descriptions of related embedding models are provided, followed by an introduction to typical downstream tasks in temporal KG scenarios. Finally, the paper concludes by summarizing the challenges faced in TKGE and outlining future research directions.}
}
@incollection{GALITSKY2025175,
title = {8 - Truth-O-Meter: Collaborating with LLM in fighting its hallucinations},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {175-210},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000043},
author = {Boris Galitsky},
keywords = {Collaborative iterative mode, Fact-checking, Handling inconsistent verification sources, Large language model hallucination, Syntax-semantic alignment},
abstract = {Large language models (LLMs), like GPT-4, often generate texts plagued with inaccuracies and fabricated information. We develop a fact-checking system known as “Truth-O-Meter” which detects erroneous facts by cross-referencing generated content with information from the web and reputable sources, and then offers appropriate corrections. We employ text mining and web mining techniques to pinpoint accurate corresponding sentences and to employ a syntactic and semantic generalization process to enhance content quality. To effectively handle the challenges posed by inconsistent information sources during fact-checking, we employ an argumentation-analysis framework based on defeasible logic programming. In a comparative evaluation with competitive approaches that rely on reinforcement learning integrated with LLM or token-based hallucination detection, our fact-checking engine demonstrates significant enhancements in the factual accuracy and meaningfulness of LLM-generated content.11https://github.com/bgalitsky/Truth-O-Meter-Making-ChatGPT-Truthful.}
}
@article{LONGHAO20231493,
title = {The method of constructing basic-element base using large language model- Take the issue of rice waste},
journal = {Procedia Computer Science},
volume = {221},
pages = {1493-1500},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923007706},
author = {WANG Long-hao and LI Ding-jie and LI Xing-sen},
keywords = {Large language model;Extenics;Basic-element base;Rice waste;ChatGPT},
abstract = {The rapid development of artificial intelligence technology has led to the emergence of large language models such as ChatGPT represented by natural language processing technology, but currently there is no effective way to input all the information to be exchanged. In this paper, a method of constructing local basic-element base of input information by combining the large language model with extenics is proposed. Taking rice waste problem as an example, the method is successfully applied to a practical project to verify the feasibility of the method.}
}
@article{MACILENTI20241289,
title = {Prompting is not all you need Evaluating GPT-4 performance on a real-world ontology alignment use case},
journal = {Procedia Computer Science},
volume = {246},
pages = {1289-1298},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.557},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026036},
author = {Giulio Macilenti and Armando Stellato and Manuel Fiorelli},
keywords = {Semantic technologies, Ontology alignment, Large language models},
abstract = {Ontology Alignment (OA) is a complex, demanding and error-prone task, requiring the intervention of domain and Semantic Web experts. Automating the alignment process thus becomes a must-do, especially when involving large datasets, to at least produce a first input for human experts. Automated ontology alignment could benefit from the outstanding language ability of Large Language Models (LLMs), which could implicitly provide the background knowledge that has been the Achilles’ heel of traditional alignment systems. However, this requires a correct evaluation of the performance of LLMs and understanding the best way to incorporate them into more specific tools. In this paper, we show that a naive prompting approach on the popular GPT-4 model could face several problems when transferred to real-world use cases. To this end, we replicated the methods of Norouzi et al. (2023), applied to the OAEI 2022 conference track, on a reference alignment between a pair of datasets (reduced versions of two popular thesauri: European Commission’s EuroVoc and TESEO, from the Italian Senate of the Republic), which has never been tested in OAEI evaluation campaigns. This reference alignment has several features common to real-world use cases: it is has a larger size than those considered in the study we replicated, it is not published online and is therefore not subject to data contamination and it involves relations between concepts that are more complex than simple equivalence. The replicated methods achieved a significantly lower performance on our reference alignment than on the OAEI 2022 conference track, suggesting that size, data contamination, and semantic complexity need to be considered when using LLMs for the alignment task.}
}
@article{JIANG2025109796,
title = {A two-stage framework for pig disease knowledge graph fusing},
journal = {Computers and Electronics in Agriculture},
volume = {229},
pages = {109796},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924011876},
author = {Tingting Jiang and Zhiyi Zhang and Shunxin Hu and Shuai Yang and Jin He and Chao Wang and Lichuan Gu},
keywords = {Pig disease knowledge graph, Knowledge graph embedding, Data augmentation, Entity alignment},
abstract = {Pig disease knowledge graphs (KGs) are crucial for the prevention and treatment of pig diseases. Due to the difficulty of knowledge mining in the field of traditional animal husbandry, there is a lack of high-quality KGs of pig diseases. To tackle this issue, a novel two-stage framework for pig disease KG fusing is proposed in this manuscript. In the first stage, a multi-view augmentation method for pig disease KGs is designed. The domain characteristics in the field of pig disease are considered and four valid strategies are utilized for augmenting triples, which not only enriches the pig disease KGs and provides abundant training data for KG embedding. In the second stage, an unsupervised entity alignment method is introduced to match entities. Importantly, the similarities of entity name, relation, attribute, and structure information are learned alternatively to avoid annotating data manually. Extensive experiments on the pig disease datasets and the public dataset MED_BBK_9K demonstrate that the proposed method can achieve state-of-the-art performance, i.e., the multi-view augmentation method improves hits@1 by 0.387 compared with the suboptimal model on the Pig1 dataset, and the entity alignment model outperforms the second-best model by 0.168 in terms of hits@1 on the Pig1_Pig2 dataset.}
}
@article{FORTH2024110312,
title = {Semantic enrichment for BIM-based building energy performance simulations using semantic textual similarity and fine-tuning multilingual LLM},
journal = {Journal of Building Engineering},
volume = {95},
pages = {110312},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110312},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224018801},
author = {Kasimir Forth and André Borrmann},
keywords = {BIM to BEM, BEPS, Semantic enrichment, Semantic textual similarity, Fine tuning LLM},
abstract = {To achieve the global targets of the Paris Agreement of limiting global warming, it is necessary to reduce the operational energy of buildings, which are responsible for around 30% of the global greenhouse gas emissions. Building Energy Performance Simulation (BEPS) is an established method to estimate the building’s energy demand in early design stages. Building Information Models (BIM) provides geometric and semantic information to create precise Building Energy Models (BEM) in early design stages. However, manual enrichment of missing semantic information is still a time-consuming and laborious process. Therefore, we propose a novel methodology to automatically enrich missing information to BIM using Semantic Textual Similarity (STS) and fine-tuned Large Language Models (LLM). For every IfcSpace, we match room-specific space types and constructions with missing thermal properties using the semantic most similar pairs of the BIM model and the according databases. We use three real-world case studies to fine-tune LLMs, and two case studies evaluate the whole methodology. Different fine-tuning strategies, such as using different loss functions, adding opposing word pairs or domain-specific abbreviations, significantly improve the accuracy of the matching. At the same time, however, findings show that semantic matching based on multilingual fine-tuned LLM performs worse than translated, monolingually fine-tuned LLM. Finally, BEPS results from automatically enriched BEM only slightly deviate from manually enriched BEM.}
}
@article{DEVARAKONDA2024104627,
title = {Clinical trial recommendations using Semantics-Based inductive inference and knowledge graph embeddings},
journal = {Journal of Biomedical Informatics},
volume = {154},
pages = {104627},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104627},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000455},
author = {Murthy V. Devarakonda and Smita Mohanty and Raja Rao Sunkishala and Nag Mallampalli and Xiong Liu},
keywords = {Clinical trials, Knowledge graphs, Graph embeddings, Transductive inference, Inductive inference, Recommendation systems, Graph Neural Networks (GNNs), Graph Attention Networks (GATs)},
abstract = {Objective
Designing a new clinical trial entails many decisions, such as defining a cohort and setting the study objectives to name a few, and therefore can benefit from recommendations based on exhaustive mining of past clinical trial records. This study proposes an approach based on knowledge graph embeddings and semantics-driven inductive inference for generating such recommendations.
Method
The proposed recommendation methodology is based on neural embeddings trained on first-of-its-kind knowledge graph constructed from clinical trials data. The methodology includes design of a knowledge graph for clinical trial data, evaluation of various knowledge graph embedding techniques for it, application of a novel inductive inference method using these embeddings, and generation of recommendations for clinical trial design. The study uses freely available data from clinicaltrials.gov and related sources.
Results
The proposed approach for recommendations obtained relevance scores ranging from 70% to 83%. These scores were determined by evaluating the text similarity of recommended elements to actual elements used in clinical trials that are in progress. Furthermore, the most pertinent recommendations were consistently located towards the top of the list, indicating the effectiveness of our method.
Conclusion
Our study suggests that inductive inference using node semantics is a viable approach for generating recommendations using graphs neural embeddings, and that there is a potential for improvement in training graph embeddings using node semantics.}
}
@article{PAN2025103971,
title = {Concept-aware embedding for logical query reasoning over knowledge graphs},
journal = {Information Processing & Management},
volume = {62},
number = {2},
pages = {103971},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103971},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003303},
author = {Pengwei Pan and Jingpei Lei and Jiaan Wang and Dantong Ouyang and Jianfeng Qu and Zhixu Li},
keywords = {Knowledge graph, Complex query answering, Information enhancement},
abstract = {Logical query reasoning over knowledge graphs (KGs) is an important task for querying some information upon specified conditions. Despite recent advancements, existing methods typically focus on the inherent structure of logical queries and fail to capture the commonality among entities and relations, resulting in cascading errors during multi-hop inference. To mitigate this issue, we resort to inferring relations’ domain constraints based on the commonality of their connected entities implicitly. Specifically, to capture the domain constraints of relations, we treat the set of relations emitted by an entity as its implicit concept information and derive a relation’s domain constraint by aggregating the implicit concept information of its head entities. Employing a geometric-based embedding strategy, we enrich the representations of entities in the query with their implicit concept information. Additionally, we design a straightforward yet effective curriculum learning strategy to refine its reasoning skills. Notably, our model can be integrated into any existing query embedding-based logical query reasoning methods in a plug-and-play manner, enhancing their understanding of the entities as well as relations in queries. Experiments on three widely used datasets show that our model can achieve comparable outcomes and improve the performance of existing logical query reasoning models. Particularly, as a plug-in, it achieves an absolute improvement of the maximum 8.4% Hits@3 compared to the original model on the FB15k dataset, and it surpasses the former state-of-the-art plug-and-play logical query reasoning model in most scenes, exceeding it by up to 2.1% average Hits@3 results.}
}
@article{YAN2024108259,
title = {Enhancing large language model capabilities for rumor detection with Knowledge-Powered Prompting},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108259},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108259},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624004172},
author = {Yeqing Yan and Peng Zheng and Yongjun Wang},
keywords = {Social networks, Rumor detection, Knowledge augmentation, Prompt tuning, Large language model},
abstract = {Amid the proliferation of misinformation on social networks, automated rumor detection has emerged as a pivotal and pressing research domain. Nonetheless, current methodologies are hindered by constrained feature representations and limited adaptability in effectively addressing diverse and unconventional rumors. The incorporation of large-scale language models holds the promise of delivering heightened semantic comprehension and broader adaptability. Regrettably, prevailing general-purpose prompting approaches frequently fall short in furnishing adequate domain-specific context and guidance, thereby restricting their utility in the context of rumor detection. To ameliorate these concerns, we introduce the Knowledge-Powered Prompting strategy, which imparts task-relevant prompts and context to the model by amalgamating domain expertise with large-scale language models. This fusion equips the model to better align with the exigencies of rumor detection, mitigating the challenges posed by sensitivity to semantic subtleties and a paucity of training samples. In particular, we devise exploration prompts and bolster the prompt representation with a dynamic knowledge injection module, thereby facilitating profound reasoning about pivotal entities. Subsequently, we extract valuable external knowledge through the filtration of interactions between knowledge and claim, thereby diminishing the impact of noise. Concurrently, we undertake joint optimization, encompassing multi-task prompt population and categorical judgment objectives, fostering synergistic semantic modeling and discriminative assessments. Empirical evaluations reveal that our methodology substantially outperforms existing models.}
}
@article{ZHONG2024105316,
title = {Domain-specific language models pre-trained on construction management systems corpora},
journal = {Automation in Construction},
volume = {160},
pages = {105316},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105316},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524000529},
author = {Yunshun Zhong and Sebastian D. Goodfellow},
keywords = {Construction management, Domain-specific large language models, Pre-training, Natural language processing (NLP), Transfer learning, Text classification (TC), Named entity recognition (NER), Corpus development},
abstract = {The rising demand for automated methods in the Construction Management Systems (CMS) sector highlights opportunities for the Transformer architecture, which enables pre-training Deep Learning models on large, unlabeled datasets for Natural Language Processing (NLP) tasks, outperforming traditional Recurrent Neural Network models. However, their potential in the CMS domain remains underexplored. Therefore, this research produced the first CMS domain corpora from academic papers and introduced an end-to-end pipeline for pre-training and fine-tuning domain-specific Pre-trained Language Models. Four corpora were constructed and transfer learning was employed to pre-train BERT and RoBERTa using the corpora. The best-performing models were then fine-tuned and outperformed models pre-trained on general corpora. In two key NLP tasks, text classification using an infrastructure condition prediction dataset and named entity recognition using an automatic construction control dataset, domain-specific pre-training improved F1 scores by 5.9% and 8.5%, respectively. These promising results demonstrate extended applicability beyond CMS to the Architecture, Engineering, and Construction sectors.}
}
@article{ZHANG2024308,
title = {Empathetic Language in LLMs under Prompt Engineering: A Comparative Study in the Legal Field},
journal = {Procedia Computer Science},
volume = {244},
pages = {308-317},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030059},
author = {Yifan Zhang and Christopher Radishian and Sabine Brunswicker and Dan Whitenack and Daniel W. Linna},
keywords = {LLM, Human-AI Interaction, Empathetic Response},
abstract = {The demand for empathetic conversations increases with conversational AIs’ rise and exponentially spreading applications. In areas like law and healthcare, where professional and empathetic conversations are essential, conversational AIs must strive to retain the correctness of information and logic while improving on empathetic language use. When addressing such an issue, we focus on linguistic empathy, relating only to syntactic and rhetoric choices in language while disregarding the emotional aspect of influence. By performing this study, we are interested in finding whether current open-sourced Large Language Models (LLMs) can match human experts in the legal field by using empathetic language while not compromising facts and logic in responses. We compare responses from three open-sourced LLMs under four prompting strategies with the expert responses. In the comparison, we use metrics from three aspects: text and semantic similarity, factual consistency, and ten rules of linguistic empathy from previous research literature. After statistical tests, the comparison results show that language models can use empathetic language without compromising the default knowledge base of LLMs when properly prompt-engineered. To accomplish this, additional domain knowledge is still needed to match factually. The data supporting this study is publicly available at huggingface.co/datasets/RCODI/empathy-prompt and code is available at github.com/RCODI-ConversationalAI/Empathy-Prompt.}
}
@article{ZHANG2025105938,
title = {Dynamic hazard analysis on construction sites using knowledge graphs integrated with real-time information},
journal = {Automation in Construction},
volume = {170},
pages = {105938},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105938},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006745},
author = {Juntong Zhang and Xin Ruan and Han Si and Xiangyu Wang},
keywords = {Hazard analysis, Knowledge graph, Real-time information, Bridge construction},
abstract = {Construction, as a significant production activity, is inherently prone to accidents. These accidents often result from a chain of multiple hazards. However, existing methods of hazard analysis are limited to single-dimensional network modeling and static analysis, which makes them inadequate for addressing the complexity and variability of construction sites. This paper presents a dynamic construction hazard analysis method that integrates real-time information into knowledge graphs. In this approach, label entities are added to general knowledge graphs, linking hazard entities to their labels. Labels identified through vision-based methods are then incorporated into the graphs, allowing for the effective extraction and updating of subgraphs in response to spatiotemporal changes in the scenario. Additionally, graph analysis metrics have been proposed to evaluate the system from multiple levels. Finally, the method was applied to a bridge foundation construction case, demonstrating its practicality and significance in preventing accidents by enabling dynamic hazard analysis.}
}
@article{TRILLO2024452,
title = {A Group Decision-Making Approach Leveraging Preference Relations Derived from Large Language Model},
journal = {Procedia Computer Science},
volume = {242},
pages = {452-459},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.161},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018805},
author = {José Ramón Trillo and María Ángeles Martínez and Sławomir Zadrożny and Janusz Kacprzyk and Enrique Herrera-Viedma and Francisco Javier Cabrerizo},
keywords = {Sentiment Analysis, Large Language Model, Group Decision-Making, Detection System, Consensus Method},
abstract = {Group decision-making involves selecting among limited options. Experts share their perspectives in a comparative debate but then evaluate alternatives using preference relations, which can result in inconsistencies between their expressions and assessments. To address this, a method is proposed that automates the generation of these relationships from the debate comments, classifying them into positive and negative using sentiment analysis, namely with the Large Language Model. A new operator is introduced that weights these comments to calculate preference relations. Furthermore, modification of the relationships is allowed if the experts so wish. Moreover, another operator is incorporated that adjusts the weight of each expert according to his or her active participation in the discussion, assigning more weight to those who contribute more comments. Finally, this innovative method promotes coherence and equal participation in group decision-making by employing an innovative sentiment analysis detection system.}
}
@article{SELLAMI2025100716,
title = {Knowledge graph representation learning: A comprehensive and experimental overview},
journal = {Computer Science Review},
volume = {56},
pages = {100716},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100716},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000996},
author = {Dorsaf Sellami and Wissem Inoubli and Imed Riadh Farah and Sabeur Aridhi},
keywords = {Knowledge graphs, Knowledge graph embedding, Representation space, Link prediction, Scalability},
abstract = {Knowledge graph embedding (KGE) is a hot topic in the field of Knowledge graphs (KG). It aims to transform KG entities and relations into vector representations, facilitating their manipulation in various application tasks and real-world scenarios. So far, numerous models have been developed in KGE to perform KG embedding. However, several challenges must be addressed when designing effective KGE models. The most discussed challenges in the literature include scalability (KGs contain millions of entities and relations), incompleteness (missing links), the complexity of relations (symmetries, inversion, composition, etc.), and the sparsity of some entities and relations. The purpose of this paper is to provide a comprehensive overview of KGE models. We begin with a theoretical analysis and comparison of the existing methods proposed so far for generating KGE, which we have classified into four categories. We then conducted experiments using four benchmark datasets to compare the efficacy, efficiency, inductiveness, the electricity and the CO2 emission of five state-of-the-art methods in the link prediction task, providing a comprehensive analysis of the most commonly used benchmarks in the literature.}
}
@article{BUEHLER2023105454,
title = {MeLM, a generative pretrained language modeling framework that solves forward and inverse mechanics problems},
journal = {Journal of the Mechanics and Physics of Solids},
volume = {181},
pages = {105454},
year = {2023},
issn = {0022-5096},
doi = {https://doi.org/10.1016/j.jmps.2023.105454},
url = {https://www.sciencedirect.com/science/article/pii/S0022509623002582},
author = {Markus J. Buehler},
keywords = {Mechanics, Attention, Transformer, Language model, Forward, Inverse, Design, Modeling, Multiscale, Atomistic, Encoding, Representation, Causal, Emergent, Collective, Graph neural network, GPT, Human-machine interaction},
abstract = {We report a flexible multi-modal mechanics language model, MeLM, applied to solve various nonlinear forward and inverse problems, that can deal with a set of instructions, numbers and microstructure data. The framework is applied to various examples including bio-inspired hierarchical honeycomb design, carbon nanotube mechanics, and protein unfolding. In spite of the adaptable nature of the model-which allows us to easily incorporate diverse materials, scales, and mechanical features-it performs well across disparate forward and inverse tasks. Based on an autoregressive attention-model, MeLM effectively represents a large multi-particle system consisting of hundreds of millions of neurons, where the interaction potentials are discovered through graph-forming self-attention mechanisms that are then used to identify relationships from emergent structures, while taking advantage of synergies discovered in the training data. We show that the model can solve complex degenerate mechanics design problems and determine novel material architectures across a range of hierarchical levels, providing an avenue for materials discovery and analysis. To illustrate the use case for broader possibilities, we outline a human-machine interactive MechGPT model, here trained on a set of 1,103 Wikipedia articles related to mechanics, showing how the general framework can be used not only to solve forward and inverse problems but in addition, for complex language tasks like summarization, generation of new research concepts, and knowledge extraction. Looking beyond the demonstrations reported in this paper, we discuss other opportunities in applied mechanics and general considerations about the use of large language models in modeling, design, and analysis that can span a broad spectrum of material properties from mechanical, thermal, optical, to electronic.}
}
@article{CHEN2025103068,
title = {Meet2Mitigate: An LLM-powered framework for real-time issue identification and mitigation from construction meeting discourse},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103068},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103068},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007195},
author = {Gongfan Chen and Abdullah Alsharef and Anto Ovid and Alex Albert and Edward Jaselskis},
keywords = {Construction meeting, Speaker diarization, Automatic speech recognition, Large language model, Retrieval augmented generation, Human-AI interaction},
abstract = {Construction meetings are essential for bringing together project participants to coordinate efforts, identify problems, and make decisions. Previous studies on meeting analysis relied on manual approaches to identify isolated pieces of information but struggled with providing a high-level overview that targeted real-time problem identification and resolution. Despite the rich discussions that occur, the sheer volume of information exchanged can make it difficult to discern key issues, decisions, and action items. Recent advancements in large language models (LLMs) provide sophisticated natural language processing capabilities that can effectively distill essential information and highlight actionable insights from meeting transcripts. However, these technologies are often underutilized in practice, despite their potential to significantly enhance the analysis and management of meeting data. This study introduced the Meet2Mitigate (M2M) framework, which integrates cutting-edge technologies, including speaker diarization, automatic speech recognition (ASR), LLMs, and retrieval-augmented generation (RAG) to revolutionize how construction meetings are captured and analyzed. In this framework, construction meeting recordings can be converted into a structured format, differentiated by timestamps, speakers, and corresponding contents. Different speakers’ dialogues are then summarized to extract the main project-related issues. For quick mitigation responses, this framework combines LLMs with a retrieval mechanism to access the Construction Industry Institute (CII) Best Practices (BPs) knowledge pool, generating detailed action items to drive problem-solving. The validation results demonstrated that the M2M prototype can automatically generate a tailored end-to-end problem-to-solution report in real time by only using a meeting recording file.}
}
@article{JAUHIAINEN2024262,
title = {The Metaverse: Innovations and generative AI},
journal = {International Journal of Innovation Studies},
volume = {8},
number = {3},
pages = {262-272},
year = {2024},
issn = {2096-2487},
doi = {https://doi.org/10.1016/j.ijis.2024.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2096248724000183},
author = {Jussi S. Jauhiainen},
keywords = {Metaverse, Innovation, Generative AI, Collaboration, Sustainability, Creativity, ChatGPT},
abstract = {Today, the Metaverse consists of various platforms, including digital twins of the physical world as well as virtual and blended digital-material environments that offer immersive experiences for individual users. By going beyond solely physical or virtual realms, these platforms unlock new possibilities for exploration, experimentation, and interaction. This makes it possible to transcend the limitations of innovation processes confined to physical locations, so the Metaverse is thus poised to drive groundbreaking innovations. This article explores the Metaverse as an innovation platform, its opportunities and challenges, including the role of generative AI in it. It discusses how the Metaverse, as a collaboration, creativity, and technological platform, supports innovation potential. By embracing the possibilities and challenges offered by the Metaverse and leveraging the capabilities of generative AI within it, a future in which individuals can truly explore novel synergies between the physical and digital realms, thriving various kinds of innovations. It is crucial to achieve holistic sustainability impacts both within the Metaverse innovation platform and as its outputs.}
}
@article{MORENO2024916,
title = {Toward Clinical-Grade Evaluation of Large Language Models},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {118},
number = {4},
pages = {916-920},
year = {2024},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2023.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0360301623081348},
author = {Amy C. Moreno and Danielle S. Bitterman}
}
@article{WANG2024843,
title = {Large language models assisted multi-effect variants mining on cerebral cavernous malformation familial whole genome sequencing},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {843-858},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S200103702400014X},
author = {Yiqi Wang and Jinmei Zuo and Chao Duan and Hao Peng and Jia Huang and Liang Zhao and Li Zhang and Zhiqiang Dong},
keywords = {Whole genome sequencing, Cerebral cavernous malformation, Deep learning, Large language model, Natural language processing},
abstract = {Cerebral cavernous malformation (CCM) is a polygenic disease with intricate genetic interactions contributing to quantitative pathogenesis across multiple factors. The principal pathogenic genes of CCM, specifically KRIT1, CCM2, and PDCD10, have been reported, accompanied by a growing wealth of genetic data related to mutations. Furthermore, numerous other molecules associated with CCM have been unearthed. However, tackling such massive volumes of unstructured data remains challenging until the advent of advanced large language models. In this study, we developed an automated analytical pipeline specialized in single nucleotide variants (SNVs) related biomedical text analysis called BRLM. To facilitate this, BioBERT was employed to vectorize the rich information of SNVs, while a deep residue network was used to discriminate the classes of the SNVs. BRLM was initially constructed on mutations from 12 different types of TCGA cancers, achieving an accuracy exceeding 99%. It was further examined for CCM mutations in familial sequencing data analysis, highlighting an upstream master regulator gene fibroblast growth factor 1 (FGF1). With multi-omics characterization and validation in biological function, FGF1 demonstrated to play a significant role in the development of CCMs, which proved the effectiveness of our model. The BRLM web server is available at http://1.117.230.196.}
}
@article{SCHAFER2024639,
title = {BioKGrapher: Initial evaluation of automated knowledge graph construction from biomedical literature},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {639-660},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024003386},
author = {Henning Schäfer and Ahmad Idrissi-Yaghir and Kamyar Arzideh and Hendrik Damm and Tabea M.G. Pakull and Cynthia S. Schmidt and Mikel Bahn and Georg Lodde and Elisabeth Livingstone and Dirk Schadendorf and Felix Nensa and Peter A. Horn and Christoph M. Friedrich},
keywords = {Knowledge graph, Named entity recognition, Entity linking, Clinical guidelines, Software},
abstract = {Background The growth of biomedical literature presents challenges in extracting and structuring knowledge. Knowledge Graphs (KGs) offer a solution by representing relationships between biomedical entities. However, manual construction of KGs is labor-intensive and time-consuming, highlighting the need for automated methods. This work introduces BioKGrapher, a tool for automatic KG construction using large-scale publication data, with a focus on biomedical concepts related to specific medical conditions. BioKGrapher allows researchers to construct KGs from PubMed IDs. Methods The BioKGrapher pipeline begins with Named Entity Recognition and Linking (NER+NEL) to extract and normalize biomedical concepts from PubMed, mapping them to the Unified Medical Language System (UMLS). Extracted concepts are weighted and re-ranked using Kullback-Leibler divergence and local frequency balancing. These concepts are then integrated into hierarchical KGs, with relationships formed using terminologies like SNOMED CT and NCIt. Downstream applications include multi-label document classification using Adapter-infused Transformer models. Results BioKGrapher effectively aligns generated concepts with clinical practice guidelines from the German Guideline Program in Oncology (GGPO), achieving F1-Scores of up to 0.6. In multi-label classification, Adapter-infused models using a BioKGrapher cancer-specific KG improved micro F1-Scores by up to 0.89 percentage points over a non-specific KG and 2.16 points over base models across three BERT variants. The drug-disease extraction case study identified indications for Nivolumab and Rituximab. Conclusion BioKGrapher is a tool for automatic KG construction, aligning with the GGPO and enhancing downstream task performance. It offers a scalable solution for managing biomedical knowledge, with potential applications in literature recommendation, decision support, and drug repurposing.}
}
@article{HAMED2024108782,
title = {Safeguarding authenticity for mitigating the harms of generative AI: Issues, research agenda, and policies for detection, fact-checking, and ethical AI},
journal = {iScience},
volume = {27},
number = {2},
pages = {108782},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.108782},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224000038},
author = {Ahmed Abdeen Hamed and Malgorzata Zachara-Szymanska and Xindong Wu},
keywords = {Biocomputational method, Bioinformatics, Biological sciences, Computational bioinformatics, Natural sciences, Neural networks, Artificial intelligence, Artificial intelligence applications},
abstract = {Summary
As the influence of transformer-based approaches in general and generative artificial intelligence (AI) in particular continues to expand across various domains, concerns regarding authenticity and explainability are on the rise. Here, we share our perspective on the necessity of implementing effective detection, verification, and explainability mechanisms to counteract the potential harms arising from the proliferation of AI-generated inauthentic content and science. We recognize the transformative potential of generative AI, exemplified by ChatGPT, in the scientific landscape. However, we also emphasize the urgency of addressing associated challenges, particularly in light of the risks posed by disinformation, misinformation, and unreproducible science. This perspective serves as a response to the call for concerted efforts to safeguard the authenticity of information in the age of AI. By prioritizing detection, fact-checking, and explainability policies, we aim to foster a climate of trust, uphold ethical standards, and harness the full potential of AI for the betterment of science and society.}
}
@article{SONG2025129019,
title = {Unsupervised fuzzy temporal knowledge graph entity alignment via joint fuzzy semantics learning and global structure learning},
journal = {Neurocomputing},
volume = {617},
pages = {129019},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129019},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224017909},
author = {Jingni Song and Luyi Bai and Xuanxuan An and Longlong Zhou},
keywords = {Entity alignment, Fuzzy temporal knowledge graph, Fuzzy semantics learning, Global structure learning, Unsupervised},
abstract = {Temporal Knowledge Graph Entity Alignment (TKGEA) aims to identify the equivalent entities between different Temporal Knowledge Graphs (TKGs), which is important to knowledge fusion. The current mainstream TKGEA models are supervised embedding-based models that rely on pre-aligned seeds and implicitly encode structural information into entity embedding space for identifying equivalent entities. To deal with the TKGs structural information, some models use Graph Neural Network (GNN) encoding. But they ignore the design of decoders, failing to fully leverage the TKGs structural information. In addition, they primarily focus on crisp TKGs with clear entity semantics. However, many real-world TKGs exhibit fuzzy semantics. This fuzzy information makes existing TKGEA models face the challenge of handling the fuzzy semantics when aligning the equivalent fuzzy entities. To solve the above problems, we propose a novel unsupervised Fuzzy Temporal Knowledge Graphs Entity Alignment (EA) framework that jointly performs Fuzzy Semantics Learning and Global Structure Learning, namely FTFS. In this framework, we convert the EA task into an unsupervised optimal transport task between two intra-graph matrices, eliminating the necessity for pre-aligned seeds and thereby avoiding intensive labor. Since we further consider the relation between graph structure and entities during the optimal-transport-based decoder module, it can make better use of the global structural information rather than simply encoding it implicitly into the embedding space. Moreover, unlike TKGEA models, which use binary classification to represent temporal relational facts, we introduce fuzzy semantics learning to embed membership degrees of fuzzy temporal relational facts. Extensive experiments on five FTKG datasets show that our unsupervised method is superior to the state-of-the-art EA methods.}
}
@article{PHOGAT202555,
title = {ZFP-CanPred: Predicting the effect of mutations in zinc-finger proteins in cancers using protein language models},
journal = {Methods},
volume = {235},
pages = {55-63},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000295},
author = {Amit Phogat and Sowmya Ramaswamy Krishnan and Medha Pandey and M. Michael Gromiha},
keywords = {ZFP-CanPred, Zinc-fingers, Cancer, Driver, Neutral, Mutations, Neural network, Protein language model},
abstract = {Zinc-finger proteins (ZNFs) constitute the largest family of transcription factors and play crucial roles in various cellular processes. Missense mutations in ZNFs significantly alter protein-DNA interactions, potentially leading to the development of various types of cancers. This study presents ZFP-CanPred, a novel deep learning-based model for predicting cancer-associated driver mutations in ZNFs. The representations derived from protein language models (PLMs) from the structural neighbourhood of mutated sites were utilized to train ZFP-CanPred for differentiating between cancer-causing and neutral mutations. ZFP-CanPred, achieved a superior performance with an accuracy of 0.72, F1-score of 0.79, and area under the Receiver Operating Characteristics (ROC) Curve (AUC) of 0.74, on an independent test set. In a comparative analysis against 11 existing prediction tools using a curated dataset of 331 mutations, ZFP-CanPred demonstrated the highest AU-ROC of 0.74, outperforming both generic and cancer-specific methods. The model’s balanced performance across specificity and sensitivity addresses a significant limitation of current methodologies. The source code and other related files are available on GitHub at https://github.com/amitphogat/ZFP-CanPred.git. We envisage that the present study contributes to understand the oncogenic processes and developing targeted therapeutic strategies.}
}@article{SYRIANI2024101287,
title = {Screening articles for systematic reviews with ChatGPT},
journal = {Journal of Computer Languages},
volume = {80},
pages = {101287},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101287},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000303},
author = {Eugene Syriani and Istvan David and Gauransh Kumar},
keywords = {Generative AI, GPT, Empirical research, Large language model, Literature review, Mapping study, Screening},
abstract = {Systematic reviews (SRs) provide valuable evidence for guiding new research directions. However, the manual effort involved in selecting articles for inclusion in an SR is error-prone and time-consuming. While screening articles has traditionally been considered challenging to automate, the advent of large language models offers new possibilities. In this paper, we discuss the effect of using ChatGPT on the SR process. In particular, we investigate the effectiveness of different prompt strategies for automating the article screening process using five real SR datasets. Our results show that ChatGPT can reach up to 82% accuracy. The best performing prompts specify exclusion criteria and avoid negative shots. However, prompts should be adapted to different corpus characteristics.}
}
@article{KOVARI2025e42077,
title = {Explainable AI chatbots towards XAI ChatGPT: A review},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e42077},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e42077},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025004578},
author = {Attila Kovari},
keywords = {Explainable AI (XAI), ChatGPT, AI chatbots, Natural language processing (NLP), Transparency, Controllable AI},
abstract = {Advances in artificial intelligence (AI) have had a major impact on natural language processing (NLP), even more so with the emergence of large-scale language models like ChatGPT. This paper aims to provide a critical review of explainable AI (XAI) methodologies for AI chatbots, with a particular focus on ChatGPT. Its main objectives are to investigate the applied methods that improve the explainability of AI chatbots, identify the challenges and limitations within them, and explore future research directions. Such goals emphasize the need for transparency and interpretability of AI systems to build trust with users and allow for accountability. While integrating such interdisciplinary methods, such as hybrid methods combining knowledge graphs with ChatGPT, enhancing explainability, they also highlight industry needs for explainability and user-centred design. This will be followed by a discussion of the balance between explainability and performance, then the role of human judgement, and finally the future of verifiable AI. These are the avenues through which insights can be used to guide the development of transparent, reliable and efficient AI chatbots.}
}
@article{SCHAEFER2024108796,
title = {GPT-4 as a biomedical simulator},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108796},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524008813},
author = {Moritz Schaefer and Stephan Reichl and Rob {ter Horst} and Adele M. Nicolas and Thomas Krausgruber and Francesco Piras and Peter Stepper and Christoph Bock and Matthias Samwald},
keywords = {Biomedical simulation, Large language models, GPT-4, Computational biology, Artificial intelligence},
abstract = {Background
Computational simulation of biological processes can be a valuable tool for accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks. This study provides proof-of-concept for the use of GPT-4 as a versatile simulator of biological systems.
Methods
We introduce SimulateGPT, a proof-of-concept for knowledge-driven simulation across levels of biological organization through structured prompting of GPT-4. We benchmarked our approach against direct GPT-4 inference in blinded qualitative evaluations by domain experts in four scenarios and in two quantitative scenarios with experimental ground truth. The qualitative scenarios included mouse experiments with known outcomes and treatment decision support in sepsis. The quantitative scenarios included prediction of gene essentiality in cancer cells and progression-free survival in cancer patients.
Results
In qualitative experiments, biomedical scientists rated SimulateGPT's predictions favorably over direct GPT-4 inference. In quantitative experiments, SimulateGPT substantially improved classification accuracy for predicting the essentiality of individual genes and increased correlation coefficients and precision in the regression task of predicting progression-free survival.
Conclusion
This proof-of-concept study suggests that LLMs may enable a new class of biomedical simulators. Such text-based simulations appear well suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulations, but for which extensive knowledge is available as written text. Finally, we propose several directions for further development of LLM-based biomedical simulators, including augmentation through web search retrieval, integrated mathematical modeling, and fine-tuning on experimental data.}
}
@incollection{DAVID2024251,
title = {Chapter Thirteen - Automatic programming (source code generator) based on an ontological model},
editor = {Preetha Evangeline David and P. Anandhakumar},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {132},
pages = {251-272},
year = {2024},
booktitle = {Applying Computational Intelligence for Social Good},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245823000748},
author = {Preetha Evangeline David and S. Malathi and P. Anandhakumar},
keywords = {Deep learning, Language model, Source code, Software engineering, Natural language processing, Ontological modeling},
abstract = {Source AI is an AI-powered tool that can generate code in any programming language from any human language description. It can also simplify, find errors and fix them and debug your code. Automatic code generation capabilities continue to evolve within programming languages, IDEs and tools that work at compile time. This coding technique has proliferated because it can reduce mundane programming grunt work, and developers have found that it improves turnaround times and accuracy. Auto generated code usually becomes a hindrance for developers who want to tweak it later on Teams should plan to restrict these tools to only certain parts of the SDLC, such as where they can act as facilitators in smaller, less complex situations.}
}
@article{DWIVEDI2023102642,
title = {Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy},
journal = {International Journal of Information Management},
volume = {71},
pages = {102642},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102642},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000233},
author = {Yogesh K. Dwivedi and Nir Kshetri and Laurie Hughes and Emma Louise Slade and Anand Jeyaraj and Arpan Kumar Kar and Abdullah M. Baabdullah and Alex Koohang and Vishnupriya Raghavan and Manju Ahuja and Hanaa Albanna and Mousa Ahmad Albashrawi and Adil S. Al-Busaidi and Janarthanan Balakrishnan and Yves Barlette and Sriparna Basu and Indranil Bose and Laurence Brooks and Dimitrios Buhalis and Lemuria Carter and Soumyadeb Chowdhury and Tom Crick and Scott W. Cunningham and Gareth H. Davies and Robert M. Davison and Rahul Dé and Denis Dennehy and Yanqing Duan and Rameshwar Dubey and Rohita Dwivedi and John S. Edwards and Carlos Flavián and Robin Gauld and Varun Grover and Mei-Chih Hu and Marijn Janssen and Paul Jones and Iris Junglas and Sangeeta Khorana and Sascha Kraus and Kai R. Larsen and Paul Latreille and Sven Laumer and F. Tegwen Malik and Abbas Mardani and Marcello Mariani and Sunil Mithas and Emmanuel Mogaji and Jeretta Horn Nord and Siobhan O’Connor and Fevzi Okumus and Margherita Pagani and Neeraj Pandey and Savvas Papagiannidis and Ilias O. Pappas and Nishith Pathak and Jan Pries-Heje and Ramakrishnan Raman and Nripendra P. Rana and Sven-Volker Rehm and Samuel Ribeiro-Navarrete and Alexander Richter and Frantz Rowe and Suprateek Sarker and Bernd Carsten Stahl and Manoj Kumar Tiwari and Wil {van der Aalst} and Viswanath Venkatesh and Giampaolo Viglia and Michael Wade and Paul Walton and Jochen Wirtz and Ryan Wright},
keywords = {Conversational agent, Generative artificial intelligence, Generative AI, ChatGPT, Large language models},
abstract = {Transformative artificially intelligent tools, such as ChatGPT, designed to generate sophisticated text indistinguishable from that produced by a human, are applicable across a wide range of contexts. The technology presents opportunities as well as, often ethical and legal, challenges, and has the potential for both positive and negative impacts for organisations, society, and individuals. Offering multi-disciplinary insight into some of these, this article brings together 43 contributions from experts in fields such as computer science, marketing, information systems, education, policy, hospitality and tourism, management, publishing, and nursing. The contributors acknowledge ChatGPT’s capabilities to enhance productivity and suggest that it is likely to offer significant gains in the banking, hospitality and tourism, and information technology industries, and enhance business activities, such as management and marketing. Nevertheless, they also consider its limitations, disruptions to practices, threats to privacy and security, and consequences of biases, misuse, and misinformation. However, opinion is split on whether ChatGPT’s use should be restricted or legislated. Drawing on these contributions, the article identifies questions requiring further research across three thematic areas: knowledge, transparency, and ethics; digital transformation of organisations and societies; and teaching, learning, and scholarly research. The avenues for further research include: identifying skills, resources, and capabilities needed to handle generative AI; examining biases of generative AI attributable to training datasets and processes; exploring business and societal contexts best suited for generative AI implementation; determining optimal combinations of human and generative AI for various tasks; identifying ways to assess accuracy of text produced by generative AI; and uncovering the ethical and legal issues in using generative AI across different contexts.}
}
@article{WU2025103158,
title = {Retrieval augmented generation-driven information retrieval and question answering in construction management},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103158},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103158},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000515},
author = {Chengke Wu and Wenjun Ding and Qisen Jin and Junjie Jiang and Rui Jiang and Qinge Xiao and Longhui Liao and Xiao Li},
keywords = {Large language model, Retrieval augmented generation, Construction management},
abstract = {Construction management is a communication-intensive field, requiring prompt responses to queries from various stakeholders to ensure project continuity. However, retrieving accurate information from project documents is hampered by the mismatch in granularity between queries and vast contents and by inherent ambiguities in information. Large language models (LLMs) and retrieval-augmented generation (RAG) offer new opportunities to address the challenges. However, their effectiveness is limited by the segmentation of documents and insufficient consideration of engineers’ preferences. Therefore, we propose a novel paradigm: RAG for Construction Management (RAG4CM). It includes three components: 1) a pipeline that parses project documents into hierarchical structures to establish a knowledge pool; 2) novel RAG search algorithms; and 3) a user preference learning mechanism. The first two components enhance granularity alignment and RAG results by integrating document-level hierarchical features with raw contents. The preference learning realizes continuously improved responses along with user-system interactions. We developed a prototype system and conducted extensive experiments, demonstrating that the knowledge pool efficiently accommodates texts, tables, and images. RAG4CM realized a 0.924 Top-3 and 0.898 answer accuracy, surpassing both open-source frameworks and commercial products. In addition, preference learning further increases answer accuracy by 1.3 % to 9.5 %. Consequently, RAG4CM enables multi-source information retrieval in a user-friendly manner, improving communication efficiency and facilitating construction management activities.}
}
@article{ZHANG2025102816,
title = {Active in-context learning for cross-domain entity resolution},
journal = {Information Fusion},
volume = {117},
pages = {102816},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102816},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005943},
author = {Ziheng Zhang and Weixin Zeng and Jiuyang Tang and Hongbin Huang and Xiang Zhao},
keywords = {Entity resolution, Cross-domain entity resolution, In-context learning},
abstract = {Entity resolution (ER) is the task of determining the equivalence between two entity descriptions. In traditional settings, the testing data and training data come from the same domain, e.g., sharing the same attribute structure. Nevertheless, in practical situations, the testing and training data often span different domains, hence calling for the study of the cross-domain ER problem. To tackle the domain shift in cross-domain ER, state-of-the-art solutions devise neural models to utilize the information from the entity pairs in the target domain to guide the feature modeling in the source domain and also the model training. Nevertheless, these approaches require excessive computational resources and fine-tuning efforts to achieve effective matching. To mitigate these issues, in this work, we for the first time investigate the in-context learning (ICL) capabilities of large language models (LLMs) for cross-domain ER and introduce a new framework, CiDER. CiDER consists of three main modules, i.e., active candidate source data generation, in-context demonstration selection, and prompt generation, which can select optimal demonstrations from the source data to enhance LLM inference performance on ER in the target domain. Comprehensive experiments on multiple benchmarks demonstrate that CiDER offers significant improvements over existing methods on cross-domain ER.}
}
@article{LUO2025126579,
title = {DeBERTA-Att-LMCQA: A hybrid model of DeBERTA and attention for legal multi-choice question answering},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126579},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126579},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002015},
author = {Ying Luo and Xudong Luo and Guibin Chen},
keywords = {Question answering, Legal AI, Natural language processing, Large language model, Neural network},
abstract = {This paper presents a Multi-Choice Question Answering (MCQA) model designed specifically for the legal domain, utilising a DeBERTa-based framework combined with a bilinear attention mechanism. MCQA plays a critical role in the legal field by enabling the automated interpretation and analysis of complex legal articles, which is essential for tasks such as legal research, judicial examination preparation, and decision-making. The proposed model addresses the intricacies of legal language by employing vector similarity searches to align questions with relevant statutes, while a neural network processes these connections for accurate answer selection. Additionally, a binary classifier is integrated to enhance decision-making accuracy. Experimental evaluations on the JEC-QA dataset, a large-scale Legal MCQA (LMCQA) dataset for the Chinese judicial examination, demonstrate the model’s superior performance in accuracy, precision, recall, and F1 score when compared to existing methods. These findings underscore the model’s potential to advance legal Artificial Intelligence (AI) applications significantly, offering robust tools for legal professionals and improving efficiency in high-stakes legal tasks.}
}
@article{QI2024106417,
title = {KEMoS: A knowledge-enhanced multi-modal summarizing framework for Chinese online meetings},
journal = {Neural Networks},
volume = {178},
pages = {106417},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106417},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024003411},
author = {Peng Qi and Yan Sun and Muyan Yao and Dan Tao},
keywords = {Multi-modal meeting knowledge graph, Topic-based hierarchical clustering approach, Multi-modal enhanced encoding strategy, Topic-enhanced decoding strategy},
abstract = {The demand for “online meetings” and “collaborative office work” keeps surging recently, producing an abundant amount of relevant data. How to provide participants with accurate and fast summarizing service has attracted extensive attention. Existing meeting summarizing models overlook the utilization of multi-modal information and the information offsetting during summarizing. In this paper, we develop a knowledge-enhanced multi-modal summarizing framework. Firstly, we construct a three-layer multi-modal meeting knowledge graph, including basic, knowledge, and multi-modal layer, to integrate meeting information thoroughly. Then, we raise a topic-based hierarchical clustering approach, which considers information entropy and difference simultaneously, to capture the semantic evolution of meetings. Next, we devise a multi-modal enhanced encoding strategy, including a sentence-level cross-modal encoder, a joint loss function, and a knowledge graph embedding module, to learn the meeting and topic-level presentations. Finally, when generating summaries, we design a topic-enhanced decoding strategy for the Transformer decoder which mitigates semantic offsetting with the aid of topic information. Extensive experiments show that our proposed work consistently outperforms state-of-the-art solutions on the Chinese meeting dataset, where the ROUGE-1, ROUGE-2, and ROUGE-L are 49.98%, 21.03%, and 32.03% respectively.}
}
@article{LIU2025102486,
title = {Automatic medical report generation based on deep learning: A state of the art survey},
journal = {Computerized Medical Imaging and Graphics},
volume = {120},
pages = {102486},
year = {2025},
issn = {0895-6111},
doi = {https://doi.org/10.1016/j.compmedimag.2024.102486},
url = {https://www.sciencedirect.com/science/article/pii/S0895611124001630},
author = {Xinyao Liu and Junchang Xin and Qi Shen and Zhihong Huang and Zhiqiong Wang},
keywords = {Automatic medical report generation, Deep learning, Encoder–decoder framework, Medical image, Natural language process},
abstract = {With the increasing popularity of medical imaging and its expanding applications, posing significant challenges for radiologists. Radiologists need to spend substantial time and effort to review images and manually writing reports every day. To address these challenges and speed up the process of patient care, researchers have employed deep learning methods to automatically generate medical reports. In recent years, researchers have been increasingly focusing on this task and a large amount of related work has emerged. Although there have been some review articles summarizing the state of the art in this field, their discussions remain relatively limited. Therefore, this paper provides a comprehensive review of the latest advancements in automatic medical report generation, focusing on four key aspects: (1) describing the problem of automatic medical report generation, (2) introducing datasets of different modalities, (3) thoroughly analyzing existing evaluation metrics, (4) classifying existing studies into five categories: retrieval-based, domain knowledge-based, attention-based, reinforcement learning-based, large language models-based, and merged model. In addition, we point out the problems in this field and discuss the directions of future challenges. We hope that this review provides a thorough understanding of automatic medical report generation and encourages the continued development in this area.}
}
@article{MUMUNI2024101188,
title = {Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101188},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101188},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001225},
author = {Fuseini Mumuni and Alhassan Mumuni},
keywords = {Domain knowledge, Cognitive architecture, Brain-inspired neural network, Explainable AI, Adversarial attack, Zero-shot generalization},
abstract = {We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-shot learning. Data-driven machine learning models have achieved remarkable performance and demonstrated capabilities surpassing humans in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural networks and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms like mathematical relations, logic rules, knowledge graphs, and large language models (LLMs). and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human brain to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience—that is, to deepen human understanding on how the brain works in general, and how it handles these problems.}
}
@article{FENG2025109587,
title = {Cognitive Digital Twins of the natural environment: Framework and application},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109587},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109587},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017457},
author = {Jun Feng and Hailin Tang and Siyuan Zhou and Yang Cai and Jianxin Zhang},
keywords = {Cognitive digital twin, Natural environment, Framework, Knowledge graph, Ontology},
abstract = {Digital Twin (DT) technology offers a method of creating digital models of natural systems to enhance their ability to withstand natural disasters. Currently, DT of the natural environment is in its initial phases, lacking adaptive capabilities and relying on human-assisted modeling. The key to endowing DT of the natural environment with greater autonomy lies in the integration of expert knowledge. Knowledge graphs can efficiently arrange and structurally store expert knowledge, thereby supporting the autonomous functionality of DT. This paper introduces the concept of Cognitive Digital Twin(CDT) derived from the industrial domain and presents a framework for CDT of the natural environment. This framework is centered around knowledge graph technology, aiming to provide more insights and guidance for system development. This framework integrates human cognition by constructing knowledge graphs of objects, models, events, and scene modes. Moreover, these knowledge graphs support agents for the dynamic adjustment of processes, as well as the adaptation and parameter optimization of related models. As a use case, we utilize this framework to implement digital twin watersheds. We develop appropriate ontologies and agents to facilitate the construction of cognitive digital watersheds for various regions. Cognitive digital watersheds effectively fulfill the application needs of integrated flood forecasting and control scheduling. This application validates the framework’s effectiveness and provides a reference for constructing CDTs of other natural systems.}
}
@article{TIBAU2024101331,
title = {ChatGPT for chatting and searching: Repurposing search behavior},
journal = {Library & Information Science Research},
volume = {46},
number = {4},
pages = {101331},
year = {2024},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2024.101331},
url = {https://www.sciencedirect.com/science/article/pii/S0740818824000525},
author = {Marcelo Tibau and Sean Wolfgand Matsui Siqueira and Bernardo Pereira Nunes},
keywords = {ChatGPT, Large language models (LLMs), Searching as learning (SaL), Search tactics, Search strategy adaptation, Conversational information systems},
abstract = {Generative AI tools, exemplified by ChatGPT, are transforming the way users interact with information by enabling dialogue-based querying instead of traditional keyword searches. While this conversational approach can simplify user interactions, it also presents challenges in structuring effective searches, refining prompts, and verifying AI-generated content. This study addresses these complexities by repurposing traditional search tactics for use in conversational AI environments, specifically to support the Searching as Learning (SaL) paradigm. Forty-five adapted tactics are introduced to aid users in defining information needs, refining queries, and evaluating ChatGPT's responses for relevance, utility, and reliability. Using the Efficient Search Tactic Identification (ESTI) method and constant comparison analysis, these tactics were mapped into a stratified model with seven categories. The framework provides a structured approach for users to leverage conversational agents more effectively, promoting critical thinking and iterative learning. This research underscores the importance of developing robust search strategies tailored to conversational AI environments, facilitating deeper learning and reflective information engagement. Additionally, it highlights the need for ongoing research into the design and evaluation of future chat-and-search systems.}
}
@article{TRIGUERO2024102135,
title = {General Purpose Artificial Intelligence Systems (GPAIS): Properties, definition, taxonomy, societal implications and responsible governance},
journal = {Information Fusion},
volume = {103},
pages = {102135},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102135},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004517},
author = {Isaac Triguero and Daniel Molina and Javier Poyatos and Javier {Del Ser} and Francisco Herrera},
keywords = {General-purpose AI, Meta-learning, Reinforcement learning, Neuroevolution, Few-shot learning, AutoML, Transfer learning, Generative AI, Large language models},
abstract = {Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research. This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and ability based on several factors such as adaptation to new tasks, competence in domains not intentionally trained for, ability to learn from few data, or proactive acknowledgement of their own limitations. We then propose a taxonomy of approaches to realise GPAIS, describing research trends such as the use of AI techniques to improve another AI (commonly referred to as AI-powered AI) or (single) foundation models. As a prime example, we delve into generative AI (GenAI), aligning them with the terms and concepts presented in the taxonomy. Similarly, we explore the challenges and prospects of multi-modality, which involves fusing various types of data sources to expand the capabilities of GPAIS. Through the proposed definition and taxonomy, our aim is to facilitate research collaboration across different areas that are tackling general purpose tasks, as they share many common aspects. Finally, with the goal of providing a holistic view of GPAIS, we discuss the current state of GPAIS, its prospects, implications for our society, and the need for regulation and governance of GPAIS to ensure their responsible and trustworthy development.}
}
@article{HU2024103279,
title = {Interpretable medical image Visual Question Answering via multi-modal relationship graph learning},
journal = {Medical Image Analysis},
volume = {97},
pages = {103279},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103279},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002044},
author = {Xinyue Hu and Lin Gu and Kazuma Kobayashi and Liangchen Liu and Mengliang Zhang and Tatsuya Harada and Ronald M. Summers and Yingying Zhu},
keywords = {Visual Question Answering, Medical dataset, Graph neural network, Multi-modal large vision language model, Large Language Model, Chain of thought},
abstract = {Medical Visual Question Answering (VQA) is an important task in medical multi-modal Large Language Models (LLMs), aiming to answer clinically relevant questions regarding input medical images. This technique has the potential to improve the efficiency of medical professionals while relieving the burden on the public health system, particularly in resource-poor countries. However, existing medical VQA datasets are small and only contain simple questions (equivalent to classification tasks), which lack semantic reasoning and clinical knowledge. Our previous work proposed a clinical knowledge-driven image difference VQA benchmark using a rule-based approach (Hu et al., 2023). However, given the same breadth of information coverage, the rule-based approach shows an 85% error rate on extracted labels. We trained an LLM method to extract labels with 62% increased accuracy. We also comprehensively evaluated our labels with 2 clinical experts on 100 samples to help us fine-tune the LLM. Based on the trained LLM model, we proposed a large-scale medical VQA dataset, Medical-CXR-VQA, using LLMs focused on chest X-ray images. The questions involved detailed information, such as abnormalities, locations, levels, and types. Based on this dataset, we proposed a novel VQA method by constructing three different relationship graphs: spatial relationships, semantic relationships, and implicit relationship graphs on the image regions, questions, and semantic labels. We leveraged graph attention to learn the logical reasoning paths for different questions. These learned graph VQA reasoning paths can be further used for LLM prompt engineering and chain-of-thought, which are crucial for further fine-tuning and training multi-modal large language models. Moreover, we demonstrate that our approach has the qualities of evidence and faithfulness, which are crucial in the clinical field. The code and the dataset is available at https://github.com/Holipori/Medical-CXR-VQA.}
}
@article{FENG2025106833,
title = {Retrieval In Decoder benefits generative models for explainable complex question answering},
journal = {Neural Networks},
volume = {181},
pages = {106833},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106833},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024007573},
author = {Jianzhou Feng and Qin Wang and Huaxiao Qiu and Lirong Liu},
keywords = {Explainable AI, Information retrieval, Generative decoding, Knowledge distillation, Question answering},
abstract = {Large-scale Language Models (LLMs) utilizing the Chain-of-Thought prompting demonstrate exceptional performance in a variety of tasks. However, the persistence of factual hallucinations remains a significant challenge in practical applications. Prevailing retrieval-augmented methods treat the retriever and generator as separate components, which inadvertently restricts the generator’s capabilities to those of the retriever through intensive supervised training. In this work, we propose an unsupervised Retrieval In Decoder framework for multi-granularity decoding called RID, which integrates retrieval directly into the decoding process of generative models. It dynamically adjusts decoding granularity based on retrieval outcomes, and duly corrects the decoding direction through its direct impact on the next token. Moreover, we introduce a reinforcement learning-driven knowledge distillation method for adaptive explanation generation to better apply to Small-scale Language Models (SLMs). The experimental results across six public benchmarks surpass popular LLMs and existing retrieval-augmented methods, which demonstrates the effectiveness of RID in models of different scales and verifies its applicability and scalability.}
}
@article{XU2024102636,
title = {A representation learning-based approach to enhancing manufacturing quality for low-voltage electrical products},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102636},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102636},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002842},
author = {Yuming Xu and Tao Peng and Jiaqi Tao and Ao Bai and Ningyu Zhang and Kendrik Lim},
keywords = {Low-voltage electrical product, Manufacturing quality, Word embedding, Graph embedding, Defect case recommendation},
abstract = {In low-voltage electrical product manufacturing, resolving quality issues is heavily reliant on engineering experience, and can be time-consuming and error-prone. Through quality management systems, a large number of historical defect cases can be consolidated for analysis along with relevant causes. However, these defect descriptions are often casually described with a mix of Chinese and English language, containing domain-specific terms. Additionally, defect product features have varieties and complex relationships. Therefore, historical defect cases have not been effectively utilized to support manufacturing quality issues. To address this challenge, this study proposes a representation learning-based approach to enhance manufacturing quality. Key research contributions include: (1) A two-stage word embedding technique based on the pre-trained language model. First, TSDAE is utilized for unsupervised pre-training on a large amount of unlabeled data. Then, Sentence-BERT is utilized for fine-tuning on a small set of labeled similar sentence pairs. This process yields a pre-trained language model specific to low-voltage electrical product defects. (2) NSHPSAGE graph embedding model based on the constructed product feature knowledge graph. We select more valuable neighboring nodes during sampling and explore different aggregation functions to enhance graph embedding performance. This model effectively aggregates product feature information into “Defect_Case” nodes, yielding graph embedding vectors. The model exhibits good Weighted-Precision and Weighted-Recall with a short training duration, and it can handle new nodes, addressing the issue of heterogeneous graph embedding. (3) A defect case recommendation technique that fuses word embedding and graph embedding. We use Multi-Head Attention Fusion in the late-fusion to obtain defect case vectors. This approach comprehensively considers defect description semantic knowledge and complex product feature relationships, enabling accurate defect case recommendation with the prototype system.}
}
@article{MCSHANE2025101335,
title = {A neurosymbolic approach to authorship anonymization},
journal = {Cognitive Systems Research},
pages = {101335},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2025.101335},
url = {https://www.sciencedirect.com/science/article/pii/S1389041725000154},
author = {Marjorie McShane and Sergei Nirenburg and Christian Arndt and Sanjay Oruganti and Jesse English},
abstract = {We report a neurosymbolic approach to authorship anonymization that combines knowledge-based paraphrasing, grounded in cognitive modeling, with support functions provided by a large language model (LLM). The cognitive model accounts for four things: what it means to faithfully retain meaning and discourse coherence in a paraphrase, how do deal with polysemy given that full semantic analysis of open text is beyond the state of the art, how to define and characterize an author’s style, and how to leverage human linguistic capabilities when preparing systems to automatically anonymize texts. LLMs augment the knowledge-based paraphrases in three ways: by filtering out atypical formulations, by selecting the best from multiple candidate paraphrases, and by offering additional paraphrases in case the knowledge-based paraphrasing fails to adequately anonymize the text. This neurosymbolic architecture favors knowledge-based processing for being reliable and explainable, while exploiting LLMs for what they do best: manipulate regularities in the surface form of language.}
}
@article{TIAN2024101875,
title = {Foundation model of ECG diagnosis: Diagnostics and explanations of any form and rhythm on ECG},
journal = {Cell Reports Medicine},
volume = {5},
number = {12},
pages = {101875},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101875},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124006463},
author = {Yuanyuan Tian and Zhiyuan Li and Yanrui Jin and Mengxiao Wang and Xiaoyang Wei and Liqun Zhao and Yunqing Liu and Jinlei Liu and Chengliang Liu},
keywords = {electrocardiograms, foundation model, zero-shot diagnosis, signal-language model, contrast learning, knowledge enhancement, cardiovascular diseases, multimodal},
abstract = {Summary
We propose a knowledge-enhanced electrocardiogram (ECG) diagnosis foundation model (KED) that utilizes large language models to incorporate domain-specific knowledge of ECG signals. This model is trained on 800,000 ECGs from nearly 160,000 unique patients. Despite being trained on single-center data, KED demonstrates exceptional zero-shot diagnosis performance across various regions, including different locales in China, the United States, and other regions. This performance spans across all age groups for various conditions such as morphological abnormalities, rhythm abnormalities, conduction blocks, hypertrophy, myocardial ischemia, and infarction. Moreover, KED exhibits robust performance on diseases it has not encountered during its training. When compared to three experienced cardiologists on real clinical datasets, the model achieves comparable performance in zero-shot diagnosis of seven common clinical ECG types. We concentrate on the zero-shot diagnostic capability and the generalization performance of the proposed ECG foundation model, particularly in the context of external multi-center data and previously unseen disease.}
}
@article{CORDEIRO2024105714,
title = {Petro NLP: Resources for natural language processing and information extraction for the oil and gas industry},
journal = {Computers & Geosciences},
volume = {193},
pages = {105714},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105714},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424001973},
author = {Fábio Corrêa Cordeiro and Patrícia Ferreira {da Silva} and Alexandre Tessarollo and Cláudia Freitas and Elvis {de Souza} and Diogo {da Silva Magalhaes Gomes} and Renato Rocha Souza and Flávio Codeço Coelho},
keywords = {Natural language processing, Information extraction, Ontology, Knowledge graphs, Linguistic corpora},
abstract = {Most companies struggle to find and extract relevant information from their technical documents. In particular, the Oil and Gas (O&G) industry faces the challenge of dealing with large amounts of data hidden within old and new geoscientific reports collected over decades of operation. Making this information available in a structured format can unlock valuable information among these mountains of data, which is crucial to support a wide range of industrial and academic applications. However, most natural language processing resources were built from general domain corpora extracted from the Internet and primarily written in English. This paper presents Petro NLP, a comprehensive set of natural language processing and information extraction resources for the oil and gas industry in Portuguese. We connected an interdisciplinary team of geoscientists, linguists, computer scientists, petroleum engineers, librarians, and ontologists to build a knowledge graph and several annotated corpora. The Petro NLP resources comprise: (i) Petro KGraph– a knowledge graph populated with entities and relations commonly found on technical reports; and (ii) Petrolês, PetroGold, PetroNER, and PetroRE– sets of corpora containing raw text and documents annotated with morphosyntactic labels, named entities, and relations. These resources are fundamental infrastructure for future research in natural language processing and information extraction in the oil industry. Our ongoing research uses these datasets to train and enhance pre-trained machine learning models that automatically extract information from geoscientific technical documents.}
}
@article{LIAO2024112507,
title = {Zero-shot relation triplet extraction as Next-Sentence Prediction},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112507},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112507},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124011419},
author = {Wenxiong Liao and Zhengliang Liu and Yiyang Zhang and Xiaoke Huang and Ninghao Liu and Tianming Liu and Quanzheng Li and Xiang Li and Hongmin Cai},
keywords = {Zero-shot learning, Relation triplet extraction, Information extraction, Knowledge graph},
abstract = {Zero-shot relation triplet extraction (ZeroRTE) endeavors to extract relation triplets from a test set using a model trained on a training set with disjoint relations from the test set. Current ZeroRTE approaches primarily rely on two strategies: 1) Combining pre-trained language models to generate additional training samples; 2) Adding a large number of parameters that require training from scratch on top of a pre-trained language model. However, the former approach does not ensure the quality of generated samples, and the latter often struggles to generalize to unseen relations in the test set, particularly when the training set is small. In this paper, we introduce a novel method, Next Sentence Prediction for Relation Triplet Extraction (NSP-RTE), abstracting ZeroRTE as a higher-level next sentence prediction (NSP) task to enhance its generalization ability to unseen relation categories. NSP-RTE integrates modules for relation recognition, entity detection, and triplet classification, leveraging pre-trained BERT models with fewer parameters requiring training from scratch, while eliminating the need for additional sample generation. Our experiments on the FewRel and Wiki-ZSL datasets demonstrate that NSP-RTE, with its simple and efficient design, significantly outperforms previous methods.}
}
@article{PARK2024578,
title = {Can ChatGPT be used to generate scientific hypotheses?},
journal = {Journal of Materiomics},
volume = {10},
number = {3},
pages = {578-584},
year = {2024},
issn = {2352-8478},
doi = {https://doi.org/10.1016/j.jmat.2023.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352847823001557},
author = {Yang Jeong Park and Daniel Kaplan and Zhichu Ren and Chia-Wei Hsu and Changhao Li and Haowei Xu and Sipei Li and Ju Li},
keywords = {large language models, scientific hypothesis generation, generative AI, GPT-4},
abstract = {We investigate whether large language models can perform the creative hypothesis generation that human researchers regularly do. While the error rate is high, generative AI seems to be able to effectively structure vast amounts of scientific knowledge and provide interesting and testable hypotheses. The future scientific enterprise may include synergistic efforts with a swarm of “hypothesis machines”, challenged by automated experimentation and adversarial peer reviews.}
}
@article{ZONG2024104716,
title = {Advancing Chinese biomedical text mining with community challenges},
journal = {Journal of Biomedical Informatics},
volume = {157},
pages = {104716},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104716},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001345},
author = {Hui Zong and Rongrong Wu and Jiaxue Cha and Weizhe Feng and Erman Wu and Jiakun Li and Aibin Shao and Liang Tao and Zuofeng Li and Buzhou Tang and Bairong Shen},
keywords = {Biomedical text mining, Health information processing, Natural language processing, Artificial intelligence, Large language model},
abstract = {Objective
This study aims to review the recent advances in community challenges for biomedical text mining in China.
Methods
We collected information of evaluation tasks released in community challenges of biomedical text mining, including task description, dataset description, data source, task type and related links. A systematic summary and comparative analysis were conducted on various biomedical natural language processing tasks, such as named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation.
Results
We identified 39 evaluation tasks from 6 community challenges that spanned from 2017 to 2023. Our analysis revealed the diverse range of evaluation task types and data sources in biomedical text mining. We explored the potential clinical applications of these community challenge tasks from a translational biomedical informatics perspective. We compared with their English counterparts, and discussed the contributions, limitations, lessons and guidelines of these community challenges, while highlighting future directions in the era of large language models.
Conclusion
Community challenge evaluation competitions have played a crucial role in promoting technology innovation and fostering interdisciplinary collaboration in the field of biomedical text mining. These challenges provide valuable platforms for researchers to develop state-of-the-art solutions.}
}
@article{FELIX2025100296,
title = {Why are you traveling? Inferring trip profiles from online reviews and domain-knowledge},
journal = {Online Social Networks and Media},
volume = {45},
pages = {100296},
year = {2025},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2024.100296},
url = {https://www.sciencedirect.com/science/article/pii/S2468696424000211},
author = {Lucas G.S. Félix and Washington Cunha and Claudio M.V. {de Andrade} and Marcos André Gonçalves and Jussara M. Almeida},
keywords = {Trip purpose, Tourism, Machine learning, Large language models, Text classification},
abstract = {This paper addresses the task of inferring trip profiles (TPs), which consists of determining the profile of travelers engaged in a particular trip given a set of possible categories. TPs may include working trips, leisure journeys with friends, or family vacations. Travelers with different TPs typically have varied plans regarding destinations and timing. TP inference may provide significant insights for numerous tourism-related services, such as geo-recommender systems and tour planning. We focus on TP inference using TripAdvisor, a prominent tourism-centric social media platform, as our data source. Our goal is to evaluate how effectively we can automatically discern the TP from a user review on this platform. A user review encompasses both textual feedback and domain-specific data (such as a user’s previous visits to the location), which are crucial for accurately characterizing the trip. To achieve this, we assess various feature sets (including text and domain-specific) and implement advanced machine learning models, such as neural Transformers and open-source Large Language Models (Llama 2, Bloom). We examine two variants of the TP inference task—binary and multi-class. Surprisingly, our findings reveal that combining domain-specific features with TF-IDF-based representation in an LGBM model performs as well as more complex Transformer and LLM models, while being much more efficient and interpretable.}
}
@article{TIAN2024112625,
title = {Agent-DA: Enhancing low-resource event extraction with collaborative multi-agent data augmentation},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112625},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112625},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124012590},
author = {Xuemeng Tian and Yikai Guo and Bin Ge and Xiaoguang Yuan and Hang Zhang and Yuting Yang and Wenjun Ke and Guozheng Li},
keywords = {Event extraction, Data augmentation, Multi-agent},
abstract = {Low-resource event extraction presents a significant challenge in real-world applications, particularly in domains like pharmaceuticals, military and law, where data is frequently insufficient. Data augmentation, as a direct method for expanding samples, is considered an effective solution. However, existing data augmentation methods often suffer from text fluency issues and label hallucination. To address these challenges, we propose a framework called Agent-DA, which leverages multi-agent collaboration for event extraction data augmentation. Specifically, Agent-DA follows a three-step process: data generation by the large language model, collaborative filtering by both the large language model and small language model to discriminate easy samples, and the use of an adjudicator to identify hard samples. Through iterative and selective augmentation, our method significantly enhances both the quantity and quality of event samples, improving text fluency and label consistency. Extensive experiments on the ACE2005-EN and ACE2005-EN+ datasets demonstrate the effectiveness of Agent-DA, with F1-score improvements ranging from 0.15% to 16.18% in trigger classification and from 2.2% to 15.67% in argument classification.}
}
@article{HEREDIAALVARO2025103007,
title = {An advanced retrieval-augmented generation system for manufacturing quality control},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103007},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103007},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400658X},
author = {José Antonio {Heredia Álvaro} and Javier González Barreda},
keywords = {Knowledge-based systems, Quality control, Ceramic tile, Retrieval-augmented generation, Large language models},
abstract = {The rise of Large Language Models (LLMs) with generative artificial intelligence has revolutionized the development of knowledge-based systems, enabling intuitive interactions through natural language. This paper explores the implementation of an advanced Retrieval-Augmented Generation (RAG) system, designed to improve manufacturing quality control by utilizing the capabilities of LLMs, particularly OpenAI’s GPT models. We focus on the ceramic tile manufacturing process, where the system retrieves and analyzes specialized bibliographic sources to diagnose defects and propose solutions. In addition to core RAG functionalities, the system incorporates tailored pre-processing and post-processing mechanisms to optimize document retrieval and response generation. The system’s effectiveness in solving quality issues is demonstrated through its application in identifying defect causes and generating actionable solutions, significantly improving non-conformities management. This approach not only streamlines troubleshooting but also enhances the quality control system, providing a comprehensive, scalable tool for manufacturers.}
}
@article{NIU2024102069,
title = {EHR-KnowGen: Knowledge-enhanced multimodal learning for disease diagnosis generation},
journal = {Information Fusion},
volume = {102},
pages = {102069},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102069},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003858},
author = {Shuai Niu and Jing Ma and Liang Bai and Zhihua Wang and Li Guo and Xian Yang},
keywords = {Multimodal learning, Multimodal electronic health records, Knowledge enhancement, Generative large language model, Disease diagnosis},
abstract = {Electronic health records (EHRs) contain diverse patient information, including medical notes, clinical events, and laboratory test results. Integrating this multimodal data can improve disease diagnoses using deep learning models. However, effectively combining different modalities for diagnosis remains challenging. Previous approaches, such as attention mechanisms and contrastive learning, have attempted to address this but do not fully integrate the modalities into a unified feature space. This paper presents EHR-KnowGen, a multimodal learning model enhanced with external domain knowledge, for improved disease diagnosis generation from diverse patient information in EHRs. Unlike previous approaches, our model integrates different modalities into a unified feature space with soft prompts learning and leverages large language models (LLMs) to generate disease diagnoses. By incorporating external domain knowledge from different levels of granularity, we enhance the extraction and fusion of multimodal information, resulting in more accurate diagnosis generation. Experimental results on real-world EHR datasets demonstrate the superiority of our generative model over comparative methods, providing explainable evidence to enhance the understanding of diagnosis results.}
}
@article{SAHADEVAN2025103141,
title = {Knowledge augmented generalizer specializer: A framework for early stage design exploration},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103141},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000345},
author = {Vijayalaxmi Sahadevan and Rohin Joshi and Kane Borg and Vishal Singh and Abhishek Raj Singh and Bilal Muhammed and Soban Babu Beemaraj and Amol Joshi},
abstract = {In non-routine engineering design projects, the design outcome is determined by how the problem is formulated and represented in the early conceptual stage. The problem representation comprises schemas, ontologies, variables, and parameters relevant to the given problem class. Despite the critical role of early conceptual decisions in shaping the eventual design outcome, most of the computational support and automation are focused on the latter stages of parametric modelling, problem-solving, and optimization. There is inadequate support for aiding and automating problem formulation, variable and parameter identification and representation, and early-stage conceptual decisions. Therefore, this paper presents an innovative, transparent, and explainable method employing semantic reasoning to automate the step-by-step conceptual design generation process, including problem formulation, identification and representation of the variables and parameters and their dependencies. The method is realized through a novel framework called Knowledge Augmented Generalizer Specializer (KAGS). KAGS employs the Function-Behavior-Structure (FBS) ontology and the Graph-of-Thought (GoT) mechanism to enable automated reasoning with a Large Language Model (LLM). The workflow comprises various stages: problem breakdown, design prototype creation, assessment, and prototype merging. The framework is implemented and tested on a Subsea Layout (SSL) planning problem, a special class of infrastructure planning projects in deep-sea oil and gas production systems. The experimentations with KAGS demonstrate its capacity to support problem formulation, hierarchical decomposition, and solution generation. The research also provides new insights into the FBS framework and meta-level reasoning in early design stages.}
}
@article{VIRVOU2024120759,
title = {VIRTSI: A novel trust dynamics model enhancing Artificial Intelligence collaboration with human users – Insights from a ChatGPT evaluation study},
journal = {Information Sciences},
volume = {675},
pages = {120759},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120759},
url = {https://www.sciencedirect.com/science/article/pii/S002002552400673X},
author = {Maria Virvou and George A. Tsihrintzis and Evangelia-Aikaterini Tsichrintzi},
keywords = {Artificial Intelligence, AI-Empowered Software, Autonomous Systems, AI Trust, Human-AI Interaction, Human-Centered Artificial Intelligence, User Modelling, Finite State Modelling, Confusion Matrix, AI in Education},
abstract = {The rapid integration of intelligent processes and methods into information systems in the Artificial Intelligence (AI) era has led to a substantial shift towards autonomous software decision-making. This evolution necessitates robust human oversight, especially in critical domains like Healthcare, Education, and Energy. Human trust in AI plays a vital role in influencing decision-making processes of users interacting with AI. This paper presents VIRTSI (Variability and Impact of Reciprocal Trust States towards Intelligent systems), a novel rigorous computational model for human-AI Interaction. VIRTSI simulates human trust states, spanning from overtrust to distrust, through user modelling. It comprises: 1. A trust dynamics representational model based on Deterministic Finite State Automata (DFAs), illustrating transitions among cognitive trust states in response to AI-generated replies. 2. A trust evaluation model based on Confusion Matrices, originating from machine learning and Accuracy Metrics, providing a quantitative framework for analysing human trust dynamics. As a result, this is the first time that trust dynamics have been thoroughly traced in a representational model and a method has been developed to assess the impact of possibly harmful states like overtrust and distrust. An empirical study on the recently launched Large Language Model of generative AI, ChatGPT (version 3.5), provides a radical underexplored AI-generated platform for evaluating the human-AI interaction through VIRTSI. The study involved 1200 interactions of real users as well as AI experts together with experts in two very different domains of evaluation, namely software engineering and poetry. This study traces trust dynamics and the emerging human-AI interaction, in concrete examples of real user synergies with generative AI. The research reveals the vital role of maintaining normal trust states for optimal human-AI interaction and that both AI and human users need further steps towards this goal. The real-world implications of this research can guide the creation and evaluation of user interfaces with AI and the incorporation of functionalities in the development of generative AI chatbots in terms of trust by providing a new rigorous DFA representational method of trust dynamics and a corresponding new perspective of confusion matrix evaluation method of the dynamics’ impact in the efficiency of human-AI dialogues.}
}
@article{DHANDA2025102937,
title = {Reviewing human-robot collaboration in manufacturing: Opportunities and challenges in the context of industry 5.0},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {93},
pages = {102937},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102937},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524002242},
author = {Mandeep Dhanda and Benedict Alexander Rogers and Stephanie Hall and Elies Dekoninck and Vimal Dhokia},
keywords = {Human Robot Collaboration, Digital Manufacturing, Industry 5.0, Artificial Intelligence, Ontology},
abstract = {Industry 4.0 (I4.0) has been characterized by the increasing use of automation, artificial intelligence, and big data in manufacturing. It has brought different machines, tools, robots and devices together through integration with cyber-physical systems as well as Internet of Things and computer systems. This has dramatically improved efficiency, productivity, and flexibility of automated systems, but it has also raised concerns about the impact of automation on jobs, the ethical considerations and the future of work in general. Industry 5.0 (I5.0) is the next manufacturing paradigm evolution and builds on I4.0 with the addition of ‘people’, in which robots will be designed to work alongside humans in a safe and efficient manner. Human-robot collaboration (HRC) is its key enabler. In manufacturing, HRC has the potential to improve safety, efficiency, and productivity by allowing humans to focus on tasks that require creativity, judgment, and flexibility, while robots perform more repetitive and dangerous tasks. This paper explores the concept of HRC and its advancement within 21st century industry. It identifies the opportunities and challenges arising from the interactions between robots and humans in manufacturing applications, assembly, and inspection. It also highlights the significance of HRC in I4.0 and its potential in I5.0. In addition, the role of artificial intelligence, machine learning, large language models, information modelling (ontologies) and new emerging digital technologies (augmented reality, virtual reality, digital twins, cyber-physical system) in the development of HRC and I5.0 is documented and discussed adding new perspectives to the growing literature in this area. This investigation sheds light on the emerging paradigms that have come about as parts of I5.0 and the transformative role of human-robot interaction in shaping the future of manufacturing. This critical review provides a realistic picture of manufacturing automation and the benefits and weaknesses of current HRC systems. It presents a researched view on the concept, needs, enabling technologies and system frameworks of human-robot interaction in manufacturing, providing a practical vision and research agenda for future work in this area and its associated systems.}
}
@article{LIU2024103768,
title = {Enhancing Chinese abbreviation prediction with LLM generation and contrastive evaluation},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103768},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103768},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001286},
author = {Jingping Liu and Xianyang Tian and Hanwen Tong and Chenhao Xie and Tong Ruan and Lin Cong and Baohua Wu and Haofen Wang},
keywords = {Chinese abbreviation prediction, LLM generation, Contrastive evaluation},
abstract = {Chinese abbreviation prediction plays an important role in natural language processing. The prevalent approach often utilizes generation models to predict abbreviations for full forms, but relying solely on a single generation model may not yield high-quality abbreviations. We emphasize the importance of introducing an evaluation model after the generation model to assess the rationality of generated abbreviations. Hence, in this paper, we propose a novel two-stage method with LLM generation and contrastive evaluation for Chinese abbreviation prediction. In the first stage, we design a type discriminator to determine the abbreviation type and then introduce a pre-trained and fine-tuned LLM to generate multiple candidate abbreviations. In the second stage, we propose a contrastive evaluation model to assess the rationality of the candidates based on the abbreviation scorer and phrase scorer with a joint learning strategy. Experiments on two public datasets indicate that our method outperforms the current state-of-the-art method, achieving improvements of 3.32% and 1.73%, respectively. More importantly, we deploy it on the Fliggy application and the 20-day online A/B testing shows a 0.65% increase in Point of Interest Recognition Rate and a 1.37% increase in Page View Click-Through Rate when using abbreviations predicted by our method in the search system.}
}
@article{DWIVEDI2024102725,
title = {Artificial intelligence (AI) futures: India-UK collaborations emerging from the 4th Royal Society Yusuf Hamied workshop},
journal = {International Journal of Information Management},
volume = {76},
pages = {102725},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102725},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223001068},
author = {Yogesh K. Dwivedi and Laurie Hughes and Harshad K.D.H. Bhadeshia and Sophia Ananiadou and Anthony G. Cohn and Jacqueline M. Cole and Gareth J. Conduit and Maunendra Sankar Desarkar and Xinwei Wang},
keywords = {Artificial intelligence, ChatGPT, Generative AI, Gen AI, Large language models, Technological disruption, uncertainties, Natural Language Processing},
abstract = {“Artificial Intelligence” in all its forms has emerged as a transformative technology that is in the process of reshaping many aspects of industry and wider society at a global level. It has evolved from a concept to a technology that is driving innovation, transforming productivity and disrupting existing business models across numerous sectors. The industrial and societal impact of AI is profound and multifaceted, offering opportunities for growth, efficiency, and improved healthcare, but also raising ethical and societal challenges as the method is integrated into many aspects of human life and work. This editorial is developed by contributors of the 4th Royal Society Yusef Hamied Workshop ( in 2023 devoted to Artificial Intelligence), designed to enhance collaboration between Indian and the UK scientists and to explore future research opportunities. The insights shared at the workshop are shared here.}
}
@article{ELCHAFEI2024229,
title = {Arabic NER Evaluation: Pre-Trained Models via Contrastive Learning vs. LLM Few-Shot Prompting},
journal = {Procedia Computer Science},
volume = {244},
pages = {229-237},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.196},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029971},
author = {Passant Elchafei and Amany Fashwan},
keywords = {Arabic NER, Named Entity Recognition, Contrastive Learning, BERT, LLM, Few-Shot, LLaMA, GPT3.5, Prompt Engineering},
abstract = {Developing Natural Language Processing (NLP) tools for the Arabic language and its dialects is very challenging. Named Entity Recognition (NER) is one of these challenges, which serves as the core component in many NLP systems such as information extraction, question answering, machine translation and knowledge graph building. This paper sheds light on applying diferent approaches for Arabic NER (Flat and Nested) using a large and rich Arabic NER corpus, Wojood dataset, which consists of about 550K tokens annotated with 21 entity types. First, we apply the Wojood base model, AraBERTv2, along with various other Arabic BERT models such as MARBERTv2, CaMelBert, mBert, ..etc. Next, we utilize the Bi-Encoder Contrastive Learning (CL) approach, a framework developed by Microsoft, which maps candidate text spans and entity types into the same vector representation space. The primary challenge in this approach is distinguishing non-entity spans from entity mentions. This approach could achieve F1 score 91.25% for Flat and 91.40% for Nested NER. Additionally, for evaluating the predicted NER, we employ Few-Shot prompting on LLaMA, and GPT-3.5 using refined prompt-based strategy. Our findings reveal that LLaMA outperforms GPT3.5.}
}
@article{LIVNE20248380,
title = {nach0: multimodal natural and chemical languages foundation model††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d4sc00966e},
journal = {Chemical Science},
volume = {15},
number = {22},
pages = {8380-8389},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc00966e},
url = {https://www.sciencedirect.com/science/article/pii/S204165202400717X},
author = {Micha Livne and Zulfat Miftahutdinov and Elena Tutubalina and Maksim Kuznetsov and Daniil Polykovskiy and Annika Brundyn and Aastha Jhunjhunwala and Anthony Costa and Alex Aliper and Alán Aspuru-Guzik and Alex Zhavoronkov},
abstract = {Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder–decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups.}
}
@article{LONGO2024102301,
title = {Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions},
journal = {Information Fusion},
volume = {106},
pages = {102301},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102301},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000794},
author = {Luca Longo and Mario Brcic and Federico Cabitza and Jaesik Choi and Roberto Confalonieri and Javier Del Ser and Riccardo Guidotti and Yoichi Hayashi and Francisco Herrera and Andreas Holzinger and Richard Jiang and Hassan Khosravi and Freddy Lecue and Gianclaudio Malgieri and Andrés Páez and Wojciech Samek and Johannes Schneider and Timo Speith and Simone Stumpf},
keywords = {Explainable artificial intelligence, XAI, Interpretability, Manifesto, Open challenges, Interdisciplinarity, Ethical AI, Large language models, Trustworthy AI, Responsible AI, Generative AI, Multi-faceted explanations, Concept-based explanations, Causality, Actionable XAI, Falsifiability},
abstract = {Understanding black box models has become paramount as systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper highlights the advancements in XAI and its application in real-world scenarios and addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. We aim to develop a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 28 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.}
}
@article{LU2024109451,
title = {Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy},
journal = {iScience},
volume = {27},
number = {4},
pages = {109451},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109451},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224006722},
author = {Muyu Lu and Fengyu Gao and Xiaolong Tang and Linjiang Chen},
keywords = {Natural sciences, Chemistry, Computer science},
abstract = {Summary
This study explores the use of large language models (LLMs) in interpreting and predicting experimental outcomes based on given experimental variables, leveraging the human-like reasoning and inference capabilities of LLMs, using selective catalytic reduction of NOx with NH3 as a case study. We implement the chain of thought (CoT) concept to formulate logical steps for uncovering connections within the data, introducing an “Ordered-and-Structured” CoT (OSCoT) prompting strategy. We compare the OSCoT strategy with the more conventional “One-Pot” CoT (OPCoT) approach and with human experts. We demonstrate that GPT-4, equipped with this new OSCoT prompting strategy, outperforms the other two settings and accurately predicts experimental outcomes and provides intuitive reasoning for its predictions.}
}
@article{BAO2025102616,
title = {Data-driven stock forecasting models based on neural networks: A review},
journal = {Information Fusion},
volume = {113},
pages = {102616},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102616},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003944},
author = {Wuzhida Bao and Yuting Cao and Yin Yang and Hangjun Che and Junjian Huang and Shiping Wen},
keywords = {Stock forecast, Finance, Financial market, Neural network, Deep learning},
abstract = {As a core branch of financial forecasting, stock forecasting plays a crucial role for financial analysts, investors, and policymakers in managing risks and optimizing investment strategies, significantly enhancing the efficiency and effectiveness of economic decision-making. With the rapid development of information technology and computer science, data-driven neural network technologies have increasingly become the mainstream method for stock forecasting. Although recent review studies have provided a basic introduction to deep learning methods, they still lack detailed discussion on network architecture design and innovative details. Additionally, the latest research on emerging large language models and neural network structures has yet to be included in existing review literature. In light of this, this paper comprehensively reviews the literature on data-driven neural networks in the field of stock forecasting from 2015 to 2023, discussing various classic and innovative neural network structures, including Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Transformers, Graph Neural Networks (GNNs), Generative Adversarial Networks (GANs), and Large Language Models (LLMs). It analyzes the application and achievements of these models in stock market forecasting. Moreover, the article also outlines the commonly used datasets and various evaluation metrics in the field of stock forecasting, further exploring unresolved issues and potential future research directions, aiming to provide clear guidance and reference for researchers in stock forecasting.}
}
@article{GONG2024108399,
title = {Advancing microbial production through artificial intelligence-aided biology},
journal = {Biotechnology Advances},
volume = {74},
pages = {108399},
year = {2024},
issn = {0734-9750},
doi = {https://doi.org/10.1016/j.biotechadv.2024.108399},
url = {https://www.sciencedirect.com/science/article/pii/S0734975024000934},
author = {Xinyu Gong and Jianli Zhang and Qi Gan and Yuxi Teng and Jixin Hou and Yanjun Lyu and Zhengliang Liu and Zihao Wu and Runpeng Dai and Yusong Zou and Xianqiao Wang and Dajiang Zhu and Hongtu Zhu and Tianming Liu and Yajun Yan},
keywords = {Microbial production, Synthetic biology, Genome annotation, Enzyme function prediction, Artificial protein design, Pathway prediction, Artificial intelligence (AI), Large language models (LLMs)},
abstract = {Microbial cell factories (MCFs) have been leveraged to construct sustainable platforms for value-added compound production. To optimize metabolism and reach optimal productivity, synthetic biology has developed various genetic devices to engineer microbial systems by gene editing, high-throughput protein engineering, and dynamic regulation. However, current synthetic biology methodologies still rely heavily on manual design, laborious testing, and exhaustive analysis. The emerging interdisciplinary field of artificial intelligence (AI) and biology has become pivotal in addressing the remaining challenges. AI-aided microbial production harnesses the power of processing, learning, and predicting vast amounts of biological data within seconds, providing outputs with high probability. With well-trained AI models, the conventional Design-Build-Test (DBT) cycle has been transformed into a multidimensional Design-Build-Test-Learn-Predict (DBTLP) workflow, leading to significantly improved operational efficiency and reduced labor consumption. Here, we comprehensively review the main components and recent advances in AI-aided microbial production, focusing on genome annotation, AI-aided protein engineering, artificial functional protein design, and AI-enabled pathway prediction. Finally, we discuss the challenges of integrating novel AI techniques into biology and propose the potential of large language models (LLMs) in advancing microbial production.}
}
@article{MUZANENHAMO2024102735,
title = {ChatGPT and accounting in African contexts: Amplifying epistemic injustice},
journal = {Critical Perspectives on Accounting},
volume = {99},
pages = {102735},
year = {2024},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2024.102735},
url = {https://www.sciencedirect.com/science/article/pii/S1045235424000340},
author = {Penelope Muzanenhamo and Sean Bradley Power},
keywords = {ChatGPT, Epistemic injustice, Large language models, Pluriversality, Accounting, Africa},
abstract = {Large Language Models (LLMs) such as ChatGPT are likely to amplify epistemic injustice through the lack of transparency and traceability of data sources. The unethical alienation of original knowledge producers from their intellectual products, which are repackaged by LLMs as artificial intelligence, conceals power asymmetries in the global knowledge production and dissemination system. As elaborated by Miranda Fricker (2010), Western White male actors traditionally dominate knowledge production; therefore, ChatGPT and other LLMs are inclined to reproduce patriarchal perspectives as universal understandings of the World. Our commentary applies this logic to accounting practice and research in Africa, and asserts that epistemic injustice, resulting from colonization and racism, means that ontological and epistemological approaches situated in the accounting needs and experiences of African communities are missing from or poorly articulated by ChatGPT and other LLMs. If LLMs are to attain legitimacy as (ethical) sources of knowledge, regulation must be enforced to ensure transparency—as a foundation for promoting pluriversality and eliminating epistemic injustice.}
}
@article{DENG2024103001,
title = {OphGLM: An ophthalmology large language-and-vision assistant},
journal = {Artificial Intelligence in Medicine},
volume = {157},
pages = {103001},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103001},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002434},
author = {Zhuo Deng and Weihao Gao and Chucheng Chen and Zhiyuan Niu and Zheng Gong and Ruiheng Zhang and Zhenjie Cao and Fang Li and Zhaoyi Ma and Wenbin Wei and Lan Ma},
keywords = {Ophthalmology, Visual dialogue interaction, Large language models},
abstract = {Vision computer-aided diagnostic methods have been used in early ophthalmic disease screening and diagnosis. However, the limited output formats of these methods lead to poor human–computer interaction and low clinical applicability value. Thus, ophthalmic visual question answering is worth studying. Unfortunately, no practical solutions exist before Large Language Models(LLMs). In this paper, we investigate the ophthalmic visual diagnostic interaction problem. We construct an ophthalmology large language-and-vision assistant, OphGLM, consisting of an image encoder, a text encoder, a fusion module, and an LLM module. We establish a new Chinese ophthalmic fine-tuning dataset, FundusTuning-CN, including the fundus instruction and conversation sets. Based on FundusTuning-CN, we establish a novel LLM-tuning strategy to introduce visual model understanding and ophthalmic knowledge into LLMs at a low cost and high efficiency. Leveraging the pre-training of the image encoder, OphGLM demonstrates strong visual understanding and surpasses open-source visual language models in common fundus disease classification tasks. The FundusTuning-CN enables OphGLM to surpass open-source medical LLMs in both ophthalmic knowledge and interactive capabilities. Our proposed OphGLM has the potential to revolutionize clinical applications in ophthalmology. The dataset, code, and models will be publicly available at https://github.com/ML-AILab/OphGLM.}
}
@article{KWON2024488,
title = {On knowing a gene: A distributional hypothesis of gene function},
journal = {Cell Systems},
volume = {15},
number = {6},
pages = {488-496},
year = {2024},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2024.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405471224001236},
author = {Jason J. Kwon and Joshua Pan and Guadalupe Gonzalez and William C. Hahn and Marinka Zitnik},
keywords = {lexical semantics, gene function, machine learning, artificial intelligence, distributed representations, word embeddings, large language models, transformers},
abstract = {Summary
As words can have multiple meanings that depend on sentence context, genes can have various functions that depend on the surrounding biological system. This pleiotropic nature of gene function is limited by ontologies, which annotate gene functions without considering biological contexts. We contend that the gene function problem in genetics may be informed by recent technological leaps in natural language processing, in which representations of word semantics can be automatically learned from diverse language contexts. In contrast to efforts to model semantics as “is-a” relationships in the 1990s, modern distributional semantics represents words as vectors in a learned semantic space and fuels current advances in transformer-based models such as large language models and generative pre-trained transformers. A similar shift in thinking of gene functions as distributions over cellular contexts may enable a similar breakthrough in data-driven learning from large biological datasets to inform gene function.}
}
@article{ZHANG20241050,
title = {Accelerating drug discovery, development, and clinical trials by artificial intelligence},
journal = {Med},
volume = {5},
number = {9},
pages = {1050-1070},
year = {2024},
issn = {2666-6340},
doi = {https://doi.org/10.1016/j.medj.2024.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S2666634024003088},
author = {Yilun Zhang and Mohamed Mastouri and Yang Zhang},
keywords = {artificial intelligence, deep learning, drug development, clinical trial, small molecule, antibody, RNA},
abstract = {Summary
Artificial intelligence (AI) has profoundly advanced the field of biomedical research, which also demonstrates transformative capacity for innovation in drug development. This paper aims to deliver a comprehensive analysis of the progress in AI-assisted drug development, particularly focusing on small molecules, RNA, and antibodies. Moreover, this paper elucidates the current integration of AI methodologies within the industrial drug development framework. This encompasses a detailed examination of the industry-standard drug development process, supplemented by a review of medications presently undergoing clinical trials. Conclusively, the paper tackles a predominant obstacle within the AI pharmaceutical sector: the absence of AI-conceived drugs receiving approval. This paper also advocates for the adoption of large language models and diffusion models as a viable strategy to surmount this challenge. This review not only underscores the significant potential of AI in drug discovery but also deliberates on the challenges and prospects within this dynamically progressing field.}
}
@article{DING2025128849,
title = {Adversarial contrastive representation training with external knowledge injection for zero-shot stance detection},
journal = {Neurocomputing},
volume = {614},
pages = {128849},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128849},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224016205},
author = {Yifan Ding and Ying Lei and Anqi Wang and Xiangrun Liu and Tuanfei Zhu and Yizhou Li},
keywords = {Natural language process, Stance detection, Contrastive learning, Zero-shot, Adversarial learning, Large language model},
abstract = {Zero-shot stance detection (ZSSD) is a task that involves identifying the author’s perspective on specific issues in text, particularly when the target topic has not been encountered during the model training process, to address rapidly evolving topics on social media. This paper introduces a ZSSD framework named KEL-CA. To enable the model to more effectively utilize transferable stance features for representing unseen targets, the framework incorporates a multi-layer contrastive learning and adversarial domain transfer module. Unlike traditional contrastive or adversarial learning, our framework captures both correlations and distinctions between invariant and specific features, as well as between different stance labels, and enhances the generalization ability and robustness of the features. Subsequently, to address the problem of insufficient information about the target context, we designed a dual external knowledge injection module that uses a large language model (LLM) to extract external knowledge from a Wikipedia-based local knowledge base and a Chain-of-Thought (COT) process to ensure the timeliness and relevance of the knowledge to infer the stances of unseen targets. Experimental results demonstrate that our approach outperforms existing models on two benchmark datasets, thereby validating its efficacy in ZSSD tasks.}
}
@article{CHATTERJEE2024570,
title = {Checking Counterfeit Critiques on Commodities using Ensemble Classifiers Enhancing Information Credibility},
journal = {Procedia Computer Science},
volume = {233},
pages = {570-579},
year = {2024},
note = {5th International Conference on Innovative Data Communication Technologies and Application (ICIDCA 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.246},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924006057},
author = {Ram Chatterjee and Mrinal Pandey and Hardeo Kumar Thakur and Anand Gupta},
keywords = {Information credibility, Large Language Models, Amazon Mechanical Turks, Elastic-net Classifier, LightGBM Trees Classifier},
abstract = {The conundrum of the ubiquitous deceptive reviews has overruled the online ontology with the obsession of obscure but obligatory posting of product reviews for the customers to believe, behold and beget the online product marketing. This mandates contemporary research in the direction to delve deeper on the application and analysis of deceiving online reviews with matured and advanced AI models functional on large scale datasets to effectively and efficiently demarcate between the genuine and the sham. The research counteracts the counterfeiting product reviews via the applications, assessment and analysis of the befitting AI models - Elastic-net Classifier model based on block coordinate descent with Wordcloud and its further performance enhancement through LightGBM Trees Classifier with Grid Search and Early Stopping support, with Log-Loss as performance metric for experimentation to gain insight into the intricacies of detection, diagnosis and diminution of fake product reviews. The paper also delineates discriminative and affirmative aspects of the dataset quality, statistics, stability and standards inherent and coherent to the creation of the dataset using Large Language Models (LLMs) intrinsic to the zeitgeist juncture of recent times promoting machines to produce large scale, cost effective bogus reviews in lieu of the Amazon Mechanical Turks. The results obtained with the Log-Loss holdout score of 0.1462 conforming the LightGBM classifier proves its performance better than the Elastic-Net classifier, conforming it as better than the ROC-AUC in terms of its proximity to the prediction probability for the matching actual/true value.}
}
@article{LI2025112913,
title = {Context-enhanced framework for medical image report generation using multimodal contexts},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112913},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112913},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124015478},
author = {Hongzhao Li and Hongyu Wang and Xia Sun and Hua He and Jun Feng},
keywords = {Medical image, Report generation, Multimodal, Context, Large language model},
abstract = {As deep learning technology continues to advance, including large language models and multimodal models, its application in the medical field has become a widely recognized research topic. In this context, a series of automated systems based on deep learning have been developed, aiming to generate corresponding text reports from medical images. However, these current methods often generate text reports solely based on patients’ images, overlooking the multimodal medical context, which encompasses various factors such as clinical information, diagnostic results, and medical knowledge. This limitation restricts the clinical application of automatically generated reports. To address this issue, we propose a novel Context-Enhanced Framework for medical image report generation. Our approach integrates various multimodal contextual elements, including but not limited to clinical text, medical knowledge, diagnostic results, and image data, to enrich the report generation process. We evaluated this framework on two public chest X-ray datasets, IU-Xray and MIMIC-CXR, using standard natural language generation and clinical effectiveness metrics. The results showed state-of-the-art performance, indicating improved quality in language and clinical accuracy. Our source code is available here.}
}
@article{YANG2024156116,
title = {PresRecRF: Herbal prescription recommendation via the representation fusion of large TCM semantics and molecular knowledge},
journal = {Phytomedicine},
volume = {135},
pages = {156116},
year = {2024},
issn = {0944-7113},
doi = {https://doi.org/10.1016/j.phymed.2024.156116},
url = {https://www.sciencedirect.com/science/article/pii/S0944711324007736},
author = {Kuo Yang and Xin Dong and Shuhan Zhang and Haibin Yu and Liqun Zhong and Lei Zhang and He Zhao and Yutong Hou and Xinpeng Song and Xuezhong Zhou},
keywords = {Herbal prescription recommendation, TCM semantics, Molecular knowledge, Herb dosage prediction, Feature fusion, Large language model},
abstract = {Background: Herbal prescription recommendation (HPR) is a hotspot in the research of clinical intelligent decision support. Recently plentiful HPR models based on deep neural networks have been proposed. Owing to insufficient data, e.g., lack of knowledge of molecular, TCM theory, and herbal dosage in HPR modeling, the existing models suffer from challenges, e.g., plain prediction precision, and are far from real-world clinics. Purpose: To address these problems, we proposed a novel herbal prescription recommendation model with the representation fusion of large TCM semantics and molecular knowledge (termed PresRecRF). Study Design and Methods: PresRecRF comprises three key modules. The representation learning module consists of two key components: a molecular knowledge representation component, integrating molecular knowledge into the herb-symptom-protein knowledge graph to enhance representations for herbs and symptoms; and a TCM knowledge representation component, leveraging BERT and ChatGPT to acquire TCM knowledge-enriched semantic representations. We introduced a representation fusion module to effectively merge molecular and TCM semantic representations. In the herb recommendation module, a multi-task objective loss is implemented to predict both herbs and dosages simultaneously. Results: The experimental results on two clinical datasets show that PresRecRF can achieve the optimal performance. Further analysis of ablation, hyper-parameters, and case studies indicate the effectiveness and reliability of the proposed model, suggesting that it can help precision medicine and treatment recommendations. Conclusion: The entire process of the proposed PresRecRF model closely mirrors the actual diagnosis and treatment procedures carried out by doctors, which are better applied in real clinical scenarios. The source codes of PresRecRF is available at https://github.com/2020MEAI/PresRecRF.}
}
@article{LI2024124760,
title = {mt4CrossOIE: Multi-stage tuning for cross-lingual open information extraction},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124760},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124760},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424016270},
author = {Tongliang Li and Zixiang Wang and Linzheng Chai and Jian Yang and Jiaqi Bai and Yuwei Yin and Jiaheng Liu and Hongcheng Guo and Liqun Yang and Hebboul {Zine el-abidine} and Zhoujun Li},
keywords = {Information extraction, Cross-lingual transfer, Disentangled training, Multi-lingual training, Mixture of LoRA},
abstract = {Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multi-stage tuning framework called mt4CrossOIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture of LoRAs for model-based cross-lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual raw data for data-based cross-lingual transfer. The model is trained with multi-lingual objectives on our proposed dataset OpenIE4++ by combining the model-based and data-based transfer techniques. Experimental results on various benchmarks emphasize the importance of aggregating multiple plug-in-and-play language-specific modules and demonstrate the effectiveness of mt4CrossOIE in cross-lingual OIE.22https://github.com/CSJianYang/Multilingual-Multimodal-NLP.}
}
@article{HOSEINI2024100819,
title = {A survey on semantic data management as intersection of ontology-based data access, semantic modeling and data lakes},
journal = {Journal of Web Semantics},
volume = {81},
pages = {100819},
year = {2024},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100819},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000052},
author = {Sayed Hoseini and Johannes Theissen-Lipp and Christoph Quix},
keywords = {Semantic data management, Semantic web, Big data, Data lakes, Ontology-based data-access},
abstract = {In recent years, data lakes emerged as a way to manage large amounts of heterogeneous data for modern data analytics. One way to prevent data lakes from turning into inoperable data swamps is semantic data management. Such approaches propose the linkage of metadata to knowledge graphs based on the Linked Data principles to provide more meaning and semantics to the data in the lake. Such a semantic layer may be utilized not only for data management but also to tackle the problem of data integration from heterogeneous sources, in order to make data access more expressive and interoperable. In this survey, we review recent approaches with a specific focus on the application within data lake systems and scalability to Big Data. We classify the approaches into (i) basic semantic data management, (ii) semantic modeling approaches for enriching metadata in data lakes, and (iii) methods for ontology-based data access. In each category, we cover the main techniques and their background, and compare latest research. Finally, we point out challenges for future work in this research area, which needs a closer integration of Big Data and Semantic Web technologies.}
}
@article{QIAO2025104877,
title = {Food recommendation towards personalized wellbeing},
journal = {Trends in Food Science & Technology},
volume = {156},
pages = {104877},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.104877},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425000135},
author = {Guanhua Qiao and Dachuan Zhang and Nana Zhang and Xiaotao Shen and Xidong Jiao and Wenwei Lu and Daming Fan and Jianxin Zhao and Hao Zhang and Wei Chen and Jinlin Zhu},
keywords = {Food recommendation system, Machine learning, Personalized diet, Recommendation algorithm, Precision nutrition},
abstract = {Background
The intersection of nutrition and technology gave birth to the research of food recommendation system (FRS), which marked the transformation of traditional diet to a more personalized and healthy direction. The FRS uses advanced data analysis and machine learning technology to provide customized dietary advice according to users' personal preferences, and nutritional needs, which plays a vital role in promoting public health and reducing disease risks.
Scope and approach
This review presents the architecture of FRS and deeply discusses various recommendation algorithms, including the content-based method, collaborative filtering method, knowledge graph-based method, and hybrid methods. The review further introduces existing data resources and evaluation metrics, and highlights key technologies in user profiling and food analysis. In addition, the wide application of personalized FRS is summarized, and the importance of these systems in satisfying users' dietary preferences and maintaining balanced nutrition is emphasized. Finally, the key challenges and development trends of FRS are deeply analyzed from data level, model level and user experience level.
Key findings and conclusions
Personalized FRS shows great potential in helping users make healthier dietary decisions. Although there are still many challenges, such as dealing with heterogeneous data and interpretability. But with the progress of technology, there will be broader development in the future. For example, the powerful data processing ability of deep learning will effectively improve the accuracy of the system. In addition, the application of interactive recommendation system and large language model will also provide strong support for satisfying user experience and improving acceptance.}
}
@article{GALUSZA2024124990,
title = {Graph-based document-level relationship extraction for risk analysis: A transitive and dialog coherence approach},
journal = {Expert Systems with Applications},
volume = {257},
pages = {124990},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124990},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424018578},
author = {Michał Gałusza and Andrzej Walczak},
keywords = {Risk analysis, Relationship extraction, Graph representation, Language models, Knowledge acquisition, Semantic networks, Knowledge graphs, Entity recognition},
abstract = {This paper proposes a solution to extracting relationships at the document level in the context of risk analysis. It addresses the problem of identifying the flow of hazard’s impact in the system’s description or the description of the consequences of its failure. The problem is challenging, as information on impact may form complex, interwoven relations distributed across many sentences within a description and across many sources. The proposed approach involves a stepwise decomposition of the descriptions: first into a Semantic Frames Graph (SFG) to detect risk-relevant relationships, then into the Intermediate Relationship Graph (IRG), which is built upon detected relations, and finally, the aggregation of risk interaction represented in Asset-Vulnerability-Hazard (A-V-H) graph. This approach allows for the modeling of risk interactions without needing a dedicated training set, as the authors present a method for relationship detection based on the verbalization of transitive relationships and dialog consistency validated through prompt engineering over the generative language model. Overall, this research provides insights into a novel approach to acquiring risk interactions using document-level relationship extraction. It demonstrates its potential in graph-based representations and transitive relationships to understand complex risk interactions.}
}
@article{SHANG2025113017,
title = {From local to global: Leveraging document graph for named entity recognition},
journal = {Knowledge-Based Systems},
pages = {113017},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113017},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000656},
author = {Yu-Ming Shang and Hongli Mao and Tian Tian and Heyan Huang and Xian-Ling Mao},
keywords = {Named entity recognition, Document-level, Span graph, LLM},
abstract = {Named Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that aims to identify the span and category of entities within text. Recent advancements have demonstrated significant improvements in NER performance by incorporating document-level context. However, due to input length limitations, these models only consider the context of nearby sentences, failing to capture global long-range dependencies within the entire document. To address this issue, we propose a novel span-based two-stage method that formulates the document as a span graph, enabling the capture of global long-range dependencies at both token and span levels. Specifically, (1) we first train a binary classifier without considering entity types to extract candidate spans from each sentence. (2) Then, we leverage the robust contextual understanding and structural reasoning capabilities of Large Language Models (LLMs) like GPT to incrementally integrate these spans into the document-level span graph. By utilizing this span graph as a guide, we retrieve relevant contextual sentences for each target sentence and jointly encode them using BERT to capture token-level dependencies. Furthermore, by employing a Graph Transformer with well-designed position encoding to incorporate graph structure, our model effectively exploits span-level dependencies throughout the document. Extensive experiments on resource-rich nested and flat NER datasets, as well as low-resource distantly supervised NER datasets, demonstrate that our proposed model outperforms previous state-of-the-art models, showcasing its effectiveness in capturing long-range dependencies and enhancing NER accuracy.}
}
@article{ZHAO2025103951,
title = {ME3A: A Multimodal Entity Entailment framework for multimodal Entity Alignment},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103951},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103951},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003108},
author = {Yu Zhao and Ying Zhang and Xuhui Sui and Xiangrui Cai},
keywords = {Entity Alignment, Multimodal learning, Knowledge graph, Prompt learning},
abstract = {Current methods for multimodal entity alignment (MEA) primarily rely on entity representation learning, which undermines entity alignment performance because of cross-KG interaction deficiency and multimodal heterogeneity. In this paper, we propose a Multimodal Entity Entailment framework of multimodal Entity Alignment task, ME3A, and recast the MEA task as an entailment problem about entities in the two KGs. This way, the cross-KG modality information directly interacts with each other in the unified textual space. Specifically, we construct the multimodal information in the unified textual space as textual sequences: for relational and attribute modalities, we combine the neighbors and attribute values of entities as sentences; for visual modality, we map the entity image as trainable prefixes and insert them into sequences. Then, we input the concatenated sequences of two entities into the pre-trained language model (PLM) as an entailment reasoner to capture the unified fine-grained correlation pattern of the multimodal tokens between entities. Two types of entity aligners are proposed to model the bi-directional entailment probability as the entity similarity. Extensive experiments conducted on nine MEA datasets with various modality combination settings demonstrate that our ME3A effectively incorporates multimodal information and surpasses the performance of the state-of-the-art MEA methods by 16.5% at most.}
}
@article{QIU2024200308,
title = {ChatGPT and finetuned BERT: A comparative study for developing intelligent design support systems},
journal = {Intelligent Systems with Applications},
volume = {21},
pages = {200308},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200308},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323001333},
author = {Yunjian Qiu and Yan Jin},
keywords = {Language model, Knowledge transferring, Knowledge elicitation, Text classification, Text generation},
abstract = {Large Language Models (LLMs), like ChatGPT, have sparked considerable interest among researchers across diverse disciplines owing to their remarkable text processing and generation capabilities. While ChatGPT is typically employed for tasks involving general knowledge, researchers increasingly explore the potential of this LLM-based tool in specific domains to enhance productivity. This study aims to compare the performance of a finetuned BERT model with that of ChatGPT on a domain-specific dataset in the context of developing an intelligent design support system. Through experiments conducted on classification and generation tasks, the knowledge transfer and elicitation abilities of ChatGPT are examined and contrasted with those of the finetuned BERT model. The findings indicate that ChatGPT exhibits comparable performance to the finetuned BERT model in sentence-level classification tasks but struggles with short sequences. However, ChatGPT's classification performance significantly improves when a few-shot setting is applied. Moreover, it can filter out unrelated data and enhance dataset quality by assimilating the underlying domain knowledge. Regarding content generation, ChatGPT with a zero-shot setting produces informative and readable output for domain-specific questions, albeit with an excessive amount of unrelated information, which can burden readers. In conclusion, ChatGPT demonstrates a promising potential for application in facilitating data labeling, knowledge transfer, and knowledge elicitation tasks. With minimal guidance, ChatGPT can substantially enhance the efficiency of domain experts in accomplishing their objectives. The findings suggest a nuanced integration of artificial intelligence (AI) with human expertise, bridging the gap from mere classification models to sophisticated human-analogous text generation systems. This signals a future in AI-augmented engineering design where the robust capabilities of AI technologies integrate with human creativity and innovation, creating a dynamic interactions to redefine how we tackle design challenges.}
}
@article{ARSLAN20244534,
title = {Exploring Business Events using Multi-source RAG},
journal = {Procedia Computer Science},
volume = {246},
pages = {4534-4540},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023226},
author = {Muhammad Arslan and Saba Munawar and Christophe Cruz},
keywords = {Business events, Extraction methods, Large Language Models, Retrieval-Augmented Generation, Dynamic business environments},
abstract = {Business events signify crucial activities within a company, indicating growth opportunities and investment prospects. They encompass various developments such as recruitment drives, market expansions, mergers, and product launches. Understanding these events is vital for businesses seeking to stay updated with market dynamics, as they provide real-time insights into a company’s trajectory. Moreover, comprehending the business events of one company can offer strategic advantages to others, facilitating informed decision-making and fostering collaboration within the business ecosystem. Extracting information about these events involves diverse structured, semi-structured, and unstructured data sources, posing challenges for traditional extraction methods. Despite the promise shown by existing openly available LLMs driven by Generative Artificial Intelligence (GenAI), they face challenges when dealing with domain-specific queries. Retrieval-Augmented Generation (RAG) addresses this challenge by seamlessly integrating multiple external data sources of varying structures. In our study, we demonstrate how RAG with LLM facilitates precise extraction of business events, ensuring adaptability in dynamic business environments where datasets are constantly evolving.}
}
@article{DOO2023877,
title = {Exploring the Clinical Translation of Generative Models Like ChatGPT: Promise and Pitfalls in Radiology, From Patients to Population Health},
journal = {Journal of the American College of Radiology},
volume = {20},
number = {9},
pages = {877-885},
year = {2023},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2023.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1546144023005161},
author = {Florence X. Doo and Tessa S. Cook and Eliot L. Siegel and Anupam Joshi and Vishwa Parekh and Ameena Elahi and Paul H. Yi},
keywords = {generative artificial intelligence, radiology, limitations, large language models, ChatGPT},
abstract = {Generative artificial intelligence (AI) tools such as GPT-4, and the chatbot interface ChatGPT, show promise for a variety of applications in radiology and health care. However, like other AI tools, ChatGPT has limitations and potential pitfalls that must be considered before adopting it for teaching, clinical practice, and beyond. We summarize five major emerging use cases for ChatGPT and generative AI in radiology across the levels of increasing data complexity, along with pitfalls associated with each. As the use of AI in health care continues to grow, it is crucial for radiologists (and all physicians) to stay informed and ensure the safe translation of these new technologies.}
}
@article{CHIANG2024110918,
title = {Customized GPT model largely increases surgery decision accuracy for pharmaco-resistant epilepsy},
journal = {Journal of Clinical Neuroscience},
volume = {130},
pages = {110918},
year = {2024},
issn = {0967-5868},
doi = {https://doi.org/10.1016/j.jocn.2024.110918},
url = {https://www.sciencedirect.com/science/article/pii/S0967586824004570},
author = {Kuo-Liang Chiang and Yu-Cheng Chou and Hsin Tung and Chin-Yin Huang and Liang-Po Hsieh and Kai-Ping Chang and Shang-Yeong Kwan and Wan-Yu Huang},
keywords = {Semiology, Seizure descriptors, Localization, Generative pre-trained transformer, Large-scale language model},
abstract = {Background
To develop an enhanced epilepsy diagnosis system by integrating an expert-informed ontology with a custom generative pre-trained transformer (GPT), validated by inferring possible seizure lateralization and localization using retrospective textual data from the pre-surgical assessments of patients with pharmaco-resistant epilepsy (PRE).
Methods
We developed an AI system for epilepsy diagnosis using Protégé with OWL/SWRL, integrating a knowledge base with seizure semiology, seizure types EEG descriptors, expert insights, and literature to pinpoint seizure locations. A customized GPT model was then tailored for specific diagnostic needs. Validated through 16 surgical cases, the system’s accuracy in seizure localization and the JSON (JavaScript Object Notation) Epilepsy Matcher’s term matching capabilities were confirmed against a Protégé-based knowledge base.
Results
A total of 117 patients with PRE underwent video-EEG monitoring at a single institution. However, only 16 of these patients received epilepsy surgery. The Protégé system achieved 75 % accuracy in diagnosing epilepsy from 16 cases using semiology, which increased to 87.5 % with EEG data. The Json Epilepsy Matcher further improved accuracy to 87.5 % with symptoms alone and 93.8 % when including EEG data, highlighting the benefits of applying GPT techniques.
Conclusions
This study highlights the efficacy of the JSON Epilepsy Matcher in improving seizure diagnosis accuracy. When combined with EEG data, it achieves a 93.8 % accuracy rate, suggesting a potential improvement in the practicality and generalizability of the original ontology expert system, boosting physicians’ confidence in confirming surgery and potentially sparing many children from prolonged suffering. This innovative approach not only improves diagnostic accuracy but also sets a precedent for future applications of AI in neurology.}
}
@article{PAN2024111224,
title = {A semantic augmented approach to FEMA P-58 based dynamic regional seismic loss estimation application},
journal = {Journal of Building Engineering},
volume = {98},
pages = {111224},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.111224},
url = {https://www.sciencedirect.com/science/article/pii/S235271022402792X},
author = {Zeyu Pan and Jianyong Shi and Liu Jiang},
keywords = {Dynamic regional seismic loss estimation, Mainshock-aftershock sequence, FEMA P-58, Semantic web, Large language model},
abstract = {Regional seismic loss estimation (RSLE) is a crucial process in both immediate post-earthquake emergency response and long-term reconstruction endeavors. Over the years, significant progress has been made in RSLE: sensing approaches such as field investigation and remote sensing offers a comprehensive overview necessary for real-time disaster response, while simulation techniques such as the methodology proposed in Federal Emergency Management Agency (FEMA) P-58 series provide insights into the mechanism of disaster development and its potential long-term impacts on urban assets. Nonetheless, challenges persist in the realm of practical RSLE applications. Firstly, a dynamic understanding of a disaster event and its influence is deemed important for effective emergency responses while challenging to achieve in current approaches. Secondly, stakeholders with varying roles, including administrators, rescue teams and ordinary citizens have distinct information requirements for RSLE. Thirdly, the complexity of seismic loss estimation, involving diverse data sources such as building information models, performance models, fragility data and sensor observations, poses interoperability issue. To tackle these issues, this article introduces a dynamic, multi-granularity, ontological representation scheme tailored for RSLE decision-supporting systems. This scheme operates across various scales, from individual building components to broader regional scales by synergistically employing FEMA P-58 guidelines and Semantic Web technologies. Upon the corresponding semantics, a question-and-answer agent powered by large language model is further developed to facilitate interaction requirements within the RSLE process via FEMA P-58 pipeline. The practical efficacy of this approach is validated through a prototype deployed under a real earthquake event, demonstrating its value in real-world scenarios.}
}
@article{LI2024e04051,
title = {Classification and application of deep learning in construction engineering and management – A systematic literature review and future innovations},
journal = {Case Studies in Construction Materials},
volume = {21},
pages = {e04051},
year = {2024},
issn = {2214-5095},
doi = {https://doi.org/10.1016/j.cscm.2024.e04051},
url = {https://www.sciencedirect.com/science/article/pii/S2214509524012038},
author = {Qingze Li and Yang Yang and Gang Yao and Fujia Wei and Rui Li and Mingtao Zhu and Huiwen Hou},
keywords = {Deep learning(DL), Construction engineering management(CEM), Condition monitoring, Damage detection, Large language models (LLM)},
abstract = {In the ever-evolving landscape of construction engineering and management (CEM), the dynamic and unique characteristics of construction project environments constantly present multifaceted challenges. These challenges are characterized by the extensive volume of project-specific information and intricate engineering data. Deep learning (DL), with its advanced analytical capabilities, has been emerging as a robust solution to these complexities. While the application of DL in CEM is on an upward trajectory, a systematic review of its implementation is conspicuously lacking. This paper, therefore, embarks on a scientometric and qualitative analysis of 296 DL-based studies related to CEM from 2014 to 2024 in the renowned data science repositories Scopus, Science Direct and Web of Science to explore the characteristics of journals, keywords and clusters. It is found that six research topics have fully utilized the advantages of DL in CEM in the last decade, including construction equipment management, structural health monitoring, construction site safety management, construction schedule management, worker health management and workforce assessment and intelligent design. Then, the studies under each research topic are summarized separately and a searchable taxonomy is proposed that secondarily categorizes each study according to the specific CEM task and DL method used to facilitate understanding and access. Finally, the primary obstacles encountered in DL itself and in its practical application in CEM are discussed. It further articulates five critical future research directions that are evolving in tandem with advances in CEM, multimodal construction site management, real-time structural health monitoring and prediction, project progress visualization and management, intelligent design with data sharing and the incorporating large language models (LLM) for text data analysis. The three goals of this study are providing CEM researchers and practitioners with an in-depth and nuanced understanding of DL, elucidating the diverse nature of CEM activities and the resulting benefits of applying DL, and identifying future opportunities for applying DL in CEM to inform subsequent ongoing academic inquiry and pragmatic applications.}
}
@article{MANN2023108446,
title = {SUSIE: Pharmaceutical CMC ontology-based information extraction for drug development using machine learning},
journal = {Computers & Chemical Engineering},
volume = {179},
pages = {108446},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108446},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003162},
author = {Vipul Mann and Shekhar Viswanath and Shankar Vaidyaraman and Jeya Balakrishnan and Venkat Venkatasubramanian},
keywords = {Ontology, Pharmaceutical drug development, Information extraction, Hybrid machine learning, Chemistry manufacturing and control},
abstract = {Automatically extracting information from unstructured text in pharmaceutical documents is important for drug discovery and development. This information can be integrated with structured datasets to ultimately accelerate pharmaceutical product development. To this end, we report an end-to-end information extraction framework based on a custom-built pharmaceutical drug development ontology, a weak supervision framework, contextualization algorithms, and a fine-tuned BioBERT model (adaptation of BERT or Bidirectional Encoder Representations from Transformers for biomedical text). The proposed framework, SUSIE (Schema-based Unsupervised Semantic Information Extraction), was trained on ICH (International Conference on Harmonization) documents to identify important entities and relations from unstructured text and auto-generate knowledge graphs representing crucial information in a structured format. On the entity identification task, the framework achieves a test accuracy and F1-score of 96% and 88%, respectively, on out-of-sample documents. A major contribution of this work is to build an automated, unsupervised information extraction framework around a domain-specific, custom-built pharmaceutical drug development ontology without the need for manual curation of training datasets for specific tasks. The efficacy of the approach was tested on out-of-sample documents including an internal Eli Lilly technical document.}
}
@article{DASILVEIRA2024102286,
title = {A knowledge-sharing platform for space resources},
journal = {Data & Knowledge Engineering},
volume = {151},
pages = {102286},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102286},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000107},
author = {Marcos {Da Silveira} and Louis Deladiennee and Emmanuel Scolan and Cedric Pruski},
keywords = {Knowledge engineering, Knowledge graph, Ontology, Space resources},
abstract = {The ever-increasing interest of academia, industry, and government institutions in space resource information highlights the difficulty of finding, accessing, integrating, and reusing this information. Although information is regularly published on the internet, it is disseminated on many different websites and in different formats, including scientific publications, patents, news, and reports. We are currently developing a knowledge management and sharing platform for space resources. This tool, which relies on the combined use of knowledge graphs and ontologies, formalises the domain knowledge contained in the above-mentioned documents and makes it more readily available to the community. In this article, we describe the concepts and techniques of knowledge extraction and management adopted during the design and implementation of the platform.}
}
@article{GUILLAUMET2024131,
title = {The power of generative AI for CRIS systems: a new paradigm for scientific information management},
journal = {Procedia Computer Science},
volume = {249},
pages = {131-149},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.057},
url = {https://www.sciencedirect.com/science/article/pii/S187705092403268X},
author = {Anna Guillaumet},
keywords = {CRIS, AI, GenerativeAI, euroCRIS, FECYT, DRIS, OpenAccess, Research, CERIF, FAIR, AI-Act, ENIA, Sandbox, Law, Regulations, Standards, ethics},
abstract = {The paper analyses the implications of the emergence of artificial intelligence (AI), especially generative AI, on current research information systems (CRIS). It reviews the recent European regulations for high-risk AI systems, the Spanish AI strategy, and the IntelComp project as use cases. The study found that the maturity of CRIS systems, coupled with the increasing complexity due to data aggregation, sets the stage for innovative AI applications. The paper proposes key domains where AI can impact and be applied in CRIS, including data management, research assessment, and advanced analytics. It also provides examples of how generative AI can be leveraged to enhance scientific information management within CRIS. The findings highlight the need to ensure the responsible and ethical development of AI technologies in the research domain.}
}
@article{ZOU2025102606,
title = {Deep learning for cross-domain data fusion in urban computing: Taxonomy, advances, and outlook},
journal = {Information Fusion},
volume = {113},
pages = {102606},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102606},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003841},
author = {Xingchen Zou and Yibo Yan and Xixuan Hao and Yuehong Hu and Haomin Wen and Erdong Liu and Junbo Zhang and Yong Li and Tianrui Li and Yu Zheng and Yuxuan Liang},
keywords = {Urban computing, Data fusion, Deep learning, Multi-modal data, Large language models, Sustainable development},
abstract = {As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.}
}
@article{TING2025100893,
title = {AstroMLab 1: Who wins astronomy jeopardy!?},
journal = {Astronomy and Computing},
volume = {51},
pages = {100893},
year = {2025},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2024.100893},
url = {https://www.sciencedirect.com/science/article/pii/S2213133724001082},
author = {Y.-S. Ting and T.D. Nguyen and T. Ghosal and R. Pan and H. Arora and Z. Sun and T. {de Haan} and N. Ramachandra and A. Wells and S. Madireddy and A. Accomazzi},
keywords = {Large Language Models, Astronomy, Benchmarking, Question Answering, Scientific Knowledge Assessment},
abstract = {We present a comprehensive evaluation of proprietary and open-weights large language models using the first astronomy-specific benchmarking dataset. This dataset comprises 4,425 multiple-choice questions curated from the Annual Review of Astronomy and Astrophysics, covering a broad range of astrophysical topics.11Jeopardy is a popular American quiz show where contestants are tested on their knowledge across various subjects. Other similar shows include Who’s Still Standing () in China and University Challenge in the UK, among others. Our analysis examines model performance across various astronomical subfields and assesses response calibration, crucial for potential deployment in research environments. Claude-3.5-Sonnet outperforms competitors by up to 4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we observed a universal reduction in cost every 3-to-12 months to achieve similar score in this particular astronomy benchmark. open-weights models have rapidly improved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with some of the best proprietary models. We identify performance variations across topics, with non-English-focused models generally struggling more in exoplanet-related fields, stellar astrophysics, and instrumentation related questions. These challenges likely stem from less abundant training data, limited historical context, and rapid recent developments in these areas. This pattern is observed across both open-weights and proprietary models, with regional dependencies evident, highlighting the impact of training data diversity on model performance in specialized scientific domains. Top-performing models demonstrate well-calibrated confidence, with correlations above 0.9 between confidence and correctness, though they tend to be slightly underconfident. The development for fast, low-cost inference of open-weights models presents new opportunities for affordable deployment in astronomy. The rapid progress observed suggests that LLM-driven research in astronomy may become feasible in the near future.}
}
@article{BERTHON2024461,
title = {Trajectories of AI technologies: Insights for managers},
journal = {Business Horizons},
volume = {67},
number = {5},
pages = {461-470},
year = {2024},
note = {SPECIAL ISSUE: WRITTEN BY CHATGPT},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2024.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0007681324000284},
author = {Pierre Berthon and Taylan Yalcin and Ekin Pehlivan and Tamara Rabinovich},
keywords = {Trajectories of technology, Generative AI, Chatbots, ChatGPT, Large language models, Social media},
abstract = {Generative artificial intelligence (GenAI) has long been considered a technology for the future. With the release of the chatbot ChatGPT 4, many now feel the future has arrived. Long in gestation, this new technology promises many benefits to humankind, but worries persist that as AI technology scales and comes to rival or exceed human intelligence, the servant may become the master. Amid such hyperbole, the more nuanced trajectories of this technology have been neglected. In this article, we use the Trajectories of Technology (ToT) framework developed by Berthon and colleagues to explore the disparate paths that AI has taken and will take in the coming years, especially in the form of chatbots. This framework provides managers with a conceptual tool to strategically plan for the enormous promises and perils of AI in general and of chatbots specifically.}
}
@article{KPODO2024109349,
title = {AgXQA: A benchmark for advanced Agricultural Extension question answering},
journal = {Computers and Electronics in Agriculture},
volume = {225},
pages = {109349},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109349},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924007403},
author = {Josué Kpodo and Parisa Kordjamshidi and A. Pouyan Nejadhashemi},
keywords = {Agricultural Extension, Question-Answering, Annotated Dataset, Large Language Models, Zero-Shot Learning},
abstract = {Large language models (LLMs) have revolutionized various scientific fields in the past few years, thanks to their generative and extractive abilities. However, their applications in the Agricultural Extension (AE) domain remain sparse and limited due to the unique challenges of unstructured agricultural data. Furthermore, mainstream LLMs excel at general and open-ended tasks but struggle with domain-specific tasks. We proposed a novel QA benchmark dataset, AgXQA, for the AE domain to address these issues. We trained and evaluated our domain-specific LM, AgRoBERTa, which outperformed other mainstream encoder- and decoder- LMs, on the extractive QA downstream task by achieving an EM score of 55.15% and an F1 score of 78.89%. Besides automated metrics, we also introduced a custom human evaluation metric, AgEES, which confirmed AgRoBERTa’s performance, as demonstrated by a 94.37% agreement rate with expert assessments, compared to 92.62% for GPT 3.5. Notably, we conducted a comprehensive qualitative analysis, whose results provide further insights into the weaknesses and strengths of both domain-specific and general LMs when evaluated on in-domain NLP tasks. Thanks to this novel dataset and specialized LM, our research enhanced further development of specialized LMs for the agriculture domain as a whole and AE in particular, thus fostering sustainable agricultural practices through improved extractive question answering.}
}
@article{SIVARAJKUMAR2024,
title = {Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/52289},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000322},
author = {Sonish Sivarajkumar and Fengyi Gao and Parker Denny and Bayan Aldhahwani and Shyam Visweswaran and Allyn Bove and Yanshan Wang},
keywords = {natural language processing, electronic health records, rehabilitation, physical exercise, ChatGPT, artificial intelligence, stroke, physical rehabilitation, rehabilitation therapy, exercise, machine learning},
abstract = {Background
The rehabilitation of a patient who had a stroke requires precise, personalized treatment plans. Natural language processing (NLP) offers the potential to extract valuable exercise information from clinical notes, aiding in the development of more effective rehabilitation strategies.
Objective
This study aims to develop and evaluate a variety of NLP algorithms to extract and categorize physical rehabilitation exercise information from the clinical notes of patients who had a stroke treated at the University of Pittsburgh Medical Center.
Methods
A cohort of 13,605 patients diagnosed with stroke was identified, and their clinical notes containing rehabilitation therapy notes were retrieved. A comprehensive clinical ontology was created to represent various aspects of physical rehabilitation exercises. State-of-the-art NLP algorithms were then developed and compared, including rule-based, machine learning–based algorithms (support vector machine, logistic regression, gradient boosting, and AdaBoost) and large language model (LLM)–based algorithms (ChatGPT [OpenAI]). The study focused on key performance metrics, particularly F1-scores, to evaluate algorithm effectiveness.
Results
The analysis was conducted on a data set comprising 23,724 notes with detailed demographic and clinical characteristics. The rule-based NLP algorithm demonstrated superior performance in most areas, particularly in detecting the “Right Side” location with an F1-score of 0.975, outperforming gradient boosting by 0.063. Gradient boosting excelled in “Lower Extremity” location detection (F1-score: 0.978), surpassing rule-based NLP by 0.023. It also showed notable performance in the “Passive Range of Motion” detection with an F1-score of 0.970, a 0.032 improvement over rule-based NLP. The rule-based algorithm efficiently handled “Duration,” “Sets,” and “Reps” with F1-scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot prompts, achieved high recall but generally lower precision and F1-scores. However, it notably excelled in “Backward Plane” motion detection, achieving an F1-score of 0.846, surpassing the rule-based algorithm’s 0.720.
Conclusions
The study successfully developed and evaluated multiple NLP algorithms, revealing the strengths and weaknesses of each in extracting physical rehabilitation exercise information from clinical notes. The detailed ontology and the robust performance of the rule-based and gradient boosting algorithms demonstrate significant potential for enhancing precision rehabilitation. These findings contribute to the ongoing efforts to integrate advanced NLP techniques into health care, moving toward predictive models that can recommend personalized rehabilitation treatments for optimal patient outcomes.}
}
@article{HUANG2024362,
title = {From explainable to interpretable deep learning for natural language processing in healthcare: How far from reality?},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {362-373},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001508},
author = {Guangming Huang and Yingya Li and Shoaib Jameel and Yunfei Long and Giorgos Papanastasiou},
keywords = {Explainable, Interpretable, Deep learning, NLP, Healthcare},
abstract = {Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term “eXplainable and Interpretable Artificial Intelligence” (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore “global” modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare.}
}
@article{WU2025113057,
title = {CoT-driven framework for short text classification: Enhancing and transferring capabilities from large to smaller model},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {113057},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113057},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125001042},
author = {Hui Wu and Yuanben Zhang and Zhonghe Han and Yingyan Hou and Lei Wang and Siye Liu and Qihang Gong and Yunping Ge},
keywords = {Short text classification, Large language models, Chain-of-thought},
abstract = {Short Text Classification (STC) is crucial for processing and understanding the brief but substantial content prevalent on contemporary digital platforms. The STC encounters difficulties in grasping the semantic and syntactic intricacies, an issue that is apparent in traditional pre-trained language models. Although Graph Convolutional Networks enhance performance by integrating external knowledge bases, these methods are limited by the quality and extent of the knowledge applied. Recently, the emergence of Large Language Models (LLMs) and Chain-of-Thought (CoT) has significantly improved the performance of complex reasoning tasks. However, some studies have highlighted the limitations of their application in fundamental NLP tasks. Consequently, this study first employs CoT to investigate and enhance the capabilities of LLMs in STC tasks. We propose the Syntactic and Semantic Enrichment CoT (SSE-CoT) method, effectively decomposing the STC tasks into four distinct steps: (i) essential concept identification, (ii) common-sense knowledge retrieval, (iii) text rewriting, and (iv) classification. Furthermore, recognizing resource constraints in sectors like finance and healthcare, we then introduce the CoT-Driven Multi-Task Learning (CDMT) framework to extend these capabilities to smaller models. This framework begins by extracting rationales from LLMs and subsequently fine-tunes smaller models to optimize their performance. Extensive experimentation across six short-text benchmarks validated the efficacy of the proposed methods. In particular, SSE-CoT achieved state-of-the-art performance with substantial improvements on all datasets, particularly on the Ohsumed and TagMyNews datasets.}
}
@article{KERRE2025111255,
title = {An instruction dataset for extracting quantum cascade laser properties from scientific text},
journal = {Data in Brief},
volume = {58},
pages = {111255},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.111255},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924012174},
author = {Deperias Kerre and Anne Laurent and Kenneth Maussang and Dickson Owuor},
keywords = {Information extraction, Large language models, Machine learning, Quantum cascade lasers},
abstract = {Quantum Cascade Lasers (QCL) are promising semiconductor lasers, compact and powerful, but of complex design. Availability of structured data of the QCL properties can support data mining activities that seek to understand the relationship between these properties, for instance between the design and performance features. The main open source of QCL data is in scientific text which in most cases is usually unstructured. One of the ways to extract and organize this data is by utilizing Information Extraction techniques. These techniques can accelerate the process of curating QCL properties data from scientific articles for further analysis. One of the main challenges in developing machine learning algorithms for extraction of QCL properties from text is lack of quality training data for these algorithms. Large Language Models (LLMs) have demonstrated great capabilities in materials property extraction from text. They however experience challenges with domain specific properties, for instance the heterostructure and design types in the QCL domain hence for adaptation. In this paper, we present an original instruction dataset for training and evaluation of LLMs for QCL properties extraction from text. The data is generated by augmenting sample sentences from scientific articles with GPT-3.5 instruct with a few shot strategy. The dataset then is manually annotated with the help of QCL experts and is composed of 1300 rows of training examples consisting of an Instruction, Input Text and the Output.}
}
@article{SIVAKUMAR2024124653,
title = {Prompting GPT–4 to support automatic safety case generation},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124653},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124653},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424015203},
author = {Mithila Sivakumar and Alvine B. Belle and Jinjun Shan and Kimya {Khakzad Shahandashti}},
keywords = {Safety cases, Safety assurance, Machine learning, Large language models, Generative AI, Requirements engineering},
abstract = {In the ever-evolving field of software engineering, the advent of large language models and conversational interfaces, exemplified by ChatGPT, represents a significant revolution. While their potential is evident in various domains, this paper expands upon our previous research, where we experimented with GPT–4, on its ability to create safety cases. A safety case is a structured argument supported by a body of evidence to demonstrate that a given system is safe to operate in a given environment. In this paper, we first determine GPT–4’s comprehension of the Goal Structuring Notation (GSN), a well-established notation for visually representing safety cases. Additionally, we conduct four distinct experiments using GPT–4 to evaluate its ability to generate safety cases within a specified system and application domain. To assess GPT–4’s performance in this context, we compare the results it produces with the ground-truth safety cases developed for an X-ray system, a machine learning-enabled component for tire noise recognition in a vehicle, and a lane management system from the automotive domain. This comparison enables us to gain valuable insights into the model’s generative capabilities. Our findings indicate that GPT–4 is able to generate moderately accurate and reasonable safety cases.}
}
@article{MORALESSANCHEZ2024108830,
title = {Early diagnosis of HIV cases by means of text mining and machine learning models on clinical notes},
journal = {Computers in Biology and Medicine},
volume = {179},
pages = {108830},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108830},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524009156},
author = {Rodrigo Morales-Sánchez and Soto Montalvo and Adrián Riaño and Raquel Martínez and María Velasco},
keywords = {HIV, Text mining, Automated screening, Electronic Health Records (EHRs), Large Language Models (LLMs), Machine Learning (ML)},
abstract = {Undiagnosed and untreated human immunodeficiency virus (HIV) infection increases morbidity in the HIV-positive person and allows onward transmission of the virus. Minimizing missed opportunities for HIV diagnosis when a patient visits a healthcare facility is essential in restraining the epidemic and working toward its eventual elimination. Most state-of-the-art proposals employ machine learning (ML) methods and structured data to enhance HIV diagnoses, however, there is a dearth of recent proposals utilizing unstructured textual data from Electronic Health Records (EHRs). In this work, we propose to use only the unstructured text of the clinical notes as evidence for the classification of patients as suspected or not suspected. For this purpose, we first compile a dataset of real clinical notes from a hospital with patients classified as suspects and non-suspects of having HIV. Then, we evaluate the effectiveness of two types of classification models to identify patients suspected of being infected with the virus: classical ML algorithms and two Large Language Models (LLMs) from the biomedical domain in Spanish. The results show that both LLMs outperform classical ML algorithms in the two settings we explore: one dataset version is balanced, containing an equal number of suspicious and non-suspicious patients, while the other reflects the real distribution of patients in the hospital, being unbalanced. We obtain F1 score figures of 94.7 with both LLMs in the unbalanced setting, while in the balance one, RoBERTaBio model outperforms the other one with a F1 score of 95.7. The findings indicate that leveraging unstructured text with LLMs in the biomedical domain yields promising outcomes in diminishing missed opportunities for HIV diagnosis. A tool based on our system could assist a doctor in deciding whether a patient in consultation should undergo a serological test.}
}
@article{HUANG2025104033,
title = {A knowledge-enhanced network for joint multimodal entity-relation extraction},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104033},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104033},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003923},
author = {Shubin Huang and Yi Cai and Li Yuan and Jiexin Wang},
keywords = {Joint multimodal entity-relation extraction, Knowledge graphs, Large language models},
abstract = {In the domain of multimodal analysis, Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE) are two pivotal tasks aiming at identifying named entities and their relations by leveraging integrated information from text–image pairs. Recently, Joint Multimodal Entity-Relation Extraction (JMERE) has emerged as a unified task to combine MNER and MRE, exploiting the bidirectional interactions between the two tasks for enhanced performance. However, existing JMERE studies primarily focus on improving visual information utilization through feature alignment, often falling short when social media texts and accompanying images lack sufficient information, leading to inaccuracies in entity type and relation recognition. To tackle this challenge, we propose KEJME, a knowledge-enhanced network tailored for the JMERE task. KEJME utilizes GPT-3.5 and ConceptNet as knowledge sources to supplement the missing context information of text–image pairs by infusing external knowledge. Moreover, to ensure the relevance of imported knowledge, we introduce a knowledge feature selection module, which performs a fine-grained selection of knowledge features according to different visual objects and textual entities. Numerous experiments have demonstrated that KEJME significantly surpasses state-of-the-art methods, achieving substantial improvements in both recall and F1 score. These findings highlight the critical role of external knowledge integration and fine-grained knowledge selection in advancing multimodal entity-relation extraction.}
}
@article{SHOHAM2024109089,
title = {MedConceptsQA: Open source medical concepts QA benchmark},
journal = {Computers in Biology and Medicine},
volume = {182},
pages = {109089},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109089},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524011740},
author = {Ofir Ben Shoham and Nadav Rappoport},
keywords = {Benchmark, Large Language Models, LLM, Machine learning, Clinical knowledge, Health care},
abstract = {Background:
Clinical data often includes both standardized medical codes and natural language texts. This highlights the need for Clinical Large Language Models to understand these codes and their differences. We introduce a benchmark for evaluating the understanding of medical codes by various Large Language Models.
Methods:
We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs. The questions are categorized into three levels of difficulty: easy, medium, and hard. We conduct evaluations of the benchmark using various Large Language Models.
Results:
Our findings show that most of the pre-trained clinical Large Language Models achieved accuracy levels close to random guessing on this benchmark, despite being pre-trained on medical data. However, GPT-4 achieves an absolute average improvement of 9-11% (9% for few-shot learning and 11% for zero-shot learning) compared to Llama3-OpenBioLLM-70B, the clinical Large Language Model that achieved the best results.
Conclusion:
Our benchmark serves as a valuable resource for evaluating the abilities of Large Language Models to interpret medical codes and distinguish between medical concepts. We demonstrate that most of the current state-of-the-art clinical Large Language Models achieve random guess performance, whereas GPT-3.5, GPT-4, and Llama3-70B outperform these clinical models, despite their primary focus during pre-training not being on the medical domain. Our benchmark is available at https://huggingface.co/datasets/ofir408/MedConceptsQA.}
}
@incollection{NGUYEN2025169,
title = {Chapter 11 - Towards human digital twins for healthcare agent-based modeling in the Metaverse☆☆Disclaimer: This book chapter and my related published materials reflect my personal views only, and do not necessarily reflect the views of the US HHS nor the FDA. The chapter contains a part of my earlier published paper at https://xmed.jmir.org/2022/2/e33502.},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {169-194},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.00013-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962000139},
author = {Tam N. Nguyen},
keywords = {Human digital twins, Metaverse, Agent-based modeling, Cognitive computing, Smart agents},
abstract = {Agent-based modeling (ABM) has been increasingly used to model complex real-life issues, such as informing prompt COVID-19 response policies. ABM represents subsystems and their entities as agents while employing flexible rules to describe complex relationships and interactions among the agents. The Metaverse, with its sophisticated agents like digital twins (DTs) and human digital twins (HDTs), can significantly boost ABM performance. However, current cognitive architectures are not ready for HDTs use in the Metaverse. Here we show that extending current digital cognitive architectures is a crucial first step towards building more robust HDTs. We introduce Cybonto, a novel ontology that packages 108 psychology constructs and thousands of related paths based on 20 time-tested psychology theories. Using 20 network science centrality algorithms, we rank the Cybonto psychology constructs by their influences, identifying the top 10 constructs: behavior, arousal, goals, perception, self-efficacy, circumstances, evaluating, behavior-controllability, knowledge, and intentional modality. These findings confirm the need for specific extensions of current digital cognitive architectures in preparation for future HDTs in the Metaverse. Additionally, Cybonto can be used to develop cognitive evaluation metrics for large language models, moving the field forward in terms of practical applications and theoretical advancements.}
}
@article{PAPATHEODOSIOU20242539,
title = {Leveraging Artificial Intelligence for personalised insomnia-sleep calibration via the Big Five Personality Traits},
journal = {Procedia Computer Science},
volume = {246},
pages = {2539-2548},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.723},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027923},
author = {Persephone Papatheodosiou and Dimitrios P. Panagoulias and Maria Virvou and George A. Tsihrintzis and Anastasios Bonakis and Dimitrios Dikeos},
keywords = {AI-empowered software engineering, Large Language Models, GPT, BERT, Insomnia, Sleep-therapy, Personalisation},
abstract = {This paper introduces Morpheas, an AI-empowered sleep evaluation and calibration system that leverages state-of-the-art technologies, like Large Language Models (LLMs) and Named Entity Recognition (NER). Morpheas integrates Sequential Language Model Integration (SLMI) workflows to simulate the initial steps of sleep disorder diagnosis, utilizing the GPT-4 engine enhanced with Rules of Conduct. Using SLMI and medical and psychological diagnostic tools, we propose a novel multi-step personalisation methodology for creating a gradation system for the improvement of patient-AI interactions. To test and showcase this personalisation approach, we simulate keeping a sleep diary for diagnosing insomnia using a trait-based personalised LLM aimed at addressing sleep concerns. For this purpose, we apply the Big Five Personality Traits (BFPT) where LLMs are again used to extract the responses and facilitate the patient throughout the process, providing guidance and explanation. We then extract the entities from these interactions with NER, in order to identify patterns, provide an explainability basis for patients and effectively customize Cognitive Behavioral Therapy for Insomnia (CBTi), whether through one-on-one sessions or via digital platforms.}
}
@article{GAO2024878,
title = {Integrating IoT and visual question answering in smart cities: Enhancing educational outcomes},
journal = {Alexandria Engineering Journal},
volume = {108},
pages = {878-888},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.09.059},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824010834},
author = {Tian Gao and Guanqi Wang},
keywords = {Smart cities, IoT framework, Visual question answering, Large language models, Smart education technology},
abstract = {Emerging as a paradigmatic shift in urban development, smart cities harness the potential of advanced information and communication technologies to seamlessly integrate urban functions, optimize resource allocation, and improve the effectiveness of city management. Within the domain of smart education, the imperative application of Visual Question Answering (VQA) technology encounters significant limitations at the prevailing stage, particularly the absence of a robust Internet of Things (IoT) framework and the inadequate incorporation of large pre-trained language models (LLMs) within contemporary smart education paradigms, especially in addressing zero-shot VQA scenarios, which pose considerable challenges. In response to these constraints, this paper introduces an IoT-based smart city framework that is designed to refine the functionality and efficacy of educational systems. This framework is delineated into four cardinal layers: the data collection layer, data transmission layer, data management layer, and application layer. Furthermore, we introduce the innovative TeachVQA methodology at the application layer, synergizing VQA technology with extensive pre-trained language models, thereby considerably enhancing the dissemination and assimilation of educational content. Evaluative metrics in the VQAv2 and OKVQA datasets substantiate that the TeachVQA methodology not only outperforms existing VQA approaches, but also underscores its profound potential and practical relevance in the educational sector.}
}
@article{LIU2025104062,
title = {Beyond expression: Comprehensive visualization of knowledge triplet facts},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104062},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104062},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000044},
author = {Wei Liu and Yixue He and Chao Wang and Shaorong Xie and Weimin Li},
keywords = {Generating model, Multi-modal Knowledge Graph, Non-visual relation, Visual element, Visual relation},
abstract = {Multi-modal Knowledge Graphs (KGs) enhance traditional KGs by incorporating multi-modal data to bridge the information gap in natural language processing (NLP) tasks. One direct method to incorporate multi-modal data is to associate structured KG with corresponding image modalities, thereby visualizing entities and triplet facts. However, existing visualization methods for triplet facts often exclude triplet facts containing abstract entities and non-visual relations, resulting in their disassociation from corresponding image modalities. This exclusion compromises the completeness and utility of multi-modal KGs. In this paper, we aim to construct a comprehensive multi-modal KG that includes abstract entities and non-visual relations, ensuring complete visualization of every triplet fact. To achieve this purpose, we propose a method for the integration of image Retrieval-Generation-Editing (RGE) to completely and accurately visualize each triplet fact. Initially, we correct the triplet facts by integrating a Large Language Model (LLM) with a retrieved knowledge database about triplet facts. Subsequently, by providing appropriate contextual examples to the LLM, we generate visual elements of relations, enriching the semantics of the triplet facts. We then employ image retrieval to obtain images that reflect the semantics of each triplet fact. For those triplet facts for which images cannot be directly retrieved, we utilize image generation and editing to create and modify images that can express the semantics of the triplet facts. Through the RGE method, we construct a multi-modal KG named DB15kFact, which includes 86,722 triplet facts, 274 relations, 12,842 entities, and 387,096 images. The construction of DB15kFact has resulted in a fourfold increase in the number of relations compared to the previous multi-modal KG, ImgFact. In experiments, both automatic and manual evaluations confirm the quality of DB15kFact. The results demonstrate that the DB15kFact significantly enhances model performance in link prediction and relation classification. Notably, in link prediction, the model optimized with DB15kFact achieves a 7.12% improvement in the H@10 metric compared to existing solutions.}
}
@article{WALDOCK2024,
title = {The Accuracy and Capability of Artificial Intelligence Solutions in Health Care Examinations and Certificates: Systematic Review and Meta-Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/56532},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007520},
author = {William J Waldock and Joe Zhang and Ahmad Guni and Ahmad Nabeel and Ara Darzi and Hutan Ashrafian},
keywords = {large language model, LLM, artificial intelligence, AI, health care exam, narrative medical response, health care examination, clinical commissioning, health services, safety},
abstract = {Background
Large language models (LLMs) have dominated public interest due to their apparent capability to accurately replicate learned knowledge in narrative text. However, there is a lack of clarity about the accuracy and capability standards of LLMs in health care examinations.
Objective
We conducted a systematic review of LLM accuracy, as tested under health care examination conditions, as compared to known human performance standards.
Methods
We quantified the accuracy of LLMs in responding to health care examination questions and evaluated the consistency and quality of study reporting. The search included all papers up until September 10, 2023, with all LLMs published in English journals that report clear LLM accuracy standards. The exclusion criteria were as follows: the assessment was not a health care exam, there was no LLM, there was no evaluation of comparable success accuracy, and the literature was not original research.The literature search included the following Medical Subject Headings (MeSH) terms used in all possible combinations: “artificial intelligence,” “ChatGPT,” “GPT,” “LLM,” “large language model,” “machine learning,” “neural network,” “Generative Pre-trained Transformer,” “Generative Transformer,” “Generative Language Model,” “Generative Model,” “medical exam,” “healthcare exam,” and “clinical exam.” Sensitivity, accuracy, and precision data were extracted, including relevant CIs.
Results
The search identified 1673 relevant citations. After removing duplicate results, 1268 (75.8%) papers were screened for titles and abstracts, and 32 (2.5%) studies were included for full-text review. Our meta-analysis suggested that LLMs are able to perform with an overall medical examination accuracy of 0.61 (CI 0.58-0.64) and a United States Medical Licensing Examination (USMLE) accuracy of 0.51 (CI 0.46-0.56), while Chat Generative Pretrained Transformer (ChatGPT) can perform with an overall medical examination accuracy of 0.64 (CI 0.6-0.67).
Conclusions
LLMs offer promise to remediate health care demand and staffing challenges by providing accurate and efficient context-specific information to critical decision makers. For policy and deployment decisions about LLMs to advance health care, we proposed a new framework called RUBRICC (Regulatory, Usability, Bias, Reliability [Evidence and Safety], Interoperability, Cost, and Codesign–Patient and Public Involvement and Engagement [PPIE]). This presents a valuable opportunity to direct the clinical commissioning of new LLM capabilities into health services, while respecting patient safety considerations.
Trial Registration
OSF Registries osf.io/xqzkw; https://osf.io/xqzkw}
}
@article{CHEN2024104651,
title = {Chat-ePRO: Development and pilot study of an electronic patient-reported outcomes system based on ChatGPT},
journal = {Journal of Biomedical Informatics},
volume = {154},
pages = {104651},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104651},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000698},
author = {Zikang Chen and Qinchuan Wang and Yaoqian Sun and Hailing Cai and Xudong Lu},
keywords = {Patient Reported Outcome Measures, Large Language Model, Knowledge Distillation, Breast Cancer, Mobile Health},
abstract = {Objective
Chatbots have the potential to improve user compliance in electronic Patient-Reported Outcome (ePRO) system. Compared to rule-based chatbots, Large Language Model (LLM) offers advantages such as simplifying the development process and increasing conversational flexibility. However, there is currently a lack of practical applications of LLMs in ePRO systems. Therefore, this study utilized ChatGPT to develop the Chat-ePRO system and designed a pilot study to explore the feasibility of building an ePRO system based on LLM.
Materials and Methods
This study employed prompt engineering and offline knowledge distillation to design a dialogue algorithm and built the Chat-ePRO system on the WeChat Mini Program platform. In order to compare Chat-ePRO with the form-based ePRO and rule-based chatbot ePRO used in previous studies, we conducted a pilot study applying the three ePRO systems sequentially at the Sir Run Run Shaw Hospital to collect patients’ PRO data.
Result
Chat-ePRO is capable of correctly generating conversation based on PRO forms (success rate: 95.7 %) and accurately extracting the PRO data instantaneously from conversation (Macro-F1: 0.95). The majority of subjective evaluations from doctors (>70 %) suggest that Chat-ePRO is able to comprehend questions and consistently generate responses. Pilot study shows that Chat-ePRO demonstrates higher response rate (9/10, 90 %) and longer interaction time (10.86 s/turn) compared to the other two methods.
Conclusion
Our study demonstrated the feasibility of utilizing algorithms such as prompt engineering to drive LLM in completing ePRO data collection tasks, and validated that the Chat-ePRO system can effectively enhance patient compliance.}
}
@article{FILTER2024100309,
title = {Food Safety Knowledge Exchange (FSKX) format: Current status and strategic development plans based on a SWOT analysis},
journal = {Microbial Risk Analysis},
volume = {27-28},
pages = {100309},
year = {2024},
issn = {2352-3522},
doi = {https://doi.org/10.1016/j.mran.2024.100309},
url = {https://www.sciencedirect.com/science/article/pii/S2352352224000203},
author = {Matthias Filter and Thomas Schüler and Racem {Ben Romdhane}},
keywords = {FAIR data, Knowledge exchange, Data standards, Linked models},
abstract = {The Food Safety Knowledge Exchange (FSKX) format is a community-driven effort initially created to promote the efficient exchange of data and models in the food safety domain. Over the past years this effort was driven by the Risk Assessment Knowledge Integration Platform (RAKIP) Initiative that also provided a number of software tools and FSKX-compliant model files via their website https://foodrisklabs.bfr.bund.de/rakip-initiative/. This paper describes the results of a SWOT analysis that was conducted to identify strategic avenues for enhancing FSKX's usability and adoption. The SWOT analysis identified a number of recommendations for the future evolution of FSKX. First, it is recommended to reduce the complexity of the annotation schema to ease the adoption of the format. Second, a clear distinction between the descriptive part of FSKX and the executable part is proposed. To promote the broad usage of FSKX-compliant models, it is also recommended to develop and provide FSKX-compliant APIs and resources that facilitate cloud-based execution. As part of the research to prioritize future FSKX development options, we also considered the implications of the emerging generative AI technologies, particularly which impact large language models (LLMs) might have in supporting the adoption of FSKX by the research community. Recognizing the format's application potential beyond the food safety domain, we then proposed to re-brand the FSKX acronym as "FAIR Scientific Knowledge Exchange Format" which better reflects its broad applicability in various scientific domains. Our research findings suggest that with the implementation of the improvements identified by the SWOT analysis and the broader availability of generative AI technologies the broad adoption of FSKX as a method to share data and models in a FAIR way comes into reach.}
}
@article{KOH20243454,
title = {Confronting the data deluge: How artificial intelligence can be used in the study of plant stress},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3454-3466},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024002988},
author = {Eugene Koh and Rohan Shawn Sunil and Hilbert Yuen In Lam and Marek Mutwil},
keywords = {Plant stress resilience, Large-scale data, Artificial intelligence, Large language models},
abstract = {The advent of the genomics era enabled the generation of high-throughput data and computational methods that serve as powerful hypothesis-generating tools to understand the genomic and gene functional basis of plant stress resilience. The proliferation of experimental and analytical methods used in biology has resulted in a situation where plentiful data exists, but the volume and heterogeneity of this data has made analysis a significant challenge. Current advanced deep-learning models have displayed an unprecedented level of comprehension and problem-solving ability, and have been used to predict gene structure, function and expression based on DNA or protein sequence, and prominently also their use in high-throughput phenomics in agriculture. However, the application of deep-learning models to understand gene regulatory and signalling behaviour is still in its infancy. We discuss in this review the availability of data resources and bioinformatic tools, and several applications of these advanced ML/AI models in the context of plant stress response, and demonstrate the use of a publicly available LLM (ChatGPT) to derive a knowledge graph of various experimental and computational methods used in the study of plant stress. We hope this will stimulate further interest in collaboration between computer scientists, computational biologists and plant scientists to distil the deluge of genomic, transcriptomic, proteomic, metabolomic and phenomic data into meaningful knowledge that can be used for the benefit of humanity.}
}
@article{LIU2024269,
title = {LLM technologies and information search},
journal = {Journal of Economy and Technology},
volume = {2},
pages = {269-277},
year = {2024},
issn = {2949-9488},
doi = {https://doi.org/10.1016/j.ject.2024.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2949948824000398},
author = {Lin Liu and Jiajun Meng and Yongliang Yang},
keywords = {LLM technologies, Information search, Google, Online advertising},
abstract = {With the booming of LLM technologies (e.g., ChatGPT), people’s goals and behaviors in information search have been reshaped significantly. This paper attempts to conceptually discuss how LLM technologies might revolutionize these important aspects in information search and provides a comprehensive analysis of the technological advancements and capabilities of ChatGPT, highlighting its potential to disrupt traditional search engines like Google. In addition, this paper contrasts ChatGPT’s conversational approach with Google’s link-based search model, offering a detailed examination of the implications for online search advertising and user behavior and explaining why Google is concerned about ChatGPT as well as its potential reactions.}
}
@article{RODRIGUES2024100248,
title = {Assessing the quality of automatic-generated short answers using GPT-4},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100248},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100248},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000511},
author = {Luiz Rodrigues and Filipe {Dwan Pereira} and Luciano Cabral and Dragan Gašević and Geber Ramalho and Rafael {Ferreira Mello}},
keywords = {Automatic answer generation, Question-answering, Large language models, GPT-4, Natural language processing},
abstract = {Open-ended assessments play a pivotal role in enabling instructors to evaluate student knowledge acquisition and provide constructive feedback. Integrating large language models (LLMs) such as GPT-4 in educational settings presents a transformative opportunity for assessment methodologies. However, existing literature on LLMs addressing open-ended questions lacks breadth, relying on limited data or overlooking question difficulty levels. This study evaluates GPT-4's proficiency in responding to open-ended questions spanning diverse topics and cognitive complexities in comparison to human responses. To facilitate this assessment, we generated a dataset of 738 open-ended questions across Biology, Earth Sciences, and Physics and systematically categorized it based on Bloom's Taxonomy. Each question included eight human-generated responses and two from GPT-4. The outcomes indicate GPT-4's superior performance over humans, encompassing both native and non-native speakers, irrespective of gender. Nevertheless, this advantage was not sustained in ’remembering’ or ’creating’ questions aligned with Bloom's Taxonomy. These results highlight GPT-4's potential for underpinning advanced question-answering systems, its promising role in supporting non-native speakers, and its capacity to augment teacher assistance in assessments. However, limitations in nuanced argumentation and creativity underscore areas necessitating refinement in these models, guiding future research toward bolstering pedagogical support.}
}
@article{YANG2024105817,
title = {Prompt-based automation of building code information transformation for compliance checking},
journal = {Automation in Construction},
volume = {168},
pages = {105817},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105817},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524005533},
author = {Fan Yang and Jiansong Zhang},
keywords = {Building code, Information transformation, Automated compliance checking, Prompt engineering, Natural language processing, Large language models (LLMs)},
abstract = {Transforming building code information into a machine-processable format is essential for automated compliance checking, yet it presents significant challenges. A prompt-based framework was developed to automate the conversion into a logic programming language. Its effectiveness was assessed by testing the framework on 51 requirements from the International Building Code (IBC) 2015, achieving 97.37 % precision and 95.88 % recall at the logic clause level, with only 2 % of the data used for training. Further testing on crash report transformation enhanced efficiency, reducing the average code generation time to approximately 60.8 s, thereby achieving a 27.8 % time savings compared to existing rule-based methods. This paper contributes to the body of knowledge by introducing an effective, versatile, and user-friendly approach to automated building code information transformation, markedly decreasing the reliance on training data, time, and manual efforts.}
}
@article{SONG2025102378,
title = {A goal-oriented document-grounded dialogue based on evidence generation},
journal = {Data & Knowledge Engineering},
volume = {155},
pages = {102378},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102378},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001022},
author = {Yong Song and Hongjie Fan and Junfei Liu and Yunxin Liu and Xiaozhou Ye and Ye Ouyang},
keywords = {Document-grounded dialogue, Evidence generation, Large language model, Vector representation, Retrieval augmented generation},
abstract = {Goal-oriented Document-grounded Dialogue (DGD) is used for retrieving specific domain documents, assisting users in document content retrieval, question answering, and document management. Existing methods typically employ keyword extraction and vector space models to understand the content of documents, identify the intent of questions, and generate answers based on the capabilities of generation models. However, challenges remain in semantic understanding, long text processing, and context understanding. The emergence of Large Language Models (LLMs) has brought new capabilities in context learning and step-by-step reasoning. These models, combined with Retrieval Augmented Generation(RAG) methods, have made significant breakthroughs in text comprehension, intent detection, language organization, offering exciting prospects for DGD research. However, the “hallucination” issue arising from LLMs requires complementary methods to ensure the credibility of their outputs. In this paper we propose a goal-oriented document-grounded dialogue approach based on evidence generation using LLMs. It designs and implements methods for document content retrieval & reranking, fine-tuning and inference, and evidence generation. Through experiments, the method of combining LLMs with vector space model, or with key information matching technique is used as a comparison, the accuracy of the proposed method is improved by 21.91% and 12.81%, while the comprehensiveness is increased by 10.89% and 69.83%, coherence is enhanced by 38.98% and 53.27%, and completeness is boosted by 16.13% and 36.97%, respectively, on average. Additional, ablation analysis conducted reveals that the evidence generation method also contributes significantly to the comprehensiveness and completeness.}
}
@article{GUTIERREZMAQUILON2024,
title = {Integrating GPT-Based AI into Virtual Patients to Facilitate Communication Training Among Medical First Responders: Usability Study of Mixed Reality Simulation},
journal = {JMIR Formative Research},
volume = {8},
year = {2024},
issn = {2561-326X},
doi = {https://doi.org/10.2196/58623},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X24007133},
author = {Rodrigo {Gutiérrez Maquilón} and Jakob Uhl and Helmut Schrom-Feiertag and Manfred Tscheligi},
keywords = {medical first responders, verbal communication skills, training, virtual patient, generative artificial intelligence, GPT, large language models, prompt engineering, mixed reality},
abstract = {Background
Training in social-verbal interactions is crucial for medical first responders (MFRs) to assess a patient’s condition and perform urgent treatment during emergency medical service administration. Integrating conversational agents (CAs) in virtual patients (VPs), that is, digital simulations, is a cost-effective alternative to resource-intensive human role-playing. There is moderate evidence that CAs improve communication skills more effectively when used with instructional interventions. However, more recent GPT-based artificial intelligence (AI) produces richer, more diverse, and more natural responses than previous CAs and has control of prosodic voice qualities like pitch and duration. These functionalities have the potential to better match the interaction expectations of MFRs regarding habitability.
Objective
We aimed to study how the integration of GPT-based AI in a mixed reality (MR)–VP could support communication training of MFRs.
Methods
We developed an MR simulation of a traffic accident with a VP. ChatGPT (OpenAI) was integrated into the VP and prompted with verified characteristics of accident victims. MFRs (N=24) were instructed on how to interact with the MR scenario. After assessing and treating the VP, the MFRs were administered the Mean Opinion Scale-Expanded, version 2, and the Subjective Assessment of Speech System Interfaces questionnaires to study their perception of the voice quality and the usability of the voice interactions, respectively. Open-ended questions were asked after completing the questionnaires. The observed and logged interactions with the VP, descriptive statistics of the questionnaires, and the output of the open-ended questions are reported.
Results
The usability assessment of the VP resulted in moderate positive ratings, especially in habitability (median 4.25, IQR 4-4.81) and likeability (median 4.50, IQR 3.97-5.91). Interactions were negatively affected by the approximately 3-second latency of the responses. MFRs acknowledged the naturalness of determining the physiological states of the VP through verbal communication, for example, with questions such as “Where does it hurt?” However, the question-answer dynamic in the verbal exchange with the VP and the lack of the VP’s ability to start the verbal exchange were noticed. Noteworthy insights highlighted the potential of domain-knowledge prompt engineering to steer the actions of MFRs for effective training.
Conclusions
Generative AI in VPs facilitates MFRs’ training but continues to rely on instructions for effective verbal interactions. Therefore, the capabilities of the GPT-VP and a training protocol need to be communicated to trainees. Future interactions should implement triggers based on keyword recognition, the VP pointing to the hurting area, conversational turn-taking techniques, and add the ability for the VP to start a verbal exchange. Furthermore, a local AI server, chunk processing, and lowering the audio resolution of the VP’s voice could ameliorate the delay in response and allay privacy concerns. Prompting could be used in future studies to create a virtual MFR capable of assisting trainees.}
}
@article{SHAKIL2024128255,
title = {Abstractive text summarization: State of the art, challenges, and improvements},
journal = {Neurocomputing},
volume = {603},
pages = {128255},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128255},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224010269},
author = {Hassan Shakil and Ahmad Farooq and Jugal Kalita},
keywords = {Automatic summarization, Abstractive summarization, Extractive summarization, Knowledge representation, Text generation},
abstract = {Specifically focusing on the landscape of abstractive text summarization, as opposed to extractive techniques, this survey presents a comprehensive overview, delving into state-of-the-art techniques, prevailing challenges, and prospective research directions. We categorize the techniques into traditional sequence-to-sequence models, pre-trained large language models, reinforcement learning, hierarchical methods, and multi-modal summarization. Unlike prior works that did not examine complexities, scalability and comparisons of techniques in detail, this review takes a comprehensive approach encompassing state-of-the-art methods, challenges, solutions, comparisons, limitations and charts out future improvements — providing researchers an extensive overview to advance abstractive summarization research. We provide vital comparison tables across techniques categorized — offering insights into model complexity, scalability and appropriate applications. The paper highlights challenges such as inadequate meaning representation, factual consistency, controllable text summarization, cross-lingual summarization, and evaluation metrics, among others. Solutions leveraging knowledge incorporation and other innovative strategies are proposed to address these challenges. The paper concludes by highlighting emerging research areas like factual inconsistency, domain-specific, cross-lingual, multilingual, and long-document summarization, as well as handling noisy data. Our objective is to provide researchers and practitioners with a structured overview of the domain, enabling them to better understand the current landscape and identify potential areas for further research and improvement.}
}
@article{ZENG2025112784,
title = {Explainable next POI recommendation based on spatial–temporal disentanglement representation and pseudo profile generation},
journal = {Knowledge-Based Systems},
volume = {309},
pages = {112784},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112784},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124014187},
author = {Jun Zeng and Hongjin Tao and Junhao Wen and Min Gao},
keywords = {POI recommendation, Large language model, Disentangled representation, Graph neural network},
abstract = {The current research in Point-of-Interest (POI) recommendation primarily aims to decipher users’ transitional patterns to predict their future location visits. Traditional approaches often intertwine various features to model these check-in transitions, which inadvertently compromises the quality of the resulting representations. This issue is compounded in both industrial and academic settings, where user-generated textual data is frequently inaccessible or restricted due to privacy concerns. Such limitations in user profiles pose significant challenges to the effectiveness of subsequent applications. In response to these challenges, the recent rise of Large Language Models (LLMs) offers a novel perspective. Diverging from the conventional approach of leveraging LLMs for semantic-based next check-in predictions, our research investigates the potential of integrating LLMs with sequential recommendation systems. This integration aims to augment feature dimensions and facilitate the generation of explicit explanations. To this end, we introduce CrossDR-Gen, a Cross-sequence Location Disentanglement Representation methodology. CrossDR-Gen is specifically designed for next POI recommendation and explanation generation. It uniquely considers spatial and temporal factors in shaping check-in behaviors, offering a comprehensive global view of location transitions. Crucially, CrossDR-Gen utilizes LLMs for pseudo profile generation in scenarios with limited semantic context, thereby enriching user features without relying on additional textual profiles or conversational data. Our experiments on real-world datasets demonstrate that CrossDR-Gen not only excels in addressing cold-start scenarios but also showcases robust recommendation capabilities. These findings validate the effectiveness of our proposed cooperative paradigm between LLMs and sequential recommendation models, highlighting a promising avenue for future research in POI recommendation systems.}
}
@article{YANG2024,
title = {Ascle—A Python Natural Language Processing Toolkit for Medical Text Generation: Development and Evaluation Study},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/60601},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006289},
author = {Rui Yang and Qingcheng Zeng and Keen You and Yujie Qiao and Lucas Huang and Chia-Chun Hsieh and Benjamin Rosand and Jeremy Goldwasser and Amisha Dave and Tiarnan Keenan and Yuhe Ke and Chuan Hong and Nan Liu and Emily Chew and Dragomir Radev and Zhiyong Lu and Hua Xu and Qingyu Chen and Irene Li},
keywords = {natural language processing, machine learning, deep learning, generative artificial intelligence, large language models, retrieval-augmented generation, healthcare},
abstract = {Background
Medical texts present significant domain-specific challenges, and manually curating these texts is a time-consuming and labor-intensive process. To address this, natural language processing (NLP) algorithms have been developed to automate text processing. In the biomedical field, various toolkits for text processing exist, which have greatly improved the efficiency of handling unstructured text. However, these existing toolkits tend to emphasize different perspectives, and none of them offer generation capabilities, leaving a significant gap in the current offerings.
Objective
This study aims to describe the development and preliminary evaluation of Ascle. Ascle is tailored for biomedical researchers and clinical staff with an easy-to-use, all-in-one solution that requires minimal programming expertise. For the first time, Ascle provides 4 advanced and challenging generative functions: question-answering, text summarization, text simplification, and machine translation. In addition, Ascle integrates 12 essential NLP functions, along with query and search capabilities for clinical databases.
Methods
We fine-tuned 32 domain-specific language models and evaluated them thoroughly on 27 established benchmarks. In addition, for the question-answering task, we developed a retrieval-augmented generation (RAG) framework for large language models that incorporated a medical knowledge graph with ranking techniques to enhance the reliability of generated answers. Additionally, we conducted a physician validation to assess the quality of generated content beyond automated metrics.
Results
The fine-tuned models and RAG framework consistently enhanced text generation tasks. For example, the fine-tuned models improved the machine translation task by 20.27 in terms of BLEU score. In the question-answering task, the RAG framework raised the ROUGE-L score by 18% over the vanilla models. Physician validation of generated answers showed high scores for readability (4.95/5) and relevancy (4.43/5), with a lower score for accuracy (3.90/5) and completeness (3.31/5).
Conclusions
This study introduces the development and evaluation of Ascle, a user-friendly NLP toolkit designed for medical text generation. All code is publicly available through the Ascle GitHub repository. All fine-tuned language models can be accessed through Hugging Face.}
}
@article{LIU20241049,
title = {Research status and application of artificial intelligence large models in the oil and gas industry},
journal = {Petroleum Exploration and Development},
volume = {51},
number = {4},
pages = {1049-1065},
year = {2024},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(24)60524-0},
url = {https://www.sciencedirect.com/science/article/pii/S1876380424605240},
author = {He LIU and Yili REN and Xin LI and Yue DENG and Yongtao WANG and Qianwen CAO and Jinyang DU and Zhiwei LIN and Wenjie WANG},
keywords = {foundation model, large language mode, visual large model, multimodal large model, large model of oil and gas industry, pre-training, fine-tuning},
abstract = {This article elucidates the concept of large model technology, summarizes the research status of large model technology both domestically and internationally, provides an overview of the application status of large models in vertical industries, outlines the challenges and issues confronted in applying large models in the oil and gas sector, and offers prospects for the application of large models in the oil and gas industry. The existing large models can be briefly divided into three categories: large language models, visual large models, and multimodal large models. The application of large models in the oil and gas industry is still in its infancy. Based on open-source large language models, some oil and gas enterprises have released large language model products using methods like fine-tuning and retrieval augmented generation. Scholars have attempted to develop scenario-specific models for oil and gas operations by using visual/multimodal foundation models. A few researchers have constructed pre-trained foundation models for seismic data processing and interpretation, as well as core analysis. The application of large models in the oil and gas industry faces challenges such as current data quantity and quality being difficult to support the training of large models, high research and development costs, and poor algorithm autonomy and control. The application of large models should be guided by the needs of oil and gas business, taking the application of large models as an opportunity to improve data lifecycle management, enhance data governance capabilities, promote the construction of computing power, strengthen the construction of “artificial intelligence + energy” composite teams, and boost the autonomy and control of large model technology.}
}
@article{SONG2025126558,
title = {HeteroHTC: Enhancing Hierarchical Text Classification via Heterogeneity Encoding of Label Hierarchy},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126558},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126558},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001800},
author = {Junru Song and Tianlei Chen and Yang Yang and Feifei Wang},
keywords = {Hierarchical Text Classification, Heterogeneous Graph Transformer, Large Language Models},
abstract = {Hierarchical Text Classification (HTC) is a challenging subtask of multi-label text classification, where labels are organized into a pre-defined hierarchy. Recent works primarily encode documents and labels separately before cross attention-based feature extraction, and in the process, they collectively overlook a crucial characteristic of label hierarchies: “heterogeneity”. Specifically, labels on different levels hold different granularities, and they should be projected onto distinct feature spaces; The relationships among labels are various, dictating that the message transmission among them should occur in unique feature spaces. We term these properties “granularity heterogeneity” and “relationship heterogeneity”, respectively. To fully exploit these ubiquitous yet overlooked properties, we propose HeteroHTC, which features a heterogeneous label hierarchy encoder. Additionally, we leverage pre-trained Large Language Models (LLMs) to generate high-quality label descriptions with strategically designed prompts. HeteroHTC outperforms almost all baselines in our extensive experiments on three datasets, proving its effectiveness and the necessity to take “granularity and relationship heterogeneity” into consideration.}
}
@article{HE2025103875,
title = {The more quality information the better: Hierarchical generation of multi-evidence alignment and fusion model for multimodal entity and relation extraction},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103875},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103875},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002346},
author = {Xinyu He and Shixin Li and Yuning Zhang and Binhe Li and Sifan Xu and Yuqing Zhou},
keywords = {Multimodal entity and relation extraction, Hierarchical generation, Multi-evidence fusion, LLM},
abstract = {Multimodal Entity and Relation Extraction (MERE) encompasses tasks, including Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction (MRE), aiming to extract valuable information from environments rich in multimodal data. Currently, many research endeavors face various challenges, including the insufficient utilization of emotional information in multimodal data, mismatches between textual and visual content, ambiguous meanings, and difficulties achieving precise alignment across different semantic levels. To address these issues, we propose the Hierarchical Generation of Multi Evidence Alignment Fusion Model for Multimodal Entity and Relation Extraction (HGMAF). This model comprises a hierarchical diffusion semantic generation stage and a multi-evidence alignment fusion module. Initially, we designed different prompt templates for the original text, using the Large Language Model (LLM) to generate corresponding hierarchical textual content. Subsequently, the generated hierarchical content is diffused to obtain images with rich hierarchical semantic information. This stage contributes to enhancing the model's understanding of hierarchical information in the original content. Following this, we design the multi-evidence alignment fusion module, which combines the generated textual and image evidence, fully leveraging information from different sources to improve extraction accuracy. Experimental results demonstrate that our model achieves F1 scores of 76.29 %, 87.66 %, and 87.34 % on the Twitter2015, Twitter2017, and MNRE datasets, respectively. These results surpass the previous state-of-the-art models by 0.29 %, 0.1 %, and 2.77 %. Furthermore, our model demonstrates superior performance in low-resource scenarios, confirming its effectiveness. The related code can be found at https://github.com/lsx314/HGMAF.}
}
@article{JANG2025103100,
title = {Semantic elaboration of low-LOD BIMs: Inferring functional requirements using graph neural networks},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103100},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103100},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007511},
author = {Suhyung Jang and Ghang Lee and Minkyeong Park and Jaekun Lee and Seungah Suh and Bonsang Koo},
keywords = {Semantic elaboration, Building information model (BIM), Threshold-enhanced triangle intersection (TETI), Graph neural network (GNN), Large language model (LLM) embedding},
abstract = {This study proposes a method to automatically subcategorize early object types in low levels of development (LODs) into detailed types (i.e., subtypes) with distinct functional requirements, such as insulation, waterproofing, and load-bearing. While rough cost estimation is possible in the early design phase without detailed object classifications, its accuracy is often limited. Subcategorizing generic objects like walls and columns into more detailed types enhances the precision of early-stage engineering analyses, including cost estimation, load assessments, and material takeoffs. Existing automated object subclassification methods rely on information extracted from highly detailed models, which are unavailable in early-stage building information models (BIMs) due to a lack of geometric and attributive distinctions. This study addresses these limitations by leveraging functional requirements inferred from object connections and placement in early BIMs, achieved using a graph neural network (GNN). To convert BIMs into graphs, a novel threshold-enhanced triangle intersection (TETI) algorithm is introduced, overcoming inaccuracies and exception-handling issues in existing methods. The study explores two GNN-based approaches: node property prediction and node prediction. The former distinguished generic object types into 14 detailed categories, but cost estimation required greater specificity. The latter successfully classified objects into 42 subtypes, with the best results achieved using semantically rich embeddings from a large language model (LLM) and GraphSAGE with three SAGE convolution layers, three hops, and 1,024 dimensions, yielding a weighted F1-score of 0.8766. This approach significantly reduces input data requirements compared to existing methods, enabling more accurate early identification of functional requirements in low-LOD BIMs and supporting both early engineering analyses and detailing processes.}
}
@article{LIU2025101818,
title = {Harnessing AI for understanding scientific literature: Innovations and applications of chat-agent system in battery recycling research},
journal = {Materials Today Energy},
volume = {49},
pages = {101818},
year = {2025},
issn = {2468-6069},
doi = {https://doi.org/10.1016/j.mtener.2025.101818},
url = {https://www.sciencedirect.com/science/article/pii/S2468606925000267},
author = {Rongfan Liu and Zhi Zou and Sihui Chen and Yang Liu and Jiayu Wan},
abstract = {Scientific research heavily relies on literature and patent reviews to build upon existing knowledge and innovation, particularly in rapidly advancing fields like battery recycling. Traditional methods for conducting these reviews, which involve manual collection and analysis, are becoming increasingly inefficient due to the overwhelming volume of information. Recent advancements in Artificial Intelligence (AI), particularly in the form of large language models (LLMs) and retrieval-augmented generation (RAG) architectures, offer promising solutions to these challenges. LLMs, enhanced by RAG, can automate the tedious tasks of literature review, providing more accurate, timely, and comprehensive insights. Despite their great potential, current RAG systems still face limitations, such as the reliance on manual database updates and challenges in providing precise answers to specific professional queries. To address these issues, we developed an innovative Agent-Based literature research system that integrates actor-critic architecture to enhance the accuracy and relevance of responses. This system is validated in the context of battery recycling, demonstrating superior performance in literature review tasks. Our findings indicate that AI-empowered literature research agents can significantly enhance the efficiency and depth of scientific inquiry, although further advancements are necessary to fully release their potential across various scientific domains.}
}
@article{PEREZPEREZ2024,
title = {Tracking the Spread of Pollen on Social Media Using Pollen-Related Messages From Twitter: Retrospective Analysis},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/58309},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006848},
author = {Martín Pérez-Pérez and María {Fernandez Gonzalez} and Francisco Javier Rodriguez-Rajo and Florentino Fdez-Riverola},
keywords = {pollen, respiratory allergies, Twitter, large language model, LLM, knowledge reconstruction, text mining},
abstract = {Background
Allergy disorders caused by biological particles, such as the proteins in some airborne pollen grains, are currently considered one of the most common chronic diseases, and European Academy of Allergy and Clinical Immunology forecasts indicate that within 15 years 50% of Europeans will have some kind of allergy as a consequence of urbanization, industrialization, pollution, and climate change.
Objective
The aim of this study was to monitor and analyze the dissemination of information about pollen symptoms from December 2006 to January 2022. By conducting a comprehensive evaluation of public comments and trends on Twitter, the research sought to provide valuable insights into the impact of pollen on sensitive individuals, ultimately enhancing our understanding of how pollen-related information spreads and its implications for public health awareness.
Methods
Using a blend of large language models, dimensionality reduction, unsupervised clustering, and term frequency–inverse document frequency, alongside visual representations such as word clouds and semantic interaction graphs, our study analyzed Twitter data to uncover insights on respiratory allergies. This concise methodology enabled the extraction of significant themes and patterns, offering a deep dive into public knowledge and discussions surrounding respiratory allergies on Twitter.
Results
The months between March and August had the highest volume of messages. The percentage of patient tweets appeared to increase notably during the later years, and there was also a potential increase in the prevalence of symptoms, mainly in the morning hours, indicating a potential rise in pollen allergies and related discussions on social media. While pollen allergy is a global issue, specific sociocultural, political, and economic contexts mean that patients experience symptomatology at a localized level, needing appropriate localized responses.
Conclusions
The interpretation of tweet information represents a valuable tool to take preventive measures to mitigate the impact of pollen allergy on sensitive patients to achieve equity in living conditions and enhance access to health information and services.}
}
@article{ARDEKANI2024103291,
title = {FinSentGPT: A universal financial sentiment engine?},
journal = {International Review of Financial Analysis},
volume = {94},
pages = {103291},
year = {2024},
issn = {1057-5219},
doi = {https://doi.org/10.1016/j.irfa.2024.103291},
url = {https://www.sciencedirect.com/science/article/pii/S1057521924002230},
author = {Aref Mahdavi Ardekani and Julie Bertz and Cormac Bryce and Michael Dowling and Suwan(Cheng) Long},
keywords = {ChatGPT, Large language models, Financial sentiment, Monetary policy, Fine-tuning},
abstract = {We present FinSentGPT, a financial sentiment prediction model based on a fine-tuned version of the artificial intelligence language model, ChatGPT. To assess the model’s effectiveness, we analyse a sample of US media news and a multi-language dataset of European Central Bank Monetary Policy Decisions. Our findings demonstrate that FinSentGPT’s sentiment classification ability aligns well with a prominent English-language finance sentiment model, surpasses an established alternative machine learning model, and is capable of predicting sentiment across various languages. Consequently, we offer preliminary evidence that advanced large-language AI models can facilitate flexible and contextual financial sentiment determination, transcending language barriers.}
}
@article{BREITUNG2025104007,
title = {Global Business Networks},
journal = {Journal of Financial Economics},
volume = {166},
pages = {104007},
year = {2025},
issn = {0304-405X},
doi = {https://doi.org/10.1016/j.jfineco.2025.104007},
url = {https://www.sciencedirect.com/science/article/pii/S0304405X25000157},
author = {Christian Breitung and Sebastian Müller},
keywords = {Business network, Textual analysis, Natural language processing, GPT-3, Large language models},
abstract = {We leverage the capabilities of GPT-3 to generate historical business descriptions for over 63,000 global firms. Utilizing these descriptions and advanced embedding models from OpenAI, we construct time-varying business networks that represent business links across the globe. We showcase the performance of these networks by studying the lead–lag effect for global stocks and predicting target firms in M&A deals. We demonstrate how masking firm-specific details can mitigate look-ahead bias concerns that may arise from the use of embedding models with a recent knowledge cutoff, and how to differentiate between competitor, supplier, and customer links by fine-tuning an open-source language model.}
}
@article{COX2024100130,
title = {An AI assistant to help review and improve causal reasoning in epidemiological documents},
journal = {Global Epidemiology},
volume = {7},
pages = {100130},
year = {2024},
issn = {2590-1133},
doi = {https://doi.org/10.1016/j.gloepi.2023.100130},
url = {https://www.sciencedirect.com/science/article/pii/S2590113323000330},
author = {Louis Anthony Cox},
keywords = {Artificial intelligence, Causality, Review methodology, Causal AI boosting, Large language models (LLMs)},
abstract = {Drawing sound causal inferences from observational data is often challenging for both authors and reviewers. This paper discusses the design and application of an Artificial Intelligence Causal Research Assistant (AIA) that seeks to help authors improve causal inferences and conclusions drawn from epidemiological data in health risk assessments. The AIA-assisted review process provides structured reviews and recommendations for improving the causal reasoning, analyses and interpretations made in scientific papers based on epidemiological data. Causal analysis methodologies range from earlier Bradford-Hill considerations to current causal directed acyclic graph (DAG) and related models. AIA seeks to make these methods more accessible and useful to researchers. AIA uses an external script (a “Causal AI Booster” (CAB) program based on classical AI concepts of slot-filling in frames organized into task hierarchies to complete goals) to guide Large Language Models (LLMs), such as OpenAI's ChatGPT or Google's LaMDA (Bard), to systematically review manuscripts and create both (a) recommendations for what to do to improve analyses and reporting; and (b) explanations and support for the recommendations. Review tables and summaries are completed systematically by the LLM in order. For example, recommendations for how to state and caveat causal conclusions in the Abstract and Discussion sections reflect previous analyses of the Study Design and Data Analysis sections. This work illustrates how current AI can contribute to reviewing and providing constructive feedback on research documents. We believe that such AI-assisted review shows promise for enhancing the quality of causal reasoning and exposition in epidemiological studies. It suggests the potential for effective human-AI collaboration in scientific authoring and review processes.}
}
@article{GUO2024108709,
title = {A survey on advancements in image–text multimodal models: From general techniques to biomedical implementations},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108709},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108709},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007947},
author = {Ruifeng Guo and Jingxuan Wei and Linzhuang Sun and Bihui Yu and Guiyong Chang and Dawei Liu and Sibo Zhang and Zhengbing Yao and Mingjun Xu and Liping Bu},
keywords = {Image–text multimodal models, Artificial intelligence, Technological evolution, Biomedical applications, Challenges and strategies},
abstract = {With the significant advancements of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), the development of image–text multimodal models has garnered widespread attention. Current surveys on image–text multimodal models mainly focus on representative models or application domains, but lack a review on how general technical models influence the development of domain-specific models, which is crucial for domain researchers. Based on this, this paper first reviews the technological evolution of image–text multimodal models, from early explorations of feature space to visual language encoding structures, and then to the latest large model architectures. Next, from the perspective of technological evolution, we explain how the development of general image–text multimodal technologies promotes the progress of multimodal technologies in the biomedical field, as well as the importance and complexity of specific datasets in the biomedical domain. Then, centered on the tasks of image–text multimodal models, we analyze their common components and challenges. After that, we summarize the architecture, components, and data of general image–text multimodal models, and introduce the applications and improvements of image–text multimodal models in the biomedical field. Finally, we categorize the challenges faced in the development and application of general models into external factors and intrinsic factors, further refining them into 2 external factors and 5 intrinsic factors, and propose targeted solutions, providing guidance for future research directions. For more details and data, please visit our GitHub page: https://github.com/i2vec/A-survey-on-image-text-multimodal-models.}
}
@article{LIGA2023105864,
title = {Fine-tuning GPT-3 for legal rule classification},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105864},
year = {2023},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2023.105864},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923000742},
author = {Davide Liga and Livio Robaldo},
keywords = {Rule classification, GPT-3, AI&Law},
abstract = {In this paper, we propose a Legal Rule Classification (LRC) task using one of the most discussed language model in the field of Artificial Intelligence, namely GPT-3, a generative pretrained language model. We train and test the proposed LRC task on the GDPR encoded in LegalDocML (Palmirani and Vitali, 2011) and LegalRuleML (Athan et al., 2013), two widely used XML standards for the legal domain. We use the LegalDocML and LegalRuleML annotations provided in Robaldo et al. (2020) to fine-tuned GPT-3. While showing the ability of large language models (LLMs) to easily learn to classify legal and deontic rules even on small amount of data, we show that GPT-3 can significantly outperform previous experiments on the same task. Our work focused on a multiclass task, showing that GPT-3 is capable to recognize the difference between obligation rules, permission rules and constitutive rules with performances that overcome previous scores in LRC.}
}@article{LI2025191,
title = {An interactive address matching method based on a graph attention mechanism},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {6},
pages = {191-200},
year = {2025},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2024.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S266630742400055X},
author = {Ming Li and Jialin Su and Zhiyu Song and Juping Qiu and Yongping Lin},
keywords = {Address matching, Interactive address matching graph attention model, Attention-based feature interaction method, Directed graph},
abstract = {Problem:
Modernizing and standardizing place names and addresses is a key challenge in the development of smart cities.
Purpose:
This paper proposes a solution to address matching challenges, such as incomplete descriptions, reversed word order, and the diverse descriptions often found in Chinese addresses.
Method:
Leveraging the hierarchical structure of Chinese addresses, this study introduces the interactive address matching graph attention model (IAMGAM). In the IAMGAM, an attention-based feature interaction method (AFIM) is employed. To reflect the hierarchical nature of address elements, a directed graph is used to model the address data, and the model is trained and tested using a graph attention mechanism.
Results:
Experiments demonstrate that the IAMGAM achieves an accuracy and F1-score of 99.61%. Compared with the existing address matching methods, the IAMGAM improves the accuracy by 0.66% to 2.57%, and the F1-score by 0.68% to 2.55%, outperforming baseline models. Additionally, ablation experiments confirm the effectiveness of each component within the model. Furthermore, when fine-tuned using ChatGLM2-6B, the results show that the IAMGAM still outperforms ChatGLM2-6B.
Conclusion:
IAMGAM demonstrates excellent performance in Chinese address matching tasks, and the Large Language Model (LLM)-based methods, such as ChatGLM2-6B, show great potential for future development in this area.}
}
@article{WANG2025112351,
title = {MPLinker: Multi-template Prompt-tuning with adversarial training for Issue–commit Link recovery},
journal = {Journal of Systems and Software},
volume = {223},
pages = {112351},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112351},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225000196},
author = {Bangchao Wang and Yang Deng and Ruiqi Luo and Peng Liang and Tingting Bi},
keywords = {Prompt-tuning, Issue–commit Link recovery, Pre-trained language model, Natural language processing},
abstract = {In recent years, the pre-training, prompting and prediction paradigm, known as prompt-tuning, has achieved significant success in Natural Language Processing (NLP). Issue–commit Link Recovery (ILR) in Software Traceability (ST) plays an important role in improving the reliability, quality, and security of software systems. The current ILR methods convert the ILR into a classification task using pre-trained language models (PLMs) and dedicated neural networks. These methods do not fully utilize the semantic information embedded in PLMs, failing to achieve acceptable performance. To address this limitation, we introduce a novel paradigm: Multi-template Prompt-tuning with adversarial training for issue–commit Link recovery (MPLinker). MPLinker redefines the ILR task as a cloze task via template-based prompt-tuning and incorporates adversarial training to enhance model generalization and reduce overfitting. We evaluated MPLinker on six open-source projects using a comprehensive set of performance metrics. The experiment results demonstrate that MPLinker achieves an average F1-score of 96.10%, Precision of 96.49%, Recall of 95.92%, MCC of 94.04%, AUC of 96.05%, and ACC of 98.15%, significantly outperforming existing state-of-the-art methods. Overall, MPLinker improves the performance and generalization of ILR models and introduces innovative concepts and methods for ILR. The replication package for MPLinker is available at https://github.com/WTU-intelligent-software-development/MPLinker.}
}
@article{CHEN2024105158,
title = {Augmented reality, deep learning and vision-language query system for construction worker safety},
journal = {Automation in Construction},
volume = {157},
pages = {105158},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105158},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004181},
author = {Haosen Chen and Lei Hou and Shaoze Wu and Guomin Zhang and Yang Zou and Sungkon Moon and Muhammed Bhuiyan},
keywords = {Construction safety, Deep learning, Vision-language models, Augmented reality},
abstract = {Low situational awareness contributes to safety incidents in construction. Existing Deep Learning (DL)-based applications lack the capability to provide context-specific and interactive feedback that is essential for workers to fully understand their surrounding environments. This paper proposes the Visual Construction Safety Query (VCSQ) system. The system encompasses real-time Image Captioning (IC), safety-centric Visual Question Answering (VQA), and keyword-based Image-Text Retrieval (ITR), integrated with head-mounted Augmented Reality (AR) devices. System validation includes benchmarks and real-world images. The ITR module posted high recall rates of 0.801 and 0.835 for Recall@5 and @10. The VQA module achieved an 89.7% accuracy rate, and the IC module had a SPICE score of 0.449. Feasibility tests and surveys confirmed the system's practical advantages in different construction scenarios. This study establishes an integration roadmap adaptable to future advancements in interactive DL and immersive AR.}
}
@article{SINGLA2024,
title = {Developing a Chatbot to Support Individuals With Neurodevelopmental Disorders: Tutorial},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/50182},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124003224},
author = {Ashwani Singla and Ritvik Khanna and Manpreet Kaur and Karen Kelm and Osmar Zaiane and Cory Scott Rosenfelt and Truong An Bui and Navid Rezaei and David Nicholas and Marek Z Reformat and Annette Majnemer and Tatiana Ogourtsova and Francois Bolduc},
keywords = {chatbot, user interface, knowledge graph, neurodevelopmental disability, autism, intellectual disability, attention-deficit/hyperactivity disorder},
abstract = {Families of individuals with neurodevelopmental disabilities or differences (NDDs) often struggle to find reliable health information on the web. NDDs encompass various conditions affecting up to 14% of children in high-income countries, and most individuals present with complex phenotypes and related conditions. It is challenging for their families to develop literacy solely by searching information on the internet. While in-person coaching can enhance care, it is only available to a minority of those with NDDs. Chatbots, or computer programs that simulate conversation, have emerged in the commercial sector as useful tools for answering questions, but their use in health care remains limited. To address this challenge, the researchers developed a chatbot named CAMI (Coaching Assistant for Medical/Health Information) that can provide information about trusted resources covering core knowledge and services relevant to families of individuals with NDDs. The chatbot was developed, in collaboration with individuals with lived experience, to provide information about trusted resources covering core knowledge and services that may be of interest. The developers used the Django framework (Django Software Foundation) for the development and used a knowledge graph to depict the key entities in NDDs and their relationships to allow the chatbot to suggest web resources that may be related to the user queries. To identify NDD domain–specific entities from user input, a combination of standard sources (the Unified Medical Language System) and other entities were used which were identified by health professionals as well as collaborators. Although most entities were identified in the text, some were not captured in the system and therefore went undetected. Nonetheless, the chatbot was able to provide resources addressing most user queries related to NDDs. The researchers found that enriching the vocabulary with synonyms and lay language terms for specific subdomains enhanced entity detection. By using a data set of numerous individuals with NDDs, the researchers developed a knowledge graph that established meaningful connections between entities, allowing the chatbot to present related symptoms, diagnoses, and resources. To the researchers’ knowledge, CAMI is the first chatbot to provide resources related to NDDs. Our work highlighted the importance of engaging end users to supplement standard generic ontologies to named entities for language recognition. It also demonstrates that complex medical and health-related information can be integrated using knowledge graphs and leveraging existing large datasets. This has multiple implications: generalizability to other health domains as well as reducing the need for experts and optimizing their input while keeping health care professionals in the loop. The researchers' work also shows how health and computer science domains need to collaborate to achieve the granularity needed to make chatbots truly useful and impactful.}
}
@article{JAHANIYEKTA2024100078,
title = {The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance},
journal = {Meta-Radiology},
volume = {2},
number = {2},
pages = {100078},
year = {2024},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2950162824000316},
author = {Mohammad Mahdi {Jahani Yekta}},
keywords = {GPT–4, Artificial general intelligence, Knowledge diffusion, Interpretability and explainability, Societal influences, Governance},
abstract = {Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.}
}
@article{RABBI2024105443,
title = {AI integration in construction safety: Current state, challenges, and future opportunities in text, vision, and audio based applications},
journal = {Automation in Construction},
volume = {164},
pages = {105443},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105443},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001791},
author = {Ahmed Bin Kabir Rabbi and Idris Jeelani},
keywords = {Artificial intelligence, Construction safety, Safety management, Automated safety},
abstract = {High occupational injury and fatality rate in the construction industry is a serious global concern. Recognizing AI as a solution to enhance safety performance, this study reviews 153 papers to assess and categorize current AI applications in construction, focusing on text, visual, and audio data, while also identifying challenges and future research opportunities. Real-time monitoring, hazard detection, and information extraction are identified as key areas where AI is applied, with a notable reliance on deep neural networks, object recognition, and Natural Language Processing. The review highlights major challenges, including the need for high-quality data management, semantic feature representation, and occluded object detection. Additionally, it underscores the untapped potential of audio-based AI and the advancements possible with Large Language Models for text interpretation. The findings emphasize the need for integrated, multi-faceted AI systems and advocate for responsible AI deployment to mitigate safety risks on construction sites.}
}
@article{BIBI2024101865,
title = {Enhancing source code retrieval with joint Bi-LSTM-GNN architecture: A comparative study with ChatGPT-LLM},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {2},
pages = {101865},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101865},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823004196},
author = {Nazia Bibi and Ayesha Maqbool and Tauseef Rana},
keywords = {Code reuse, Recommendation systems, Code recommendation, Joint model, Source code retrieval, Deep learning, LSTM, GNN, Bi-directional LSTM},
abstract = {Retrieving relevant source code from large repositories is a significant and ongoing challenge in the field of software engineering, primarily due to the vast and ever-expanding amount of available code. Existing deep learning methods, although effective to some extent, exhibit limitations in capturing the intricate and complex structural information embedded within source code, which hinders their ability to provide highly accurate retrieval results. This study endeavors to tackle this prominent issue by introducing a novel and innovative approach known as the Joint Bi-directional LSTM and Graph Neural Networks (JBLG) model for source code retrieval. The central aim is to harness the combined strengths and capabilities of Bi-directional Long Short-Term Memory (LSTM) networks and Graph Neural Networks (GNNs) to significantly enhance the model’s capacity to capture and interpret the complex structural characteristics intrinsic to source code. The proposed JBLG model employs a unique fusion of Bi-directional LSTM, which excels in capturing sequential and temporal dependencies within code, and GNN, which is adept at modeling the intricate graph structure of the code. By leveraging this hybrid architecture, the model aims to provide a comprehensive and highly effective solution for source code retrieval tasks. To assess the efficacy of the JBLG model, extensive experiments are conducted, and the model’s performance is evaluated against well-established benchmarks, including LSTM, GNN, and ChatGPT, using two diverse datasets: CodeSearchNet and CosBench datasets. These evaluations span multiple programming languages, ensuring a comprehensive and robust assessment of the model’s capabilities. The experimental results indicate that the JBLG model consistently outperforms its counterparts, including Bi-LSTM, GNN, ChatGPT, and DGMS, across various evaluation metrics. the JBLG model showcases an exceptional ability to handle and extract the intricate structural information inherent in source code, resulting in significantly enhanced retrieval accuracy. The JBLG model emerges as a highly promising solution for real-world source code retrieval applications, with the potential to revolutionize the field. The success of this model underscores the importance of combining deep learning techniques like Bi-directional LSTM and GNNs for tackling complex software engineering challenges. Furthermore, future research directions could involve exploring advanced techniques such as attention mechanisms and extending the model’s applicability to other software engineering tasks like code summarization and code completion. The findings of this study are expected to have a lasting impact on the advancement of source code retrieval methodologies.}
}
@article{MAO2024101988,
title = {A survey on semantic processing techniques},
journal = {Information Fusion},
volume = {101},
pages = {101988},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101988},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003044},
author = {Rui Mao and Kai He and Xulang Zhang and Guanyi Chen and Jinjie Ni and Zonglin Yang and Erik Cambria},
keywords = {Semantic processing, Word sense disambiguation, Anaphora resolution, Named entity recognition, Concept extraction, Subjectivity detection},
abstract = {Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.}
}
@article{ZERMATTEN2025621,
title = {Learning transferable land cover semantics for open vocabulary interactions with remote sensing images},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {220},
pages = {621-636},
year = {2025},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2025.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271625000061},
author = {Valérie Zermatten and Javiera Castillo-Navarro and Diego Marcos and Devis Tuia},
keywords = {Land cover mapping, Open vocabulary semantic segmentation, Vision-language model for remote sensing},
abstract = {Why should we confine land cover classes to rigid and arbitrary definitions? Land cover mapping is a central task in remote sensing image processing, but the rigorous class definitions can sometimes restrict the transferability of annotations between datasets. Open vocabulary recognition, i.e. using natural language to define a specific object or pattern in an image, breaks free from predefined nomenclature and offers flexible recognition of diverse categories with a more general image understanding across datasets and labels. The open vocabulary framework opens doors to search for concepts of interest, beyond individual class boundaries. In this work, we propose to use Text As supervision for COntrastive Semantic Segmentation (TACOSS), and we design an open vocabulary semantic segmentation model that extends its capacities beyond that of a traditional model for land cover mapping: In addition to visual pattern recognition, TACOSS leverages the common sense knowledge captured by language models and is capable of interpreting the image at the pixel level, attributing semantics to each pixel and removing the constraints of a fixed set of land cover labels. By learning to match visual representations with text embeddings, TACOSS can transition smoothly from one set of labels to another and enables the interaction with remote sensing images in natural language. Our approach combines a pretrained text encoder with a visual encoder and adopts supervised contrastive learning to align the visual and textual modalities. We explore several text encoders and label representation methods and compare their abilities to encode transferable land cover semantics. The model’s capacity to predict a set of different land cover labels on an unseen dataset is also explored to illustrate the generalization capacities across domains of our approach. Overall, TACOSS is a general method and permits adapting between different sets of land cover labels with minimal computational overhead. Code is publicly available online11https://github.com/eceo-epfl/RS-OVSS..}
}
@article{LI2025103996,
title = {Basis is also explanation: Interpretable Legal Judgment Reasoning prompted by multi-source knowledge},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {103996},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103996},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003558},
author = {Shangyuan Li and Shiman Zhao and Zhuoran Zhang and Zihao Fang and Wei Chen and Tengjiao Wang},
keywords = {Legal judgment prediction, Prompt learning, Contrastive learning, Knowledge encoding, Interpretability},
abstract = {The task of Legal Judgment Prediction (LJP) aims to forecast case outcomes by analyzing fact descriptions, playing a pivotal role in enhancing judicial system efficiency and fairness. Existing LJP methods primarily focus on improving representations of fact descriptions to enhance judgment performance. However, these methods typically depend on the superficial case information and neglect the underlying legal basis, resulting in a lack of in-depth reasoning and interpretability in the judgment process of long-tail or confusing cases. Recognizing that the basis for judgments in real-world legal contexts encompasses both factual logic and related legal knowledge, we introduce the interpretable legal judgment reasoning framework with multi-source knowledge prompted. The essence of this framework is to transform the implicit factual logic of cases and external legal knowledge into explicit basis for judgment, aiming to enhance not only the accuracy of judgment predictions but also the interpretability of the reasoning process. Specifically, we design a chain prompt reasoning module that guides a large language model to elucidate factual logic basis through incremental reasoning, aligning the model prior knowledge with task-oriented knowledge in the process. To match the above fact-based information with legal knowledge basis, we propose a contrastive knowledge fusing module to inject external statutes knowledge into the fact description embedding. It pushes away the distance of similar knowledge in the semantic space during the encoding of external knowledge base without manual annotation, thus improving the judgment prediction performance of long-tail and confusing cases. Experimental results on two real datasets indicate that our framework significantly outperforms existing LJP baseline methods in accuracy and interpretability, achieving new state-of-the-art performance. In addition, tests on specially constructed long-tail and confusing case datasets demonstrate that the proposed framework possesses improved generalization abilities for predicting these complex cases.}
}
@article{ASKARIZADE2025125649,
title = {Enhancing rumor detection with data augmentation and generative pre-trained transformer},
journal = {Expert Systems with Applications},
volume = {262},
pages = {125649},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125649},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424025168},
author = {Mojgan Askarizade},
keywords = {Fake news detection, Finetuned language model, Neural network classifier, Rumor detection, Generative pre-trained transformer, Data augmentation},
abstract = {The advent of social networks has facilitated the rapid dissemination of false information, including rumors, leading to significant societal and individual damages. Extensive research has been dedicated to rumor detection, ranging from machine learning techniques to neural networks. However, the existing methods could not learn the deep concepts of the rumor text to detect the rumor. In addition, imbalanced datasets in the rumor domain reduce the effectiveness of these algorithms. This study addresses this challenge by leveraging the Generative Pre-trained Transformer 2 (GPT-2) model to generate rumor-like texts, thus creating a balanced dataset. Subsequently, a novel approach for classifying rumor texts is proposed by modifying the GPT-2 model. We compare our results with state-of-art machine learning and deep learning methods as well as pre-trained models on the PHEME, Twitter15, and Twitter16 datasets. Our findings demonstrate that the proposed model, implementing advanced artificial intelligence techniques, has improved accuracy and F-measure in the application of detecting rumors compared to previous methods.}
}
@article{ZHU2025109589,
title = {Soft Prompt-tuning with Self-Resource Verbalizer for short text streams},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109589},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109589},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017470},
author = {Yi Zhu and Ye Wang and Yun Li and Jipeng Qiang and Yunhao Yuan},
keywords = {Short text streams, Prompt-tuning, Soft template, Verbalizer, Text classification},
abstract = {Short text streams such as real-time news and search snippets have attained vast amounts of attention and research in recent decades, the characteristics of high generation velocity, feature sparsity, and high ambiguity accentuate both the importance and challenges to language models. However, most of the existing short text stream classification methods can neither automatically select relevant knowledge components for arbitrary samples, nor expand knowledge internally instead of rely on external open knowledge base to address the inherent limitations of short text stream. In this paper, we propose a Soft Prompt-tuning with Self-Resource Verbalizer (SPSV for short) for short text stream classification, the soft prompt with self-resource knowledgeable expansion is conducted for updating label words space to address evolved semantic topics in the data streams. Specifically, the automatic constructed prompt is first generated to instruct the model prediction, which is optimized to address the problem of high velocity and topic drift in short text streams. Then, in each chunk, the projection between category names and label words space, i.e. verbalizer, is updated, which is constructed by internal knowledge expansion from the short text itself. Through comprehensive experiments on four well-known benchmark datasets, we validate the superb performance of our method compared to other short text stream classification and fine-tuning PLMs methods, which achieves up to more than 90% classification accuracy with the counts of data chunk increased.}
}
@article{JIM2024100059,
title = {Recent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review},
journal = {Natural Language Processing Journal},
volume = {6},
pages = {100059},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100059},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000074},
author = {Jamin Rahman Jim and Md Apon Riaz Talukder and Partha Malakar and Md Mohsin Kabir and Kamruddin Nur and M.F. Mridha},
keywords = {Sentiment classification, Text classification, Natural language processing, Emotion detection, Sentiment analysis},
abstract = {Sentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. Scrutinizing words and phrases categorizes them into positive, negative, or neutral sentiments. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative. Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. We then delved into prevalent pre-processing techniques, datasets, and evaluation metrics to enhance comprehension. We also explored Machine Learning, Deep Learning, Large Language Models and Pre-trained models in sentiment analysis, providing insights into their advantages and drawbacks. Subsequently, we precisely reviewed the experimental results and limitations of recent state-of-the-art articles. Finally, we discussed the diverse challenges encountered in sentiment analysis and proposed future research directions to mitigate these concerns. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.}
}
@article{NASARIAN2024102412,
title = {Designing interpretable ML system to enhance trust in healthcare: A systematic review to proposed responsible clinician-AI-collaboration framework},
journal = {Information Fusion},
volume = {108},
pages = {102412},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102412},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524001908},
author = {Elham Nasarian and Roohallah Alizadehsani and U.Rajendra Acharya and Kwok-Leung Tsui},
keywords = {Interpretable ML, AI-based medical devices, Unstructured data, Medical large language models, Human-computer-interaction, Wearable medical devices, Explainable casual analysis, Responsible AI},
abstract = {Background
Artificial intelligence (AI)-based medical devices and digital health technologies, including medical sensors, wearable health trackers, telemedicine, mobile health (mHealth), large language models (LLMs), and digital care twins (DCTs), significantly influence the process of clinical decision support systems (CDSS) in healthcare and medical applications. However, given the complexity of medical decisions, it is crucial that results generated by AI tools not only be correct but also carefully evaluated, understandable, and explainable to end-users, especially clinicians. The lack of interpretability in communicating AI clinical decisions can lead to mistrust among decision-makers and a reluctance to use these technologies.
Objective
This paper systematically reviews the processes and challenges associated with interpretable machine learning (IML) and explainable artificial intelligence (XAI) within the healthcare and medical domains. Its main goals are to examine the processes of IML and XAI, their related methods, applications, and the implementation challenges they pose in digital health interventions (DHIs), particularly from a quality control perspective, to help understand and improve communication between AI systems and clinicians. The IML process is categorized into pre-processing interpretability, interpretable modeling, and post-processing interpretability. This paper aims to foster a comprehensive understanding of the significance of a robust interpretability approach in clinical decision support systems (CDSS) by reviewing related experimental results. The goal is to provide future researchers with insights for creating clinician-AI tools that are more communicable in healthcare decision support systems and offer a deeper understanding of their challenges.
Methods
Our research questions, eligibility criteria, and primary goals were proved using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline and the PICO (population, intervention, control, and outcomes) method. We systematically searched PubMed, Scopus, and Web of Science databases using sensitive and specific search strings. Subsequently, duplicate papers were removed using EndNote and Covidence. A two-phase selection process was then carried out on Covidence, starting with screening by title and abstract, followed by a full-text appraisal. The Meta Quality Appraisal Tool (MetaQAT) was used to assess the quality and risk of bias. Finally, a standardized data extraction tool was employed for reliable data mining.
Results
The searches yielded 2,241 records, from which 555 duplicate papers were removed. During the title and abstract screening step, 958 papers were excluded, and the full-text review step excluded 482 studies. Subsequently, in quality and risk of bias assessment, 172 papers were removed. 74 publications were selected for data extraction, which formed 10 insightful reviews and 64 related experimental studies.
Conclusion
The paper provides general definitions of explainable artificial intelligence (XAI) in the medical domain and introduces a framework for interpretability in clinical decision support systems structured across three levels. It explores XAI-related health applications within each tier of this framework, underpinned by a review of related experimental findings. Furthermore, the paper engages in a detailed discussion of quality assessment tools for evaluating XAI in intelligent health systems. It also presents a step-by-step roadmap for implementing XAI in clinical settings. To direct future research toward bridging current gaps, the paper examines the importance of XAI models from various angles and acknowledges their limitations.}
}
@article{MANZANARESSALOR2025112945,
title = {Enhancing text anonymization via re-identification risk-based explainability},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112945},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112945},
url = {https://www.sciencedirect.com/science/article/pii/S095070512401579X},
author = {Benet Manzanares-Salor and David Sánchez},
keywords = {Privacy protection, Neural language models, Explainability, Text anonymization, Re-identification},
abstract = {Text anonymization is a challenging task usually carried out by human annotators, thereby incurring significant economic and temporal costs. Even though automated approaches have been proposed to mitigate those costs, practical mechanisms for text anonymization mostly rely on named entity recognition (NER), which is acknowledged to offer insufficient privacy protection. To tackle this issue, we propose a methodology to enhance the privacy protection attained by any text anonymization mechanism –with a focus on NER-based ones–, while providing empirical guarantees against re-identification rooted on the k-anonymity privacy model. Our method relies on a neural language model trained on the aggregated background knowledge that can be leveraged to conduct re-identification attacks. Then, it employs explainability techniques to detect and iteratively mask the unprotected terms that caused the greatest re-identification risk until a user-defined k-anonymity level is reached. On the contrary to most existing methods in the text anonymization literature, our approach allows to intuitively configure the desired level of protection, and to tune the trade-off between privacy and data utility preservation. Experiments show that our method is able to significantly and consistently lower the re-identification risk of NER-based anonymizations, and to compete against more sophisticated state-of-the-art text anonymization methods while being free of their costs and external dependencies.}
}
@article{RAMONI202431,
title = {Artificial intelligence in scientific medical writing: Legitimate and deceptive uses and ethical concerns},
journal = {European Journal of Internal Medicine},
volume = {127},
pages = {31-35},
year = {2024},
issn = {0953-6205},
doi = {https://doi.org/10.1016/j.ejim.2024.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0953620524002954},
author = {Davide Ramoni and Cosimo Sgura and Luca Liberale and Fabrizio Montecucco and John P.A. Ioannidis and Federico Carbone},
keywords = {Artificial intelligence, Chatbots, ChatGPT, Medical writing, Natural language understanding, Large language models},
abstract = {The debate surrounding the integration of artificial intelligence (AI) into scientific writing has already attracted significant interest in medical and life sciences. While AI can undoubtedly expedite the process of manuscript creation and correction, it raises several criticisms. The crossover between AI and health sciences is relatively recent, but the use of AI tools among physicians and other scientists who work in the life sciences is growing very fast. Within this whirlwind, it is becoming essential to realize where we are heading and what the limits are, including an ethical perspective. Modern conversational AIs exhibit a context awareness that enables them to understand and remember any conversation beyond any predefined script. Even more impressively, they can learn and adapt as they engage with a growing volume of human language input. They all share neural networks as background mathematical models and differ from old chatbots for their use of a specific network architecture called transformer model [1]. Some of them exceed 100 terabytes (TB) (e.g., Bloom, LaMDA) or even 500 TB (e.g., Megatron-Turing NLG) of text data, the 4.0 version of ChatGPT (GPT-4) was trained with nearly 45 TB, but stays updated by the internet connection and may integrate with different plugins that enhance its functionality, making it multimodal.}
}
@incollection{GHANDIKOTA2024171,
title = {Chapter Seven - Application of artificial intelligence and machine learning in drug repurposing},
editor = {Vijai Singh},
series = {Progress in Molecular Biology and Translational Science},
publisher = {Academic Press},
volume = {205},
pages = {171-211},
year = {2024},
booktitle = {New Approach for Drug Repurposing Part A},
issn = {1877-1173},
doi = {https://doi.org/10.1016/bs.pmbts.2024.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S1877117324000851},
author = {Sudhir K. Ghandikota and Anil G. Jegga},
keywords = {Drug repurposing, Drug repositioning, Artificial Intelligence, De novo drug discovery, Machine Learning, Deep Learning, Network analysis},
abstract = {The purpose of drug repurposing is to leverage previously approved drugs for a particular disease indication and apply them to another disease. It can be seen as a faster and more cost-effective approach to drug discovery and a powerful tool for achieving precision medicine. In addition, drug repurposing can be used to identify therapeutic candidates for rare diseases and phenotypic conditions with limited information on disease biology. Machine learning and artificial intelligence (AI) methodologies have enabled the construction of effective, data-driven repurposing pipelines by integrating and analyzing large-scale biomedical data. Recent technological advances, especially in heterogeneous network mining and natural language processing, have opened up exciting new opportunities and analytical strategies for drug repurposing. In this review, we first introduce the challenges in repurposing approaches and highlight some success stories, including those during the COVID-19 pandemic. Next, we review some existing computational frameworks in the literature, organized on the basis of the type of biomedical input data analyzed and the computational algorithms involved. In conclusion, we outline some exciting new directions that drug repurposing research may take, as pioneered by the generative AI revolution.}
}
@article{PANOUTSOPOULOS2024109268,
title = {Investigating the effect of different fine-tuning configuration scenarios on agricultural term extraction using BERT},
journal = {Computers and Electronics in Agriculture},
volume = {225},
pages = {109268},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109268},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924006598},
author = {Hercules Panoutsopoulos and Borja Espejo-Garcia and Stephan Raaijmakers and Xu Wang and Spyros Fountas and Christopher Brewster},
keywords = {Automatic term extraction, Agriculture, Agriculture-BERT, Fine tuning configuration scenarios, Silver standard corpus},
abstract = {This paper compares different transformer-based language models for automatic term extraction from agriculture-related texts. Agriculture is an important economic sector faced with severe environmental and societal challenges. The collection, annotation and sharing of agricultural scientific knowledge is key to enabling the agricultural sector to address its challenges. Automatic term extraction is a Natural Language Processing task that can provide solutions to text tagging and annotation towards better knowledge and information exchange. It is concerned with the identification of terms pertaining to a domain, or area of expertise, in text and is an important step in knowledge base creation and update pipelines. Transformer-based language modeling technologies like BERT have become popular for automatic term extraction, but limited work has been undertaken so far in applying these methods to agriculture. This paper systematically compares Agriculture-BERT to Sci-BERT, RoBERTa, and vanilla BERT, which were fine-tuned for the automatic extraction of agricultural terms from English texts. The greatest challenge faced in our research was the scarcity of agriculture-related gold standard corpora for measuring automatic term extraction performance. Our results show that, with a few exceptions, Agriculture-BERT performs better than the other models considered in our research. Our main contribution and novelty of the presented research is the investigation of the impact that different language model fine-tuning configuration scenarios had on the term extraction task. More specifically, we tested different scenarios related to the model layers kept frozen, or being updated, during training, to measure the impact they may have on Agriculture-BERT’s performance in automatic term extraction. Our results show that the best performance was achieved by: (i) the “embedding layer updated + all encoder layers updated” scenario for the identification of terms also seen during training; (ii) the “embedding layer frozen + all encoder layers updated” scenario for the identification of terms being synonyms to those seen during training; and (iii) the “embedding layer updated + top 4 encoder layers updated” scenario for identifying terms neither seen during training nor being synonyms to those seen during training (novel terms).}
}
@incollection{HARALAMBOUS2024,
title = {Natural Language Processing},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00090-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041000909},
author = {Yannis Haralambous},
keywords = {Attention mechanism, Chatbots, ChatGPT, Computational linguistics, Deep learning, ELIZA, Ethics in AI, Interpretability, Language generation, Large language models (LLM), Machine learning, Machine translation, Natural language processing (NLP), Neural networks, Sentiment analysis, Speech recognition, Statistical methods, Summarization, Text classification, Word embeddings},
abstract = {Natural Language Processing (NLP) evolved from rule-based systems in the 1960s–70s to statistical methods in the 1980s–90s, and finally to the deep learning revolution of the 21st century. The key milestones of its evolution include ELIZA, an early chatbot (1966), a focus on formal grammars and knowledge-based systems (1980s), the rise of statistical methods and machine learning (1990s), the popularization of word embeddings (2013), sequence-to-sequence models and attention mechanisms (2014–2016) and large language models like BERT, GPT-3, and ChatGPT (in the recent years). Major NLP applications can be classified in three categories: those that have linguistic input and non-linguistic output (e.g., text classification, sentiment analysis), those that transform linguistic input to linguistic output (e.g., machine translation, summarization), and those that are based on non-linguistic input (or no input at all) and have linguistic output (e.g., weather report generation, poetry generation). As for chatbots, they are comprehensive NLP applications, encompassing multiple tasks. Key challenges in NLP are the difficulty of machine translation and the importance of context in language understanding. Significant progress has been made with deep learning and large language models, but issues remain, like the lack of interpretability and ethical concerns. We provide resources for further learning, including popular textbooks, online courses, scientific journals, and mention platforms like Hugging Face and Kaggle for accessing datasets and models. Looking to the future, we anticipate increased multimodal integration, efforts to support under-resourced languages, and a focus on addressing interpretability and ethical issues in large language models. Overall, NLP is as a rapidly evolving field that has made significant strides in understanding and generating human language, with exciting possibilities and important challenges ahead.}
}
@article{ALREFAIE2024108654,
title = {Chemical, biological, radiological and nuclear event detection and classification using ontology interrogation and social media data},
journal = {Engineering Applications of Artificial Intelligence},
volume = {135},
pages = {108654},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108654},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624008121},
author = {Mohamed Taher Alrefaie and Tom W. Jackson and Ejovwoke Onojeharho and Suzanne Elayan},
keywords = {Ontology, Disaster management, Information retrieval, Social media analysis, Machine learning, Natural language processing},
abstract = {In an era where chemical, biological, radiological, and nuclear (CBRN) incidents present a grave threat to public safety, timely and accurate information is paramount. The complexity of the CBRN concept encompasses a range of incidents, each with unique and overlapping symptoms, related substances, and event descriptions. This study introduces an innovative approach to the development of a CBRN-specific ontology, uniting diverse data sources and domain expertise to construct a comprehensive repository of CBRN events, sub-events, their causes, symptoms, and toxic substances. Unlike prior methodologies reliant on keyword searches and predefined categories, our approach enables a holistic analysis of textual data by capturing intricate relationships between symptoms and toxic substances. We leverage this ontology in conjunction with a tailored interrogation algorithm to detect potential CBRN incidents through social media data. The algorithm was then tested on datasets of three actual CBRN incidents, one fictional incident (TV show) that simulated a nuclear incident and one non-CBRN. The interrogation algorithm was able to detect the five CBRN incidents accurately. However, the study showcased the need to extend the algorithm to distinguish between real and fictional CBRN incidents. These findings underscore the potential of this approach to deliver timely information on potential CBRN incidents. Nevertheless, the study acknowledged the inherent challenges and limitations in utilizing social media data, including the risk of misinformation, fictional events, fake news, and interference from malicious actors, all of which can affect the accuracy and reliability of the information collected.}
}
@article{NIELSEN2025,
title = {Investigating the Classification of Living Kidney Donation Experiences on Reddit and Understanding the Sensitivity of ChatGPT to Prompt Engineering: Content Analysis},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/57319},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000067},
author = {Joshua Nielsen and Xiaoyu Chen and LaShara Davis and Amy Waterman and Monica Gentili},
keywords = {prompt engineering, generative artificial intelligence, kidney donation, transplant, living donor},
abstract = {Background
Living kidney donation (LKD), where individuals donate one kidney while alive, plays a critical role in increasing the number of kidneys available for those experiencing kidney failure. Previous studies show that many generous people are interested in becoming living donors; however, a huge gap exists between the number of patients on the waiting list and the number of living donors yearly.
Objective
To bridge this gap, we aimed to investigate how to identify potential living donors from discussions on public social media forums so that educational interventions could later be directed to them.
Methods
Using Reddit forums as an example, this study described the classification of Reddit content shared about LKD into three classes: (1) present (presently dealing with LKD personally), (2) past (dealt with LKD personally in the past), and (3) other (LKD general comments). An evaluation was conducted comparing a fine-tuned distilled version of the Bidirectional Encoder Representations from Transformers (BERT) model with inference using GPT-3.5 (ChatGPT). To systematically evaluate ChatGPT’s sensitivity to distinguishing between the 3 prompt categories, we used a comprehensive prompt engineering strategy encompassing a full factorial analysis in 48 runs. A novel prompt engineering approach, dialogue until classification consensus, was introduced to simulate a deliberation between 2 domain experts until a consensus on classification was achieved.
Results
BERT and GPT-3.5 exhibited classification accuracies of approximately 75% and 78%, respectively. Recognizing the inherent ambiguity between classes, a post hoc analysis of incorrect predictions revealed sensible reasoning and acceptable errors in the predictive models. Considering these acceptable mismatched predictions, the accuracy improved to 89.3% for BERT and 90.7% for GPT-3.5.
Conclusions
Large language models, such as GPT-3.5, are highly capable of detecting and categorizing LKD-targeted content on social media forums. They are sensitive to instructions, and the introduced dialogue until classification consensus method exhibited superior performance over stand-alone reasoning, highlighting the merit in advancing prompt engineering methodologies. The models can produce appropriate contextual reasoning, even when final conclusions differ from their human counterparts.}
}
@article{JO2025298,
title = {Comprehensive review of advances in machine-learning-driven optimization and characterization of perovskite materials for photovoltaic devices},
journal = {Journal of Energy Chemistry},
volume = {101},
pages = {298-323},
year = {2025},
issn = {2095-4956},
doi = {https://doi.org/10.1016/j.jechem.2024.09.043},
url = {https://www.sciencedirect.com/science/article/pii/S2095495624006673},
author = {Bonghyun Jo and Wenning Chen and Hyun Suk Jung},
keywords = {Perovskite solar cell, Data-driven machine learning, Characterization, Perovskite materials},
abstract = {Perovskite solar cells (PSCs) have developed rapidly, positioning them as potential candidates for next-generation renewable energy sources. However, conventional trial-and-error approaches and the vast compositional parameter space continue to pose challenges in the pursuit of exceptional performance and high stability of perovskite-based optoelectronics. The increasing demand for novel materials in optoelectronic devices and establishment of substantial databases has enabled data-driven machine-learning (ML) approaches to swiftly advance in the materials field. This review succinctly outlines the fundamental ML procedures, techniques, and recent breakthroughs, particularly in predicting the physical characteristics of perovskite materials. Moreover, it highlights research endeavors aimed at optimizing and screening materials to enhance the efficiency and stability of PSCs. Additionally, this review highlights recent efforts in using characterization data for ML, exploring their correlations with material properties and device performance, which are actively being researched, but they have yet to receive significant attention. Lastly, we provide future perspectives, such as leveraging Large Language Models (LLMs) and text-mining, to expedite the discovery of novel perovskite materials and expand their utilization across various optoelectronic fields.}
}
@article{PAPASTRATIS2024112291,
title = {Can ChatGPT provide appropriate meal plans for NCD patients?},
journal = {Nutrition},
volume = {121},
pages = {112291},
year = {2024},
issn = {0899-9007},
doi = {https://doi.org/10.1016/j.nut.2023.112291},
url = {https://www.sciencedirect.com/science/article/pii/S0899900723003192},
author = {Ilias Papastratis and Andreas Stergioulas and Dimitrios Konstantinidis and Petros Daras and Kosmas Dimitropoulos},
keywords = {ChatGPT, Nutrition, Artificial intelligence, Recommendation systems},
abstract = {Objectives
Dietary habits significantly affect health conditions and are closely related to the onset and progression of non-communicable diseases (NCDs). Consequently, a well-balanced diet plays an important role in lessening the effects of various disorders, including NCDs. Several artificial intelligence recommendation systems have been developed to propose healthy and nutritious diets. Most of these systems use expert knowledge and guidelines to provide tailored diets and encourage healthier eating habits. However, new advances in large language models such as ChatGPT, with their ability to produce human-like responses, have led individuals to search for advice in several tasks, including diet recommendations. This study aimed to determine the ability of ChatGPT models to generate appropriate personalized meal plans for patients with obesity, cardiovascular diseases, and type 2 diabetes.
Methods
Using a state-of-the-art knowledge-based recommendation system as a reference, we assessed the meal plans generated by two large language models in terms of energy intake, nutrient accuracy, and meal variability.
Results
Experimental results with different user profiles revealed the potential of ChatGPT models to provide personalized nutritional advice.
Conclusion
Additional supervision and guidance by nutrition experts or knowledge-based systems are required to ensure meal appropriateness for users with NCDs.}
}
@article{SHAMSHIRI2024105200,
title = {Text mining and natural language processing in construction},
journal = {Automation in Construction},
volume = {158},
pages = {105200},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105200},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004600},
author = {Alireza Shamshiri and Kyeong Rok Ryu and June Young Park},
keywords = {Text mining, Natural language processing, Machine learning, Computational linguistics, Language models, Construction, Project management},
abstract = {Text mining (TM) and natural language processing (NLP) have stirred interest within the construction field, as they offer enhanced capabilities for managing and analyzing text-based information. This highlights the need for a systematic review to identify the status quo, gaps, and future directions from the perspective of construction management. A review was conducted by aligning the objectives of 205 publications with the specific domains, areas, tasks, and processes outlined in construction management practices. This review reveals multiple facets of the construction sector empowered by TM/NLP approaches and highlights essential voids demanding consideration for automation possibilities and minimizing manual tasks. Ultimately, following identified obstacles, the review results indicate potential research opportunities: (1) strengthening overlooked construction aspects, (2) coupling diverse data formats, and (3) leveraging pre-trained language models and reinforcement learning. The findings will provide vital insights, fostering further progress in TM/NLP research and its applications in academia and industry.}
}
@article{MAO2025102712,
title = {A survey on pragmatic processing techniques},
journal = {Information Fusion},
volume = {114},
pages = {102712},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102712},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004901},
author = {Rui Mao and Mengshi Ge and Sooji Han and Wei Li and Kai He and Luyao Zhu and Erik Cambria},
keywords = {Pragmatic processing, Metaphor understanding, Sarcasm detection, Personality recognition, Aspect extraction, Sentiment polarity detection},
abstract = {Pragmatics, situated in the domains of linguistics and computational linguistics, explores the influence of context on language interpretation, extending beyond the literal meaning of expressions. It constitutes a fundamental element for natural language understanding in machine intelligence. With the advancement of large language models, the research focus in natural language processing has predominantly shifted toward high-level task processing, inadvertently downplaying the importance of foundational pragmatic processing tasks. Nevertheless, pragmatics serves as a crucial medium for unraveling human language cognition. The exploration of pragmatic processing stands as a pivotal facet in realizing linguistic intelligence. This survey encompasses important pragmatic processing techniques for subjective and emotive tasks, such as personality recognition, sarcasm detection, metaphor understanding, aspect extraction, and sentiment polarity detection. It spans theoretical research, the forefront of pragmatic processing techniques, and downstream applications, aiming to highlight the significance of these low-level tasks in advancing natural language understanding and linguistic intelligence.}
}
@article{ZHU2025129008,
title = {Soft prompt-tuning for unsupervised domain adaptation via self-supervision},
journal = {Neurocomputing},
volume = {617},
pages = {129008},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129008},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401779X},
author = {Yi Zhu and Shuqin Wang and Yun Li and Yunhao Yuan and Jipeng Qiang},
keywords = {Unsupervised domain adaptation, Soft prompt-tuning, Cross-domain text classification, Verbalizer construction},
abstract = {Unsupervised domain adaptation methods aim to facilitate learning tasks in unlabeled target domains using labeled information from related source domains. Recently, prompt-tuning has emerged as a powerful instrument to incorporate templates that reformulate input examples into equivalent cloze-style phrases. However, there are still two great challenges for domain adaptation: (1) Existing prompt-tuning methods only rely on the general knowledge distributed in upstream pre-trained language models to alleviate the domain discrepancy. How to incorporate specific features in the source and target domains into prompt-tuning model is still divergent and under-explored; (2) In the prompt-tuning, either the crafted template methods are time-consuming and labor-intensive, or automatic prompt generation methods cannot achieve satisfied performance. To address these issues, in this paper, we propose an innovative Soft Prompt-tuning method for Unsupervised Domain Adaptation via Self-Supervision, which combines two novel ideas: Firstly, instead of only stimulating knowledge distributed in the pre-trained model, we further employ hierarchically clustered optimization strategies in a self-supervised manner to retrieve knowledge for the verbalizer construction in prompt-tuning. Secondly, we construct prompts with the special designed verbalizer that facilitate the transfer of learning representations across domains, which can consider both the automatic template generation and cross-domain classification performance. Extensive experimental results demonstrate that our method even outperforms SOTA baselines that utilize external open knowledge with much less computational time.}
}
@article{DEMURO20241,
title = {Artificial intelligence and the ethnographic encounter: Transhuman language ontologies, or what it means “to write like a human, think like a machine”},
journal = {Language & Communication},
volume = {96},
pages = {1-12},
year = {2024},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0271530924000119},
author = {Eugenia Demuro and Laura Gurney},
keywords = {Artificial intelligence, Language ontologies, Ethnographic encounter, Transhumanism, Posthumanism},
abstract = {In this paper, we employ the language ontologies framework to artificial intelligence (specifically, OpenAI's ChatGPT) to investigate the ‘ethnographic encounter’ between human and non-human language users. Our focus is on the exchange and interplay between human language users and non-human artificial language generators in the production of written text. We analyse how such programs transform our understanding of what language is or might be; their practices to create language are unfamiliar, and yet they make sense to human interlocutors. Drawing from, and building on, the language ontologies framework, we discuss the practices involved in such encounters and suggest the need for an updated ‘toolkit’ in our understanding of language to account for transhuman interactions.}
}
@article{RATHORE2024108926,
title = {ToxinPred 3.0: An improved method for predicting the toxicity of peptides},
journal = {Computers in Biology and Medicine},
volume = {179},
pages = {108926},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108926},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524010114},
author = {Anand Singh Rathore and Shubham Choudhury and Akanksha Arora and Purva Tijare and Gajendra P.S. Raghava},
keywords = {Toxic motifs, Virtual screening, Machine learning, Deep learning, Large language models, Ensemble/hybrid method},
abstract = {Toxicity emerges as a prominent challenge in the design of therapeutic peptides, causing the failure of numerous peptides during clinical trials. In 2013, our group developed ToxinPred, a computational method that has been extensively adopted by the scientific community for predicting peptide toxicity. In this paper, we propose a refined variant of ToxinPred that showcases improved reliability and accuracy in predicting peptide toxicity. Initially, we utilized a similarity/alignment-based approach employing BLAST to predict toxic peptides, which yielded satisfactory accuracy; however, the method suffered from inadequate coverage. Subsequently, we employed a motif-based approach using MERCI software to uncover specific patterns or motifs that are exclusively observed in toxic peptides. The search for these motifs in peptides allowed us to predict toxic peptides with a high level of specificity with poor sensitivity. To overcome the coverage limitations, we developed alignment-free methods using machine/deep learning techniques to balance sensitivity and specificity of prediction. Deep learning model (ANN - LSTM with fixed sequence length) developed using one-hot encoding achieved a maximum AUROC of 0.93 with MCC of 0.71 on an independent dataset. Machine learning model (extra tree) developed using compositional features of peptides achieved a maximum AUROC of 0.95 with MCC of 0.78. We also developed large language models and achieved maximum AUC of 0.93 using ESM2-t33. Finally, we developed hybrid or ensemble methods combining two or more methods to enhance performance. Our specific hybrid method, which combines a motif-based approach with a machine learning-based model, achieved a maximum AUROC of 0.98 with MCC 0.81 on an independent dataset. In this study, all models were trained and tested on 80 % of data using five-fold cross-validation and evaluated on the remaining 20 % of data called independent dataset. The evaluation of all methods on an independent dataset revealed that the method proposed in this study exhibited better performance than existing methods. To cater to the needs of the scientific community, we have developed a standalone software, pip package and web-based server ToxinPred3 (https://github.com/raghavagps/toxinpred3 and https://webs.iiitd.edu.in/raghava/toxinpred3/).}
}
@article{KIM2025112269,
title = {Technological applications of social robots to create healthy and comfortable smart home environment},
journal = {Building and Environment},
volume = {267},
pages = {112269},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2024.112269},
url = {https://www.sciencedirect.com/science/article/pii/S0360132324011119},
author = {Hakpyeong Kim and Minjin Kong and Seunghoon Jung and Jaewon Jeoung and Hyuna Kang and Taehoon Hong},
keywords = {Smart home, Home automation, Social robot, Emotion recognition, Personal assistance},
abstract = {The increasing demand for healthy and comfortable living environments has driven significant advancements in smart home technology. However, current developments often overlook the importance of users’ social-emotional needs and the contextual dynamics within the home environment. This study proposes the integration of social robots as a promising solution to address these gaps. By enhancing smart home functionalities, social robots offer a more holistic approach to smart home design. A comprehensive literature review was conducted using the PRISMA methodology combined with large language model-based topic modeling to identify current research trends in social robotics. The analysis revealed five key research areas: (i) social-emotional intelligence, (ii) physical embodiment, (iii) elderly care, (iv) pediatric care, and (v) therapeutic applications. The study discusses how the core functionalities of social robots can enhance user experience by positively influencing the sensing, perception, and action layers of smart home systems. The findings suggest that the evolution of smart home technology should prioritize not only functional improvements but also the social and emotional well-being of users. Integrating social robots into smart homes will foster more human-centric, interactive, and satisfying living environments.}
}
@article{WANG2024102664,
title = {A few-shot word-structure embedded model for bridge inspection reports learning},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102664},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102664},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003124},
author = {Yuchen Wang and Yanjie Zhu and Wen Xiong and C.S. Cai},
keywords = {Bridge maintenance, Deep learning, Information extraction, Integrated embedding, Pre-trained language model, Smart bridge},
abstract = {Intelligent bridge maintenance requires the comprehensive utilization of inspection records, as they contain valuable insights into structures’ long-term service conditions. To efficiently focus and utilize this data from extensive reports, reliable extraction methods are highly sought after. However, the heavy reliance on large amounts of manually annotated data limits the applicability and practicability of existing information extraction methods from bridge inspection reports, especially given the non-uniformity in report formats and expressions. To address this issue, this study proposed a few-shot information extraction model for bridge inspection reports understanding, comprising Word and Structure Integration Embeddings, Bi-directional Long Short-Term Memory (BiLSTM), and Condition Random Field (CRF). The model’s strength lies in its easy-to-implement word-structure embedding approach, which combines domain-specific word representations and sentence structure information. Specifically, the bridge inspection-domain pre-trained language model was further pre-trained and fine-tuned to obtain word embeddings, containing prior knowledge of the domain and tasks. Moreover, a novel encoding method was designed to generate sentence structure embeddings from dependency syntactic analysis results, providing textual representation information. Finally, the integrated word-structure embeddings, created by aligning dimensions for concatenation, were fed into the BiLSTM-CRF architecture to capture contextual dependencies and constrain extraction results. Empirical evaluations conducted on four few-shot datasets with 10, 30, 50, and 100 samples demonstrate that the proposed model achieved high accuracy and F1 score, outperforming prior methods, general domain models, and large language models. Specifically, in a dataset containing 50 sentences, our model achieved an accuracy of up to 0.9357 and an F1 score of 0.8683, representing an average increase of 38.4% higher than these methods. Ablation experiments revealed the contributions of each model component. These results suggest that the proposed model can accurately extract key information from bridge inspection reports even with limited training data scenarios, thereby facilitating applications such as structural condition evaluation and maintenance decision-making.}
}
@article{JOSHI2024124283,
title = {Saliency infused dialogue response generation: Improving task oriented text generation using feature attribution},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124283},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124283},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424011497},
author = {Ratnesh Kumar Joshi and Arindam Chatterjee and Asif Ekbal},
keywords = {Natural language generation, Saliency, Dialogue Systems, End-to-End generation},
abstract = {Challenges persist in dialogue scenarios, particularly in multi-turn dialogues where response generation often disregards contextual information beyond the last user utterance, resulting in fluent yet inadequate responses. This paper addresses these issues by identifying and resolving common shortcomings in base model responses during response generation and proposes methods to enhance response quality in unannotated dialogue settings. Our approach involves augmenting information from multiple sources, including keywords, salient features, and knowledge graph triples. We compare the effectiveness of these methods against both the base model and human annotation, which includes dialogue acts and entities. Our findings demonstrate that appending extracted tokens significantly enhances response quality compared to annotated information. In task-oriented dialogue, models perform best when infused with saliency and knowledge graph triples, as shown in the MultiWOZ dataset. Conversely, focusing solely on saliency yields better results for open-domain dialogue, as demonstrated with the DailyDialog dataset. For contextual relevance, the information infusion could also approach the performance of the LLama2 model with only a tenth of the available parameters.}
}
@article{LIU2024100099,
title = {A systematic evaluation of GPT-4V's multimodal capability for chest X-ray image analysis},
journal = {Meta-Radiology},
volume = {2},
number = {4},
pages = {100099},
year = {2024},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2024.100099},
url = {https://www.sciencedirect.com/science/article/pii/S2950162824000535},
author = {Yunyi Liu and Yingshu Li and Zhanyu Wang and Xinyu Liang and Lingqiao Liu and Lei Wang and Leyang Cui and Zhaopeng Tu and Longyue Wang and Luping Zhou},
keywords = {GPT-4V, Medical image, Radiology report generation medical visual question answering medical visual grounding, Large language model evaluation},
abstract = {This work evaluates GPT-4V's multimodal capability for medical image analysis, focusing on three representative tasks radiology report generation, medical visual question answering, and medical visual grounding. For the evaluation, a set of prompts is designed for each task to induce the corresponding capability of GPT-4V to produce sufficiently good outputs. Three evaluation ways including quantitative analysis, human evaluation, and case study are employed to achieve an in-depth and extensive evaluation. Our evaluation shows that GPT-4V excels in understanding medical images can generate high-quality radiology reports and effectively answer questions about medical images. Meanwhile, it is found that its performance for medical visual grounding needs to be substantially improved. In addition, we observe the discrepancy between the evaluation outcome from quantitative analysis and that from human evaluation. This discrepancy suggests the limitations of conventional metrics in assessing the performance of large language models like GPT-4V and the necessity of developing new metrics for automatic quantitative analysis.}
}
@article{BENFENATI2024586,
title = {A Retrieval-augmented Generation application for Question-Answering in Nutrigenetics Domain},
journal = {Procedia Computer Science},
volume = {246},
pages = {586-595},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.467},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025092},
author = {Domenico Benfenati and Giovanni Maria {De Filippis} and Antonio Maria Rinaldi and Cristiano Russo and Cristian Tommasino},
keywords = {Retrieval-augmented generation, AI-generated content, Large language models, Information Retrieval, Nutrigenetics, Personalized nutrition},
abstract = {The domain of nutrigenetics investigates the complex relationship between genetic variations and individual dietary responses, encompassing a wide array of disciplines, including genomics, nutrition science, bioinformatics, and personalized medicine. This field is marked by its intricate data landscape, necessitating innovative approaches to effectively manage and interpret the vast volumes of information involved. Given nutrigenetic data sheer volume and complexity, traditional AI models often struggle to maintain comprehensive and up-to-date knowledge. In this paper, we propose an implementation of the Retrieval-Augmented Generation (RAG) strategy to address the question-answering task in nutrigenetic domain. This framework enhances the accuracy and relevancy of outputs produced by an advanced Large Language Model, circumventing the exhaustive model fine-tuning process. As a result, our RAG approach not only alleviates the computational demand but also fortifies against data leakage concerns, particularly critical in the sensitive area of nutrigenetics. The implementation of RAG in the nutrigenetic domain not only addresses the existing challenges but also paves the way for more advanced and efficient exploration of nutrigenetic data. Our proposed workflow could advance the understanding of nutrigenetic interactions and personalized nutrition.}
}
@article{SIKSTROM2024105140,
title = {Pedagogical agents communicating and scaffolding students' learning: High school teachers' and students' perspectives},
journal = {Computers & Education},
volume = {222},
pages = {105140},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105140},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001544},
author = {Pieta Sikström and Chiara Valentini and Anu Sivunen and Tommi Kärkkäinen},
keywords = {Pedagogical agent, Secondary education, User-centered design, Human–machine communication (HMC), Human-to-human communication script},
abstract = {Pedagogical agents (PAs) communicate verbally and non-verbally with students in digital and virtual reality/augmented reality learning environments. PAs have been shown to be beneficial for learning, and generative artificial intelligence, such as large language models, can improve PAs' communication abilities significantly. K-12 education is underrepresented in learning technology research and teachers' and students' insights have not been considered when developing PA communication. The current study addresses this research gap by conducting and analyzing semi-structured, in-depth interviews with eleven high school teachers and sixteen high school students about their expectations for PAs' communication capabilities. The interviewees identified relational and task-related communication capabilities that a PA should perform to communicate effectively with students and scaffold their learning. PA communication that is simultaneously affirmative and relational can induce immediacy, foster the relationship and engagement with a PA, and support students' learning management. Additionally, the teachers and students described the activities and technological aspects that should be considered when designing conversational PAs. The study showed that teachers and students applied human-to-human communication scripts when outlining their desired PA communication characteristics. The study offers novel insights and recommendations to researchers and developers on the communicational, pedagogical, and technological aspects that must be considered when designing communicative PAs that scaffold students’ learning, and discusses the contributions on human–machine communication in education.}
}
@article{DINGIL2024e33645,
title = {Understanding state-of-the-art situation of transport planning strategies in earthquake-prone areas by using AI-supported literature review methodology},
journal = {Heliyon},
volume = {10},
number = {13},
pages = {e33645},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e33645},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024096762},
author = {Ali Enes Dingil and Ondrej Pribyl},
keywords = {artificial intelligence, Information retrieval, AI-Supported review, Earth-quake prone areas, Seismic risk, Transport planning, Transport system},
abstract = {Aim
This review aims to explore earthquake-based transport strategies in seismic areas, providing state-of-the-art insights into the components necessary to guide urban planners and policymakers in their decision-making processes.
Outputs
The review provides a variety of methodologies and approaches employed for the reinforcement planning and emergency demand management to analyze and evaluate the impact of seismic events on transportation systems, in turn to develop strategies for preparedness, mitigation, response, and recovery phases. The selection of the appropriate approach depends on factors such as the specific transport system, urbanization level and type, built environment, and critical components involved.
Originality and value
Besides providing a distinctive illustration of the integration of transportation and seismic literature as a valuable consolidated resource, this article introduces a novel methodology named ALARM for conducting state-of-the-art reviews on any topic, incorporating AI through the utilization of large language models (LLMs) built upon transformer deep neural networks, along with indexing data structures (in this study mainly OPEN-AI DAVINCI-003 model and vector-storing index). Hence, it is of paramount significance as the first instance of implementing LLMs within academic review standards. This paves the way for the potential integration of AI and human collaboration to become a standard practice under enhanced criteria for comprehending and analyzing specific information.}
}
@article{SONNENSCHEIN2024120232,
title = {Validating and constructing behavioral models for simulation and projection using automated knowledge extraction},
journal = {Information Sciences},
volume = {662},
pages = {120232},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120232},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524001452},
author = {Tabea S. Sonnenschein and G. Ardine {de Wit} and Nicolette R. {den Braver} and Roel C.H. Vermeulen and Simon Scheider},
keywords = {Validation, Knowledge extraction, Knowledge synthesis, Knowledge graph, Behavior modeling, Simulation, Ontology, BERT, Named-entity recognition},
abstract = {Human behavior may be one of the most challenging phenomena to model and validate. This paper proposes a method for automatically extracting and compiling evidence on human behavior determinants into a knowledge graph. The method (1) extracts associations of behavior determinants and choice options in relation to study groups and moderators from published studies using Natural Language Processing and Deep Learning, (2) synthesizes the extracted evidence into a knowledge graph, and (3) sub-selects the model components and relationships that are relevant and robust. The method can be used to either (4a) construct a structurally valid simulation model before proceeding with calibration or (4b) to validate the structure of existing simulation models. To demonstrate the feasibility of the method, we discuss an example implementation with mode of transport as behavior choice. We find that including non-frequently studied significant behavior determinants drastically improves the model's explanatory power in comparison to only including frequently studied variables. The paper serves as a proof-of-concept which can be reused, extended or adapted for various purposes.}
}
@article{ZHANG2025129307,
title = {ChatGPT in threefold: As attacker, target, and evaluator in adversarial attacks},
journal = {Neurocomputing},
volume = {621},
pages = {129307},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129307},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224020782},
author = {Yunting Zhang and Lin Ye and Kai Tan and Zeshu Tian and Baisong Li and Hongli Zhang},
keywords = {Adversarial example, Textual adversarial attack, ChatGPT, Text classification, Perturbation},
abstract = {ChatGPT is one of the most popular large language models (LLMs). It has achieved state-of-the-art (SOTA) performance in various natural language processing (NLP) tasks. Recent studies have shown that deep learning (DL) models reveal vulnerability to adversarial texts. However, current research on the security of ChatGPT is still in its nascent stages. In this paper, ChatGPT plays three roles in adversarial attacks: an attack method, a target model, and an evaluation tool. We propose a novel black-box word-level adversarial text generation method, ChatGPT Tricker (CGT), for the text classification task. In the framework of adversarial text generation based on word importance, CGT introduces imperceptible perturbations to the original text by utilizing ChatGPT prompts, aiming to craft adversarial texts. Subsequently, we leverage the transferability of adversarial texts to attack ChatGPT and assess its robustness against such attacks. Additionally, we introduce an innovative fluency evaluation method for adversarial texts based on ChatGPT. We define a novel concept termed offset average difference (OAD) to provide a practical formula for the computation of fluency scores. The proposed method enables automated evaluation without human intervention. We utilize CGT to attack both BERT and ChatGPT on real-world English and Chinese text classification datasets. Experimental results demonstrate that CGT balances various aspects of attack performance well. CGT produces more fluent adversarial texts while achieving an attack success rate close to the SOTA baselines. Concurrently, the results also indicate that ChatGPT resists English adversarial texts but is vulnerable to Chinese ones. On the Chinese dataset, CGT can attack ChatGPT with a success rate exceeding 30%.}
}
@article{CORLATESCU2024108154,
title = {The automated model of comprehension version 4.0 – Validation studies and integration of ChatGPT},
journal = {Computers in Human Behavior},
volume = {154},
pages = {108154},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108154},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224000219},
author = {Dragos-Georgian Corlatescu and Micah Watanabe and Stefan Ruseti and Mihai Dascalu and Danielle S. McNamara},
keywords = {Natural language processing, Reading comprehension, Automated model of comprehension, ChatGPT, Large language models},
abstract = {Modeling reading comprehension processes is a critical task for Learning Analytics, as accurate models of the reading process can be used to match students to texts, identify appropriate interventions, and predict learning outcomes. This paper introduces an improved version of the Automated Model of Comprehension, namely version 4.0. AMoC has its roots in two theoretical models of the comprehension process (i.e., the Construction-Integration model and the Landscape model), and the new version leverages state-of-the-art Large Language models, more specifically ChatGPT, to have a better contextualization of the text and a simplified construction of the underlying graph model. Besides showcasing the usage of the model, the study introduces three in-depth psychological validations that argue for the model's adequacy in modeling reading comprehension. In these studies, we demonstrated that AMoC is in line with the theoretical background proposed by the Construction-Integration and Landscape models, and it is better at replicating results from previous human psychological experiments than its predecessor. Thus, AMoC v4.0 can be further used as an educational tool to, for example, help teachers design better learning materials personalized for student profiles. Additionally, we release the code from AMoC v4.0 as open source in a Google Collab Notebook and a GitHub repository.}
}
@article{ZHAO2025104040,
title = {INSNER: A generative instruction-based prompting method for boosting performance in few-shot NER},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104040},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104040},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324003996},
author = {Peiwen Zhao and Chong Feng and Peiguang Li and Guanting Dong and Sirui Wang},
keywords = {Named Entity Recognition, Information extraction, Few-shot learning, Prompt-based learning},
abstract = {Most existing Named Entity Recognition (NER) methods require a large scale of labeled data and exhibit poor performance in low-resource scenarios. Thus in this paper, we propose INSNER, a generative INStruction-based prompting method for few-shot NER. Specifically, we introduce a unified instruction to guide the model in extracting correct entities in response to the instruction, and construct synthetic verbalizers, which support complex types, to encourage effective knowledge transfer. We organize the NER results in natural language form, which mitigates the gap between pre-training and fine-tuning of language models. Furthermore, to facilitate the model to learn task-related knowledge and rich label semantics, we introduce entity-oriented prompt-tuning as an auxiliary task. We conduct in-domain and cross-domain experiments in few-shot settings on 4 datasets, and extensive analyses to validate the effectiveness and generalization ability of INSNER. Experimental results demonstrate that INSNER significantly outperforms current methods in few-shot settings, especially huge improvements(+12.0% F1) over the powerful ChatGPT in MIT Movie Complex both under a 10-shot setting.}
}
@article{GIORDANO2025104186,
title = {Decomposing maintenance actions into sub-tasks using natural language processing: A case study in an Italian automotive company},
journal = {Computers in Industry},
volume = {164},
pages = {104186},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104186},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524001143},
author = {Vito Giordano and Gualtiero Fantoni},
keywords = {Natural language processing, Text mining, Maintenance work order, Industrial applications, Association rule mining, Large language model},
abstract = {Industry 4.0 has led to a huge increase in data coming from machine maintenance. At the same time, advances in Natural Language Processing (NLP) and Large Language Models provide new ways to analyse this data. In our research, we use NLP to analyse maintenance work orders, and specifically the descriptions of failures and the corresponding repair actions. Many NLP studies have focused on failure descriptions for categorising them, extracting specific information about failure, or supporting failure analysis methodologies (such as FMEA). Whereas, the analysis of repair actions and its relationship with failure remains underexplored. Addressing this gap, our study makes three significant contributions. Firstly, we focused on the Italian language, which presents additional challenges due to the dominance of NLP systems that are mainly designed for English. Secondly, it proposes a method for automatically subdividing a repair action into a set of sub-tasks. Lastly, it introduces an approach that employs association rule mining to recommend sub-tasks to maintainers when addressing failures. We tested our approach with a case study from an automotive company in Italy. The case study provides insights into the current barriers faced by NLP applications in maintenance, offering a glimpse into the future opportunities for smart maintenance systems.}
}
@article{ZONG2025100916,
title = {Recent progress on machine learning with limited materials data: Using tools from data science and domain knowledge},
journal = {Journal of Materiomics},
volume = {11},
number = {3},
pages = {100916},
year = {2025},
issn = {2352-8478},
doi = {https://doi.org/10.1016/j.jmat.2024.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352847824001552},
author = {Bangtan Zong and Jinshan Li and Tinghuan Yuan and Jun Wang and Ruihao Yuan},
keywords = {Small datasets, Data augmentation, Transfer learning, Active learning, Domain knowledge},
abstract = {One key challenge in materials informatics is how to effectively use the material data of small size to search for desired materials from a huge unexplored material space. We review the recent progress on the use of tools from data science and domain knowledge to mitigate the issues arising from limited materials data. The enhancement of data quality and amount via data augmentation and feature engineering is first summarized and discussed. Then the strategies that use ensemble model and transfer learning for improved machine learning model are overviewed. Next, we move to the active learning with emphasis on the uncertainty quantification and evaluation. Subsequently, the merits of the combination of domain knowledge and machine learning are stressed. Finally, we discuss some applications of large language models in the field of materials science. We summarize this review by posing the challenges and opportunities in the field of machine learning for small material data.}
}
@article{LI2025112016,
title = {The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing},
journal = {iScience},
pages = {112016},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112016},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225002767},
author = {Yuannan Li and Shan Xu and Jia Liu},
keywords = {logical-mathematical symbols processing, natural language processing, spatial cognition, language of thought, large language model},
abstract = {SUMMARY
The ability to use logical-mathematical symbols (LMS), encompassing tasks such as calculation, reasoning, and programming, is special to humans with recent emergence. LMS processing was suggested to build upon fundamental cognitive systems through neuronal recycling, with natural language processing and spatial cognition as key candidates. This study used meta-analyses and synthesized neural maps of representative LMS tasks, including reasoning, calculation, and mental programming, to compare their neural correlates with those of the two systems. Our results revealed greater activation overlap and multivariate similarity between LMS and spatial cognition than with language processing. Hierarchical clustering further indicated that LMS tasks were indistinguishable from spatial tasks at the neural level, suggesting an inherent connection. Our findings support the hypothesis that spatial cognition is the basis of LMS processing, shedding light on the logical reasoning limitations of large language models, particularly those lacking explicit spatial representations.}
}
@article{DAI2024292,
title = {Facilitating Students’ Adaptive Help-seeking and Peer Interactions through an Analytics-enhanced Forum in Engineering Design Education},
journal = {Procedia CIRP},
volume = {128},
pages = {292-297},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124006735},
author = {Yun Dai and Ziyan Lin and Ang Liu},
keywords = {help-seeking, peer support, learning analytics, design thinking, engineering education},
abstract = {Design often takes place in collective and collaborative settings, and interactions and mutual support among peers have been a critical component of design education. However, in most of the existing design courses, students often work in small groups and peer interactions are limited to group members, which limits the range and depth of knowledge exchange. To complement the group-based activities, this study designs and assesses an analytics-enhanced discussion forum for whole-class interactions. The forum adopts ontology-based recommender systems and anomaly detection techniques to tailor the threads and contents for individual students in a personalized way. This analytics-enhanced forum was implemented in a large-size undergraduate design course (n = 313), and data about student responses to this forum was compared with data from the previous year’s course that adopted a conventional forum (n = 280). From the statistical analysis, students learning with the analytics-enhanced forum demonstrated significantly higher degrees of design practices (specifically, empathize, define, ideate, and test), collaborative learning, and course satisfaction. Qualitative analysis of students’ focus-group interviews shows their perceived benefits and concerns of the analytics-enhanced forum. The study also suggests integrating generative artificial intelligence and large language models to support students’ design thinking and collaborative design.}
}
@article{CHEN2024102593,
title = {AskNatureNet: A divergent thinking tool based on bio-inspired design knowledge},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102593},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102593},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002416},
author = {Liuqing Chen and Zebin Cai and Zhaojun Jiang and Jianxi Luo and Lingyun Sun and Peter Childs and Haoyu Zuo},
keywords = {Bio-inspired design, Semantic network, Divergent thinking, Design creativity, Design ideation},
abstract = {Divergent thinking is a process in design by exploring multiple possible solutions, is crucial in the early stages of design to break fixation and expand the design ideation. Design-by-Analogy promotes divergent thinking, by studying solutions have solved similar problems and using this knowledge to make inferences and solve problems in new and unfamiliar situations. Bio-inspired design (BID) is a form of design by analogy and its knowledge provides diverse sources for analogy, making BID knowledge as a potential source for divergent thinking. Existing BID database has focused on collecting BID cases and facilitating the retrieval of biological knowledge. Despite its success, applying BID knowledge into divergent thinking still encounters challenge, as the association between source domain and target domain are always limited within a single case. In this work, a novel approach is proposed to support divergent thinking from three subsequent phases: encoding, retrieval and mapping. Specifically, biological knowledge is encoded in a triple form by employing a large language model (LLM) to extract key information from a well-known BID knowledge base. The created triples are implemented in a semantic network to facilitate bidirectional retrieval modes: problem-driven and solution-driven, as well as mapping for divergent thinking. The mapping algorithm calculates the semantic similarity between nodes in the semantic network based on their attributes in three progressive steps by following the paradigm of divergent thinking. The proposed approach is implemented as tool called AskNatureNet,11https://www.asknaturenet.com/. which supports divergent thinking by retrieving and mapping knowledge in a visualized interactive semantic network. An ideation case study on evaluating the effectiveness of AskNatureNet shows that our tool is capable of supporting divergent thinking efficiently.}
}
@incollection{GURTU20251617,
title = {Chapter 101 - Use of Artificial Intelligence (AI) in Cybersecurity},
editor = {John R. Vacca},
booktitle = {Computer and Information Security Handbook (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {1617-1624},
year = {2025},
isbn = {978-0-443-13223-0},
doi = {https://doi.org/10.1016/B978-0-443-13223-0.00101-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443132230001016},
author = {Anurag Gurtu and Damien Lim},
keywords = {AI algorithm, AI fuzzing, AI model, Automation, Bayesian, Cyber threats, Cyberattacks, Cybersecurity, Data poisoning, Deep learning, Deep neural networks, Generative AI, Knowledge graph, Learning mode, Least squares, Machine learning, Malware, Malware sandbox, Natural language processing, Next generation firewall, Password cracking, Random forest, Reinforcement learning, Security posture, Supervised learning, Threat actor, Threat detection, Threat response, Training data, Unsupervised learning, User entity behavior analytics, Virtual assistants, Worm generation},
abstract = {Artificial Intelligence (AI) is actively utilized and works in the background to benefit consumers and businesses. This chapter focuses on how AI can be used to enhance cybersecurity and the challenges and risks involved. It covers the fundamentals, algorithms, and learning modes of AI and how they can be applied to various security technologies and processes. There are many different AI models, but specific examples stand out as proven applications to cybersecurity, such as machine learning (ML), deep learning (DL), generative AI (GenAI), natural language processing (NLP), and knowledge graph (KG). They can work together to create powerful technologies, including virtual assistants for cybersecurity. We balance the conversation with reviewing inherent weaknesses found in AI. The impact of AI on cybersecurity is quite significant in that it helps fill the skill gap, accelerate threat detection, automate processes, and improve security posture. On the other hand, threat actors can misuse and exploit AI to create more sophisticated and evasive cyberattacks. Some examples of AI abuse include AI fuzzing, password cracking, worm generation, and data poisoning. Lastly, we examine the factors that influence the adoption of AI and its future, such as organization size, industry vertical, barriers, and governance.}
}
@article{NGO2025102966,
title = {Integrating personalized and contextual information in fine-grained emotion recognition in text: A multi-source fusion approach with explainability},
journal = {Information Fusion},
volume = {118},
pages = {102966},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102966},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000399},
author = {Anh Ngo and Jan Kocoń},
keywords = {Emotion recognition, Sentence sequence classification, Personalization, Data cartography, Natural Language Processing (NLP), Explainable artificial, Intelligence (XAI)},
abstract = {Emotion recognition in textual data is a rapidly evolving field with diverse applications. While the state-of-the-art (SOTA) models based on pre-trained large language models (LLMs) have demonstrated significant achievements, the existing approaches often overlook fine-grained emotional nuances within individual sentences and the influence of contextual information. Additionally, despite the growing interest in personalized Natural Language Processing, recent studies have highlighted limitations in the literature, particularly the lack of explainability methods to interpret the improvements observed in these models. This study explores the CLARIN-Emo dataset to demonstrate the effectiveness of integrating personalized and contextual information for accurate emotion detection. By framing textual emotion recognition as a sequence sentence classification (SSC) task and leveraging transformer-based architectures, the proposed multi-source fusion approach significantly outperformed the baseline model, which considers each sentence in isolation. Furthermore, a personalized method, referred to as UserID, captures user-specific characteristics by assigning each annotator a unique identifier, significantly enhancing emotion prediction accuracy. This work also introduces an extension of Data Maps by differentiating dynamic training metrics to analyze the models’ training behaviors. The results validate the capability of this approach in visually interpreting and facilitating performance comparisons between models.}
}
@article{QIN2025102994,
title = {A Comprehensive Taxonomy of Machine Consciousness},
journal = {Information Fusion},
pages = {102994},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102994},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000673},
author = {Ruilin Qin and Changle Zhou and Mengjie He},
keywords = {Consciousness, machine consciousness, artificial consciousness, conscious robot, qualia, large language model},
abstract = {Machine consciousness (MC) is the ultimate challenge to artificial intelligence. Although great progress has been made in artificial intelligence and robotics, consciousness is still an enigma and machines are far from having it. To clarify the concepts of consciousness and the research directions of machine consciousness, in this review, a comprehensive taxonomy for machine consciousness is proposed, categorizing it into seven types: MC-Perception, MC-Cognition, MC-Behavior, MC-Mechanism, MC-Self, MC-Qualia and MC-Test, where the first six types aim to achieve a certain kind of conscious ability, and the last type aims to provide evaluation methods and criteria for machine consciousness. For each type, the specific research contents and future developments are discussed in detail. Especially, the machine implementations of three influential consciousness theories, i.e. global workspace theory, integrated information theory and higher-order theory, are elaborated in depth. Moreover, the challenges and outlook of machine consciousness are analyzed in detail from both theoretical and technical perspectives, with emphasis on new methods and technologies that have the potential to realize machine consciousness, such as brain-inspired computing, quantum computing and hybrid intelligence. The ethical implications of machine consciousness are also discussed. Finally, a comprehensive implementation framework of machine consciousness is provided, integrating five suggested research perspectives: consciousness theories, computational methods, cognitive architectures, experimental systems, and test platforms, paving the way for the future developments of machine consciousness.}
}
@article{BRASE20241923,
title = {Digital chemistry: navigating the confluence of computation and experimentation – definition, status quo, and future perspective},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {1923-1932},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00130c},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001530},
author = {Stefan Bräse},
abstract = {Digital chemistry represents a transformative approach integrating computational methods, digital data, and automation within the chemical sciences. It is defined by using digital toolkits and algorithms to simulate, predict, accelerate, and analyze chemical processes and properties, augmenting traditional experimental methods. The current status quo of digital chemistry is marked by rapid advancements in several key areas: high-throughput screening, machine learning models, quantum chemistry, and laboratory automation. These technologies have enabled unprecedented speeds in discovering and optimizing new molecules, materials, and reactions. Digital retrosynthesis and structure–active prediction tools have supported these endeavors. Furthermore, integrating large-language models and robotics in chemistry labs (e.g. demonstrated in self-driving labs) have begun to automate routine tasks and complex decision-making processes. Looking forward, the future of digital and digitalized chemistry is poised for significant growth, driven by the increasing accessibility of computational resources, the expansion of chemical databases, and the refinement of artificial intelligence algorithms. This evolution promises to accelerate innovation in drug discovery, materials science, and sustainable manufacturing, ultimately leading to more efficient, cost-effective, and environmentally friendly chemical research and production. The challenge lies in advancing the technology itself, fostering interdisciplinary collaboration, and ensuring the ethical use of digital tools in chemical research.}
}
@article{CHARI2023102498,
title = {Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes},
journal = {Artificial Intelligence in Medicine},
volume = {137},
pages = {102498},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102498},
url = {https://www.sciencedirect.com/science/article/pii/S093336572300012X},
author = {Shruthi Chari and Prasant Acharya and Daniel M. Gruen and Olivia Zhang and Elif K. Eyigoz and Mohamed Ghalwash and Oshani Seneviratne and Fernando Suarez Saiz and Pablo Meyer and Prithwish Chakraborty and Deborah L. McGuinness},
keywords = {User-driven, Clinical explainability, Contextual explanations, Question-answering approach, Type-2 diabetes comorbidity risk prediction},
abstract = {Medical experts may use Artificial Intelligence (AI) systems with greater trust if these are supported by ‘contextual explanations’ that let the practitioner connect system inferences to their context of use. However, their importance in improving model usage and understanding has not been extensively studied. Hence, we consider a comorbidity risk prediction scenario and focus on contexts regarding the patients’ clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We explore how relevant information for such dimensions can be extracted from Medical guidelines to answer typical questions from clinical practitioners. We identify this as a question answering (QA) task and employ several state-of-the-art Large Language Models (LLM) to present contexts around risk prediction model inferences and evaluate their acceptability. Finally, we study the benefits of contextual explanations by building an end-to-end AI pipeline including data cohorting, AI risk modeling, post-hoc model explanations, and prototyped a visual dashboard to present the combined insights from different context dimensions and data sources, while predicting and identifying the drivers of risk of Chronic Kidney Disease (CKD) - a common type-2 diabetes (T2DM) comorbidity. All of these steps were performed in deep engagement with medical experts, including a final evaluation of the dashboard results by an expert medical panel. We show that LLMs, in particular BERT and SciBERT, can be readily deployed to extract some relevant explanations to support clinical usage. To understand the value-add of the contextual explanations, the expert panel evaluated these regarding actionable insights in the relevant clinical setting. Overall, our paper is one of the first end-to-end analyses identifying the feasibility and benefits of contextual explanations in a real-world clinical use case. Our findings can help improve clinicians’ usage of AI models.}
}
@article{WANG2025100759,
title = {Towards cognitive intelligence-enabled product design: The evolution, state-of-the-art, and future of AI-enabled product design},
journal = {Journal of Industrial Information Integration},
volume = {43},
pages = {100759},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100759},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24002024},
author = {Zuoxu Wang and Xinxin Liang and Mingrui Li and Shufei Li and Jihong Liu and Lianyu Zheng},
keywords = {Engineering product design, Cognitive computing, Knowledge graph, Industrial products and services design, Design knowledge support, Knowledge reasoning},
abstract = {Engineering design researchers have increasing interests in leveraging artificial intelligence (AI) techniques to a wide range of product design tasks, such as customer requirement analysis, product concept generation, design synthesis, and decision-making in product design. Indeed, AI techniques perform excellently on well-defined design tasks with clear problem definition, specialized solutions, and abundant training data. However, facing the ever-evolving AI techniques rapidly and radically changing the product design manner, there is still a lack of a systematic summary about the current stage of AI-enabled product design. Besides, although the current AI-enabled product design performs excellently on the well-defined tasks, the other advanced design tasks that need cognitive capability can still hardly be satisfyingly completed by the current product design system. This study systematically reviewed the literature on AI-enabled product design to understand its evolution and state-of-the-arts. To bridge the semantic gap between humans and systems, a novel cognitive intelligence-enabled product design (CIPD) framework is proposed, in which cognitive intelligence is the key enabler. The CIPD's key aspects, including its system architecture, human-like capabilities, enabling technologies, and potential applications, are also systematically discussed. It is hoped that this study could contribute to the future directions of the product design field and offer insightful guidance to the practitioners and researchers in their product design process.}
}
@article{WU2024106666,
title = {Generative commonsense knowledge subgraph retrieval for open-domain dialogue response generation},
journal = {Neural Networks},
volume = {180},
pages = {106666},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106666},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024005902},
author = {Sixing Wu and Jiong Yu and Jiahao Chen and Wei Zhou},
keywords = {Response generation, Knowledge grounded response generation, Commonsense knowledge},
abstract = {Grounding on a commonsense knowledge subgraph can help the model generate more informative and diverse dialogue responses. Prior Traverse-based works explicitly retrieve a subgraph from the external knowledge base (eKB). Notably, the available knowledge is strictly restricted by the eKB. To break this restriction, Generative Retrieval methods externalize knowledge from the language model. However, they always generate boring knowledge due to their one-pass externalization procedure. This work proposes a novel TiLM Traverse in Language Model (TiLM), which uses three ‘Chain-of-Thought’ sub-tasks, i.e., Query Entity Production, Topic Entity Prediction, and Knowledge Subgraph Completion, to build a high-quality knowledge subgraph to ground the next Response Generation without explicitly accessing the eKB in inference. Experimental results on both Chinese and English datasets demonstrate TiLM’s outstanding performance even only with a small scale of parameters.}
}
@article{SONG2025109572,
title = {Korean football in-game conversation state tracking dataset for dialogue and turn level evaluation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109572},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109572},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017305},
author = {Sangmin Song and Juhyoung Park and Juhwan Choi and Junho Lee and Kyohoon Jin and YoungBin Kim},
keywords = {Dialogue state tracking, Data annotation, Large language model},
abstract = {Recent research in dialogue state tracking has made significant progress in tracking user goals through dialogue-level and turn-level approaches, but existing research primarily focused on predicting dialogue-level belief states. In this study, we present the KICK: Korean football In-game Conversation state tracKing dataset, which introduces a conversation-based approach. This approach leverages the roles of casters and commentators within the self-contained context of sports broadcasting to examine how utterances impact the belief state at both the dialogue-level and turn-level. Towards this end, we propose a task that aims to track the states of a specific time turn and understand conversations during the entire game. The proposed dataset comprises 228 games and 2463 events over one season, with a larger number of tokens per dialogue and turn, making it more challenging than existing datasets. Experiments revealed that the roles and interactions of casters and commentators are important for improving the zero-shot state tracking performance. By better understanding role-based utterances, we identify distinct approaches to the overall game process and events at specific turns.}
}
@article{CHOI2024115042,
title = {GPT-based data-driven urban building energy modeling (GPT-UBEM): Concept, methodology, and case studies},
journal = {Energy and Buildings},
volume = {325},
pages = {115042},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115042},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011587},
author = {Sebin Choi and Sungmin Yoon},
keywords = {Urban building energy modeling (UBEM), Urban informatics, Top-down, Large language model (LLM), GPT, Technical exploration},
abstract = {Achieving carbon neutrality is a critical global goal, with urban building energy modeling (UBEM) playing a pivotal role by providing data-driven insights to optimize energy consumption and reduce emissions. This paper introduces GPT-based urban building energy modeling (GPT-UBEM), a novel approach utilizing GPT’s advanced capabilities to address key UBEM challenges using GPT-4o. The study aimed to demonstrate the effectiveness of GPT-UBEM in performing UBEM tasks and to explore its potential in overcoming traditional limitations. Specifically, (1) basic analytics of urban data, (2) data analysis and energy prediction, (3) building feature engineering and optimization, and (4) energy signature analysis were conducted in four case studies. These analyses were applied to 2,000 buildings in Seoul and 31 buildings in Gangwon-do, South Korea. Through case study, the findings highlighted the ability of GPT-UBEM to integrate diverse data sources, optimize building features for high accuracy in prediction models, and provide valuable insights for urban planners and policymakers through the use of expert domain knowledge and intervention. Additionally, based on the results derived from GPT-UBEM in this study, the current limitations of GPT-UBEM (L1 to L3) and future research directions (F1 to F4) have been outlined.}
}
@article{KEFALIDIS2024104203,
title = {The question answering system GeoQA2 and a new benchmark for its evaluation},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {134},
pages = {104203},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.104203},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224005594},
author = {Sergios-Anestis Kefalidis and Dharmen Punjani and Eleni Tsalapati and Konstantinos Plas and Maria-Aggeliki Pollali and Pierre Maret and Manolis Koubarakis},
keywords = {Geospatial knowledge graphs, Geospatial question answering},
abstract = {We present the question answering engine GeoQA2 which is able to answer geospatial questions over the union of knowledge graphs YAGO2 and YAGO2geo. We also present the dataset GeoQuestions1089 which consists of 1089 natural language questions, their corresponding SPARQL or GeoSPARQL queries and their answers over the union of the same knowledge graphs. We use this dataset to compare the effectiveness of GeoQA2 and the system of Hamzei et al. 2022 and make it publicly available to be used by other researchers. Our evaluation shows that although the engine GeoQA2 performs better than the engine of Hamzei et al. 2022, both engines have ample room for improvement in their question answering performance.}
}
@article{HAN2025112805,
title = {A plug-and-play knowledge-enhanced module for medical reports generation},
journal = {Knowledge-Based Systems},
volume = {309},
pages = {112805},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112805},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124014394},
author = {Qinyu Han and Zhihao Yang and Hongfei Lin and Tian Qin},
keywords = {Medical report generation, Dialogue summarization, Knowledge graph, Heterogeneous graph network},
abstract = {Medical reports generation from patient–doctor conversations aims to capture the salient contents from dialogues in the form of medical reports, to assist the healthcare archiving and follow-ups. Unlike general summarization tasks, summaries in specialized domains, such as medical report generation, require stronger support from domain-specific knowledge to enhance the internal logic specific to their field. However, existing methods either model the context sequentially without incorporating knowledge graphs, resulting in unclear representation of domain-specific logical structures, or introduce the expert medical knowledge using homogeneous graphs, which fail to effectively fuse information from different semantic spaces. To address this, we propose to incorporate the knowledge into the step of context modeling by introducing a plug-and-play heterogeneous graph encoder. Furthermore, a fact-aware module is introduced to help our model in integrating key context information filtered by external knowledge during decoding. Compared with previous works, our method makes full use of knowledge, and the proposed modules demonstrate easy adaption on existing frameworks. Experimental results on two public medical dialogue summarization datasets indicate that our method significantly outperforms a range of baselines while being smaller and capable of achieving the state-of-the-art performance11Code is available at https://github.com/salvatoreferragamo/Med-HGE..}
}
@article{LIN2024122254,
title = {Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122254},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122254},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027562},
author = {Baihan Lin},
keywords = {Reinforcement learning, Bandits, Speech processing, Natural language processing, Speech recognition, Large language models, Survey, Perspective},
abstract = {In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits including those in the large language models, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.}
}
@article{SUN2024111975,
title = {Harnessing domain insights: A prompt knowledge tuning method for aspect-based sentiment analysis},
journal = {Knowledge-Based Systems},
volume = {298},
pages = {111975},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111975},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124006099},
author = {Xinjie Sun and Kai Zhang and Qi Liu and Meikai Bao and Yanjiang Chen},
keywords = {LLMs, Prompt tuning, Domain knowledge base, Co-occurrence gate, Hybrid prompt},
abstract = {Aspect-based sentiment analysis (ABSA) endeavours predict the sentiment polarity of specific aspects of a given review. Recently, prompt tuning has been widely explored and has achieved remarkable success in improving semantic comprehension in several NLP tasks. However, most existing methods consider semantic tuning for various tasks and overlook domain knowledge, such as common-sense background knowledge. This not only limits the model’s ability to understand and apply domain knowledge but also often leads to the model’s inability to fully utilise domain-specific information, resulting in poor semantic quality and inferior model performance. To bridge this gap, we conducted a systematic study of Prompt Tuning with Domain Knowledge (PTDK) for ABSA, which aimed to design efficient prompts that guide the model to learn the knowledge of specific aspects in ABSA. Specifically, we first fine-tune the Large Language Models (LLMs) using hard prompts, which enhance the ability to extract enriched domain insights from the knowledge base. Additionally, we employed a Co-occurrence Gate to meticulously filter and refine the domain knowledge. This mechanism enhances the domain representation capability of the prompt template by selecting the most similar parts that are independently generated from a vast amount of domain knowledge for each comment. Simultaneously, a emphhybrid prompt templatewas constructed. This template integrates hard prompts and trainable soft prompts to compensate for the lack of specificity in hard prompts and to facilitate the integration of specific masks in various domain vectors. This hybrid strategy further enhances our ability to utilise domain-specific knowledge when performing ABSA. Experimental results on three public datasets – Restaurant, Laptop, and Twitter – demonstrate that our method consistently outperforms the current state-of-the-art baselines in all cases. The accuracies were 88.63%, 82.65%, and 81.65%, respectively, and F1-scores were 83.38%, 79.68%, and 80.36%, respectively. This translates into an average increase in accuracy of 0.97% and an enhancement in the F1-score of 1.03%. These enhancements not only validate the efficacy of our approach but also have substantial practical implications for real-world scenarios that require sophisticated sentiment analysis, such as the evaluation of customer feedback on e-commerce platforms.}
}
@article{CHAKHTOUNA2024428,
title = {Modeling Speech Emotion Recognition via ImageBind representations},
journal = {Procedia Computer Science},
volume = {236},
pages = {428-435},
year = {2024},
note = {International Symposium on Green Technologies and Applications (ISGTA’2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924010664},
author = {Adil CHAKHTOUNA and Sara SEKKATE and Abdellah ADIB},
keywords = {ImageBind, Speech Emotion Recognition, Embedding representations, IEMOCAP, Nu-SVM},
abstract = {Speech Emotion Recognition (SER) refers to the ability of Machine Learning (ML) and Deep Learning (DL) techniques to accurately predict people's emotional states from speech signals. significant progress has been achieved in the SER domain involving the incorporation of DL models to introduce novel features extraction processes. This paper introduces the use of deep representations learned from the multi-modal Large Language Model (LLM) called ImageBind. These representations were subsequently provided as input to the Nu-Support Vector Machine (Nu-SVM) with RBF kernel for the classification task. The experiments were executed using the IEMOCAP database within the context of a Speaker-Dependent (SD) scenario. The method achieved a noteworthy overall accuracy rate of 80.58% for the four emotions of IEMOCAP, representing a substantial improvement over well-established methods in the existing body of literature. Thus, affirming that the proposed methodology, founded upon ImageBind representations, introduces a novel perspective to the field of SER.}
}
@article{XIAO2024124475,
title = {TPKE-QA: A gapless few-shot extractive question answering approach via task-aware post-training and knowledge enhancement},
journal = {Expert Systems with Applications},
volume = {254},
pages = {124475},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124475},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424013411},
author = {Qiao Xiao and Ren Li and Jianxi Yang and Yu Chen and Shixin Jiang and Di Wang},
keywords = {Extractive question answering, Few-shot, Post-training, Knowledge enhancement, Task-aware, Pretrained Language Model},
abstract = {Few-shot extractive question answering (EQA) is a challenging task in natural language processing, whose current methods are mainly based on pretrained language models (PLMs). Data augmentation is often employed to improve the answer predictions of EQA models in few-shot settings. However, due to the differences between pretraining objectives and the EQA task, as well as embedding space alignment bottlenecks, the performance of few-shot EQA models must be improved. We propose TPKE-QA, a few-shot extractive Question Answering approach via Task-aware Post-training and Knowledge Enhancement, with entity-noun-oriented span selection in post-training, which can automatically generate EQA-style examples from a large-scale unlabeled corpus. By post-training based on generated examples, the gap between PLMs and the EQA task is effectively filled. To avoid embedding space alignment issues, a knowledge-enhanced sequence generation and knowledge injection approach for the EQA task enables gapless knowledge enhancement and fine-tuning on the post-trained model. In experiments, TPKE-QA achieved state-of-the-art results in most few-shot settings on the MRQA 2019 benchmark.}
}
@article{GAO2025200475,
title = {TourismNER: A Tourism Named Entity Recognition method based on entity boundary joint prediction},
journal = {Intelligent Systems with Applications},
volume = {25},
pages = {200475},
year = {2025},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2025.200475},
url = {https://www.sciencedirect.com/science/article/pii/S2667305325000018},
author = {Kai Gao and Jiahao Zhou and Yunxian Chi and Yimin Wen},
keywords = {Natural Language Processing, Tourism Named Entity Recognition, Entity boundary recognition, Joint prediction},
abstract = {Tourism named entity recognition is indispensable in tourism information extraction, and plays a crucial role in constructing tourism knowledge map and enhancing tourism knowledge quiz system. The difficulty of tourism named entity recognition lies in its complex nested structure, and the lengthy entity naming length. To address these existing problems, we propose a tourism named entity recognition model that jointly predicts entity boundaries, adopting a training strategy of data preprocessing to enhance the model’s ability for tourism named entity boundary recognition, while our model introduces a pre-trained Bert model as well as BiLSTM coding to enhance the representation of the model’s contexts, and uses a combined predictor of Biaffine and MLP to enhance the model’s recognition performance for boundaries, as well as introducing label smoothing cross entropy to smooth the target labels during the training process. Experiments are conducted on three datasets with different granularities. From the analysis of the experimental results, it can be seen that the named entity recognition method achieves higher accuracy and F1 value compared with the optimal baseline model, and also proves the effectiveness and generality of the modeling method proposed in this paper.}
}
@article{GAO2024103802,
title = {Self-supervised BGP-graph reasoning enhanced complex KBQA via SPARQL generation},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103802},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103802},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001614},
author = {Feng Gao and Yan Yang and Peng Gao and Ming Gu and Shangqing Zhao and Yuefeng Chen and Hao Yuan and Man Lan and Aimin Zhou and Liang He},
keywords = {Knowledge base question answering, Semantic parsing, Intermediate representation, SPARQL query generation, Basic graph pattern, Graph neural networks},
abstract = {Knowledge base question answering aims to answer complex questions from large-scale knowledge bases. Although existing generative language models that translate questions into SPARQL queries have achieved promising results, there are still generation errors due to redundancies or errors in the knowledge fed to the generative models and difficulties in representing the implicit logic of knowledge as the specific syntax of SPARQL. To address above issues, we propose TrackerQA, a novel self-supervised reasoning framework based on basic graph patterns (BGP) to determine precise paths and enhance SPARQL generation. First, we develop a contrastive learning semantic matching model to reduce the large knowledge searching space. Then, we built a BGP parser that parses the recalled knowledge and constraints into BGP graphs, which can deconstruct complex knowledge into BGP triples and naturally obtain supervision from gold SPARQL. Next, we design a self-supervised BGP graph neural network that encodes knowledge through graph transformation layers with directed message-passing control and employs a question-aware attention mechanism to predict the exact BGP paths. Finally, a SPARQL generator integrates the paths into a pre-trained language model to improve the performance of SPARQL generation. Experiments on the KQA Pro dataset show that our model achieves state-of-the-art answering accuracy scores of 95.32%, being the closest to the human level at 97.5%, and reasons out KB paths with F1 scores of 0.98 for nodes and 0.99 for edges.}
}
@article{OBRIEN2024249,
title = {Machine learning for hypothesis generation in biology and medicine: exploring the latent space of neuroscience and developmental bioelectricity},
journal = {Digital Discovery},
volume = {3},
number = {2},
pages = {249-263},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00185g},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X2400024X},
author = {Thomas O'Brien and Joel Stremmel and Léo Pio-Lopez and Patrick McMillen and Cody Rasmussen-Ivey and Michael Levin},
abstract = {Artificial intelligence is a powerful tool that could be deployed to accelerate the scientific enterprise. Here we address a major unmet need: use of existing scientific literature to generate novel hypotheses. We use a deep symmetry between the fields of neuroscience and developmental bioelectricity to evaluate a new tool, FieldSHIFT. FieldSHIFT is an in-context learning framework using a large language model to facilitate candidate scientific research from existing published studies, serving as a tool to generate hypotheses at scale. We release a new dataset for translating between the neuroscience and developmental bioelectricity domains and show how FieldSHIFT helps human scientists explore a latent space of papers that could exist, providing a rich field of suggested future research. We demonstrate the performance of FieldSHIFT for hypothesis generation relative to human-generated developmental biology research directions then test a key prediction of this model using bioinformatics, showing a surprising conservation of molecular mechanisms involved in cognitive behavior and developmental morphogenesis. By allowing scientists to rapidly explore symmetries and meta-parameters that exist in a corpus of scientific papers, we show how machine learning can potentiate human creativity and assist with one of the most interesting and crucial aspects of research: identifying insights from data and generating potential candidates for research agendas.}
}
@article{TCHUITCHEU2024110734,
title = {Table representation learning using heterogeneous graph embedding},
journal = {Pattern Recognition},
volume = {156},
pages = {110734},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110734},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324004850},
author = {Willy Carlos Tchuitcheu and Tan Lu and Ann Dooms},
keywords = {Document parsing, Table representation learning, Semantic meta-path, Heterogeneous graph embedding, Permutation invariance},
abstract = {Tables, especially when having complex layouts, contain rich semantic information. However, effectively learning from tables to uncover such semantic information remains challenging. The rapid progress in natural language processing does not necessarily correspond to equivalent advancements in table parsing, which often requires joint visual and language modeling. Indeed, humans can quickly derive semantic meaning from table entries by associating them with corresponding column and/or row headers. Motivated by this observation, we propose a new heterogeneous Graph-based Table Representation Learning (GTRL) framework. GTRL combines graph-based visual modeling with sequence-based language modeling to learn granular per-cell embeddings that are sensitive to the semantic meaning of cells within their corresponding table context. We systematically evaluate the proposed GTRL framework using two datasets: a new adhesive table benchmark comprising complex tables extracted from industrial documents for learning per-entry semantics, and a publicly available large-scale dataset that enables learning header semantics from column tables. Experimental results demonstrate the competitive performance of the proposed GTRL, which often exhibits reduced computational complexity compared to state-of-the-art table representation learning models.}
}
@article{GIORDANO20241170,
title = {POPCORN: Fictional and Synthetic Intelligence Reports for Named Entity Recognition and Relation Extraction Tasks},
journal = {Procedia Computer Science},
volume = {246},
pages = {1170-1180},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.542},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025870},
author = {Bastien Giordano and Maxime Prieur and Nakanyseth Vuth and Sylvain Verdy and Kévin Cousot and Gilles Sérasset and Guillaume Gadek and Didier Schwab and Cédric Lopez},
keywords = {Synthetic Data Generation, Dataset, Natural Language Processing, Large Language Models, Information Extraction},
abstract = {POPCORN is a research project aiming at maturing Information Extraction (IE) solutions for intelligence services. Due to defense security constraints, reports analyzed by intelligence services are not to be accessible to the scientific community. To address this challenge, we propose a dataset made of “fictional” (handcrafted) and “synthetic” (AI generated) French reports. Those synthetic reports are produced by an innovative approach that generates texts closely resembling real-world intelligence reports, facilitating the training and evaluation of IE tasks such as Entity and Relation Extraction. Experiments demonstrate the interest of synthetic reports to enhance the performance of IE models, showcasing their potential to augment real-world intelligence operations.}
}
@article{BOURDIN2024396,
title = {NLP in SMEs for industry 4.0: opportunities and challenges},
journal = {Procedia Computer Science},
volume = {239},
pages = {396-403},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.186},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014297},
author = {Mathieu Bourdin and Thomas Paviot and Robert Pellerin and Samir Lamouri},
keywords = {Natural Language Processing, Industry 4.0, Machine Learning, Large Language Models, SMEs},
abstract = {Natural Language Processing is the field of Computer Science that focuses on analyzing and processing natural language, mainly human text or speech. Recent trends in Natural Language Processing have led to the development of Large Language Models (LLMs): huge models trained on high amounts of data that achieve unprecedented performances in many tasks, such as answering questions, summarizing texts, or coding. These new tools have a wide range of applications and are being developed by many companies. However, Small and Medium Enterprises (SMEs) struggle to implement these new technologies, mainly because of the lack of resources. This paper aims to show the opportunities and challenges related to NLP-based solutions in SMEs based on a literature review. The main result is that NLP-based solutions have a wide range of applications in various companies, including SMEs, and may lead to many changes. However, there are still many obstacles to developing these tools in SMEs: SMEs lack specialized know-how to develop these solutions and do not often have standardized data. Moreover, there exists nearly no support for SMEs in the scientific literature to develop these tools.}
}
@article{CLAY2025109808,
title = {Natural language processing techniques applied to the electronic health record in clinical research and practice - an introduction to methodologies},
journal = {Computers in Biology and Medicine},
volume = {188},
pages = {109808},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.109808},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525001581},
author = {Benjamin Clay and Henry I. Bergman and Safa Salim and Gabriele Pergola and Joseph Shalhoub and Alun H. Davies},
keywords = {natural language processing, NLP, methodology, electronic health record, EHR},
abstract = {Natural Language Processing (NLP) has the potential to revolutionise clinical research utilising Electronic Health Records (EHR) through the automated analysis of unstructured free text. Despite this potential, relatively few applications have entered real-world clinical practice. This paper aims to introduce the whole pipeline of NLP methodologies for EHR analysis to the clinical researcher, with case studies to demonstrate the application of these methods in the existing literature. Essential pre-processing steps are introduced, followed by the two major classes of analytical frameworks: statistical methods and Artificial Neural Networks (ANNs). Case studies which apply statistical and ANN-based methods are then provided and discussed, illustrating information extraction tasks for objective and subjective information, and classification/prediction tasks using supervised and unsupervised approaches. State-of-the-art large language models and future directions for research are then discussed. This educational article aims to bridge the gap between the clinical researcher and the NLP expert, providing clinicians with a background understanding of the NLP techniques relevant to EHR analysis, allowing engagement with this rapidly evolving area of research, which is likely to have a major impact on clinical practice in coming years.}
}
@article{ZNAIDI2024408,
title = {Automatic Abstracting For Bibliographic Record Semantic Enrichment},
journal = {Procedia Computer Science},
volume = {244},
pages = {408-415},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.215},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030163},
author = {Nassreddine Znaidi and Amel Fraisse and Fadoua Ouamani},
keywords = {Bibliographic record, Multilingualism, Automatic summarizing, Natural language processing, Large Language Model},
abstract = {Cultural heritage institutions store and digitize large amounts of bibliographic data within archives to make records accessible to archivists, librarians, and the general public. However, cataloging standards vary from one archive to another, which limits the sharing and use of these data. To address this issue, it is necessary to work on multilingual bibliographic records and enrich them. This would help standardize and harmonize the information, facilitating interoperability between different databases and systems. Enriching multilingual bibliographic data is essential to open up and provide broader access to these resources, thereby enhancing the dissemination of cultural heritage and enabling more efficient use by an international audience. This paper proposes a new methodology built within the framework of the interdisciplinary ROSETTA project, dedicated to enrichment of multilingual Bibliographic Records for Transnational Works. To experiment with our approach, we select 30 transnational literary works that have been translated into 15 languages, including non-dominant languages. We aggregate multilingual bibliographic records for these works from various sources. We employ an Large language model(LLM) model to enrich these records by generating summaries of works and calculating the similarity between different translations. To ensure quality and accuracy, we had Mark Twain experts at Stanford University meticulously evaluate each summary for fidelity, clarity, and coherence, and received positive feedback highlighting their precision and ability to capture the essence of the original work.}
}
@article{PANG2025102852,
title = {Towards cognition-augmented human-centric assembly: A visual computation perspective},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {91},
pages = {102852},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102852},
url = {https://www.sciencedirect.com/science/article/pii/S073658452400139X},
author = {Jiazhen Pang and Pai Zheng and Junming Fan and Tianyuan Liu},
keywords = {Cognitive assistance, Human-centric assembly, Computer vision, Metaverse, Cloud service, Large language model, Brain computer interface},
abstract = {Human-centric assembly is emerging as a promising paradigm for achieving mass personalization in the context of Industry 5.0, as it fully capitalizes on the advantages of human flexibility with robot assistance. However, in small-batch and highly customized assembly tasks, frequently changes in production procedures pose significant cognition challenges. To address this, leveraging computer vision technology to enhance human cognition becomes a feasible solution. Therefore, this review aims to explore the cognitive characteristics of human beings and classify existing computer vision technologies in a manner that discusses the future development of cognition-augmented human-centric assembly. The concept of cognition-augmented assembly is first proposed based on the brain's functional structure - the frontal, parietal, temporal, and occipital lobes. Corresponding to these brain regions, cognitive issues in spatiality, memory, knowledge, and decision-making are summarized. Recent studies conducted between 2014 and 2023 on visual computation of assembly are categorized into four groups: position registration, multi-layer recognition, contextual perception, and mixed-reality fusion, all aimed at addressing these cognitive challenges. The applications and limitations of current computer vision technology are discussed. Furthermore, considering the rapidly evolving technologies such as the metaverse, cloud services, large language models, and brain-computer interfaces, future trends on computer vision are prospected to augment human cognition corresponding to the cognitive issues.}
}
@article{KAGAN2024100658,
title = {Toward a nomenclature consensus for diverse intelligent systems: Call for collaboration},
journal = {The Innovation},
volume = {5},
number = {5},
pages = {100658},
year = {2024},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100658},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824000961},
author = {Brett J. Kagan and Michael Mahlis and Anjali Bhat and Josh Bongard and Victor M. Cole and Phillip Corlett and Christopher Gyngell and Thomas Hartung and Bianca Jupp and Michael Levin and Tamra Lysaght and Nicholas Opie and Adeel Razi and Lena Smirnova and Ian Tennant and Peter Thestrup Wade and Ge Wang},
abstract = {Summary
Disagreements about language use are common both between and within fields. Where interests require multidisciplinary collaboration or the field of research has the potential to impact society at large, it becomes critical to minimize these disagreements where possible. The development of diverse intelligent systems, regardless of the substrate (e.g., silicon vs. biology), is a case where both conditions are met. Significant advancements have occurred in the development of technology progressing toward these diverse intelligence systems. Whether progress is silicon based, such as the use of large language models, or through synthetic biology methods, such as the development of organoids, a clear need for a community-based approach to seeking consensus on nomenclature is now vital. Here, we welcome collaboration from the wider scientific community, proposing a pathway forward to achieving this intention, highlighting key terms and fields of relevance, and suggesting potential consensus-making methods to be applied.}
}
@article{NGUYEN2025103949,
title = {Retrieve–Revise–Refine: A novel framework for retrieval of concise entailing legal article set},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103949},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103949},
url = {https://www.sciencedirect.com/science/article/pii/S030645732400308X},
author = {Chau Nguyen and Phuong Nguyen and Le-Minh Nguyen},
keywords = {Retrieval–Revise–Refine framework, Legal article set retrieval, Information retrieval, COLIEE competition, Large language models},
abstract = {The retrieval of entailing legal article sets aims to identify a concise set of legal articles that holds an entailment relationship with a legal query or its negation. Unlike traditional information retrieval that focuses on relevance ranking, this task demands conciseness. However, prior research has inadequately addressed this need by employing traditional methods. To bridge this gap, we propose a three-stage Retrieve–Revise–Refine framework which explicitly addresses the need for conciseness by utilizing both small and large language models (LMs) in distinct yet complementary roles. Empirical evaluations on the COLIEE 2022 and 2023 datasets demonstrate that our framework significantly enhances performance, achieving absolute increases in the macro F2 score by 3.17% and 4.24% over previous state-of-the-art methods, respectively. Specifically, our Retrieve stage, employing various tailored fine-tuning strategies for small LMs, achieved a recall rate exceeding 0.90 in the top-5 results alone—ensuring comprehensive coverage of entailing articles. In the subsequent Revise stage, large LMs narrow this set, improving precision while sacrificing minimal coverage. The Refine stage further enhances precision by leveraging specialized insights from small LMs, resulting in a relative improvement of up to 19.15% in the number of concise article sets retrieved compared to previous methods. Our framework offers a promising direction for further research on specialized methods for retrieving concise sets of entailing legal articles, thereby more effectively meeting the task’s demands.}
}
@article{ELZ2024104017,
title = {The IDMP Ontology – A Catalyst to Unleash the Potential of AI and Accelerate Data-Driven Decisions with Industry-Wide Standards},
journal = {Drug Discovery Today},
volume = {29},
number = {6},
pages = {104017},
year = {2024},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.104017},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624001429},
author = {Sheila R. Elz and Gerd R. Kleemann and Torsten Osthus and Martin Petracchi and Raphael Sergent}
}
@article{KEARNS2024543,
title = {The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {7},
pages = {543-551},
year = {2024},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S152515782400062X},
author = {William G. Kearns and Georgios Stamoulis and Joseph Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Bradford Wilson and Laura Kearns and Nicholas Ng and Maya Seshan and Raymond Anchan},
abstract = {Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or "dark" data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies.}
}
@article{BU2024111148,
title = {Efficient utilization of pre-trained models: A review of sentiment analysis via prompt learning},
journal = {Knowledge-Based Systems},
volume = {283},
pages = {111148},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111148},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123008985},
author = {Kun Bu and Yuanchao Liu and Xiaolong Ju},
keywords = {Sentiment analysis, Prompt learning, Word embedding, Pre-trained models, Natural language processing},
abstract = {Sentiment analysis is one of the traditional well-known tasks in Natural Language Processing (NLP) research. In recent years, Pre-trained Models (PMs) have become one of the frontiers of NLP, and the knowledge in PMs is usually leveraged to improve machine learning models' performance for a variety of downstream NLP tasks including sentiment analysis. However, there are also some shortcomings in PM-based approaches. For example, many studies pointed out there are gaps between pre-training and fine-tuning. In addition, because of the time-consuming and high-cost data annotation process, the labeled training data are usually precious and scarce, which often leads to the over-fitting of models. The recent advent of prompt learning technology provides a promising solution to the above challenges. In this paper, we first discussed the background of prompt learning and its basic principle. Prompt learning changes the model input by adding templates, allowing learning tasks to adapt actively to pre-trained models, and therefore can promote the innovation and applicability of pre-trained models. Then we investigated the evolution of sentiment analysis and explored the application of prompt learning to different sentiment analysis tasks. Our research and review show that prompt learning is more suitable for sentiment analysis tasks and can achieve good performance. Finally, we also provided some future research directions on prompt-based sentiment analysis. Our survey demonstrated that prompt learning can facilitate the efficient utilization of pre-trained models in sentiment analysis and other tasks, which makes it a new paradigm worthy of further exploration.}
}
@article{ZENG2024123400,
title = {Research on the application of knowledge mapping and knowledge structure construction based on adaptive learning model},
journal = {Expert Systems with Applications},
volume = {249},
pages = {123400},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123400},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424002653},
author = {Xiyin Zeng and Shouqiang Liu},
keywords = {Personalized learing, Pedagogy, Interactive learning environments, Applications},
abstract = {This project has developed a geometry learning software that integrates multiple computer technologies to address the challenges of deep analysis of knowledge points and establishing connections in learning software. The software combines Long Short-Term Memory (LSTM) and Residual Neural Network (ResNet101) to encode text and image features. A self-attention mechanism is used to fuse information from both modalities, enabling decoding of geometric models and classification of corresponding knowledge points.This project uses LSTM and ResNet101 models to extract text and visual features for problem-solving using the Multi Mode Thinking Chain (CoT) method. Classification labels are utilized to generate text responses for problem-solving ideas. Furthermore, a recommendation module is proposed, which combines knowledge tracking and neural collaborative filtering algorithms to capture student behavior and knowledge point vectors. Implicit factors representing students' mastery of different knowledge points are used as inputs in neural collaborative filtering for personalized recommendations. The results demonstrate improvements in accuracy using the ResNet + LSTM multimodal algorithm, achieving a 13 % increase compared to single-modal classification. The multimodal CoT approach also outperforms language models like GPT3.5 and VisualBert by 10 %. Additionally, the combined algorithm of knowledge tracking and neural collaborative filtering shows a 13.3 % higher F1 value compared to ordinary algorithms, confirming the superiority of the adopted method in this project.}
}
@article{LIU2025112278,
title = {Agent design pattern catalogue: A collection of architectural patterns for foundation model based agents},
journal = {Journal of Systems and Software},
volume = {220},
pages = {112278},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112278},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003224},
author = {Yue Liu and Sin Kit Lo and Qinghua Lu and Liming Zhu and Dehai Zhao and Xiwei Xu and Stefan Harrer and Jon Whittle},
keywords = {Agent, Foundation model, Large language model, Pattern, Software engineering, Responsible AI},
abstract = {Foundation model-enabled generative artificial intelligence facilitates the development and implementation of agents, which can leverage distinguished reasoning and language processing capabilities to takes a proactive, autonomous role to pursue users’ goals. Nevertheless, there is a lack of systematic knowledge to guide practitioners in designing the agents considering challenges of goal-seeking (including generating instrumental goals and plans), such as hallucinations inherent in foundation models, explainability of reasoning process, complex accountability, etc. To address this issue, we have performed a systematic literature review to understand the state-of-the-art foundation model-based agents and the broader ecosystem. In this paper, we present a pattern catalogue consisting of 18 architectural patterns with analyses of the context, forces, and trade-offs as the outcomes from the previous literature review. We propose a decision model for selecting the patterns. The proposed catalogue can provide holistic guidance for the effective use of patterns, and support the architecture design of foundation model-based agents by facilitating goal-seeking and plan generation.}
}
@article{ARMARY2025100693,
title = {Ontology learning towards expressiveness: A survey},
journal = {Computer Science Review},
volume = {56},
pages = {100693},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100693},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000765},
author = {Pauline Armary and Cheikh Brahim El-Vaigh and Ouassila {Labbani Narsis} and Christophe Nicolle},
keywords = {Ontology learning, Heavyweight ontology, Ontology, Axioms, Rules},
abstract = {Ontology learning, particularly axiom learning, is a challenging task that focuses on building expressive and decidable ontologies. The literature proposes several research efforts aimed to resolve the complexities inherent in axiom and rule learning, which seeks to automatically infer logical constructs from diverse data sources. The goal of this paper is to conduct a comprehensive review of existing work in this domain. It aims to critically analyze the contributions and limitations of current approaches, providing a clear understanding of the state-of-the-art and identifying areas where further research is needed.}
}
@article{ZAVARELLA2024e32479,
title = {Triplétoile: Extraction of knowledge from microblogging text},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32479},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32479},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085104},
author = {Vanni Zavarella and Sergio Consoli and Diego {Reforgiato Recupero} and Gianni Fenu and Simone Angioni and Davide Buscaldi and Danilo Dessí and Francesco Osborne},
keywords = {Information extraction, Knowledge graphs, Social media analysis, Named entity recognition, Hierarchical clustering, Word embeddings},
abstract = {Numerous methods and pipelines have recently emerged for the automatic extraction of knowledge graphs from documents such as scientific publications and patents. However, adapting these methods to incorporate alternative text sources like micro-blogging posts and news has proven challenging as they struggle to model open-domain entities and relations, typically found in these sources. In this paper, we propose an enhanced information extraction pipeline tailored to the extraction of a knowledge graph comprising open-domain entities from micro-blogging posts on social media platforms. Our pipeline leverages dependency parsing and classifies entity relations in an unsupervised manner through hierarchical clustering over word embeddings. We provide a use case on extracting semantic triples from a corpus of 100 thousand tweets about digital transformation and publicly release the generated knowledge graph. On the same dataset, we conduct two experimental evaluations, showing that the system produces triples with precision over 95% and outperforms similar pipelines of around 5% in terms of precision, while generating a comparatively higher number of triples.}
}
@article{GAVRIILIDIS20241886,
title = {A mini-review on perturbation modelling across single-cell omic modalities},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {1886-1896},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.04.058},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001417},
author = {George I. Gavriilidis and Vasileios Vasileiou and Aspasia Orfanou and Naveed Ishaque and Fotis Psomopoulos},
keywords = {Perturbation, Single-cell RNA sequencing, ScRNAseq, Machine learning, Deep learning},
abstract = {Recent advances in single-cell omics technology have transformed the landscape of cellular and molecular research, enriching the scope and intricacy of cellular characterisation. Perturbation modelling seeks to comprehensively grasp the effects of external influences like disease onset or molecular knock-outs or external stimulants on cellular physiology, specifically on transcription factors, signal transducers, biological pathways, and dynamic cell states. Machine and deep learning tools transform complex perturbational phenomena in algorithmically tractable tasks to formulate predictions based on various types of single-cell datasets. However, the recent surge in tools and datasets makes it challenging for experimental biologists and computational scientists to keep track of the recent advances in this rapidly expanding filed of single-cell modelling. Here, we recapitulate the main objectives of perturbation modelling and summarise novel single-cell perturbation technologies based on genetic manipulation like CRISPR or compounds, spanning across omic modalities. We then concisely review a burgeoning group of computational methods extending from classical statistical inference methodologies to various machine and deep learning architectures like shallow models or autoencoders, to biologically informed approaches based on gene regulatory networks, and to combinatorial efforts reminiscent of ensemble learning. We also discuss the rising trend of large foundational models in single-cell perturbation modelling inspired by large language models. Lastly, we critically assess the challenges that underline single-cell perturbation modelling while pointing towards relevant future perspectives like perturbation atlases, multi-omics and spatial datasets, causal machine learning for interpretability, multi-task learning for performance and explainability as well as prospects for solving interoperability and benchmarking pitfalls.}
}
@article{LI2025100894,
title = {A concise review of intelligent game agent},
journal = {Entertainment Computing},
volume = {52},
pages = {100894},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100894},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002623},
author = {Hui Li and Xinyi Pang and Bixia Sun and Kexin Liu},
keywords = {Intelligent agent, Artificial intelligence, Monte Carlo tree, Reinforcement learning, Large language models},
abstract = {Intelligent game agents are crafted using AI technologies to mimic player behavior and make decisions autonomously. Over the past decades, the scope of intelligent agents has broadened from chess to encompass content generation, player modeling, and result prediction, reflecting the field’s evolving and multifaceted nature. In this paper, we conduct a systematic review of recent literature on intelligent methods and applications of game agents, along with general game agent frameworks. Our findings suggest that creating general intelligent agents remains a significant challenge, yet it is worthwhile to explore methods that better integrate the strengths of different techniques to build more robust and adaptable intelligent game agents.}
}
@article{SHAHRIAR2025104358,
title = {A comprehensive review of current trends, challenges, and opportunities in text data privacy},
journal = {Computers & Security},
volume = {151},
pages = {104358},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104358},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000471},
author = {Sakib Shahriar and Rozita Dara and Rajen Akalu},
keywords = {Privacy enhancing solutions, Text data, Natural language processing, Artificial intelligence, Machine learning, Privacy risk},
abstract = {The emergence of smartphones and internet accessibility around the globe have enabled billions of people to be connected to the digital world. Due to the popularity of instant messaging applications and social media, a large quantity of personal data is in text format, and processing text data in a privacy-preserving manner poses unique challenges. While existing reviews focus on privacy concerns from specific algorithmic perspectives or target only a particular domain, such as healthcare or smart metering, they fail to provide a comprehensive view that addresses the multi-layered privacy risks inherent to text data processing. Existing works often limit their scope to specialized solutions like differential privacy, anonymization, or federated learning, neglecting a broader spectrum of challenges. To fill this gap, we present a comprehensive review of privacy-enhancing solutions for text data processing in the present literature and classify the works into six categories of privacy risks: (i) unintentional memorability, (ii) membership inference, (iii) exposure and re-identification, (iv) language models and word embeddings, (v) authorship attribution, and (vi) collaborative processing. We then analyze existing privacy-enhancing solutions for text data by considering the aforementioned privacy risks. Finally, we identified several research gaps, including the need for comprehensive privacy metrics, explainable algorithms, and privacy in social media analytics.}
}
@article{YU2024104765,
title = {Artificial intelligence in paleontology},
journal = {Earth-Science Reviews},
volume = {252},
pages = {104765},
year = {2024},
issn = {0012-8252},
doi = {https://doi.org/10.1016/j.earscirev.2024.104765},
url = {https://www.sciencedirect.com/science/article/pii/S0012825224000928},
author = {Congyu Yu and Fangbo Qin and Akinobu Watanabe and Weiqi Yao and Ying Li and Zichuan Qin and Yuming Liu and Haibing Wang and Qigao Jiangzuo and Allison Y. Hsiang and Chao Ma and Emily Rayfield and Michael J. Benton and Xing Xu},
keywords = {Paleontology, Fossil, Artificial intelligence, Machine learning, Deep learning, Classification, Segmentation, Prediction},
abstract = {The accumulation of large datasets and increasing data availability have led to the emergence of data-driven paleontological studies, which reveal an unprecedented picture of evolutionary history. However, the fast-growing quantity and complication of data modalities make data processing laborious and inconsistent, while also lacking clear benchmarks to evaluate data collection and generation, and the performances of different methods on similar tasks. Recently, artificial intelligence (AI) has become widely practiced across scientific disciplines, but not so much to date in paleontology where traditionally manual workflows have been more usual. In this study, we review >70 paleontological AI studies since the 1980s, covering major tasks including micro- and macrofossil classification, image segmentation, and prediction. These studies feature a wide range of techniques such as Knowledge-Based Systems (KBS), neural networks, transfer learning, and many other machine learning methods to automate a variety of paleontological research workflows. Here, we discuss their methods, datasets, and performance and compare them with more conventional AI studies. We attribute the recent increase in paleontological AI studies most to the lowering of the entry bar in training and deployment of AI models rather than innovations in fossil data compilation and methods. We also present recently developed AI implementations such as diffusion model content generation and Large Language Models (LLMs) that may interface with paleontological research in the future. Even though AI has not yet been a significant part of the paleontologist's toolkit, successful implementation of AI is growing and shows promise for paradigm-transformative effects on paleontological research in the years to come.}
}
@article{ZHAO2024111843,
title = {From easy to hard: Improving personalized response generation of task-oriented dialogue systems by leveraging capacity in open-domain dialogues},
journal = {Knowledge-Based Systems},
volume = {295},
pages = {111843},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111843},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124004775},
author = {Meng Zhao and Lifang Wang and Zejun Jiang and Yushuang Liu and Ronghan Li and Zhongtian Hu and Xinyu Lu},
keywords = {Task-oriented dialogue system, Personalized response generation, Knowledge base, Pre-trained language model},
abstract = {A task-oriented dialogue system (TOD) is an important application of artificial intelligence. In the past few years, works on personalized TODs have attracted increased research attention and have seen much progress. The main challenge of such dialogue systems is finding ways to exploit user profiles under conditions of fixed and monolithic dialogue style. However, most of the existing works overlook the observation that the personalization capability of the dialogue system is fundamental and generic, and they treat all attributes of the user profile equally throughout the dialogue flows, which makes them inadequate for developing a well-performing personalized TOD. In this paper, we propose a two-stage learning framework equipped with GPT2 as a backbone to alleviate the above two problems. In the first learning phase, we finetune the GPT2 model on the personalized open-domain dialogues to preliminarily acquire personalization power. Then, we transfer this power to personalized task-oriented dialogues for the second stage of learning. After that, we enable the proposed model to be capable of the desired personalization capacity. Moreover, we present a dynamic profile fusion mechanism and an auxiliary task that detects which attributes contribute to the current utterance to facilitate the model’s performance. Eventually, we rewrite the attribute descriptions of user profiles in sentences to mitigate the consistency gap between the open-domain and task-oriented dialogues. The experimental results show that the proposed model achieves superior results compared to the state-of-the-art models on two versions of the Personalized bAbI dataset.}
}
@article{ISHMAM2024102270,
title = {From image to language: A critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities},
journal = {Information Fusion},
volume = {106},
pages = {102270},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102270},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000484},
author = {Md. Farhan Ishmam and Md. Sakib Hossain Shovon and M.F. Mridha and Nilanjan Dey},
keywords = {Visual Question Answering, Vision language pre-training, Multimodal learning, Multimodal large language models},
abstract = {The multimodal task of Visual Question Answering (VQA) encompassing elements of Computer Vision (CV) and Natural Language Processing (NLP), aims to generate answers to questions on any visual input. Over time, the scope of VQA has expanded from datasets focusing on an extensive collection of natural images to datasets featuring synthetic images, video, 3D environments, and various other visual inputs. The emergence of large pre-trained networks has shifted the early VQA approaches relying on feature extraction and fusion schemes to vision language pre-training (VLP) techniques. However, there is a lack of comprehensive surveys that encompass both traditional VQA architectures and contemporary VLP-based methods. Furthermore, the VLP challenges in the lens of VQA haven’t been thoroughly explored, leaving room for potential open problems to emerge. Our work presents a survey in the domain of VQA that delves into the intricacies of VQA datasets and methods over the field’s history, introduces a detailed taxonomy to categorize the facets of VQA, and highlights the recent trends, challenges, and scopes for improvement. We further generalize VQA to multimodal question answering, explore tasks related to VQA, and present a set of open problems for future investigation. The work aims to navigate both beginners and experts by shedding light on the potential avenues of research and expanding the boundaries of the field.}
}
@article{BUGHIO20245330,
title = {GenAI in Rule-based Systems for IoMT Security: Testing and Evaluation},
journal = {Procedia Computer Science},
volume = {246},
pages = {5330-5339},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.652},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027078},
author = {Kulsoom S. Bughio and David M. Cook and Syed Afaq A. Shah},
keywords = {Artificial Intelligence, IoMT, Ruled-based Systems, Vulnerability Detection, Gen AI},
abstract = {Generative AI (GenAI) represents a significant advancement in Artificial intelligence research, offering numerous benefits and opening new avenues for innovation across various domains. In healthcare, Generative AI has shown promise in applications such as drug discovery, personalized medicine, and medical imaging. This paper examines the role of Generative AI in rule-based systems, where vulnerabilities are detected with the help of formal logic. In this context, the ruleset is generated and tested to evaluate the performance of rule-based systems with the aid of GenAI. The effectiveness of the GenAI tool was evaluated using a publicly available case study from a laboratory setting. The results show that using generative Artificial intelligence in rule-based systems leads to increased creativity, continuous learning, and robust performance. GenAI responded to each use case and provided the desired results compared to traditional rule-based systems. This integration of advanced AI techniques with traditional rule-based systems ensures that these hybrid systems perform reliably and effectively.}
}
@article{AVOGADRO2024112447,
title = {Feature/vector entity retrieval and disambiguation techniques to create a supervised and unsupervised semantic table interpretation approach},
journal = {Knowledge-Based Systems},
volume = {304},
pages = {112447},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112447},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010815},
author = {Roberto Avogadro and Fabio D’Adda and Marco Cremaschi},
keywords = {Semantic web, Knowledge base, Knowledge base construction, Knowledge base extension, Knowledge graph, Semantic table interpretation, Table annotation, Data enrichment, Tabular data},
abstract = {Recently, there has been an increasing interest in extracting and annotating tables on the Web. This activity allows the transformation of textual data into machine-readable formats to enable the execution of various artificial intelligence tasks, e.g., semantic search and dataset extension. Semantic Table Interpretation (STI) is the process of annotating elements in a table. The paper explores Semantic Table Interpretation, addressing the challenges of Entity Retrieval and Entity Disambiguation in the context of Knowledge Graphs (KGs). It introduces LamAPI, an Information Retrieval system with string/type-based filtering and s-elBat, an Entity Disambiguation technique that combines heuristic and ML-based approaches. By applying the acquired know-how in the field and extracting algorithms, techniques and components from our previous STI approaches and the state of the art, we have created a new platform capable of annotating any tabular data, ensuring a high level of quality.}
}
@article{RASHIDI2025100688,
title = {Introduction to Artificial Intelligence and Machine Learning in Pathology and Medicine: Generative and Nongenerative Artificial Intelligence Basics},
journal = {Modern Pathology},
volume = {38},
number = {4},
pages = {100688},
year = {2025},
issn = {0893-3952},
doi = {https://doi.org/10.1016/j.modpat.2024.100688},
url = {https://www.sciencedirect.com/science/article/pii/S0893395224002680},
author = {Hooman H. Rashidi and Joshua Pantanowitz and Matthew G. Hanna and Ahmad P. Tafti and Parth Sanghani and Adam Buchinsky and Brandon Fennell and Mustafa Deebajah and Sarah Wheeler and Thomas Pearce and Ibrahim Abukhiran and Scott Robertson and Octavia Palmer and Mert Gur and Nam K. Tran and Liron Pantanowitz},
keywords = {artificial intelligence, ChatGPT, generative AI, generative pretrained transformer, machine learning, supervised & unsupervised ML},
abstract = {This manuscript serves as an introduction to a comprehensive 7-part review article series on artificial intelligence (AI) and machine learning (ML) and their current and future influence within pathology and medicine. This introductory review provides a comprehensive grasp of this fast-expanding realm and its potential to transform medical diagnosis, workflow, research, and education. Fundamental terminology employed in AI-ML is covered using an extensive dictionary. The article also provides a broad overview of the main domains in the AI-ML field, encompassing both generative and nongenerative (traditional) AI, thereby serving as a primer to the other 6 review articles in this series that describe the details about statistics, regulations, bias, ethical dilemmas, and ML-Ops in AI-ML. The intent of these review articles is to better equip individuals who are or will be working in an AI-enabled health care system.}
}
@article{ZHANG2024106073,
title = {Bayesian deep learning: An enhanced AI framework for legal reasoning alignment},
journal = {Computer Law & Security Review},
volume = {55},
pages = {106073},
year = {2024},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2024.106073},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001390},
author = {Chuyue Zhang and Yuchen Meng},
keywords = {Legal AI, Legal reasoning, Deep learning, Bayesian deep learning, Bayesian neural networks},
abstract = {The integration of artificial intelligence into the field of law has penetrated the underlying logic of legal operations. Currently, legal AI systems face difficulties in representing legal knowledge, exhibit insufficient legal reasoning capabilities, have poor explainability, and are inefficient in handling causal inference and uncertainty. In legal practice, various legal reasoning methods (deductive reasoning, inductive reasoning, abductive reasoning, etc.) are often intertwined and used comprehensively. However, the reasoning modes employed by current legal AI systems are inadequate. Identifying AI models that are more suitable for legal reasoning is crucial for advancing the development of legal AI systems. Distinguished from the current high-profile large language models, we believe that Bayesian reasoning is highly compatible with legal reasoning, as it can perferm abductive reasoning, excel at causal inference, and admits the "defeasibility" of reasoning conclusions, which is consistent with the cognitive development pattern of legal professionals from apriori to posteriori. AI models based on Bayesian methods can also become the main technological support for legal AI systems. Bayesian neural networks have advantages in uncertainty modeling, avoiding overfitting, and explainability. Legal AI systems based on Bayesian deep learning frameworks can combine the advantages of deep learning and probabilistic graphical models, facilitating the exchange and supplementation of information between perception tasks and reasoning tasks. In this paper, we take perpetrator prediction systems and legal judegment prediction systems as examples to discuss the construction and basic operation modes of the Bayesian deep learning framework. Bayesian deep learning can enhance reasoning ability, improve the explainability of models, and make the reasoning process more transparent and visualizable. Furthermore, Bayesian deep learning framework is well-suited for human-machine collaborative tasks, enabling the complementary strengths of humans and machines.}
}
@article{LI2025129609,
title = {A novel rumor detection method focusing on social psychology with graph attention network},
journal = {Neurocomputing},
volume = {626},
pages = {129609},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129609},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225002814},
author = {Lina Li and Guoxing Liu and Yu Liu and Qinghe Yu and Cheng Luo and Nianfeng Li},
keywords = {Rumor detection, Social psychology, Graph attention networks, Feature fusion, Large language models},
abstract = {The proliferation of online social media has led to an increase in the spread of rumors. Current rumor detection methods have not adequately considered the impact of social psychology and have neglected to integrate text features with other characteristics. This paper introduces a multi-feature fusion rumor detection model, Social Psychology-based Graph ATtention network (SPGAT), designed to enhance the accuracy of rumor detection. In this model, social psychological features are extracted using large language models, encompassing user emotion, social identity, group emotional resonance, and social influence. These features aim to deeply capture the essential attributes of rumors. Concurrently, a multi-head dynamic graph attention convolutional network is constructed. This network amalgamates complex structural features with essential features, thereby effectively capturing spatial propagation features and significant features while paying attention to the high-dimensional hidden features of rumors. Furthermore, a neural network is designed to comprehensively integrate the high-dimensional features of rumors, and rumor detection is achieved through a fully connected layer. Extensive experiments are conducted on three public datasets. Compared to the latest typical detection methods, the proposed method demonstrates certain advantages. Specifically, the accuracy and F1 score of rumor detection are improved by 5.87% and 4.4% on average on Weibo, PHEME 5, and PHEME 9 datasets compared to the latest baselines, respectively. Meanwhile, we verify and analyze the key role of socio-psychological characteristics in rumor propagation, which provides strong support for an in-depth understanding of the rumor propagation mechanism.}
}
@article{JIN2024104988,
title = {PubMed and beyond: biomedical literature search in the age of artificial intelligence},
journal = {eBioMedicine},
volume = {100},
pages = {104988},
year = {2024},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2024.104988},
url = {https://www.sciencedirect.com/science/article/pii/S2352396424000239},
author = {Qiao Jin and Robert Leaman and Zhiyong Lu},
keywords = {Artificial intelligence, Biomedical literature search},
abstract = {Summary
Biomedical research yields vast information, much of which is only accessible through the literature. Consequently, literature search is crucial for healthcare and biomedicine. Recent improvements in artificial intelligence (AI) have expanded functionality beyond keywords, but they might be unfamiliar to clinicians and researchers. In response, we present an overview of over 30 literature search tools tailored to common biomedical use cases, aiming at helping readers efficiently fulfill their information needs. We first discuss recent improvements and continued challenges of the widely used PubMed. Then, we describe AI-based literature search tools catering to five specific information needs: 1. Evidence-based medicine. 2. Precision medicine and genomics. 3. Searching by meaning, including questions. 4. Finding related articles with literature recommendation. 5. Discovering hidden associations through literature mining. Finally, we discuss the impacts of recent developments of large language models such as ChatGPT on biomedical information seeking.}
}
@article{XIA2023313,
title = {Enhancing intelligent IoT services development by integrated multi-token code completion},
journal = {Computer Communications},
volume = {212},
pages = {313-323},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423003791},
author = {Yu Xia and Tian Liang and WeiHuan Min and Li Kuang and Honghao Gao},
keywords = {Intelligent device-free sensing, Code completion, Language model, Graph neural network},
abstract = {The Internet of Things (IoT) is a revolutionary network of interconnected devices embedded with sensors and software that enables seamless communication, data sharing, and intelligent decision-making in the form of IoT services. To facilitate the efficient development of IoT services, code completion technique provides a promising solution by providing suggestions for missing code snippets. The development trend of IoT services is to support more mobile device terminals. Mobile devices are portable and easy to use, allowing IoT device operation and management anytime and anywhere. However, the current multi-token completion methods struggle to guarantee code generation quality under the constraints of low resources and low latency, making it difficult to fully support IoT service development. We propose a multi-token code completion framework, S2RCC, which completes code from skeleton to refinement with dual encoder and dual decoder. The framework consists of two phases: first, the code skeleton, which is the simplification of code containing structure-sensitive tokens, is predicted based on the semantics of the code context; second, the broken context is repaired with the predicted skeleton, and then parsed into the code structure so that the specific tokens can be generated combining the semantics and structure of context. Furthermore, we then provide an implementation of the framework, representing the repaired code as an improved Heterogeneous code graph and fusing the semantics and structure of code context by the three-layer stacked attention. We conducted experiments on multi-token completion datasets, showing that our model has achieved the state-of-the-art with the smallest possible scale and the fastest generation speed.}
}
@article{JUST2024102883,
title = {Natural language processing for innovation search – Reviewing an emerging non-human innovation intermediary},
journal = {Technovation},
volume = {129},
pages = {102883},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2023.102883},
url = {https://www.sciencedirect.com/science/article/pii/S0166497223001943},
author = {Julian Just},
keywords = {Natural language processing, Innovation search, Innovation intermediation, Front-end of innovation, AI-based innovation management, Systematic literature review},
abstract = {Applying artificial intelligence (AI), especially natural language processing (NLP), to harness large amounts of information from patent databases, online communities, social media, or crowdsourcing platforms is becoming increasingly popular to help organizations find promising solutions. In the era of non-human innovation intermediaries, we should begin to view NLP not only as a useful technology applied in different innovation practices but also as an intermediary orchestrating valuable information. Previous research has not taken this perspective, and knowledge about its intermediation activities and functions is limited. This study reviews 167 academic articles to better understand how NLP approaches can enrich intermediation in early-stage innovation search. It identifies 18 distinctive innovation practices taking over activities like forecasting trends, illustrating technology and idea landscapes, filtering out distinctive contributions, recombining domain-specific and analogous knowledge, or matching problems with solutions. While certain NLP capabilities complement each other, the analysis shows that the choice of the most appropriate approach depends on the characteristics of the innovation practice. Innovation researchers and practitioners should rethink current roles and responsibilities in AI-based innovation processes. As seen in the recent emergence of large language models (LLMs), the rapidly evolving field offers many future research opportunities and practical benefits.}
}
@article{DING2025126232,
title = {Tagging knowledge concepts for math problems based on multi-label text classification},
journal = {Expert Systems with Applications},
volume = {267},
pages = {126232},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126232},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030999},
author = {Ziqi Ding and Xiaolu Wang and Yuzhuo Wu and Guitao Cao and Liangyu Chen},
keywords = {Hierarchical multi-label classification, Deep learning, Attention mechanism, K12 math problems},
abstract = {Tagging knowledge concepts for course problems is essential for intelligent tutoring systems. Traditional manual tagging methods, usually performed by domain experts, are time-consuming and subject to individual biases. Consequently, research on automatic tagging technology is of substantial practical importance. Recently, text classification techniques have been applied to this task; however, these methods are inadequate for math problems due to their complexity, which includes formulaic content and hierarchical relationships among knowledge concepts. Although large language models (LLMs) have also been explored for this purpose, their generative nature and high computational cost pose challenges for direct application in tutoring systems. In this paper, we propose an automatic knowledge concept tagging model LHABS based on RoBERTa. This model integrates hierarchical label-semantic attention, which captures hierarchical knowledge concepts information, and multi-label smoothing, which combines textual features to help reduce overfitting, thus enhancing text classification performance. Our experimental evaluation on four datasets demonstrates that our model outperforms state-of-the-art methods. We also validate the effectiveness of hierarchical label-semantic attention and multi-label smoothing through our experiments. The code and data are available at: https://github.com/xuqiang124/atmk_system.}
}
@article{SCHAEFFER20232106,
title = {OLAF: An Ontology Learning Applied Framework},
journal = {Procedia Computer Science},
volume = {225},
pages = {2106-2115},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013595},
author = {Marion Schaeffer and Matthias Sesboüé and Jean-Philippe Kotowicz and Nicolas Delestre and Cecilia Zanni-Merk},
keywords = {ontology learning, ontology, knowledge acquisition, ontology-based system, framework, automation, NLP},
abstract = {Since the beginning of the century, research on ontology learning has gained popularity. Automatically extracting and structuring knowledge relevant to a domain of interest from unstructured textual data is a major scientific challenge. After studying the main existing methods, such as Text2Onto, we propose a new approach with a modular ontology learning framework focusing on automatically extracting knowledge from raw text sources. We consider tasks from data pre-processing to axiom extraction. Whereas previous contributions considered ontology learning systems as tools to help the domain expert craft a reusable ontology, we developed the proposed framework with full automation in mind to build a minimum viable ontology targeted at an application. Ontology Learning Applied Framework (OLAF) has been generically designed to build specific ontologies whatever the application domain, use case and text data. We implement an initial version and test the framework on an ontology-based system, a search engine for technical products.}
}
@article{PENG2025103067,
title = {Hybrid approach for drug-target interaction predictions in ischemic stroke models},
journal = {Artificial Intelligence in Medicine},
volume = {161},
pages = {103067},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103067},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000028},
author = {Jing-Jie Peng and Yi-Yue Zhang and Rui-Feng Li and Wen-Jun Zhu and Hong-Rui Liu and Hui-Yin Li and Bin Liu and Dong-Sheng Cao and Jun Peng and Xiu-Ju Luo},
keywords = {Drug target interaction, Machine learning, Large-language-model, Cerdulatinib, Ischemic stroke, Cell death},
abstract = {Multiple cell death mechanisms are triggered during ischemic stroke and they are interconnected in a complex network with extensive crosstalk, complicating the development of targeted therapies. We therefore propose a novel framework for identifying disease-specific drug-target interaction (DTI), named strokeDTI, to extract key nodes within an interconnected graph network of activated pathways via leveraging transcriptomic sequencing data. Our findings reveal that the drugs a model can predict are highly representative of the characteristics of the database the model is trained on. However, models with comparable performance yield diametrically opposite predictions in real testing scenarios. Our analysis reveals a correlation between the reported literature on drug-target pairs and their binding scores. Leveraging this correlation, we introduced an additional module to assess the predictive validity of our model for each unique target, thereby improving the reliability of the framework's predictions. Our framework identified Cerdulatinib as a potential anti-stroke drug via targeting multiple cell death pathways, particularly necroptosis and apoptosis. Experimental validation in in vitro and in vivo models demonstrated that Cerdulatinib significantly attenuated stroke-induced brain injury via inhibiting multiple cell death pathways, improving neurological function, and reducing infarct volume. This highlights strokeDTI's potential for disease-specific drug-target identification and Cerdulatinib's potential as a potent anti-stroke drug.}
}
@article{YANG2024107924,
title = {RDmaster: A novel phenotype-oriented dialogue system supporting differential diagnosis of rare disease},
journal = {Computers in Biology and Medicine},
volume = {169},
pages = {107924},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107924},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000088},
author = {Jian Yang and Liqi Shu and Mingyu Han and Jiarong Pan and Lihua Chen and Tianming Yuan and Linhua Tan and Qiang Shu and Huilong Duan and Haomin Li},
keywords = {Rare disease, Human phenotype ontology, Differential diagnosis, Phenomic and genomic diagnostics, Electronic differential diagnostic support system},
abstract = {Background
Clinicians often lack the necessary expertise to differentially diagnose multiple underlying rare diseases (RDs) due to their complex and overlapping clinical features, leading to misdiagnoses and delayed treatments. The aim of this study is to develop a novel electronic differential diagnostic support system for RDs.
Method
Through integrating two Bayesian diagnostic methods, a candidate list was generated with enhance clinical interpretability for the further Q&A based differential diagnosis (DDX). To achieve an efficient Q&A dialogue strategy, we introduce a novel metric named the adaptive information gain and Gini index (AIGGI) to evaluate the expected gain of interrogated phenotypes within real-time diagnostic states.
Results
This DDX tool called RDmaster has been implemented as a web-based platform (http://rdmaster.nbscn.org/). A diagnostic trial involving 238 published RD patients revealed that RDmaster outperformed existing RD diagnostic tools, as well as ChatGPT, and was shown to enhance the diagnostic accuracy through its Q&A system.
Conclusions
The RDmaster offers an effective multi-omics differential diagnostic technique and outperforms existing tools and popular large language models, particularly enhancing differential diagnosis in collecting diagnostically beneficial phenotypes.}
}
@article{PFANSCHILLING2025109369,
title = {NeST: The neuro-symbolic transpiler},
journal = {International Journal of Approximate Reasoning},
volume = {179},
pages = {109369},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2025.109369},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X25000106},
author = {Viktor Pfanschilling and Hikaru Shindo and Devendra Singh Dhami and Kristian Kersting},
keywords = {Probabilistic programming, Neuro-symbolic, Large language models, Tractable probabilistic models},
abstract = {Tractable Probabilistic Models such as Sum-Product Networks are a powerful category of models that offer a rich choice of fast probabilistic queries. However, they are limited in the distributions they can represent, e.g., they cannot define distributions using loops or recursion. To move towards more complex distributions, we introduce a novel neurosymbolic programming language, Sum Product Loop Language (SPLL), along with the Neuro-Symbolic Transpiler (NeST). SPLL aims to build inference code most closely resembling Tractable Probabilistic Models. NeST is the first neuro-symbolic transpiler—a compiler from one high-level language to another. It generates inference code from SPLL but natively supports other computing platforms, too. This way, SPLL can seamlessly interface with e.g. pretrained (neural) models in PyTorch or Julia. The result is a language that can run probabilistic inference on more generalized distributions, reason on neural network outputs, and provide gradients for training.}
}
@article{XIANG2024103607,
title = {A cross-guidance cross-lingual model on generated parallel corpus for classical Chinese machine reading comprehension},
journal = {Information Processing & Management},
volume = {61},
number = {2},
pages = {103607},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103607},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323003448},
author = {Junyi Xiang and Maofu Liu and Qiyuan Li and Chen Qiu and Huijun Hu},
keywords = {Classical Chinese machine reading comprehension, Chinese diachronic gap, Cross-guidance cross-lingual model, Parallel corpus generation},
abstract = {Chinese diachronic gap is a key issue in classical Chinese machine reading comprehension (CCMRC). Preceding work on bridging this gap has been mostly restricted to limited monolingual classical Chinese corpora pre-training and lexical knowledge integration, which require a great deal of human resources. In this paper, we propose a cross-guidance cross-lingual model (CGCLM), pre-trained on a classical and modern Chinese parallel corpus generated from a large language model, to bridge the Chinese diachronic gap and reduce the manual effort. The CGCLM facilitates accurate translation by providing in-context examples and feedback based on the longest common substring between source and target sentences, thereby avoiding untranslated Chinese words. Specifically, we consider three pre-training tasks, i.e., cross-masked language modeling, linguistic label cross-prediction, and semantic cross-aware translation language modeling. The knowledge acquired from masked tokens uncovering and linguistic label predicting can lead to the implicit semantic alignment between two language styles. Taking advantage of the semantic similarity between the same syntactic levels of parallel pairs, cross-aware modeling integrates and transmits contextualized semantic information. We utilize an 18.6G monolingual corpus to create a 37.2G parallel corpus. Manual evaluation has resulted in only acceptable discrepancies between our generated and human-edited parallel corpora. Extensive experimental results show that our proposed model outperforms the state-of-the-art by an average accuracy of 3.13%, 2.44%, and 2.17% on CCMRC, classical Chinese language understanding evaluation (CCLUE), and modern Chinese language understanding evaluation (MCLUE) tasks.}
}
@article{EKAPUTRA2025100855,
title = {Pattern-based engineering of Neurosymbolic AI Systems},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100855},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100855},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000416},
author = {Fajar J. Ekaputra},
keywords = {Neurosymbolic AI, Knowledge graphs, AI system engineering, Design patterns},
abstract = {The symbiotic combination of sub-symbolic and symbolic AI techniques is a significant trend in AI, leading to the fast-paced development of various techniques that integrate these paradigms to build intelligent systems. However, the wealth of heterogeneous architectural options for combining the paradigms into Neurosymbolic AI (NeSy-AI) systems poses significant challenges. In particular, there is currently no standardized way to design, engineer, and document such systems that encompass visual and formal notations. Existing works aim to address this challenge by systematically modelling NeSy-AI systems as design patterns that include process, data, and human interactions. However, these works focus on capturing specific views of the system rather than aiming to support the broad process of AI system engineering. This paper outlines a vision of pattern-based AI Systems engineering, aiming to support the engineering process of NeSy-AI systems with tasks such as system documentation and artefact generation through interlinked visual and formal notations with Knowledge Graphs at its core.}
}
@article{KONG2024108533,
title = {Recurrent event query decoder for document-level event extraction},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108533},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108533},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624006912},
author = {Jing Kong and Zhouwang Yang},
keywords = {Document-level event extraction, Transformers, Recurrent decoder},
abstract = {Document-level event extraction is a challenging task in natural language processing, as it involves multiple events within a document and scattered event arguments across sentences. To tackle these challenges, we propose a recurrent event query decoder, i.e., a recurrent module that dynamically updates event queries to capture cross-event dependencies. Our approach then generates arguments by extracting role-argument relations using bilinear mapping, which helps address the issue of scattered arguments. Experimental results demonstrate that our proposed approach outperforms state-of-the-art models on a large-scale public dataset and actual application data, achieving significant improvements in F1-score. In domain-specific event extraction applications, our method achieves higher accuracy with fewer resources compared to general-purpose large language models.}
}
@article{VERES2023102208,
title = {Self supervised learning and the poverty of the stimulus},
journal = {Data & Knowledge Engineering},
volume = {147},
pages = {102208},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102208},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2300068X},
author = {Csaba Veres and Jennifer Sampson},
keywords = {Classification, Learnability, Text mining, Machine Learning, NLP, Language},
abstract = {Diathesis alternations are the possible expressions of the arguments of verbs in different, systematically related subcategorization frames. Semantically similar verbs such as spill and spray can behave differently with respect to the alternations they can participate in. For example one can “spill/spray water on the plant”, but while one can “spray the plant with water”, it is odd to say “spill the plant with water”. “Spray” is a verb which can alternate between syntactic frames while “spill” is not alternating. How human speakers learn the difference between such verbs is not clearly understood, because the primary linguistic data (PLD) they receive does not appear sufficient to infer the knowledge required for adult competence. More generally the poverty of the stimulus (POS) hypothesis states that the PLD is not sufficient for a learner to infer full adult competence of language. That is, learning relies on prior constraints introduced by the language faculty. We tested state-of-the-art machine learning models trained by self supervision, and found some evidence that they could in fact learn the correct pattern of acceptability judgement in the locative alternation. However, we argued that this was partially a result of fine-tuning which introduced negative evidence into the learning data, which facilitated shortcut learning. Large language models (LLMs) cannot learn some linguistic facts from normal language data, but they can compensate to some extent by learning spurious correlated features when negative feedback is introduced during the training cycle.}
}@article{LIU2024411,
title = {Self-X heterogeneous attributed graph embedding-based product configuration framework for cognitive mass personalization},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {411-428},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001778},
author = {Yangshengyan Liu and Fu Gu and Jianfeng Guo},
keywords = {Cognitive mass personalization, Industrial knowledge graph, Smart product-service system, Product configuration, Heterogeneous attributed graph embedding, Cognitive computing},
abstract = {Cognitive mass personalization (CMP) is a promising manufacturing paradigm; equipped with cognitive capabilities like reasoning, CMP satisfies changeable needs via configuring personalized products at scale. In CMP, knowledge graphs (KGs) are exploited by smart product-service systems (SPSS) to support cognitive configuration/reconfiguration processes. However, the extant KG-enabled SPSSs are built upon fixed configurations and hybrid frameworks due to lacking a graph embedding (GE) model to render cognitive configuration decisions. In fact, GE is scarcely used in SPSS configuration, because it is not only compromised by the heterogeneity of KGs entailed by content-related specifications and complex structures but also influenced by the feature randomness and feature drift problems, which are triggered by accumulative errors and inconsistent objectives due to noisy assignments and different configuration tasks, separately. To address these limitations, a Self-X Heterogeneous Attributed Graph Embedding (SXHAGE) model is proposed in a Self-X architecture, which includes 1) self-attention graph attention networks, 2) a self-adaptive autoencoder, and 3) self-optimizing training objectives, to present heterogeneous data through jointly optimizing heterogeneous attributed entities and relations. A systematic SXHAGE-based configuration framework, in which product family design and configuration recommending are enabled by graph clustering and link prediction, is developed as a continuous updating loop to proactively configure personalized products. A real-world case study, i.e., configure personalized electric clippers via a web-based sustainable configuration platform, is performed to validate the applicability of the proposed framework in the CMP context. Moreover, extensive experiments on the case study dataset demonstrate the superiority of SXHAGE over the state-of-the-art algorithms, e.g., surpassing Deep Neighbor-Aware Embedding (DNENC) by 18 % in F1-score for graph clustering and by 5 % in ROC-AUC for link prediction.}
}
@article{CHIOSA2024114802,
title = {A portable application framework for energy management and information systems (EMIS) solutions using Brick semantic schema},
journal = {Energy and Buildings},
volume = {323},
pages = {114802},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114802},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824009186},
author = {Roberto Chiosa and Marco Savino Piscitelli and Marco Pritoni and Alfonso Capozzoli},
keywords = {Energy management and information systems, Portable application, Brick metadata schema, Anomaly detection, Machine learning},
abstract = {This paper introduces a portable framework for developing, scaling and maintaining energy management and information systems (EMIS) applications using an ontology-based approach. Key contributions include an interoperable layer based on Brick schema, the formalization of application constraints pertaining metadata and data requirements, and a field demonstration. The framework allows for querying metadata models, fetching data, preprocessing, and analyzing data, thereby offering a modular and flexible workflow for application development. Its effectiveness is demonstrated through a case study involving the development and implementation of a data-driven anomaly detection tool for the photovoltaic systems installed at the Politecnico di Torino, Italy. During eight months of testing, the framework was used to tackle practical challenges including: (i) developing a machine learning-based anomaly detection pipeline, (ii) replacing data-driven models during operation, (iii) optimizing model deployment and retraining, (iv) handling critical changes in variable naming conventions and sensor availability (v) extending the pipeline from one system to additional ones.}
}
@article{GROZA2025100057,
title = {Realising the potential impact of artificial intelligence for rare diseases – A framework},
journal = {Rare},
volume = {3},
pages = {100057},
year = {2025},
issn = {2950-0087},
doi = {https://doi.org/10.1016/j.rare.2024.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2950008724000401},
author = {Tudor Groza and Chun-Hung Chan and David A. Pearce and Gareth Baynam},
keywords = {Rare diseases, Patient journey, Generative artificial intelligence, Diagnosis, Care coordination},
abstract = {Rare diseases (RD) are conditions affecting fewer than 1 in 2000 persons, with over 7000 largely genetic RDs affecting 3.5 %-5.9 % of the global population, or approximately 262.9–446.2 million people. The substantial healthcare burden and costs, such as the $1 trillion annual expense in the USA, highlight the urgent need for improved RD management. The International Rare Diseases Research Consortium (IRDiRC) addresses this need through global collaboration, aiming for timely and accurate diagnosis, development of 1000 new therapies, and methodologies to measure impact by 2027. IRDiRC's initiatives include biannual meetings and workshops, like the AI-focused workshop in October 2023. This identified AI as crucial for advancing RD research and proposed a Framework for AI to enhance the RD patient journey by addressing efficiency and quality of life through modular solutions mapped to critical stages. The Framework integrates diverse data sources to improve diagnosis, treatment, and impact assessment, reflecting a holistic, cross-sector approach. By guiding multi-stakeholder efforts, the Framework aims to harness AI’s potential to significantly improve rare disease care.}
}
@article{LI2024112294,
title = {DANTE: Dialog graph enhanced prompt learning for conversational question answering over KGs},
journal = {Knowledge-Based Systems},
volume = {301},
pages = {112294},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112294},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124009286},
author = {Jingyang Li and Shengli Song and Sitong Yan and Guangneng Hu and Chengen Lai and Yulong Zhou},
keywords = {Conversational question answering, Knowledge graphs, Pre-train prompt and predict, Graph prompt},
abstract = {In this study, we focus on the task of selecting high-quality answers selection in knowledge-graph-based (KG-based) conversational question answering (ConvQA) system. Effectively exploring a user’s intention and modeling historical interaction records are challenging. To address this challenge, we propose the Dialog grAph eNhanced prompT lEarning (DANTE) model, which simultaneously integrates sequential and structural information from questions and interactive logic. While the structural information was exploited in previous studies by simply converting it into linear strings in a “pre-train, predict” paradigm, DANTE comprises the use of a novel graph representation for jointly modeling the QA pairs, relevant KG paths, and dialog contexts. The dialog graph constructs in both the turn-level and dialog-level, where DANTE fuses the structural and sequential information deeply in a “pre-train, prompt, and predict” manner. The experimental results showed that DANTE improves the absolute points by 7.1% and 8.2% in terms of the P@1 and mean reciprocal rank metrics, respectively, on the ConvQuestions/ConvRef benchmark compared with state-of-the-art baselines.}
}
@article{MUMUNI2025113,
title = {Automated data processing and feature engineering for deep learning and big data applications: A survey},
journal = {Journal of Information and Intelligence},
volume = {3},
number = {2},
pages = {113-153},
year = {2025},
issn = {2949-7159},
doi = {https://doi.org/10.1016/j.jiixd.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S2949715924000027},
author = {Alhassan Mumuni and Fuseini Mumuni},
keywords = {AutoML, Automated data preprocessing, Data processing, Automated feature engineering, Generative artificial intelligence, Big data},
abstract = {Modern approach to artificial intelligence (AI) aims to design algorithms that learn directly from data. This approach has achieved impressive results and has contributed significantly to the progress of AI, particularly in the sphere of supervised deep learning. It has also simplified the design of machine learning systems as the learning process is highly automated. However, not all data processing tasks in conventional deep learning pipelines have been automated. In most cases data has to be manually collected, preprocessed and further extended through data augmentation before they can be effective for training. Recently, special techniques for automating these tasks have emerged. The automation of data processing tasks is driven by the need to utilize large volumes of complex, heterogeneous data for machine learning and big data applications. Today, end-to-end automated data processing systems based on automated machine learning (AutoML) techniques are capable of taking raw data and transforming them into useful features for big data tasks by automating all intermediate processing stages. In this work, we present a thorough review of approaches for automating data processing tasks in deep learning pipelines, including automated data preprocessing – e.g., data cleaning, labeling, missing data imputation, and categorical data encoding – as well as data augmentation (including synthetic data generation using generative AI methods) and feature engineering – specifically, automated feature extraction, feature construction and feature selection. In addition to automating specific data processing tasks, we discuss the use of AutoML methods and tools to simultaneously optimize all stages of the machine learning pipeline.}
}
@article{LIU2024102300,
title = {Emotion detection for misinformation: A review},
journal = {Information Fusion},
volume = {107},
pages = {102300},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102300},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000782},
author = {Zhiwei Liu and Tianlin Zhang and Kailai Yang and Paul Thompson and Zeping Yu and Sophia Ananiadou},
keywords = {Sentiment analysis, Emotion detection, Misinformation, Rumor, Fake news, Stance detection},
abstract = {With the advent of social media, an increasing number of netizens are sharing and reading posts and news online. However, the huge volumes of misinformation (e.g., fake news and rumors) that flood the internet can adversely affect people’s lives, and have resulted in the emergence of rumor and fake news detection as a hot research topic. The emotions and sentiments of netizens, as expressed in social media posts and news, constitute important factors that can help to distinguish fake news from genuine news and to understand the spread of rumors. This article comprehensively reviews emotion-based methods for misinformation detection, with a particular focus on advanced fusion methods. We begin by explaining the strong links between emotions and misinformation. We subsequently provide a detailed analysis of a range of misinformation detection methods that employ a variety of emotion, sentiment and stance-based features, and describe their strengths and weaknesses. Finally, we discuss a number of ongoing challenges in emotion-based misinformation detection based on large language models, and suggest future research directions, including data collection (multi-platform, multilingual), annotation, benchmark, multimodality, and interpretability.}
}
@article{KO2025478,
title = {MDCKE: Multimodal deep-context knowledge extractor that integrates contextual information},
journal = {Alexandria Engineering Journal},
volume = {119},
pages = {478-492},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.01.119},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825001474},
author = {Hyojin Ko and Joon Yoo and Ok-Ran Jeong},
keywords = {Multimodal knowledge graph, Multimodal data fusing, Information extraction, Named entity recognition, Relation extraction, Natural language processing, Image processing},
abstract = {Extraction of comprehensive information from diverse data sources remains a significant challenge in contemporary research. Although multimodal Named Entity Recognition (NER) and Relation Extraction (RE) tasks have garnered significant attention, existing methods often focus on surface-level information, underutilizing the potential depth of the available data. To address this issue, this study introduces a Multimodal Deep-Context Knowledge Extractor (MDCKE) that generates hierarchical multi-scale images and captions from original images. These connectors between image and text enhance information extraction by integrating more complex data relationships and contexts to build a multimodal knowledge graph. Captioning precedes feature extraction, leveraging semantic descriptions to align global and local image features and enhance inter- and intramodality alignment. Experimental validation on the Twitter2015 and Multimodal Neural Relation Extraction (MNRE) datasets demonstrated the novelty and accuracy of MDCKE, resulting in an improvement in the F1-score by up to 5.83% and 26.26%, respectively, compared to State-Of-The-Art (SOTA) models. MDCKE was compared with top models, case studies, and simulations in low-resource settings, proving its flexibility and efficacy. An ablation study further corroborated the contribution of each component, resulting in an approximately 6% enhancement in the F1-score across the datasets.}
}
@article{DING2024103637,
title = {A plug-and-play adapter for consistency identification in task-oriented dialogue systems},
journal = {Information Processing & Management},
volume = {61},
number = {3},
pages = {103637},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103637},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323003746},
author = {Zeyuan Ding and Zhihao Yang and Hongfei Lin},
keywords = {Consistency identification, Task-oriented dialogue, Knowledge injection, Fusion mechanism, Adapter module},
abstract = {Task-oriented Dialogue system (ToD) has gained significant attention due to its aim to assist users in accomplishing various tasks. However, the neural network-based dialogue system is like a black box, which may lead to erroneous responses and result in an unfriendly user experience. To address this issue, consistency identification is proposed to prevent generating inconsistent responses. However, the existing consistency identification methods require frequent interaction with the knowledge base, making them susceptible to the introduction of noise during the knowledge base fusion process, ultimately leading to a decline in performance. In this paper, we propose a plug-and-play method for consistency identification, which can introduce external knowledge into the internal reasoning process of the pre-trained language model (PLM) without modifying PLM’s structure. Additionally, we design a new fusion mechanism that effectively fuses the knowledge base information related to the current utterance, which helps the model avoid introducing noise from the irrelevant knowledge base. The experimental results demonstrate that our method achieves state-of-the-art performance on the consistency identification task, improving F1 scores by 2.9% absolute points over the previous methods. Finally, we investigate different knowledge base fusion methods and provide extensive experiments to show the advantages of our proposed method.}
}
@article{ZHAO2023100005,
title = {When brain-inspired AI meets AGI},
journal = {Meta-Radiology},
volume = {1},
number = {1},
pages = {100005},
year = {2023},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2023.100005},
url = {https://www.sciencedirect.com/science/article/pii/S295016282300005X},
author = {Lin Zhao and Lu Zhang and Zihao Wu and Yuzhong Chen and Haixing Dai and Xiaowei Yu and Zhengliang Liu and Tuo Zhang and Xintao Hu and Xi Jiang and Xiang Li and Dajiang Zhu and Dinggang Shen and Tianming Liu},
abstract = {Artificial General Intelligence (AGI) has been a long-standing goal of humanity, with the aim of creating machines capable of performing any intellectual task that humans can do. To achieve this, AGI researchers draw inspiration from the human brain and seek to replicate its principles in intelligent machines. Brain-inspired artificial intelligence is a field that has emerged from this endeavor, combining insights from neuroscience, psychology, and computer science to develop more efficient and powerful AI systems. In this article, we provide a comprehensive overview of brain-inspired AI from the perspective of AGI. We begin with the current progress in brain-inspired AI and its extensive connection with AGI. We then cover the important characteristics for both human intelligence and AGI (e.g., scaling, multimodality, and reasoning). We discuss important technologies toward achieving AGI in current AI systems, such as in-context learning and prompt tuning. We also investigate the evolution of AGI systems from both algorithmic and infrastructural perspectives. Finally, we explore the limitations and future of AGI.}
}
@article{TRAPPEY2020101980,
title = {Identify trademark legal case precedents - Using machine learning to enable semantic analysis of judgments},
journal = {World Patent Information},
volume = {62},
pages = {101980},
year = {2020},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.101980},
url = {https://www.sciencedirect.com/science/article/pii/S0172219019300638},
author = {Charles V. Trappey and Amy J.C. Trappey and Bo-Hung Liu},
keywords = {Trademark infringement, Clustering, Latent dirichlet allocation, Precedence analysis, Recommendation platform},
abstract = {Legal case precedents have a considerable impact on the development of litigation strategies. This research uses the neural network language modeling (NNLM) approach to analyze and identify judgment documents of US trademark (TM) litigation cases as precedents of a given target case. In this research, the NNLM has been trained using 4835 TM litigation documents. There are more than 800,000 words in the entire training text set including more than 150,000 vocabularies. The words in TM legal documents are vectorized to train the NN model for e-discovery of semantically correlated precedents and their features. Specifically, non-supervised machine learning (ML) methods, including clustering and Latent Dirichlet Allocation (LDA), are applied to form the TM legal document clusters, topics, and key terminologies used to characterize the TM case descriptions and precedents. The definition of the clusters, topics and corresponding key terms enhance the ability of the system to recommend and explain similar case judgments for any given TM case of interest or a cease and desist letter with detailed claims of infringement. Further, the intelligent approach provides macro and micro views for companies to research TM litigation trends as a means to better protect their brand equity.}
}
@article{DUNSIN2025100299,
title = {Reinforcement learning for an efficient and effective malware investigation during cyber incident response},
journal = {High-Confidence Computing},
pages = {100299},
year = {2025},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2025.100299},
url = {https://www.sciencedirect.com/science/article/pii/S2667295225000030},
author = {Dipo Dunsin and Mohamed Chahine Ghanem and Karim Ouazzane and Vassil Vassilev},
keywords = {Cyber incident, Digital forensics, Artificial intelligence, Reinforcement learning, Markov Chain, MDP, DFIR, Malware, Incident response},
abstract = {The ever-escalating prevalence of malware is a serious cybersecurity threat, often requiring advanced post-incident forensic investigation techniques. This paper proposes a framework to enhance malware forensics by leveraging reinforcement learning (RL). The approach combines heuristic and signature-based methods, supported by RL through a unified MDP model, which breaks down malware analysis into distinct states and actions. This optimisation enhances the identification and classification of malware variants. The framework employs Q-learning and other techniques to boost the speed and accuracy of detecting new and unknown malware, outperforming traditional methods. We tested the experimental framework across multiple virtual environments infected with various malware types. The RL agent collected forensic evidence and improved its performance through Q-tables and temporal difference learning. The epsilon-greedy exploration strategy, in conjunction with Q-learning updates, effectively facilitated transitions. The learning rate depended on the complexity of the MDP environment: higher in simpler ones for quicker convergence and lower in more complex ones for stability. This RL-enhanced model significantly reduced the time required for post-incident malware investigations, achieving a high accuracy rate of 94% in identifying malware. These results indicate RL’s potential to revolutionise post-incident forensics investigations in cybersecurity. Future work will incorporate more advanced RL algorithms and large language models (LLMs) to further enhance the effectiveness of malware forensic analysis.}
}
@article{ZHU2025125976,
title = {Pre-training graph autoencoder incorporating hierarchical topology knowledge},
journal = {Expert Systems with Applications},
volume = {265},
pages = {125976},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125976},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424028434},
author = {Hongyin Zhu and Yakun Li and Luyang Liu and Haonan Tong and Qunyang Lin and Chuang Zhang},
keywords = {Graph pre-training, Hierarchical topology knowledge, Graph autoencoder, Subgraph regularization},
abstract = {Existing graph pre-training methods demonstrate their ability to generate vertex representations beneficial for downstream machine-learning tasks. However, the quality of these representations is often influenced by the choice of learning algorithm. While masking strategies are commonly employed, random masks can lead to noisy neighborhoods and incomplete graph topology, hampering learning efficiency and increasing computational costs. When a significant portion of neighbors are randomly masked, the central vertex lacks sufficient contextual information. To address this challenge, we integrate hierarchical topology knowledge to enhance masking strategies, thereby preserving the major topology and minimizing training costs. Our method leverages 3 distinct masking techniques: global-aware, local-aware, and element-aware masking. Global-aware masking encourages the model to capture the graph’s overall topology, while local-aware masking focuses on capturing vertex interactions. Element-aware masking enhances the model’s robustness against noise and structural variations. By incorporating these 3 strategies, our method mitigates the impact of random noise and structural variations during training, yielding more robust and effective vertex representations. To further optimize the model, we introduce a fine-grained subgraph regularization, which reduces the model’s parameter count by penalizing divergence in subgraph embeddings across multiple views. We assess the effectiveness of our approach on 3 datasets, highlighting its performance improvements while achieving a reduction of 25% in model parameters.}
}
@article{QIU2024105863,
title = {Semantic information extraction and search of mineral exploration data using text mining and deep learning methods},
journal = {Ore Geology Reviews},
volume = {165},
pages = {105863},
year = {2024},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2023.105863},
url = {https://www.sciencedirect.com/science/article/pii/S0169136823005796},
author = {Qinjun Qiu and Miao Tian and Liufeng Tao and Zhong Xie and Kai Ma},
keywords = {Mineral exploration data, Semantic search, Text mining, Topic extraction, Knowledge graph},
abstract = {Large-scale mineral resource reports offer a wealth of information for geological knowledge mining and mineral explorers’ knowledge discovery. The geological conditions in which mineral deposits develop may be learned a great deal from these mineral exploration reports. Mineral exploration data may be queried and aggregated to effectively mitigate future exploration risks and reduce costs. However, due to the reports being presented in unstructured textual format, it becomes challenging to extract valuable geological data without manually scanning through a vast number of reports. This laborious process poses difficulties for geologists. To address this issue, this paper proposes a system that extracts a set of geologically relevant keywords/keyphrases of each chapter from each mineral exploration reposts using latent Dirichlet allocation (LDA), develops a topic graph, recognizes the geological entity and related relations for constructing a knowledge graph, and uses visualization of those graphs (e.g., topic graphs and knowledge graphs) to explore the contents of the report. The text mining and machine learning technique described here serves as the foundation for future research into incorporating semantic analysis into geological information extraction. The findings of this study show how automated text analysis may help with the quick processing of huge quantities of reports in order to identify target mineral systems and their related geological location and rock mineral composition. The suggested approaches can quickly and reliably convert mineral exploration data (e.g., text, figure, and table) into a structured form, which is a hitherto untouched field in geological knowledge mining.}
}
@article{WANG2024128580,
title = {Zero-shot text classification with knowledge resources under label-fully-unseen setting},
journal = {Neurocomputing},
volume = {610},
pages = {128580},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128580},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224013511},
author = {Yuqi Wang and Wei Wang and Qi Chen and Kaizhu Huang and Anh Nguyen and Suparna De},
keywords = {Zero-shot learning, Knowledge graph embedding, Natural language processing, Textual analysis, Multi-class classification},
abstract = {Classification techniques are at the heart of many real-world applications, e.g. sentiment analysis, recommender systems and automatic text annotation, to process and analyse large-scale textual data in multiple fields. However, the effectiveness of natural language processing models can only be confirmed when a large amount of up-to-date training data is available. An unprecedented amount of data is continuously created, and new topics are introduced, making it less likely or even infeasible to collect labelled samples covering all topics for training models. We attempt to study the extreme case: there is no labelled data for model training, and the model, without being adapted to any specific dataset, will be directly applied to the testing samples. We propose a transformer-based framework to encode sentences in a contextualised way and leverage the existing knowledge resources, i.e. ConceptNet and WordNet, to integrate both descriptive and structural knowledge for better performance. To enhance the robustness of the model, we design an adversarial example generator based on relations from external knowledge bases. The framework is evaluated on both general and specific domain text classification datasets. Results show that the proposed framework can outperform the existing competitive state-of-the-art baselines, delivering new benchmark results.}
}
@article{KAHN2025115027,
title = {More than 50 years of consumer behavior research: What will the future look like?},
journal = {Journal of Business Research},
volume = {186},
pages = {115027},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.115027},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324005319},
author = {Barbara E. Kahn and Anne V. Wilson},
abstract = {To understand how consumer behavior research has evolved and what the future might hold, we first summarize the intellectual trajectory of scholarship in the area and briefly describe the research paradigms that developed over time. We report on the trends in research topics over the years and the “hot topics” projected for the near future. We also discuss the internal and external forces that fundamentally shape research and scholars. These forces provide both constraints and opportunities that will define the future of the field. We predict that some forces, unfortunately, incentivize scholars to examine more minor, less influential research questions. However, new sources of data and areas of inquiry are simultaneously providing opportunities for innovation and creativity in exploring how consumer behavior will change or evolve in response to macroeconomic factors, such as social issues, political movements, or rapid technological advances.}
}
@article{TANG2024109543,
title = {ResiAdvNet: A named entity recognition model for potato diseases and pests based on progressive residual structures and adversarial training},
journal = {Computers and Electronics in Agriculture},
volume = {227},
pages = {109543},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109543},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924009347},
author = {Wentao Tang and Xianhuan Wen and Miao Li and Yuqi Chen and Zelin Hu},
keywords = {Named entity recognition, ResiAdvNet, Potato disease and pest, Progressive residual structure, Adversarial training},
abstract = {Conventional named entity recognition methods based on pretrained models often focus on utilizing the output of the final layer of a pretrained model while ignoring the linguistic features embedded in its internal layers. To utilize pretrained models more fully, this paper proposes a named entity recognition model called ResiAdvNet, which combines progressive residual structures with adversarial training. This proposed model is applied to potato disease and pest identification. MacBERT is utilized as the pretrained model and residual blocks are used to aggregate the outputs of its internal layers, obtaining a final output that emphasizes information from all layers. This output is subsequently fed into a bidirectional long-short term memory network for context modeling and finally passed through a conditional random field to obtain the globally optimal tagging sequence. Additionally, adversarial training is introduced as a means to enhance model robustness. By introducing adversarial examples during training, the model learns more robust feature representations, thereby improving its performance when facing unknown inputs. ResiAdvNet was tested with on a custom dataset called PpdKED over five trials with an average F1 score of approximately 0.9225, significantly outperforming other models. Experimental results demonstrate that the proposed model can efficiently extract entities related to potato diseases and pests, laying a solid foundation for the subsequent tasks of relation extraction and knowledge graph construction.}
}
@article{ARIKKAT2024103990,
title = {OSTIS: A novel Organization-Specific Threat Intelligence System},
journal = {Computers & Security},
volume = {145},
pages = {103990},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103990},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824002955},
author = {Dincy R. Arikkat and Vinod P. and Rafidha Rehiman K.A. and Serena Nicolazzo and Antonino Nocera and Georgiana Timpau and Mauro Conti},
keywords = {Cyber Threat Intelligence, Cybersecurity knowledge graph, Organization-specific threat intelligence, Relation extraction, Named entity recognition, Natural language processing, Explainable AI},
abstract = {With the increasing complexity and frequency of cyber attacks, organizations recognize the need for a proactive and targeted approach to safeguard their digital assets and operations. Every industry faces a distinct array of threats shaped by factors such as its industrial objective, geographic footprint, workforce size, revenue, partnerships, and the extent of its digital assets. This results in a wide heterogeneity in threat landscapes, which necessitates tailored threat intelligence sources. While some security practitioners may gravitate towards extensive sources, relying solely on volume-based solutions often leads to “alert fatigue”. For this reason, organization-specific threat intelligence has acquired a growing importance in cybersecurity defense. This work presents a complete and novel framework called OSTIS (Organization-Specific Threat Intelligence System) for generating and managing organization-specific Cyber Threat Intelligence (CTI) data. Our approach identifies reliable security blogs from which we gather CTI data through a custom and focused Web Crawler. Relevant content from such sources is, then, identified and extracted using automated deep-learning models. Moreover, our AI-driven solution maps CTI data to specific domain scenarios, such as education, finance, government, healthcare, industrial control systems, and IoT. To validate and gain insights from the trained models, we also include an explainable AI (XAI, for short) task carried out by leveraging the SHapley Additive exPlanations (SHAP) tool. This allows us to interpret the prediction process and discern influential content from data. The last step of our framework consists of the generation of an Organization Specific Threat Intelligence Knowledge Graph (OSTIKG), empowering organizations to identify and visualize attack patterns and incidents, promptly. To create this graph, we develop and adapt several techniques to extract diverse entities, including malware groups, campaigns, attack types, malware types, software tools, and so forth, and to identify relationships among them. Finally, through an extensive experimental campaign, we certify the validity and performance of all the components of our framework, which shows a 0.84 F1-score in the identification of relevant content, a 0.93 F1-score for the domain classification, and a 0.95 and 0.89 F1-score in the identification of entities and relations to build our OSTIKG graph.}
}
@article{ZHANG2025125059,
title = {Deep generative models in energy system applications: Review, challenges, and future directions},
journal = {Applied Energy},
volume = {380},
pages = {125059},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.125059},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924024437},
author = {Xiangyu Zhang and Andrew Glaws and Alexandre Cortiella and Patrick Emami and Ryan N. King},
keywords = {Generative artificial intelligence, Deep generative models, Energy systems, Smart grid},
abstract = {In recent years, with the advent of mature machine learning products like ChatGPT, Stable Diffusion, and Sora, the world has witnessed tremendous changes driven by the rapid development of generative artificial intelligence (GAI). Beyond applications in text, speech, image, and video creation, deep generative models (DGMs) underpinning these cutting-edge technologies have also been employed by domain researchers to address scientific and engineering challenges. This paper aims to fill a gap in the research community by providing a comprehensive review of how DGMs have been utilized in energy system applications. Based on five of the most popular DGMs, we review and categorize 228 research articles into five focus areas: data generation, forecasting, situational awareness, modeling, and optimal decision-making. Through this classification, we uncover trends in how DGMs are employed for each type of problem, highlighting GAI techniques that contribute to breakthroughs over traditional methods. We discuss limitations in existing literature, engineering challenges, and propose future directions, all tailored to the unique nature of problems in energy system engineering. Our goal is to offer insights for energy system domain researchers, providing a comprehensive view of existing studies and potential future opportunities.}
}
@article{LOUGE2025126641,
title = {Events-based semantic services composition in Industry 4.0 using Asset Administration Shell meta-model for digital twins},
journal = {Expert Systems with Applications},
volume = {271},
pages = {126641},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126641},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425002635},
author = {Thierry Louge and Sina Namaki Araghi and Mohamed Hedi Karray and Arkopaul Sarkar},
keywords = {Ontologies, Semantic services, Asset Administration Shell, Industry 4.0, Digital twins},
abstract = {Since the emergence of the Semantic Web concept, considerable work has focused on service composition using ontology-based approaches. Meanwhile, the concept of Industry 4.0 has emerged, emphasizing the benefits of utilizing data and computing devices in close proximity to production lines, exemplified by concepts like digital twins. However, these two fields rarely intersect, and the requirements for integrating domain-specific knowledge into business processes with event feedback during processes execution differ between these contexts. With the recent advancements in the semantization of industrial standards, such as the Asset Administration Shell, this work explores the elements of a semantic model for describing equipment, enabling the semantic composition of equipment as services. We propose an ontology, COMPAAS, designed to facilitate the composition of production lines that can react to events reported by their components, allowing the system to adjust its behavior accordingly. This approach also addresses the removal and addition of hardware or software elements within the chain, and the entire concept is validated through a minimal use case that demonstrates the improved flexibility of the production line in response to potential disturbances.}
}
@article{DURAN2025112310,
title = {A review on artificial intelligence applications for facades},
journal = {Building and Environment},
volume = {269},
pages = {112310},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2024.112310},
url = {https://www.sciencedirect.com/science/article/pii/S0360132324011521},
author = {Ayca Duran and Christoph Waibel and Valeria Piccioni and Bernd Bickel and Arno Schlueter},
keywords = {Literature review, Building facades, Computer vision, Machine learning, Deep learning},
abstract = {This review applies a transformer-based topic model to reveal trends and relationships in Artificial Intelligence (AI)-driven facade research, with a focus on architectural, environmental, and structural aspects. AI methods reviewed include Machine Learning (ML), Deep Learning (DL), and Computer Vision (CV). Overall, a significantly growing interest in applying AI methods can be observed across all research areas. However, noticeable differences exist between the three topics. While CV and DL techniques are applied to image data in research on the architectural design of facades, research on environmental aspects of facades often uses numerical data with relatively small datasets and classical ML models. Research on facade structure also tends to use image data but also incorporates numerical performance prediction. A major limitation remains a lack of generalizability, which could be addressed by more comprehensive datasets and novel DL techniques. These include concepts such as Physics-Informed Neural Networks, where domain knowledge is integrated into hybrid data-driven models, and multi-modal diffusion models, which offer generative modeling capabilities to support inverse and forward design tasks. The trends and directions outlined in this review suggest that AI will continue to advance facade research and, in line with other domains, has the potential to achieve a level of maturity suitable for adoption beyond academia and into practice.}
}
@article{DAVID2024216,
title = {Decentralized research data management: introducing SoVisu+},
journal = {Procedia Computer Science},
volume = {249},
pages = {216-223},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.067},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032782},
author = {Reymond David and Tabariès Alaric},
keywords = {SoVisu+, Decentralized research data management, CRISalid consortium, Knowledge graph, Self-archiving, Solid Pods, Linked Data, Research evaluation practices, Open science, Community collaboration},
abstract = {This research proposes SoVisu+, a new system for managing research data in a decentralized way, building upon the foundation laid by the CRISalid consortium, a community-driven approach to research information management. Traditional, centralized systems have issues like fragmented data, unchecked and outdated information. SoVisu+ tackles these problems by giving researchers more control over their data and fostering collaboration to build a shared knowledge graph. Researchers benefit from features like self-archiving accompaniment, AI driven helpers and real-time profile views, while institutions gain access to more reliable and comprehensive data. SoVisu+ expects to employ cutting-edge technologies like Solid Pods and Linked Data to streamline data management and improve research evaluation practices. Overall, SoVisu+ aims to create a more transparent and collaborative research ecosystem that ultimately accelerates scientific progress.}
}
@incollection{LENCI2024,
title = {Artificial Intelligence and Language☆},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00241-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002416},
author = {Alessandro Lenci and Andrea Vestrucci},
keywords = {Artificial intelligence (AI), Symbolic AI, Subsymbolic AI, Hybrid AI, Large language models (LLMs), Neuro-symbolic AI, Natural language inference, Machine learning (ML), Natural language reasoning, Inferential tasks, Semantic gap},
abstract = {This article explores the roles of three primary paradigms of Artificial Intelligence (AI)—symbolic, subsymbolic, and hybrid—in natural language reasoning and inferential tasks. Symbolic AI leverages explicit knowledge representations and logical rules to provide high interpretability and precision, making it suitable for tasks requiring clear and verifiable reasoning. However, its rigidity and lack of scalability limit its effectiveness in handling the nuanced nature of human language. Subsymbolic AI, driven by machine learning, deep learning, and large language models, excels in flexibility and adaptability by learning from vast datasets. While capable of generating human-like text and performing complex inferences, subsymbolic AI faces challenges such as semantic gaps, susceptibility to biases, and a lack of transparency in decision-making processes. Hybrid AI combines the strengths of both symbolic and subsymbolic approaches, aiming to create more robust and reliable systems. By integrating explicit reasoning capabilities with data-driven learning, hybrid models enhance interpretability while maintaining adaptability to diverse linguistic contexts. This article concludes by emphasizing the need for continued research to refine these integrations, address current limitations, and pave the way for more intelligent and trustworthy language reasoning systems.}
}
@article{RAGO2025104291,
title = {Argumentative review aggregation and dialogical explanations},
journal = {Artificial Intelligence},
volume = {340},
pages = {104291},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104291},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000104},
author = {Antonio Rago and Oana Cocarascu and Joel Oksanen and Francesca Toni},
keywords = {Argumentation, Argument mining, Review aggregation, Dialogical interaction, Conversational explanation},
abstract = {The aggregation of online reviews is one of the dominant methods of quality control for users in various domains, from retail to entertainment. Consequently, explainable aggregation of reviews is increasingly sought-after. We introduce quantitative argumentation technology to this setting, towards automatically generating reasoned review aggregations equipped with dialogical explanations. To this end, we define a novel form of argumentative dialogical agent (ADA), using ontologies to harbour information from reviews into argumentation frameworks. These agents may then be evaluated with a quantitative argumentation semantics and used to mediate the generation of dialogical explanations for item recommendations based on the reviews. We show how to deploy ADAs in three different contexts in which argumentation frameworks are mined from text, guided by ontologies. First, for hotel recommendations, we use a human-authored ontology and exemplify the potential range of dialogical explanations afforded by ADAs. Second, for movie recommendations, we empirically evaluate an ADA based on a bespoke ontology (extracted semi-automatically, by natural language processing), by demonstrating that its quantitative evaluations, which are shown to satisfy desirable theoretical properties, are comparable with those on a well-known movie review aggregation website. Finally, for product recommendation in e-commerce, we use another bespoke ontology (extracted fully automatically, by natural language processing, from a website's reviews) to construct an ADA which is then empirically evaluated favourably against review aggregations from the website.}
}
@article{FENG2025110897,
title = {Data-driven supply chains mapping and disruption analysis: The case of automotive SoC enterprises in China},
journal = {Computers & Industrial Engineering},
volume = {201},
pages = {110897},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.110897},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225000427},
author = {Jiawei Feng and Mengsi Cai and Fangze Dai and Shuo Liu and Tianci Bu and Xiaoyu Zhang and Huijun Zheng and Xin Lu},
keywords = {Supply chain, Complex networks, Interaction disruption models, Cascade failure},
abstract = {Effective modeling of modern supply chains is crucial for improving visibility, mitigating systemic risks, and developing resilient strategies. However, data limitations imposed by industry sensitivity and competition have hindered research in this area. Combining big data and complex network theory, this study introduces an Open Supplier Knowledge Extraction and Complement (OSKEC) approach, incorporating cross-domain named entity recognition, firm entity fuzzy matching, and supplier relation inferring, to construct highly reliable supply chain networks from limited information. Applying OSKEC on the Chinese automotive Systems-on-Chips (SoCs) industry approves its effectiveness in enhancing supply chain visibility and resilience. Topological analysis for the built supply chain network reveals a clear scale-free degree distribution, implying a strong heterogeneity for the interdependence of entities in the network. Specifically, NVIDIA, Qualcomm, and Mobileye occupy the majority share of the automotive SoC market in China, while local enterprises only hold a smaller portion. We further develop two interaction disruption models (IDMs) which simulate the impact of various disturbances on firms with different recovery capacities and risk-transfer strategies, and find that a risk-transfer enterprise strategy may lead to a rapid collapse of the network in the early stages of disruptions. In general, the study improves the understanding of modern supply chain dynamics and inform effective risk management strategies in the Chinese automotive SoC sector.}
}
@article{LIU2024,
title = {Evaluating Medical Entity Recognition in Health Care: Entity Model Quantitative Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/59782},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001455},
author = {Shengyu Liu and Anran Wang and Xiaolei Xiu and Ming Zhong and Sizhu Wu},
keywords = {natural language processing, NLP, model evaluation, macrofactors, medical named entity recognition models},
abstract = {Background
Named entity recognition (NER) models are essential for extracting structured information from unstructured medical texts by identifying entities such as diseases, treatments, and conditions, enhancing clinical decision-making and research. Innovations in machine learning, particularly those involving Bidirectional Encoder Representations From Transformers (BERT)–based deep learning and large language models, have significantly advanced NER capabilities. However, their performance varies across medical datasets due to the complexity and diversity of medical terminology. Previous studies have often focused on overall performance, neglecting specific challenges in medical contexts and the impact of macrofactors like lexical composition on prediction accuracy. These gaps hinder the development of optimized NER models for medical applications.
Objective
This study aims to meticulously evaluate the performance of various NER models in the context of medical text analysis, focusing on how complex medical terminology affects entity recognition accuracy. Additionally, we explored the influence of macrofactors on model performance, seeking to provide insights for refining NER models and enhancing their reliability for medical applications.
Methods
This study comprehensively evaluated 7 NER models—hidden Markov models, conditional random fields, BERT for Biomedical Text Mining, Big Transformer Models for Efficient Long-Sequence Attention, Decoding-enhanced BERT with Disentangled Attention, Robustly Optimized BERT Pretraining Approach, and Gemma—across 3 medical datasets: Revised Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA), BioCreative V CDR, and Anatomical Entity Mention (AnatEM). The evaluation focused on prediction accuracy, resource use (eg, central processing unit and graphics processing unit use), and the impact of fine-tuning hyperparameters. The macrofactors affecting model performance were also screened using the multilevel factor elimination algorithm.
Results
The fine-tuned BERT for Biomedical Text Mining, with balanced resource use, generally achieved the highest prediction accuracy across the Revised JNLPBA and AnatEM datasets, with microaverage (AVG_MICRO) scores of 0.932 and 0.8494, respectively, highlighting its superior proficiency in identifying medical entities. Gemma, fine-tuned using the low-rank adaptation technique, achieved the highest accuracy on the BioCreative V CDR dataset with an AVG_MICRO score of 0.9962 but exhibited variability across the other datasets (AVG_MICRO scores of 0.9088 on the Revised JNLPBA and 0.8029 on AnatEM), indicating a need for further optimization. In addition, our analysis revealed that 2 macrofactors, entity phrase length and the number of entity words in each entity phrase, significantly influenced model performance.
Conclusions
This study highlights the essential role of NER models in medical informatics, emphasizing the imperative for model optimization via precise data targeting and fine-tuning. The insights from this study will notably improve clinical decision-making and facilitate the creation of more sophisticated and effective medical NER models.}
}
@article{RIHM2024100004,
title = {Transforming research laboratories with connected digital twins},
journal = {Nexus},
volume = {1},
number = {1},
pages = {100004},
year = {2024},
issn = {2950-1601},
doi = {https://doi.org/10.1016/j.ynexs.2024.100004},
url = {https://www.sciencedirect.com/science/article/pii/S2950160124000020},
author = {Simon D. Rihm and Jiaru Bai and Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {laboratory automation, comprehensive digital twins, connected digital twins, AI scientist, dynamic knowledge graphs, self-driving laboratories, knowledge discovery, smart lab, lab management, research facility},
abstract = {To substantially expedite scientific discovery, research laboratories need to be further automated. In this regard, the scientific community envisions an “artificial intelligence scientist” capable of planning, conducting, and assessing experiments based on higher-order goals and reasoning capabilities. We argue that a paradigm shift is necessary to bridge the gap between the current trajectory of lab automation and this vision. Adopting a systems perspective reveals several key challenges that must be addressed. We argue that achieving holistic lab automation requires a network of comprehensive distributed digital twins grounded in a universal knowledge model. Dynamic knowledge graphs are expected to play an important role, and we introduce a framework encompassing all aspects of experimental research, including infrastructure and peripheries. Our framework considers human-machine interactions from the outset to empower a goal-driven approach that brings automation to autonomy.}
}
@article{AHMADI2024104211,
title = {3D scene generation for zero-shot learning using ChatGPT guided language prompts},
journal = {Computer Vision and Image Understanding},
volume = {249},
pages = {104211},
year = {2024},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2024.104211},
url = {https://www.sciencedirect.com/science/article/pii/S1077314224002923},
author = {Sahar Ahmadi and Ali Cheraghian and Townim Faisal Chowdhury and Morteza Saberi and Shafin Rahman},
keywords = {Zero-shot learning, Deep learning, Point cloud object, Contrastive learning},
abstract = {Zero-shot learning in the realm of 3D point cloud data remains relatively unexplored compared to its 2D image counterpart. This domain introduces fresh challenges due to the absence of robust pre-trained feature extraction models. To tackle this, we introduce a prompt-guided method for 3D scene generation and supervision, enhancing the network’s ability to comprehend the intricate relationships between seen and unseen objects. Initially, we utilize basic prompts resembling scene annotations generated from one or two point cloud objects. Recognizing the limited diversity of basic prompts, we employ ChatGPT to expand them, enriching the contextual information within the descriptions. Subsequently, leveraging these descriptions, we arrange point cloud objects’ coordinates to fabricate augmented 3D scenes. Lastly, employing contrastive learning, we train our proposed architecture end-to-end, utilizing pairs of 3D scenes and prompt-based captions. We posit that 3D scenes facilitate more efficient object relationships than individual objects, as demonstrated by the effectiveness of language models like BERT in contextual understanding. Our prompt-guided scene generation method amalgamates data augmentation and prompt-based annotation, thereby enhancing 3D ZSL performance. We present ZSL and generalized ZSL results on both synthetic (ModelNet40, ModelNet10, and ShapeNet) and real-scanned (ScanOjbectNN) 3D object datasets. Furthermore, we challenge the model by training with synthetic data and testing with real-scanned data, achieving state-of-the-art performance compared to existing 2D and 3D ZSL methods in the literature. Codes and models are available at: https://github.com/saharahmadisohraviyeh/ChatGPT_ZSL_3D.}
}
@article{SAKONG2024121165,
title = {Higher-order knowledge-enhanced recommendation with heterogeneous hypergraph multi-attention},
journal = {Information Sciences},
volume = {680},
pages = {121165},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121165},
url = {https://www.sciencedirect.com/science/article/pii/S002002552401079X},
author = {Darnbi Sakong and Viet Hung Vu and Thanh Trung Huynh and Phi Le Nguyen and Hongzhi Yin and Quoc Viet Hung Nguyen and Thanh Tam Nguyen},
keywords = {Hypergraph embedding, Knowledge-based recommender systems, Self-supervised learning, Graph-based collaborative filtering},
abstract = {Recent advancements in recommender systems have focused on integrating knowledge graphs (KGs) to leverage their auxiliary information. The core idea of KG-enhanced recommenders is to incorporate rich semantic information for more accurate recommendations. However, two main challenges persist: i) Neglecting complex higher-order interactions in the KG-based user-item network, potentially leading to sub-optimal recommendations, and ii) Dealing with the heterogeneous modalities of input sources, such as user-item bipartite graphs and KGs, which may introduce noise and inaccuracies. To address these issues, we present a novel Knowledge-enhanced Heterogeneous Hypergraph Recommender System (KHGRec). KHGRec captures group-wise characteristics of both the interaction network and the KG, modeling complex connections in the KG. Using a collaborative knowledge heterogeneous hypergraph (CKHG), it employs two hypergraph encoders to model group-wise interdependencies and ensure explainability. Additionally, it fuses signals from the input graphs with cross-view self-supervised learning and attention mechanisms. Extensive experiments on four real-world datasets show our model's superiority over various state-of-the-art baselines, with an average 5.18% relative improvement. Additional tests on noise resilience, missing data, and cold-start problems demonstrate the robustness of our KHGRec framework. Our model and evaluation datasets are publicly available at https://github.com/viethungvu1998/KHGRec.}
}
@article{JONNAKUTI2024100707,
title = {PolyAMiner-Bulk is a deep learning-based algorithm that decodes alternative polyadenylation dynamics from bulk RNA-seq data},
journal = {Cell Reports Methods},
volume = {4},
number = {2},
pages = {100707},
year = {2024},
issn = {2667-2375},
doi = {https://doi.org/10.1016/j.crmeth.2024.100707},
url = {https://www.sciencedirect.com/science/article/pii/S2667237524000213},
author = {Venkata Soumith Jonnakuti and Eric J. Wagner and Mirjana Maletić-Savatić and Zhandong Liu and Hari Krishna Yalamanchili},
keywords = {alternative polyadenylation (APA), post-transcriptional regulation, deep learning, large language model (LLM), bioinformatics, computational biology, gene regulation},
abstract = {Summary
Alternative polyadenylation (APA) is a key post-transcriptional regulatory mechanism; yet, its regulation and impact on human diseases remain understudied. Existing bulk RNA sequencing (RNA-seq)-based APA methods predominantly rely on predefined annotations, severely impacting their ability to decode novel tissue- and disease-specific APA changes. Furthermore, they only account for the most proximal and distal cleavage and polyadenylation sites (C/PASs). Deconvoluting overlapping C/PASs and the inherent noisy 3′ UTR coverage in bulk RNA-seq data pose additional challenges. To overcome these limitations, we introduce PolyAMiner-Bulk, an attention-based deep learning algorithm that accurately recapitulates C/PAS sequence grammar, resolves overlapping C/PASs, captures non-proximal-to-distal APA changes, and generates visualizations to illustrate APA dynamics. Evaluation on multiple datasets strongly evinces the performance merit of PolyAMiner-Bulk, accurately identifying more APA changes compared with other methods. With the growing importance of APA and the abundance of bulk RNA-seq data, PolyAMiner-Bulk establishes a robust paradigm of APA analysis.}
}
@article{LU2025129145,
title = {A novel span-based Knowledge-enhanced framework for aspect sentiment triplet extraction},
journal = {Neurocomputing},
volume = {619},
pages = {129145},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129145},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224019167},
author = {Heng-yang Lu and Rui Cong and Wei Nie and Tian-ci Liu and Wei Fang},
keywords = {Aspect sentiment triplet extraction, Sentiment analysis, Attention mechanism, Common-sense knowledge},
abstract = {Aspect Sentiment Triplet Extraction (ASTE) is a subtask of Aspect-based Sentiment Analysis, which aims to identify all aspect sentiment triplets in given sentences. ASTE is an important research task to discover sentimental opinions in online social media. Existing ASTE models are usually based on pipeline or joint strategy. The pipeline-based methods may suffer from error propagation. The joint-based methods could avoid error propagation in an end-to-end manner, which have become the more popular choice. Among these joint-based ASTE models, the syntactic information from the dependency tree is widely used. However, the dependency tree may fail to connect the aspect and opinion terms in some cases, which causes the lack of interaction in triplets. Inspired by the work of Liang et al. on the Aspect Sentiment Classification (ASC) task, we integrate common-sense knowledge into the ASTE task, which can bring more important information to the modeling of context representation. To address these limitations, this paper first involves common-sense sentiment knowledge along with syntactic dependency knowledge to get better representations of contexts. The syntactic and common-sense knowledge can contribute to the interpretability when extracting the sentiment elements. We also introduce a self-attention mechanism based on the orthogonal loss function to better capture the interactions between words. We evaluated our model on four public datasets with recently proposed baselines in F1-score metric, especially 3.62%, 4.14%, 3.58% and 2.22% higher on 14Res, 14Lap, 15Res, and 16Res, compared to the SSJE approach, respectively. Experimental results show that our method outperforms SOTA models on three datasets.}
}
@article{GILLINGS2025100121,
title = {How humans and machines identify discourse topics: A methodological triangulation},
journal = {Applied Corpus Linguistics},
volume = {5},
number = {1},
pages = {100121},
year = {2025},
issn = {2666-7991},
doi = {https://doi.org/10.1016/j.acorp.2025.100121},
url = {https://www.sciencedirect.com/science/article/pii/S2666799125000048},
author = {Mathew Gillings and Sylvia Jaworska},
keywords = {Triangulation, Topic modelling, CADS, Concordance analysis, Close reading, LLMs},
abstract = {Identifying and exploring discursive topics in texts is of interest to not only linguists, but to researchers working across the full breadth of the social sciences. This paper reports on an exploratory study assessing the influence that analytical method has on the identification and labelling of topics, which might lead to varying interpretations of texts. Using a corpus of corporate sustainability reports, totalling 98,277 words, we asked 6 different researchers to interrogate the corpus and decide on its main ‘topics’ via four different methods: LLM-assisted analyses; topic modelling; concordance analysis; and close reading. These methods differ according to the amount of data that can be analysed at once, the amount of textual context available to the researcher, and the focus of the analysis (i.e., micro to macro). The paper explores how the identified topics differed both between analysts using the same method, and between methods. We conclude with a series of tentative observations regarding the benefits and limitations of each method, and offer recommendations for researchers in choosing which analytical technique to select.}
}
@article{CREMA2023104557,
title = {Advancing Italian biomedical information extraction with transformers-based models: Methodological insights and multicenter practical application},
journal = {Journal of Biomedical Informatics},
volume = {148},
pages = {104557},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104557},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002782},
author = {Claudio Crema and Tommaso Mario Buonocore and Silvia Fostinelli and Enea Parimbelli and Federico Verde and Cira Fundarò and Marina Manera and Matteo Cotta Ramusino and Marco Capelli and Alfredo Costa and Giuliano Binetti and Riccardo Bellazzi and Alberto Redolfi},
keywords = {Natural language processing, Deep learning, Biomedical text mining, Language model, Transformer},
abstract = {The introduction of computerized medical records in hospitals has reduced burdensome activities like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting data from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation by using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Transformers-based model. Moreover, we collected and leveraged three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77 %, Precision 83.16 %, Recall 86.44 %. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a “low-resource” approach. This allowed us to establish methodological guidelines that pave the way for Natural Language Processing studies in less-resourced languages.}
}
@article{SIMA2025110941,
title = {Small and medium-sized enterprise dedicated knowledge exploitation mechanism: A recommender system based on knowledge relatedness},
journal = {Computers & Industrial Engineering},
pages = {110941},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.110941},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225000877},
author = {Xingyu Sima and Thierry Coudert and Laurent Geneste and Aymeric {de Valroger}},
keywords = {Knowledge management (KM), Knowledge exploitation, Knowledge graph (KG), Knowledge relatedness, Recommender system (RS), Small and medium-sized enterprises (SMEs)},
abstract = {Knowledge is a vital asset for organizations, especially in today’s Industry 4.0 context with the ever-increasing amount of information being produced. Organizations must consider knowledge management (KM) to create a sustainable competitive advantage. Currently, KM is applied relatively well in large organizations; however, small and medium-sized enterprises (SMEs) encounter various constraints. Knowledge exploitation is a key phase in KM for the retrieval of relevant knowledge. Therefore, a recommender system (RS), which is a promising and widely used information technology (IT) tool, is proposed in this study, for SMEs to enable effective knowledge exploitation. The RS can be adapted to SME KM specificities and a dedicated RS based on knowledge relatedness derived from different information sources is proposed herein. The proposed RS enables the recommendation of knowledge item balancing: i) historical application data, that is, information regarding how items were related during past projects, and ii) initial relatedness knowledge, which represents the relationships between knowledge items defined by knowledge experts. The proposed RS was developed in collaboration with the Axsens-bte SME, who specialize in consultancy and training in the supply chain, Industry 4.0, and quality requirements management. The proposed RS improved SME KM processes and increased efficiency in terms of exploiting knowledge assets. This demonstrated the ability of the proposed RS to assist SMEs in efficiently and effectively navigating complex information environments.}
}
@article{XU2025107580,
title = {Learning protein language contrastive models with multi-knowledge representation},
journal = {Future Generation Computer Systems},
volume = {164},
pages = {107580},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.107580},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24005442},
author = {Wenjun Xu and Yingchun Xia and Bifan Sun and Zihao Zhao and Lianggui Tang and Xiaobo Zhou and Qingyong Wang and Lichuan Gu},
keywords = {Protein representation learning, Contrastive learning, Multi-knowledge embeddings, Convex approximation},
abstract = {Protein representation learning plays a crucial role in obtaining a comprehensive understanding of biological regulatory mechanisms and in developing proteins and drugs for therapeutic purposes. However, labeled proteins, such as sequenced and functionally annotated data, are incomplete and few. Thus, contrastive learning has emerged as the preferred technique for learning meaningful representations from unlabeled data samples. In addition, at present, natural proteins cannot be fully described by extracting protein knowledge from a single domain. Therefore, Pro-CoRL, a protein contrastive models framework based on multi-knowledge representation learning, was proposed in this study. In particular, Pro-CoRL smooths the objective function using convex approximation, thereby improving the stability of training. Extensive experiments on predicting protein–protein interaction types and clustering protein families have confirmed the high accuracy and robustness of Pro-CoRL.}
}
@article{REN20241619,
title = {TCMM: A unified database for traditional Chinese medicine modernization and therapeutic innovations},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {1619-1630},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001053},
author = {Zhixiang Ren and Yiming Ren and Zeting Li and Huan Xu},
keywords = {Traditional Chinese medicine, Database, Knowledge graph, Deep learning, Graph neural network},
abstract = {Mining the potential of traditional Chinese medicine (TCM) in treating modern diseases requires a profound understanding of its action mechanism and a comprehensive knowledge system that seamlessly bridges modern medical insights with traditional theories. However, existing databases for modernizing TCM are plagued by varying degrees of information loss, which impede the multidimensional dissection of pharmacological effects. To address this challenge, we introduce traditional Chinese medicine modernization (TCMM), the currently largest modernized TCM database that integrates pioneering intelligent pipelines. By aligning high-quality TCM and modern medicine data, TCMM boasts the most extensive TCM modernization knowledge, including 20 types of modernized TCM concepts such as prescription, ingredient, target and 46 biological relations among them, totaling 3,447,023 records. We demonstrate the efficacy and reliability of TCMM with two features, prescription generation and knowledge discovery, the outcomes show consistency with biological experimental results. A publicly available web interface is at https://www.tcmm.net.cn/.}
}
@article{NYAMATHI2024,
title = {Establishing the Foundations of Emotional Intelligence in Care Companion Robots to Mitigate Agitation Among High-Risk Patients With Dementia: Protocol for an Empathetic Patient-Robot Interaction Study},
journal = {JMIR Research Protocols},
volume = {13},
year = {2024},
issn = {1929-0748},
doi = {https://doi.org/10.2196/55761},
url = {https://www.sciencedirect.com/science/article/pii/S1929074824004803},
author = {Adeline Nyamathi and Nikil Dutt and Jung-Ah Lee and Amir M Rahmani and Mahkameh Rasouli and Donna Krogh and Erik Krogh and David Sultzer and Humayun Rashid and Hamza Liaqat and Riyam Jawad and Farhan Azhar and Ali Ahmad and Bilal Qamar and Taha Yasin Bhatti and Chet Khay and Jocelyn Ludlow and Lisa Gibbs and Julie Rousseau and Mahyar Abbasian and Yutong Song and Cheonkam Jeong and Sabine Brunswicker},
keywords = {persons with dementia, empathy-based care companion robot, agitation, fall risk, artificial intelligence, AI},
abstract = {Background
An estimated 6.7 million persons are living with dementia in the United States, a number expected to double by 2060. Persons experiencing moderate to severe dementia are 4 to 5 times more likely to fall than those without dementia, due to agitation and unsteady gait. Socially assistive robots fail to address the changing emotional states associated with agitation, and it is unclear how emotional states change, how they impact agitation and gait over time, and how social robots can best respond by showing empathy.
Objective
This study aims to design and validate a foundational model of emotional intelligence for empathetic patient-robot interaction that mitigates agitation among those at the highest risk: persons experiencing moderate to severe dementia.
Methods
A design science approach will be adopted to (1) collect and store granular, personal, and chronological data using Personicle (an open-source software platform developed to automatically collect data from phones and other devices), incorporating real-time visual, audio, and physiological sensing technologies in a simulation laboratory and at board and care facilities; (2) develop statistical models to understand and forecast the emotional state, agitation level, and gait pattern of persons experiencing moderate to severe dementia in real time using machine learning and artificial intelligence and Personicle; (3) design and test an empathy-focused conversation model, focused on storytelling; and (4) test and evaluate this model for a care companion robot (CCR) in the community.
Results
The study was funded in October 2023. For aim 1, architecture development for Personicle data collection began with a search for existing open-source data in January 2024. A community advisory board was formed and met in December 2023 to provide feedback on the use of CCRs and provide personal stories. Full institutional review board approval was received in March 2024 to place cameras and CCRs at the sites. In March 2024, atomic marker development was begun. For aim 2, after a review of open-source data on patients with dementia, the development of an emotional classifier was begun. Data labeling was started in April 2024 and completed in June 2024 with ongoing validation. Moreover, the team established a baseline multimodal model trained and validated on healthy-person data sets, using transformer architecture in a semisupervised manner, and later retrained on the labeled data set of patients experiencing moderate to severe dementia. In April 2024, empathy alignment of large language models was initiated using prompt engineering and reinforcement learning.
Conclusions
This innovative caregiving approach is designed to recognize the signs of agitation and, upon recognition, intervene with empathetic verbal communication. This proposal has the potential to have a significant impact on an emerging field of computational dementia science by reducing unnecessary agitation and falls of persons experiencing moderate to severe dementia, while reducing caregiver burden.
International Registered Report Identifier (IRRID)
PRR1-10.2196/55761}
}
@article{SYED2024100238,
title = {Airline reviews processing: Abstractive summarization and rating-based sentiment classification using deep transfer learning},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100238},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000272},
author = {Ayesha Ayub Syed and Ford Lumban Gaol and Alfred Boediman and Widodo Budiharto},
keywords = {Airline reviews, Domain adaptation, Opinion summarization, Review rating, Sentiment classification, Two-stage finetuning},
abstract = {Opinion summarization and sentiment classification are key processes for understanding, analyzing, and leveraging information from customer opinions. The rapid and ceaseless increase in big data of reviews on e-commerce platforms, social media, or review portals becomes a stimulus for the automation of these processes. In recent years, deep transfer learning has opted to solve many challenging tasks in Natural Language Processing (NLP) relieving the hassles of exhaustive training and the requirement of extensive labelled datasets. In this work, we propose frameworks for Abstractive Summarization (ABS) and Sentiment Analysis (SA) of airline reviews using Pretrained Language Models (PLM). The abstractive summarization model goes through two finetuning stages, the first one, for domain adaptation and the second one, for final task learning. Several studies in the literature empirically demonstrate that review rating has a positive correlation with sentiment valence. For the sentiment classification framework, we used the rating value as a signal to determine the review sentiment, and the model is built on top of BERT (Bidirectional Encoder Representations from Transformers) architecture. We evaluated our models comprehensively with multiple metrics. Our results indicate competitive performance of the models in terms of most of the evaluation metrics.}
}
@article{KONDINSKI20241071,
title = {Hacking decarbonization with a community-operated CreatorSpace},
journal = {Chem},
volume = {10},
number = {4},
pages = {1071-1083},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.}
}
@article{MING2024104658,
title = {Enhancing the coverage of SemRep using a relation classification approach},
journal = {Journal of Biomedical Informatics},
volume = {155},
pages = {104658},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104658},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000765},
author = {Shufan Ming and Rui Zhang and Halil Kilicoglu},
keywords = {Biomedical relation extraction, Relation classification, Large language models, SemRep, SemMedDB},
abstract = {Objective:
Relation extraction is an essential task in the field of biomedical literature mining and offers significant benefits for various downstream applications, including database curation, drug repurposing, and literature-based discovery. The broad-coverage natural language processing (NLP) tool SemRep has established a solid baseline for extracting subject–predicate–object triples from biomedical text and has served as the backbone of the Semantic MEDLINE Database (SemMedDB), a PubMed-scale repository of semantic triples. While SemRep achieves reasonable precision (0.69), its recall is relatively low (0.42). In this study, we aimed to enhance SemRep using a relation classification approach, in order to eventually increase the size and the utility of SemMedDB.
Methods:
We combined and extended existing SemRep evaluation datasets to generate training data. We leveraged the pre-trained PubMedBERT model, enhancing it through additional contrastive pre-training and fine-tuning. We experimented with three entity representations: mentions, semantic types, and semantic groups. We evaluated the model performance on a portion of the SemRep Gold Standard dataset and compared it to SemRep performance. We also assessed the effect of the model on a larger set of 12K randomly selected PubMed abstracts.
Results:
Our results show that the best model yields a precision of 0.62, recall of 0.81, and F1 score of 0.70. Assessment on 12K abstracts shows that the model could double the size of SemMedDB, when applied to entire PubMed. We also manually assessed the quality of 506 triples predicted by the model that SemRep had not previously identified, and found that 67% of these triples were correct.
Conclusion:
These findings underscore the promise of our model in achieving a more comprehensive coverage of relationships mentioned in biomedical literature, thereby showing its potential in enhancing various downstream applications of biomedical literature mining. Data and code related to this study are available at https://github.com/Michelle-Mings/SemRep_RelationClassification.}
}
@incollection{KATO20242839,
title = {Prototype of Automated Physical Model Builder: Challenges and Opportunities},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {2839-2844},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50474-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241504749},
author = {Shota Kato and Manabu Kano},
keywords = {Artificial intelligence, Physical model, Digital twin, Natural language processing, Process modelling},
abstract = {In the process industry, physical models are indispensable, yet current models sometimes compromise accuracy or incur substantial computational costs. Such cases require new physical models, but the traditional approach to building physical models is reliant on expert knowledge and is time-consuming. This necessitates the development of a new efficient physical model building methodology. Our research aims to establish automated physical model builder, AutoPMoB, which builds physical models from manufacturing process literature. The realization of AutoPMoB requires developing several methods, including those for collecting documents related to the target process and accurately extracting information for physical model building from the documents. In this study, we develop an AutoPMoB prototype, employing a large language model alongside a model building approach previously proposed in our research. The prototype's application to a continuous stirred tank reactor showed its capability to extract necessary data accurately, although the initial attempts did not yield the anticipated models. Subsequent modifications in unifying expressions led to successful model building, underscoring the effectiveness of our system in leveraging literature for physical model building. Advancing AutoPMoB towards practical deployment necessitates specific enhancements, particularly in methods for equivalence judgment of definitions, retrieval of relevant documents, integration of non-documentary information, and domain-specific adaptation.}
}
@article{SONG2024104,
title = {Relation-aware deep neural network enables more efficient biomedical knowledge acquisition from massive literature},
journal = {AI Open},
volume = {5},
pages = {104-114},
year = {2024},
issn = {2666-6510},
doi = {https://doi.org/10.1016/j.aiopen.2024.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666651024000123},
author = {Chenyang Song and Zheni Zeng and Changyao Tian and Kuai Li and Yuan Yao and Suncong Zheng and Zhiyuan Liu and Maosong Sun},
keywords = {Biomedical knowledge retrieval, Biomedical relation modeling, Deep learning, Pre-trained language model},
abstract = {Biomedical knowledge is typically organized in a relational scheme, such as chemical-disease relation, gene-disease relation, and gene-pathway relation. Biomedical scientists heavily rely on search engines to acquire up-to-date relational knowledge from massive biomedical articles. The navigation efficiency of the retrieval process, however, is significantly restricted by keyword matching techniques unaware of the biomedical relations of these keywords in articles. To bridge the gap between existing retrieval techniques and practical access demands for relational knowledge, we present a novel framework, Biomedical Relation-Aware Document Ranking (BioRADR), capable of retrieving articles expressing specific relations with respect to the queried entity pair. Based on a deep neural network, BioRADR can be trained from large-scale data automatically annotated via distant supervision, and empirical evaluation reveals that it outperforms the strongest baseline by over 8 points in NDCG@1. We implement an online system (http://bioradr.ai.thunlp.org/) based on BioRADR, enabling more efficient relation-oriented retrieval of biomedical articles.}
}
@article{TAO202579,
title = {Intelligent emergency assisted decision-making method based on standard digitalization: Hazardous chemical accidents in industrial parks},
journal = {Journal of Safety Science and Resilience},
volume = {6},
number = {1},
pages = {79-92},
year = {2025},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2024.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S2666449624000513},
author = {Zhenxiang Tao and Xiaohan Liu and Ying Li and Peifeng Hu and Weitong Tang and Ning Luo and Jiansong Wu and Rui Yang},
keywords = {Public safety, Emergency response, Assisted decision, Knowledge graph, Standard digitization},
abstract = {Contemporary society is confronted with multifaceted challenges, and the intricate interplay of interconnected factors significantly complicates emergency response efforts. Current practices rely on quick decisions by domain experts; however, the limitations of individual expertise and the urgency of crises hinder both precision and standardization. To address these issues, we propose a novel approach: an intelligent method for emergency decision-making grounded in a standardized digital knowledge graph. First, our study examined the underlying theory of standardized digital transformation and event-chain evolution. This led to the construction of a knowledge graph encompassing standard emergency knowledge, as well as supplementary derivative data pertinent to event response. Second, through the application of semantic analysis and intention recognition of the decision target, coherent and interpretable query sentences for the decision system were crafted. These query sentences then served as a conduit for retrieving standard emergency knowledge relevant to the current emergency situation, as well as potential secondary disasters. The overarching goal is to provide emergency decision makers with effective support mechanisms that are both well informed and tailored to the specific demands of each situation.}
}
@article{FENG2024124462,
title = {Focusing on differences! Sample framework enhances semantic textual similarity with external knowledge},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124462},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124462},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424013289},
author = {Jianzhou Feng and Junxin Liu and Chenghan Gu and Haotian Qi and Zhongcan Ren and Kehan Xu and Yuanzhuo Wang},
keywords = {Semantic similarity task, Differential information, Sentence embeddings},
abstract = {Recently, the widespread application of pre-trained language models (PLMs) such as BERT and RoBERTa has significantly enhanced the performance of tasks related to text semantic similarity. However, methods solely based on PLMs inadequately account for the differential information between sentence pairs, thus underestimating the importance of this information in sentence matching. In this paper, we propose the enriching Differential information with External Knowledge framework (DEK), an approach that explicitly extracts differential information and enriches semantics using external knowledge. Specifically, we devise a module for extracting differential words from sentence pairs, obtain synonyms of differential words from WordNet, and construct a differential information graph. We employ Graph Convolutional Networks (GCNs) to extract features from this graph and subsequently integrate this information into sentence embeddings. In this work, we demonstrate that incorporating differential information enables PLMs-based methods to better focus on the differing aspects of sentences. Moreover, DEK seamlessly adapts to contrastive learning of sentence embeddings models, including SimCSE and PromptBert, among others. Comparing to baseline, our method has improved spearman correlation between 0.22 and 0.64, yielding competitive results in the experiments.}
}
@article{SHI2025107191,
title = {Knowledge-Guided Semantically Consistent Contrastive Learning for sequential recommendation},
journal = {Neural Networks},
volume = {185},
pages = {107191},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107191},
url = {https://www.sciencedirect.com/science/article/pii/S089360802500070X},
author = {Chenglong Shi and Surong Yan and Shuai Zhang and Haosen Wang and Kwei-Jay Lin},
keywords = {Sequential recommendation, Contrastive learning, Knowledge graph, Semantic consistency},
abstract = {Contrastive learning has gained dominance in sequential recommendation due to its ability to derive self-supervised signals for addressing data sparsity problems. However, caused by random augmentations (e.g., crop, mask, and reorder), existing methods may produce positive views with inconsistent semantics, which degrades performance. Although some efforts have been made by providing new operations (e.g., insert and substitute), challenges have not been well addressed due to information scarcity. Inspired by the massive semantic relationships in the Item Knowledge Graph (IKG), we propose a Knowledge-Guided Semantically consistent Contrastive Learning model for sequential recommendation (KGSCL). Specifically, we introduce two knowledge-guided augmentation operations, KG-substitute and KG-insert, to create semantically consistent and meaningful views. These operations add knowledge-related items from the neighbors in the IKG to augment the sequence, aligning real-world associations to retain original semantics. Meanwhile, we design a co-occurrence-based sampling strategy to complement knowledge-guided augmentations for selecting more correlated neighbors. Moreover, we introduce a view-target CL to model the correlation between semantically consistent views and target items since they exhibit similar user preferences. Experimental results on six widely used datasets demonstrate the effectiveness of our KGSCL in recommendation performance, robustness, and model convergence compared with 14 state-of-the-art competitors. Our code is available at: https://github.com/LFM-bot/KGSCL.}
}
@article{VETTER2024102831,
title = {Towards a framework for local interrogation of AI ethics: A case study on text generators, academic integrity, and composing with ChatGPT},
journal = {Computers and Composition},
volume = {71},
pages = {102831},
year = {2024},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102831},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000070},
author = {Matthew A. Vetter and Brent Lucia and Jialei Jiang and Mahmoud Othman},
keywords = {Artificial intelligence (AI), Academic integrity, Text generators, academic policy, Composition pedagogy, Ethics},
abstract = {Ethical frameworks for text generators (TGs) in education are generally concerned with personalized instruction, a dependency on data, biases in training data, academic integrity, and lack of creativity from students. While broad-level, institutional guidelines provide value in understanding the ethical dimensions of artificial intelligence (AI) for the classroom, there is a need for a more ecological understanding of how AI ethics might be constructed locally, one that takes into account the negotiation of AI between teacher and student. This article investigates how an educational ethical framework for AI use emerges through a qualitative case study of one composition student's interaction with and understanding of using ChatGPT as a type of writing partner. Analysis of interview data and student logs uncover what we term an emergent “local ethic” – a framework that is capable of exploring unique ethical considerations, values, and norms that develop at the most foundational unit of higher education – the individual classroom. Our framework is meant to provide a heuristic for other writing teacher-scholars as they interrogate issues related to pedagogy, student criticality, agency, reliability, and access within the context of powerful AI systems.}
}
@article{KUGIC2024,
title = {Processing of Short-Form Content in Clinical Narratives: Systematic Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/57852},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006009},
author = {Amila Kugic and Ingrid Martin and Luise Modersohn and Peter Pallaoro and Markus Kreuzthaler and Stefan Schulz and Martin Boeker},
keywords = {electronic health records, EHR, clinical narratives, natural language processing, machine learning, deep learning, rule-based approach, short-form expression, disambiguation, word embedding, vector representations, language modeling, human-in-the-loop, feature extraction},
abstract = {Background
Clinical narratives are essential components of electronic health records. The adoption of electronic health records has increased documentation time for hospital staff, leading to the use of abbreviations and acronyms more frequently. This brevity can potentially hinder comprehension for both professionals and patients.
Objective
This review aims to provide an overview of the types of short forms found in clinical narratives, as well as the natural language processing (NLP) techniques used for their identification, expansion, and disambiguation.
Methods
In the databases Web of Science, Embase, MEDLINE, EBMR (Evidence-Based Medicine Reviews), and ACL Anthology, publications that met the inclusion criteria were searched according to PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines for a systematic scoping review. Original, peer-reviewed publications focusing on short-form processing in human clinical narratives were included, covering the period from January 2018 to February 2023. Short-form types were extracted, and multidimensional research methodologies were assigned to each target objective (identification, expansion, and disambiguation). NLP study recommendations and study characteristics were systematically assigned occurrence rates for evaluation.
Results
Out of a total of 6639 records, only 19 articles were included in the final analysis. Rule-based approaches were predominantly used for identifying short forms, while string similarity and vector representations were applied for expansion. Embeddings and deep learning approaches were used for disambiguation.
Conclusions
The scope and types of what constitutes a clinical short form were often not explicitly defined by the authors. This lack of definition poses challenges for reproducibility and for determining whether specific methodologies are suitable for different types of short forms. Analysis of a subset of NLP recommendations for assessing quality and reproducibility revealed only partial adherence to these recommendations. Single-character abbreviations were underrepresented in studies on clinical narrative processing, as were investigations in languages other than English. Future research should focus on these 2 areas, and each paper should include descriptions of the types of content analyzed.}
}
@incollection{DANU2024,
title = {Idiolect},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00122-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041001228},
author = {Julija Danu and Krzysztof Kredens and Tim Grant},
keywords = {Idiolect, Linguistic individual, Style, Authorship, Authorship analysis, Forensic linguistics},
abstract = {We examine the development of the idea of the linguistic individual and the concept of idiolect from antiquity to contemporary theories and provide a brief survey of international approaches to these discussions, and empirical attempts to establish the evidence of existence of idiolects before looking at applications, primarily in forensic linguistic casework. The future indicates that interest in Large Language Models is likely to influence development of both theory and empirical evidence that idiolects exist.}
}
@article{BABAIAN2024114172,
title = {Entity recognition from colloquial text},
journal = {Decision Support Systems},
volume = {179},
pages = {114172},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2024.114172},
url = {https://www.sciencedirect.com/science/article/pii/S0167923624000058},
author = {Tamara Babaian and Jennifer Xu},
keywords = {Entity recognition, Symptom recognition, Natural language processing, Training strategies, Design science},
abstract = {Extraction of concepts and entities of interest from non-formal texts such as social media posts and informal communication is an important capability for decision support systems in many domains, including healthcare, customer relationship management, and others. Despite the recent advances in training large language models for a variety of natural language processing tasks, the developed models and techniques have mainly focused on formal texts and do not perform as well on colloquial data, which is characterized by a number of distinct challenges. In our research, we focus on the healthcare domain and investigate the problem of symptom recognition from colloquial texts by designing and evaluating several training strategies for BERT-based model fine-tuning. These strategies are distinguished by the choice of the base model, the training corpora, and application of term perturbations in the training data. The best-performing models trained using these strategies outperform the state-of-the-art specialized symptom recognizer by a large margin. Through a series of experiments, we have found specific patterns of model behavior associated with the training strategies we designed. We present design principles for training strategies for effective entity recognition in colloquial texts based on our findings.}
}
@article{LI2025111176,
title = {Pseudo-labeling with keyword refining for few-supervised video captioning},
journal = {Pattern Recognition},
volume = {159},
pages = {111176},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111176},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009270},
author = {Ping Li and Tao Wang and Xinkui Zhao and Xianghua Xu and Mingli Song},
keywords = {Video captioning, Few supervision, Pseudo-labeling, Keyword refiner, Gated fusion},
abstract = {Video captioning generate a sentence that describes the video content. Existing methods always require a number of captions (e.g., 10 or 20) per video to train the model, which is quite costly. In this work, we explore the possibility of using only one or very few ground-truth sentences, and introduce a new task named few-supervised video captioning. Specifically, we propose a few-supervised video captioning framework that consists of lexically constrained pseudo-labeling module and keyword-refined captioning module. Unlike the random sampling in natural language processing that may cause invalid modifications (i.e., edit words), the former module guides the model to edit words using some actions (e.g., copy, replace, insert, and delete) by a pretrained token-level classifier, and then fine-tunes candidate sentences by a pretrained language model. Meanwhile, the former employs the repetition penalized sampling to encourage the model to yield concise pseudo-labeled sentences with less repetition, and selects the most relevant sentences upon a pretrained video-text model. Moreover, to keep semantic consistency between pseudo-labeled sentences and video content, we develop the transformer-based keyword refiner with the video-keyword gated fusion strategy to emphasize more on relevant words. Extensive experiments on several benchmarks demonstrate the advantages of the proposed approach in both few-supervised and fully-supervised scenarios.}
}
@article{YANG2024120134,
title = {Information bottleneck based knowledge selection for commonsense reasoning},
journal = {Information Sciences},
volume = {660},
pages = {120134},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120134},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524000471},
author = {Zhao Yang and Yuanzhe Zhang and Pengfei Cao and Cao Liu and Jiansong Chen and Jun Zhao and Kang Liu},
keywords = {Commonsense reasoning, Knowledge selection, Information bottleneck, KG-augmented model},
abstract = {KG-augmented models usually endow existing models with external knowledge graphs, which achieve promising performance in various knowledge-intensive tasks, such as commonsense reasoning. Existing methods mainly first exploited heuristic ways for retrieving the relevant knowledge subgraphs according to the input, and then utilized some effective encoders, such as GNNs, to encode the symbolic knowledge into the neural reasoning networks. However, whether the whole retrieved knowledge subgraphs are really relevant or useful for the reasoning process was seldom considered. Actually, according to our observations and analysis, most retrieved knowledge is noisy and useless to the reasoning models, which would hurt the final performance. To remedy this, this paper proposes information bottleneck based knowledge selection (IBKS), which is able to select useful knowledge from the retrieved knowledge subgraph. Expectedly, the selected knowledge could better improve the commonsense reasoning ability of the model. Moreover, IBKS is model-agnostic and could be plugged into any existing KG-augmented model. Extensive experimental results show that IBKS could effectively improve commonsense reasoning performance.}
}
@article{CHAU2024102666,
title = {Advancing plant single-cell genomics with foundation models},
journal = {Current Opinion in Plant Biology},
volume = {82},
pages = {102666},
year = {2024},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2024.102666},
url = {https://www.sciencedirect.com/science/article/pii/S1369526624001572},
author = {Tran N. Chau and Xuan Wang and John M. McDowell and Song Li},
abstract = {Single-cell genomics, combined with advanced AI models, hold transformative potential for understanding complex biological processes in plants. This article reviews deep-learning approaches in single-cell genomics, focusing on foundation models, a type of large-scale, pretrained, multi-purpose generative AI models. We explore how these models, such as Generative Pre-trained Transformers (GPT), Bidirectional Encoder Representations from Transformers (BERT), and other Transformer-based architectures, are applied to extract meaningful biological insights from diverse single-cell datasets. These models address challenges in plant single-cell genomics, including improved cell-type annotation, gene network modeling, and multi-omics integration. Moreover, we assess the use of Generative Adversarial Networks (GANs) and diffusion models, focusing on their capacity to generate high-fidelity synthetic single-cell data, mitigate dropout events, and handle data sparsity and imbalance. Together, these AI-driven approaches hold immense potential to enhance research in plant genomics, facilitating discoveries in crop resilience, productivity, and stress adaptation.}
}
@article{JONEK2024139,
title = {Manual assembly planning with AI Image Generators},
journal = {Procedia CIRP},
volume = {130},
pages = {139-144},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.068},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012216},
author = {Michael Jonek and Malte Bast and Martin Manns},
keywords = {Production planning, AI-assisted planning, AI art, Assembly instructions, Manual assembly},
abstract = {For small and medium-sized enterprises (SMEs), the planning of manual assembly activities represents a significant cost and resource factor, requiring precision and meticulous organization. To ensure a stable competitive and economical production, the steps involved in manual assembly must be optimized. In today’s digital era, Artificial Intelligences (AI) offer innovative approaches and opportunities to streamline processes. In addition to LLM AIs, AI image generators are currently attracting a lot of attention as they generate very realistic and detailed images based on a given description. Images or visualisations of the situation are often used to make work instructions in manual assembly easier to understand. AI image generators can be used to visualise assembly process steps for automatically generated work instructions. In this research, a quantitative measure is proposed that can be used to rate how correctly the actual situation is depicted by highlighting incorrect or false objects and operations and assessing the accuracy of the context. The measure is validated by qualitatively evaluating images created with DALL-E 3 in an user study by both workers and planning experts from industry and comparing them with the quantitative measure. This will enable further research in the field of automated work planning and the comparison of different AI image generation tools for use in assembly planning.}
}
@article{PALLOTTINO2025109919,
title = {Applications and perspectives of Generative Artificial Intelligence in agriculture},
journal = {Computers and Electronics in Agriculture},
volume = {230},
pages = {109919},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2025.109919},
url = {https://www.sciencedirect.com/science/article/pii/S0168169925000250},
author = {Federico Pallottino and Simona Violino and Simone Figorilli and Catello Pane and Jacopo Aguzzi and Giacomo Colle and Eugenio {Nerio Nemmi} and Alessandro Montaghi and Damianos Chatzievangelou and Francesca Antonucci and Lavinia Moscovini and Alessandro Mei and Corrado Costa and Luciano Ortenzi},
keywords = {GAI, GAN, NLP, LLMs, ChatGPT, Microsoft Copilot},
abstract = {Artificial Intelligence (AI) applications related to agriculture have recently gained in use and attention. They are indeed valuable tools for interpreting data, improving production chains, and optimizing the use of natural resources. Among AI models, the most recent and promising area is represented by Generative Artificial Intelligence (GAI). After an initial description of its general model architectures, this work aims to review its practical uses and potentials in the following individual sectors: agriculture, precision farming, and animal farming, as well as interdisciplinary applications. The literature search was carried out using the SCOPUS, Google Scholar, and Web of Science databases. GAI holds immense potential for revolutionizing agriculture, offering solutions ranging from precision farming to pest management and supply chain optimization. Though some applications can extend beyond efficiency gains, and hallucinations occurrence i.e. false output information presented as fact, remains an open issue, GAI can be decisive for tasks like improving training datasets, refining models, and facilitating time series analysis. This review extensively describes the vital importance of these tasks for agriculture, precision and animal farming, caused by the rise of new technologies. As a result, by embracing and responsibly implementing GAI applications, it is possible to create a more sustainable and resilient future for agriculture and precision farming. GAI have the capacity to extract specific information from big data systems, offering huge potential to meet a growing global population demand and consequent environmental challenges for the future.}
}
@article{HU2023104527,
title = {Learning entity-oriented representation for biomedical relation extraction},
journal = {Journal of Biomedical Informatics},
volume = {147},
pages = {104527},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104527},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002484},
author = {Ying Hu and Yanping Chen and Yongbin Qin and Ruizhang Huang},
keywords = {Biomedical natural language processing, Overlapping semantics, Information extraction},
abstract = {Biomedical Relation Extraction (BioRE) aims to automatically extract semantic relations for given entity pairs and is of great significance in biomedical research. Current popular methods often utilize pretrained language models to extract semantic features from individual input instances, which frequently suffer from overlapping semantics. Overlapping semantics refers to the situation in which a sentence contains multiple entity pairs that share the same context, leading to highly similar information between these entity pairs. In this study, we propose a model for learning Entity-oriented Representation (EoR) that aims to improve the performance of the model by enhancing the discriminability between entity pairs. It contains three modules: sentence representation, entity-oriented representation, and output. The first module learns the global semantic information of the input instance; the second module focuses on extracting the semantic information of the sentence from the target entities; and the third module enhances distinguishability among entity pairs and classifies the relation type. We evaluated our approach on four BioRE tasks with eight datasets, and the experiments showed that our EoR achieved state-of-the-art performance for PPI, DDI, CPI, and DPI tasks. Further analysis demonstrated the benefits of entity-oriented semantic information in handling multiple entity pairs in the BioRE task.}
}
@article{ZHAO2023126708,
title = {ChatAgri: Exploring potentials of ChatGPT on cross-linguistic agricultural text classification},
journal = {Neurocomputing},
volume = {557},
pages = {126708},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126708},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223008317},
author = {Biao Zhao and Weiqiang Jin and Javier {Del Ser} and Guang Yang},
keywords = {Agricultural text classification, Very large pre-trained language model, Generative Pre-trained Transformer (GPT), ChatGPT, GPT-4},
abstract = {In the era of sustainable smart agriculture, a vast amount of agricultural news text is posted online, accumulating significant agricultural knowledge. To efficiently access this knowledge, effective text classification techniques are urgently needed. Deep learning approaches, such as fine-tuning strategies on pre-trained language models (PLMs), have shown remarkable performance gains. Nonetheless, these methods face several complex challenges, including limited agricultural training data, poor domain transferability (especially across languages), and complex and expensive deployment of large models. Inspired by the success of recent ChatGPT models (e.g., GPT-3.5, GPT-4), this work explores the potential of applying ChatGPT in the field of agricultural informatization. Various crucial factors, such as prompt construction, answer parsing, and different ChatGPT variants, are thoroughly investigated to maximize its capabilities. A preliminary comparative study is conducted, comparing ChatGPT with PLMs-based fine-tuning methods and PLMs-based prompt-tuning methods. Empirical results demonstrate that ChatGPT effectively addresses the mentioned research challenges and bottlenecks, making it an ideal solution for agricultural text classification. Moreover, ChatGPT achieves comparable performance to existing PLM-based fine-tuning methods, even without fine-tuning on agricultural data samples. We hope this preliminary study could inspire the emergence of a general-purpose AI paradigm for agricultural text processing.}
}
@article{MAHBOUBI2024104004,
title = {Evolving techniques in cyber threat hunting: A systematic review},
journal = {Journal of Network and Computer Applications},
volume = {232},
pages = {104004},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104004},
url = {https://www.sciencedirect.com/science/article/pii/S1084804524001814},
author = {Arash Mahboubi and Khanh Luong and Hamed Aboutorab and Hang Thanh Bui and Geoff Jarrad and Mohammed Bahutair and Seyit Camtepe and Ganna Pogrebna and Ejaz Ahmed and Bazara Barry and Hannah Gately},
keywords = {Threat hunting, Hypothesis, Machine learning, OpenAI voice engine, Cyber threat intelligence, Generative AI},
abstract = {In the rapidly changing cybersecurity landscape, threat hunting has become a critical proactive defense against sophisticated cyber threats. While traditional security measures are essential, their reactive nature often falls short in countering malicious actors’ increasingly advanced tactics. This paper explores the crucial role of threat hunting, a systematic, analyst-driven process aimed at uncovering hidden threats lurking within an organization’s digital infrastructure before they escalate into major incidents. Despite its importance, the cybersecurity community grapples with several challenges, including the lack of standardized methodologies, the need for specialized expertise, and the integration of cutting-edge technologies like artificial intelligence (AI) for predictive threat identification. To tackle these challenges, this survey paper offers a comprehensive overview of current threat hunting practices, emphasizing the integration of AI-driven models for proactive threat prediction. Our research explores critical questions regarding the effectiveness of various threat hunting processes and the incorporation of advanced techniques such as augmented methodologies and machine learning. Our approach involves a systematic review of existing practices, including frameworks from industry leaders like IBM and CrowdStrike. We also explore resources for intelligence ontologies and automation tools. The background section clarifies the distinction between threat hunting and anomaly detection, emphasizing systematic processes crucial for effective threat hunting. We formulate hypotheses based on hidden states and observations, examine the interplay between anomaly detection and threat hunting, and introduce iterative detection methodologies and playbooks for enhanced threat detection. Our review encompasses supervised and unsupervised machine learning approaches, reasoning techniques, graph-based and rule-based methods, as well as other innovative strategies. We identify key challenges in the field, including the scarcity of labeled data, imbalanced datasets, the need for integrating multiple data sources, the rapid evolution of adversarial techniques, and the limited availability of human expertise and data intelligence. The discussion highlights the transformative impact of artificial intelligence on both threat hunting and cybercrime, reinforcing the importance of robust hypothesis development. This paper contributes a detailed analysis of the current state and future directions of threat hunting, offering actionable insights for researchers and practitioners to enhance threat detection and mitigation strategies in the ever-evolving cybersecurity landscape.}
}
@article{LI2022102098,
title = {Ontology-based knowledge representation and semantic topic modeling for intelligent trademark legal precedent research},
journal = {World Patent Information},
volume = {68},
pages = {102098},
year = {2022},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2022.102098},
url = {https://www.sciencedirect.com/science/article/pii/S0172219022000059},
author = {Gi-Kuen J. Li and Charles V. Trappey and Amy J.C. Trappey and Annie A.S. Li},
keywords = {Legal research, Ontology-based knowledge system, Latent Dirichlet allocation (LDA), Semantic topic modeling},
abstract = {An intelligent methodology and its prototype system are developed for automatically discovering legal precedents using semantic analysis. The concept of the trademark legal precedent recommendation was originated from our TE2020 conference paper. The approach is to identify matching cases related to given seed case with respect to their legal case brief attributes using advanced text mining techniques. In the paper, dynamic topic modeling is further developed to analyze the dataset over three time-sequential cohorts to identify trademark law topics varied over time. Further, the prototype system was demonstrated and verified using real trademark case analysis with satisfactory results.}
}
@article{LIANG2024106583,
title = {An unsupervised multi-view contrastive learning framework with attention-based reranking strategy for entity alignment},
journal = {Neural Networks},
volume = {179},
pages = {106583},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106583},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024005070},
author = {Yan Liang and Weishan Cai and Minghao Yang and Yuncheng Jiang},
keywords = {Entity alignment, Knowledge graphs, Graph attention network, Contrastive learning, Reranking strategy},
abstract = {Entity alignment is a crucial task in knowledge graphs, aiming to match corresponding entities from different knowledge graphs. Due to the scarcity of pre-aligned entities in real-world scenarios, research focused on unsupervised entity alignment has become more popular. However, current unsupervised entity alignment methods suffer from a lack of informative entity guidance, hindering their ability to accurately predict challenging entities with similar names and structures. To solve these problems, we present an unsupervised multi-view contrastive learning framework with an attention-based reranking strategy for entity alignment, named AR-Align. In AR-Align, two kinds of data augmentation methods are employed to provide a complementary view for neighborhood and attribute, respectively. Next, a multi-view contrastive learning method is introduced to reduce the semantic gap between different views of the augmented entities. Moreover, an attention-based reranking strategy is proposed to rerank the hard entities through calculating their weighted sum of embedding similarities on different structures. Experimental results indicate that AR-Align outperforms most both supervised and unsupervised state-of-the-art methods on three benchmark datasets.}
}
@article{SARKER2024935,
title = {Explainable AI for cybersecurity automation, intelligence and trustworthiness in digital twin: Methods, taxonomy, challenges and prospects},
journal = {ICT Express},
volume = {10},
number = {4},
pages = {935-958},
year = {2024},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2024.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2405959524000572},
author = {Iqbal H. Sarker and Helge Janicke and Ahmad Mohsin and Asif Gill and Leandros Maglaras},
keywords = {Cybersecurity, Explainable AI, Machine learning, Data-driven, Automation, Intelligent decision-making, Trustworthiness, Digital twin},
abstract = {Digital twins (DTs) are an emerging digitalization technology with a huge impact on today’s innovations in both industry and research. DTs can significantly enhance our society and quality of life through the virtualization of a real-world physical system, providing greater insights about their operations and assets, as well as enhancing their resilience through real-time monitoring and proactive maintenance. DTs also pose significant security risks, as intellectual property is encoded and more accessible, as well as their continued synchronization to their physical counterparts. The rapid proliferation and dynamism of cyber threats in today’s digital environments motivate the development of automated and intelligent cyber solutions. Today’s industrial transformation relies heavily on artificial intelligence (AI), including machine learning (ML) and data-driven technologies that allow machines to perform tasks such as self-monitoring, investigation, diagnosis, future prediction, and decision-making intelligently. However, to effectively employ AI-based models in the context of cybersecurity, human-understandable explanations, and their trustworthiness, are significant factors when making decisions in real-world scenarios. This article provides an extensive study of explainable AI (XAI) based cybersecurity modeling through a taxonomy of AI and XAI methods that can assist security analysts and professionals in comprehending system functions, identifying potential threats and anomalies, and ultimately addressing them in DT environments in an intelligent manner. We discuss how these methods can play a key role in solving contemporary cybersecurity issues in various real-world applications. We conclude this paper by identifying crucial challenges and avenues for further research, as well as directions on how professionals and researchers might approach and model future-generation cybersecurity in this emerging field.}
}
@article{WEI2025273,
title = {Machine learning-assisted retrosynthesis planning: Current status and future prospects},
journal = {Chinese Journal of Chemical Engineering},
volume = {77},
pages = {273-292},
year = {2025},
issn = {1004-9541},
doi = {https://doi.org/10.1016/j.cjche.2024.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S1004954124003720},
author = {Yixin Wei and Leyu Shan and Tong Qiu and Diannan Lu and Zheng Liu},
keywords = {Retrosynthesis planning, Machine learning, Artificial intelligence, Synthetic pathway, Chemoinformatics},
abstract = {Machine learning-assisted retrosynthesis planning aims to utilize machine learning (ML) algorithms to find synthetic pathways for target compounds. In recent years, with the development of artificial intelligence (AI), especially ML, researchers’ interest in ML-assisted retrosynthesis planning has rapidly increased, bringing development and opportunities to the field. In this review, we aim to provide a comprehensive understanding of ML-assisted retrosynthesis planning. We first discuss the formal definition and the objective of retrosynthesis planning, and organize a modular framework which includes four modules: data preparation, data preprocessing, pathway generation and evaluation, and pathway verification. Then, we sequentially review the current status of the first three modules (except pathway verification) in the ML-assisted retrosynthesis planning framework, including ideas, methods, and latest progress. Following that, we specifically discuss large language models in retrosynthesis planning. Finally, we summarize the extant challenges that are faced by current ML-assisted retrosynthesis planning research and offer a perspective on future research directions and development.}
}
@article{WANG20237,
title = {ChatGPT for design, manufacturing, and education},
journal = {Procedia CIRP},
volume = {119},
pages = {7-14},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004262},
author = {Xingzhi Wang and Nabil Anwer and Yun Dai and Ang Liu},
keywords = {Artificial Intelligence, Engineering Design, Product Development, Smart Manufacturing, ChatGPT, AI-generated content, Education},
abstract = {The manufacturing industry involves innumerable complex tasks that require significant knowledge and experience to execute. With the rapid development of artificial intelligence, particularly with the emergence of powerful large language models such as ChatGPT, new opportunities have risen to provide knowledge through conversation. With its seemingly endless knowledge base and highly organized response style, ChatGPT is expected to revolutionize every aspect of the industry. However, the extent of ChatGPT's capabilities and how they could contribute to the industry's future revolution remains unclear. In light of this, this paper performed a systematic testing of ChatGPT to uncover its advantages and limitations. Based on the testing results, the authors provided some prospects and critical research questions of ChatGPT from a manufacturing perspective. Furthermore, the authors recommended a technology development roadmap to successfully integrate ChatGPT into the manufacturing industry.}
}
@article{GAO2023104286,
title = {DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing},
journal = {Journal of Biomedical Informatics},
volume = {138},
pages = {104286},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104286},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000072},
author = {Yanjun Gao and Dmitriy Dligach and Timothy Miller and John Caskey and Brihat Sharma and Matthew M. Churpek and Majid Afshar},
keywords = {Natural language processing, Clinical diagnostic reasoning, Clinical diagnostic decision support, Clinical natural language processing benchmark},
abstract = {The meaningful use of electronic health records (EHR) continues to progress in the digital era with clinical decision support systems augmented by artificial intelligence. A priority in improving provider experience is to overcome information overload and reduce the cognitive burden so fewer medical errors and cognitive biases are introduced during patient care. One major type of medical error is diagnostic error due to systematic or predictable errors in judgement that rely on heuristics. The potential for clinical natural language processing (cNLP) to model diagnostic reasoning in humans with forward reasoning from data to diagnosis and potentially reduce cognitive burden and medical error has not been investigated. Existing tasks to advance the science in cNLP have largely focused on information extraction and named entity recognition through classification tasks. We introduce a novel suite of tasks coined as Diagnostic Reasoning Benchmarks, Dr.Bench, as a new benchmark for developing and evaluating cNLP models with clinical diagnostic reasoning ability. The suite includes six tasks from ten publicly available datasets addressing clinical text understanding, medical knowledge reasoning, and diagnosis generation. DR.BENCH is the first clinical suite of tasks designed to be a natural language generation framework to evaluate pre-trained language models for diagnostic reasoning. The goal of DR. BENCH is to advance the science in cNLP to support downstream applications in computerized diagnostic decision support and improve the efficiency and accuracy of healthcare providers during patient care. We fine-tune and evaluate the state-of-the-art generative models on DR.BENCH. Experiments show that with domain adaptation pre-training on medical knowledge, the model demonstrated opportunities for improvement when evaluated in DR. BENCH. We share DR. BENCH as a publicly available GitLab repository with a systematic approach to load and evaluate models for the cNLP community. We also discuss the carbon footprint produced during the experiments and encourage future work on DR.BENCH to report the carbon footprint.}
}
@article{FICKO2025100257,
title = {Reflective thinking meets artificial intelligence: Synthesizing sustainability transition knowledge in left-behind mountain regions},
journal = {Geography and Sustainability},
volume = {6},
number = {1},
pages = {100257},
year = {2025},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2024.100257},
url = {https://www.sciencedirect.com/science/article/pii/S2666683924001202},
author = {Andrej Ficko and Simo Sarkki and Yasar Selman Gultekin and Antonia Egli and Juha Hiedanpää},
keywords = {Artificial intelligence, Innovation, Reflective thinking, Scientific imagination, Text mining, Text summarization},
abstract = {We demonstrate a multi-method approach towards discovering and structuring sustainability transition knowledge in marginalized mountain regions. By employing reflective thinking, artificial intelligence (AI)-powered text summarization and text mining, we synthesize experts’ narratives on sustainable development challenges and solutions in Kardüz Upland, Türkiye. We then analyze their alignment with the UN Sustainable Development Goals (SDGs) using document embedding. Investment in infrastructure, education, and resilient socio-ecological systems emerged as priority sectors to combat poor infrastructure, geographic isolation, climate change, poverty, depopulation, unemployment, low education levels, and inadequate social services. The narratives were closest in substance to SDG 1, 3, and 11. Social dimensions of sustainability were more pronounced than environmental dimensions. The presented approach supports policymakers in organizing loosely structured sustainability transition knowledge and fragmented data corpora, while also advancing AI applications for designing and planning sustainable development policies at the regional level.}
}
@article{GAN2024102337,
title = {Entity type inference based on path walking and inter-types relationships},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102337},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102337},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000612},
author = {Yi Gan and Zhihui Su and Gaoyong Lu and Pengju Zhang and Aixiang Cui and Jiawei Jiang and Duanbing Chen},
keywords = {ET-PT, Entity type inference, Path walking, Inter-types relationships},
abstract = {As a crucial task for knowledge graphs (KGs), knowledge graph entity type inference (KGET) has garnered increasing attention in recent years. However, recent methods overlook the long-distance information pertaining to entities and the inter-types relationships. The neglect of long-distance information results in the omission of crucial entity relationships and neighbors, consequently leading to the loss of path information associated with missing types. To address this, a path-walking strategy is utilized to identify two-hop triplet paths of the crucial entity for encoding long-distance entity information. Moreover, the absence of inter-types relationships can lead to the loss of the neighborhood information of types, such as co-occurrence information. To ensure a comprehensive understanding of inter-types relationships, we consider interactions not only with the types of single entity but also with different types of entities. Finally, in order to comprehensively represent entities for missing types, considering both the dimensions of path information and neighborhood information, we propose an entity type inference model based on path walking and inter-types relationships, denoted as “ET-PT”. This model effectively extracts comprehensive entity information, thereby obtaining the most complete semantic representation of entities. The experimental results on publicly available datasets demonstrate that the proposed method outperforms state-of-the-art approaches.}
}
@article{ZHANG2024104220,
title = {An event logic graph for geographic environment observation planning in disaster chain monitoring},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {134},
pages = {104220},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.104220},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224005764},
author = {Yunbo Zhang and Wenjie Chen and Bingshu Huang and Zongran Zhang and Jie Li and Ruishan Gao and Ke Wang and Chuli Hu},
keywords = {Event logic graph, Disaster chain, Geographic environment, Observation planning, Knowledge ontology, Event reasoning},
abstract = {Effective geographic environment observation planning is the key to obtain disaster monitoring and warning information. The previous researches can only make observation plans for a single disaster at some specific stages. They are difficult to apply to the dynamic evolution of the disaster chain. Timely and comprehensive geographic environment observation planning is urgently needed to provide high-value monitoring data for the identification and response of secondary disaster chains. Event logic graph (ELG) shows great potential in evolutionary law expression and chain event reasoning. Therefore, this study proposed an observation ELG (OELG), in which events and their logical relationships are modeled as nodes and edges to express the occurrence and development motivation of observation events. The disaster chain observation planning can be transformed into the reasoning of potential continuous observation events. Subsequently, an OELG-based geographic environment observation planning framework was proposed, which realizes the construction, instantiation, and plan reasoning of OELG. The observation planning experiment was carried out taking the flood disaster chain that occurred in Beijing, China and Nordrhein-Westfalen, Germany as examples. The results show that OELG can generate disaster chain observation plan more timely, comprehensively, and continuously than other models, thus providing support for disaster chain risk monitoring and emergency response.}
}
@article{ZHAO2024107055,
title = {A joint communication and computation design for semantic wireless communication with probability graph},
journal = {Journal of the Franklin Institute},
volume = {361},
number = {13},
pages = {107055},
year = {2024},
issn = {0016-0032},
doi = {https://doi.org/10.1016/j.jfranklin.2024.107055},
url = {https://www.sciencedirect.com/science/article/pii/S0016003224004769},
author = {Zhouxiang Zhao and Zhaohui Yang and Xu Gan and Quoc-Viet Pham and Chongwen Huang and Wei Xu and Zhaoyang Zhang},
keywords = {Semantic communication, Knowledge graph, Probability graph, Joint communication and computation},
abstract = {In this paper, the problem of joint communication and computation design for probability graph-based semantic communication over wireless networks is investigated. In the considered model, the base station (BS) extracts the compressed small-sized semantic data by removing redundant information based on the shared knowledge base between the transceivers. In particular, the knowledge base is represented as a probability graph, which summarizes the statistic relations of massive knowledge graphs. On the user side, the compressed information is accurately inferred on the basis of the same probability graph as the BS. Although this approach brings additional computation resource consumption for semantic information extraction, it effectively reduces communication resource consumption through the transmission of small-sized data. Both the communication and computation cost models are derived based on the inference process of the probability graph. Based on the formulated models, the problem of joint communication and computation resource allocation is proposed to minimize the total energy consumption of the network considering both latency and power limitations. To solve this problem, the closed-form solution of the transmission power is obtained with fixed semantic compression ratio. Then, an effective linear search-based algorithm is proposed to obtain the optimal solution of the considered problem with low complexity. Simulation results demonstrate the effectiveness of the proposed system compared with the conventional non-semantic schemes.}
}
@article{MARTINS2024105385,
title = {Unlocking human-like conversations: Scoping review of automation techniques for personalized healthcare interventions using conversational agents},
journal = {International Journal of Medical Informatics},
volume = {185},
pages = {105385},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105385},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624000480},
author = {Ana Martins and Ana Londral and Isabel {L. Nunes} and Luís {V. Lapão}},
keywords = {Conversational Agents, Automation, Personalization, Natural Language Processing, Artificial Intelligence, Healthcare},
abstract = {Background
Conversational agents (CAs) offer a sustainable approach to deliver personalized interventions and improve health outcomes.
Objectives
To review how human-like communication and automation techniques of CAs in personalized healthcare interventions have been implemented. It is intended for designers and developers, computational scientists, behavior scientists, and biomedical engineers who aim at developing CAs for healthcare interventions.
Methodology
A scoping review was conducted in accordance with PRISMA Extension for Scoping Review. A search was performed in May 2023 in Web of Science, Pubmed, Scopus and IEEE databases. Search results were extracted, duplicates removed, and the remaining results were screened. Studies that contained personalized and automated CAs within the healthcare domain were included. Information regarding study characterization, and human-like communication and automation techniques was extracted from articles that met the eligibility criteria.
Results
Twenty-three studies were selected. These articles described the development of CAs designed for patients to either self-manage their diseases (such as diabetes, mental health issues, cancer, asthma, COVID-19, and other chronic conditions) or to enhance healthy habits. The human-like communication characteristics studied encompassed aspects like system flexibility, personalization, and affective characteristics. Seven studies used rule-based models, eleven applied retrieval-based techniques for content delivery, five used AI models, and six integrated affective computing.
Conclusions
The increasing interest in employing CAs for personalized healthcare interventions is noteworthy. The adaptability of dialogue structures and personalization features is still limited. Unlocking human-like conversations may encompass the use of affective computing and generative AI to help improve user engagement. Future research should focus on the integration of holistic methods to describe the end-user, and the safe use of generative models.}
}
@article{KADDOURA2024101911,
title = {EnhancedBERT: A feature-rich ensemble model for Arabic word sense disambiguation with statistical analysis and optimized data collection},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {1},
pages = {101911},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101911},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823004652},
author = {Sanaa Kaddoura and Reem Nassar},
keywords = {Arabic natural language processing, Word sense disambiguation, Machine learning, Knowledge-based, BERT, Performance evaluation},
abstract = {Accurate assignment of meaning to a word based on its context, known as Word Sense Disambiguation (WSD), remains challenging across languages. Extensive research aims to develop automated methods for determining word senses in different contexts. However, the literature lacks the presence of datasets generated for the Arabic language WSD. This paper presents a dataset comprising a hundred polysemous Arabic words. Each word in the dataset encompasses 3–8 distinct senses, with ten example sentences per sense. Some statistical operations are conducted to gain insights into the dataset, enlightening its characteristics and properties. Subsequently, a novel WSD approach is proposed to utilize similarity measures and find the overlap between contextual information and dictionary definitions. The proposed method uses the power of BERT, a pre-trained language model, to enable effective Arabic word disambiguation. In training, new features are integrated to improve the model's ability to differentiate between various senses of words. The proposed BERT models are combined to compose an ensemble model architecture to improve the classification performances. The performance of the WSD system outperforms state-of-the-art systems, achieving an approximate F1-score of 96 %. Statistical analyses are performed to evaluate the overall performance of the WSD approach by providing additional information on model predictions. A case study was implemented to test the effectiveness of WSD in sentiment analysis, a downstream task.}
}
@article{BAAJ2024109206,
title = {Synergies between machine learning and reasoning - An introduction by the Kay R. Amel group},
journal = {International Journal of Approximate Reasoning},
volume = {171},
pages = {109206},
year = {2024},
note = {Synergies between Machine Learning and Reasoning},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109206},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24000938},
author = {Ismaïl Baaj and Zied Bouraoui and Antoine Cornuéjols and Thierry Denœux and Sébastien Destercke and Didier Dubois and Marie-Jeanne Lesot and João Marques-Silva and Jérôme Mengin and Henri Prade and Steven Schockaert and Mathieu Serrurier and Olivier Strauss and Christel Vrain},
keywords = {Accountability, Background knowledge, Explainability, Neurosymbolic AI, Rule-based models, Uncertainty},
abstract = {This paper proposes a tentative and original survey of meeting points between Knowledge Representation and Reasoning (KRR) and Machine Learning (ML), two areas which have been developed quite separately in the last four decades. First, some common concerns are identified and discussed such as the types of representation used, the roles of knowledge and data, the lack or the excess of information, or the need for explanations and causal understanding. Then, the survey is organised in seven sections covering most of the territory where KRR and ML meet. We start with a section dealing with prototypical approaches from the literature on learning and reasoning: Inductive Logic Programming, Statistical Relational Learning, and Neurosymbolic AI, where ideas from rule-based reasoning are combined with ML. Then we focus on the use of various forms of background knowledge in learning, ranging from additional regularisation terms in loss functions, to the problem of aligning symbolic and vector space representations, or the use of knowledge graphs for learning. Then, the next section describes how KRR notions may benefit to learning tasks. For instance, constraints can be used as in declarative data mining for influencing the learned patterns; or semantic features are exploited in low-shot learning to compensate for the lack of data; or yet we can take advantage of analogies for learning purposes. Conversely, another section investigates how ML methods may serve KRR goals. For instance, one may learn special kinds of rules such as default rules, fuzzy rules or threshold rules, or special types of information such as constraints, or preferences. The section also covers formal concept analysis and rough sets-based methods. Yet another section reviews various interactions between Automated Reasoning and ML, such as the use of ML methods in SAT solving to make reasoning faster. Then a section deals with works related to model accountability, including explainability and interpretability, fairness and robustness. Finally, a section covers works on handling imperfect or incomplete data, including the problem of learning from uncertain or coarse data, the use of belief functions for regression, a revision-based view of the EM algorithm, the use of possibility theory in statistics, or the learning of imprecise models. This paper thus aims at a better mutual understanding of research in KRR and ML, and how they can cooperate. The paper is completed by an abundant bibliography.}
}
@article{WANG2024127674,
title = {A multi-view graph learning model with dual strategies for solving math word problems},
journal = {Neurocomputing},
volume = {587},
pages = {127674},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127674},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224004454},
author = {Zhiwei Wang and Qi Lang and Xiaodong Liu and Wenlin Jing},
keywords = {Math word problem, Natural language processing, Text mining, Multi-view graph learning},
abstract = {Recently, graph-based deep learning models have exhibited remarkable performance in generating solution expressions for the math word problem (MWP). However, most of these models have not taken into account the limitations and errors in constructing prior knowledge graphs, which may affect their accuracy and reliability in practical applications. In addition, during graph learning, they focus on extracting information from each given graph, while neglecting the adaptability and unification of graph representation learning. In this paper, we propose a novel multi-view graph learning-to-tree model with dual-strategy (MVG-DS-T), in which it performs adaptive and consistent multi-view representation learning through two benchmark graphs. Specifically, we construct benchmark graphs via semantic dependency parsing of MWP text, considering both semantic and quantitative aspects, i.e., semantic graph and quantitative graph. Then, the reconstruction strategy is employed to reconstruct the structure of the benchmark graphs to capture the adaptive representation information suitable for downstream tasks, while the alignment strategy is utilized to overcome the limitation of independent view representations by unifying the semantic and quantity embedding information through graph structure. Also, an adaptive length normalized loss balancing term for the tree-based decoder is introduced to control the model focus on label length during training, resulting in better equation generation. Extensive experiments demonstrate the effectiveness of the proposed approach on the MWP task. The empirical results show that MVG-DS-T achieves performance comparable to that of the state-of-the-art graph-based models in the existing literature.}
}
@article{TZIRIDES2024100184,
title = {Combining human and artificial intelligence for enhanced AI literacy in higher education},
journal = {Computers and Education Open},
volume = {6},
pages = {100184},
year = {2024},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2024.100184},
url = {https://www.sciencedirect.com/science/article/pii/S2666557324000247},
author = {Anastasia Olga (Olnancy) Tzirides and Gabriela Zapata and Nikoleta Polyxeni Kastania and Akash K. Saini and Vania Castro and Sakinah A. Ismael and Yu-ling You and Tamara Afonso dos Santos and Duane Searsmith and Casey O'Brien and Bill Cope and Mary Kalantzis},
keywords = {Adult learning, Cooperative learning, Collaborative learning, Human-computer interface, Post-Secondary Education, Teaching/Learning Strategies},
abstract = {This paper seeks to contribute to the emergent literature on Artificial Intelligence (AI) literacy in higher education. Specifically, this convergent, mixed methods case study explores the impact of employing Generative AI (GenAI) tools and cyber-social teaching methods on the development of higher education students’ AI literacy. Three 8-week courses on advanced digital technologies for education in a graduate program in the College of Education at a mid-western US university served as the study sites. Data were based on 37 participants’ experiences with two different types of GenAI tools–a GenAI reviewer and GenAI image generator platforms. The application of the GenAI review tool relied on precision fine-tuning and transparency in AI-human interactions, while the AI image generation tools facilitated the participants’ reflection on their learning experiences and AI's role in education. Students’ interaction with both tools was designed to foster their learning regarding GenAI's strengths and limitations, and their responsible application in educational contexts. The findings revealed that the participants appeared to feel more comfortable using GenAI tools after their course experiences. The results also point to the students’ enhanced ability to understand and critically assess the value of AI applications in education. This study contributes to existing work on AI in higher education by introducing a novel pedagogical approach for AI literacy development showcasing the synergy between humans and artificial intelligence.}
}
@article{ZHENG2023105067,
title = {Dynamic prompt-based virtual assistant framework for BIM information search},
journal = {Automation in Construction},
volume = {155},
pages = {105067},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105067},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003278},
author = {Junwen Zheng and Martin Fischer},
keywords = {Building information modeling, Generative pre-trained transformer, Virtual assistant, Information search, Natural language processing, Artificial intelligence, BIMS-GPT, Prompt engineering, Large language model, Information retrieval},
abstract = {Efficient information search from building information models (BIMs) requires deep BIM knowledge or extensive engineering efforts for building natural language (NL)-based interfaces. To address this challenge, this paper introduces a dynamic prompt-based virtual assistant framework dubbed “BIMS-GPT” that integrates generative pre-trained transformer (GPT) technologies, supporting NL-based BIM search. To understand users' NL queries, extract relevant information from BIM databases, and deliver NL responses along with 3D visualizations, a dynamic prompt-based process was developed. In a case study, BIMS-GPT's functionality is demonstrated through a virtual assistant prototype for a hospital building. When evaluated with a BIM query dataset, the approach achieves accuracy rates of 99.5% for classifying NL queries with incorporating 2% of the data in prompts. This paper contributes to the advancement of effective and versatile virtual assistants for BIMs in the construction industry as it significantly enhances BIM accessibility while reducing the engineering and training data prerequisites for processing NL queries.}
}
@article{YAGER20241933,
title = {Towards a science exocortex},
journal = {Digital Discovery},
volume = {3},
number = {10},
pages = {1933-1957},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00178h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X2400158X},
author = {Kevin G. Yager},
abstract = {Artificial intelligence (AI) methods are poised to revolutionize intellectual work, with generative AI enabling automation of text analysis, text generation, and simple decision making or reasoning. The impact to science is only just beginning, but the opportunity is significant since scientific research relies fundamentally on extended chains of cognitive work. Here, we review the state of the art in agentic AI systems, and discuss how these methods could be extended to have even greater impact on science. We propose the development of an exocortex, a synthetic extension of a person's cognition. A science exocortex could be designed as a swarm of AI agents, with each agent individually streamlining specific researcher tasks, and whose inter-communication leads to emergent behavior that greatly extend the researcher's cognition and volition.}
}
@article{FIEDLER202454,
title = {Generative Pre-trained Transformer for Pediatric Stroke Research: A Pilot Study},
journal = {Pediatric Neurology},
volume = {160},
pages = {54-59},
year = {2024},
issn = {0887-8994},
doi = {https://doi.org/10.1016/j.pediatrneurol.2024.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0887899424002522},
author = {Anna K. Fiedler and Kai Zhang and Tia S. Lal and Xiaoqian Jiang and Stuart M. Fraser},
keywords = {Pediatric stroke, IPSS, GPT, PS-GPT, LLM},
abstract = {Background
Pediatric stroke is an important cause of morbidity in children. Although research can be challenging, large amounts of data have been captured through collaborative efforts in the International Pediatric Stroke Study (IPSS). This study explores the use of an advanced artificial intelligence program, the Generative Pre-trained Transformer (GPT), to enter pediatric stroke data into the IPSS.
Methods
The most recent 50 clinical notes of patients with ischemic stroke or cerebral venous sinus thrombosis at the UTHealth Pediatric Stroke Clinic were deidentified. Domain-specific prompts were engineered for an offline artificial intelligence program (GPT) to answer IPSS questions. Responses from GPT were compared with the human rater. Percent agreement was assessed across 50 patients for each of the 114 queries developed from the IPSS database outcome questionnaire.
Results
GPT demonstrated strong performance on several questions but showed variability overall. In its early iterations it was able to match human judgment occasionally with an accuracy score of 1.00 (n = 20, 17.5%), but it scored as low as 0.26 in some patients. Prompts were adjusted in four subsequent iterations to increase accuracy. In its fourth iteration, agreement was 93.6%, with a maximum agreement of 100% and minimum of 62%. Of 2400 individual items assessed, our model entered 2247 (93.6%) correctly and 153 (6.4%) incorrectly.
Conclusions
Although our tailored generative model with domain-specific prompt engineering and ontological guidance shows promise for research applications, further refinement is needed to enhance its accuracy. It cannot enter data entirely independently, but it can be employed in tandem with human oversight contributing to a collaborative approach that reduces overall effort.}
}
@incollection{LI2024149,
title = {Chapter 7 - Deployment roadmap of proactive human–robot collaboration},
editor = {Shufei Li and Pai Zheng and Lihui Wang},
booktitle = {Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing},
publisher = {Elsevier},
pages = {149-192},
year = {2024},
isbn = {978-0-443-13943-7},
doi = {https://doi.org/10.1016/B978-0-44-313943-7.00014-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443139437000144},
author = {Shufei Li and Pai Zheng and Lihui Wang},
keywords = {Deployment roadmap of proactive human–robot collaboration, Scene perception, Knowledge representation, Decision making, Collaborative control},
abstract = {This chapter presents a stepwise procedure for the development of Proactive HRC systems comprising four key modules: scene perception, knowledge representation, decision making, and collaborative control. For each module, we provide a comprehensive research roadmap of related technologies and offer an advanced algorithm as a feasible solution. The perception module is dedicated to perceiving the human–robot–workspace environment, as detailed in Section 7.1. Meanwhile, knowledge representation focuses on acquiring semantic knowledge of manufacturing tasks and transferring human expertise to robots for cognitive inference, as illustrated in Section 7.2. In Section 7.3, we delve into the decision-making module, which empowers the HRC system to make intelligent decisions for optimized trajectory planning and human information support, adapting to changing environmental conditions. Additionally, Section 7.4 provides an overview of various algorithms for robot collaborative control at the operational level. These four aspects have witnessed the widespread adoption of cutting-edge cognitive computing techniques such as deep learning, reinforcement learning, transfer learning, large language model, etc., resulting in significant enhancements to Proactive HRC system performance.}
}
@article{TRAGER2024466,
title = {Artificial intelligence for nonmelanoma skin cancer},
journal = {Clinics in Dermatology},
volume = {42},
number = {5},
pages = {466-476},
year = {2024},
note = {Artificial Intelligence II},
issn = {0738-081X},
doi = {https://doi.org/10.1016/j.clindermatol.2024.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0738081X24001007},
author = {Megan H. Trager and Emily R. Gordon and Alyssa Breneman and Chunhua Weng and Faramarz H. Samie},
abstract = {Nonmelanoma skin cancers (NMSCs) are among the top five most common cancers globally. NMSC is an area with great potential for novel application of diagnostic tools including artificial intelligence (AI). In this scoping review, we aimed to describe the applications of AI in the diagnosis and treatment of NMSC. Twenty-nine publications described AI applications to dermatopathology including lesion classification and margin assessment. Twenty-five publications discussed AI use in clinical image analysis, showing that algorithms are not superior to dermatologists and may rely on unbalanced, nonrepresentative, and nontransparent training data sets. Sixteen publications described the use of AI in cutaneous surgery for NMSC including use in margin assessment during excisions and Mohs surgery, as well as predicting procedural complexity. Eleven publications discussed spectroscopy, confocal microscopy, thermography, and the AI algorithms that analyze and interpret their data. Ten publications pertained to AI applications for the discovery and use of NMSC biomarkers. Eight publications discussed the use of smartphones and AI, specifically how they enable clinicians and patients to have increased access to instant dermatologic assessments but with varying accuracies. Five publications discussed large language models and NMSC, including how they may facilitate or hinder patient education and medical decision-making. Three publications pertaining to the skin of color and AI for NMSC discussed concerns regarding limited diverse data sets for the training of convolutional neural networks. AI demonstrates tremendous potential to improve diagnosis, patient and clinician education, and management of NMSC. Despite excitement regarding AI, data sets are often not transparently reported, may include low-quality images, and may not include diverse skin types, limiting generalizability. AI may serve as a tool to increase access to dermatology services for patients in rural areas and save health care dollars. These benefits can only be achieved, however, with consideration of potential ethical costs.}
}
@article{MODI2024104603,
title = {Extracting adverse drug events from clinical Notes: A systematic review of approaches used},
journal = {Journal of Biomedical Informatics},
volume = {151},
pages = {104603},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104603},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000212},
author = {Salisu Modi and Khairul Azhar Kasmiran and Nurfadhlina {Mohd Sharef} and Mohd Yunus Sharum},
keywords = {Adverse drug events, Pipeline approach, Joint task learning, Multi-task learning, Named entity recognition, Relation extraction},
abstract = {Background
An adverse drug event (ADE) is any unfavorable effect that occurs due to the use of a drug. Extracting ADEs from unstructured clinical notes is essential to biomedical text extraction research because it helps with pharmacovigilance and patient medication studies.
Objective
From the considerable amount of clinical narrative text, natural language processing (NLP) researchers have developed methods for extracting ADEs and their related attributes. This work presents a systematic review of current methods.
Methodology
Two biomedical databases have been searched from June 2022 until December 2023 for relevant publications regarding this review, namely the databases PubMed and Medline. Similarly, we searched the multi-disciplinary databases IEEE Xplore, Scopus, ScienceDirect, and the ACL Anthology. We adopted the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 statement guidelines and recommendations for reporting systematic reviews in conducting this review. Initially, we obtained 5,537 articles from the search results from the various databases between 2015 and 2023. Based on predefined inclusion and exclusion criteria for article selection, 100 publications have undergone full-text review, of which we consider 82 for our analysis.
Results
We determined the general pattern for extracting ADEs from clinical notes, with named entity recognition (NER) and relation extraction (RE) being the dual tasks considered. Researchers that tackled both NER and RE simultaneously have approached ADE extraction as a “pipeline extraction” problem (n = 22), as a “joint task extraction” problem (n = 7), and as a “multi-task learning” problem (n = 6), while others have tackled only NER (n = 27) or RE (n = 20). We further grouped the reviews based on the approaches for data extraction, namely rule-based (n = 8), machine learning (n = 11), deep learning (n = 32), comparison of two or more approaches (n = 11), hybrid (n = 12) and large language models (n = 8). The most used datasets are MADE 1.0, TAC 2017 and n2c2 2018.
Conclusion
Extracting ADEs is crucial, especially for pharmacovigilance studies and patient medications. This survey showcases advances in ADE extraction research, approaches, datasets, and state-of-the-art performance in them. Challenges and future research directions are highlighted. We hope this review will guide researchers in gaining background knowledge and developing more innovative ways to address the challenges.}
}
@article{WEI2024106559,
title = {A cross-temporal contrastive disentangled model for ancient Chinese understanding},
journal = {Neural Networks},
volume = {179},
pages = {106559},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106559},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024004830},
author = {Yuting Wei and Yangfu Zhu and Ting Bai and Bin Wu},
keywords = {Ancient Chinese understanding, Cross-temporal, Low-resource language, Disentangled representation, Contrastive learning},
abstract = {Ancient Chinese is a crucial bridge for understanding Chinese history and culture. Most existing works utilize high-resource modern Chinese to understand low-resource ancient Chinese, but they fail to fully consider the semantic and syntactic gaps between them due to their changes over time, resulting in the misunderstanding of ancient Chinese. Hence, we propose a novel language pre-training framework for ancient Chinese understanding based on the Cross-temporal Contrastive Disentanglement Model (CCDM), which bridges the gap between modern and ancient Chinese with their parallel corpus. Specifically, we first explore a cross-temporal data augmentation method by disentangling and reconstructing the parallel ancient-modern corpus. It is noteworthy that the proposed decoupling strategy takes full account of the cross-temporal character between ancient and modern Chinese. Then, cross-temporal contrastive learning is exploited to train the model by fully leveraging the cross-temporal information. Finally, the trained language model is utilized for downstream tasks. We conduct extensive experiments on six ancient Chinese understanding tasks. Results demonstrate that our model outperforms the state-of-the-art baselines. Our framework also holds potential applicability to other languages that have undergone evolutionary changes, leading to shifts in syntax and semantics.11Our model is available at https://github.com/yuting-wei/CCDM.}
}
@article{SUN2025111461,
title = {Knowledge Enhanced Prompt Learning Framework for Financial News Recommendation},
journal = {Pattern Recognition},
pages = {111461},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2025.111461},
url = {https://www.sciencedirect.com/science/article/pii/S0031320325001219},
author = {ShaoBo Sun and Xiaoming Pan and Shuang Qi and Jun Gao},
keywords = {News recommendation, Prompt learning, Knowledge graph, Topic, Sentiment},
abstract = {The aim of financial news recommendation systems is to deliver personalized and timely financial information. Traditional methods face challenges, including the complexity of financial news, which requires stock-related external knowledge and accounts for users' interests in various stocks, industries, and concepts. Additionally, the financial domain's timeliness necessitates adaptable recommender systems, especially in few-shot and cold-start scenarios. To address these challenges, we propose a knowledge-enhanced prompt learning framework for financial news recommendation (FNRKPL). FNRKPL incorporates a financial news knowledge graph and transforms triple information into prompt language to strengthen the recommendation model's knowledge base. Personalized prompt templates are designed to account for users' topic preferences and sentiment tendencies, integrating knowledge, topic, and sentiment prompts. Furthermore, a knowledge-enhanced prompt learning mechanism enhances the model's generalization and adaptability in few-shot and cold-start scenarios. Extensive experiments on real-world corporate datasets validate FNRKPL's effectiveness in both data-rich and resource-poor conditions.}
}
@article{KALUTHARAGE2025104318,
title = {Neurosymbolic learning and domain knowledge-driven explainable AI for enhanced IoT network attack detection and response},
journal = {Computers & Security},
volume = {151},
pages = {104318},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104318},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000070},
author = {Chathuranga Sampath Kalutharage and Xiaodong Liu and Christos Chrysoulas},
keywords = {Neurosymbolic learning, Attack detection, Explainable artificial intelligence, Expert knowledge, Threat intelligence},
abstract = {In the dynamic landscape of network security, where cyberattacks continuously evolve, robust and adaptive detection mechanisms are essential, particularly for safeguarding Internet of Things (IoT) networks. This paper introduces an advanced anomaly detection model that utilizes Artificial Intelligence (AI) to identify network anomalies based on traffic features, explaining the most influential factors behind each detected anomaly. The model integrates domain knowledge stored in a knowledge graph to verify whether the detected anomaly constitutes a legitimate attack. Upon validation, the model identifies which core cybersecurity principles—Confidentiality, Integrity, or Availability (CIA)—are violated by mapping influential feature values. This is followed by an alignment with the MITRE ATT&CK framework to provide insights into potential attack tactics, techniques, and intelligence-driven countermeasures. By leveraging explainable AI (XAI) and incorporating expert domain knowledge, our approach bridges the gap between complex AI predictions and human-understandable decision-making, thereby enhancing both detection accuracy and result interpretability. This transparency facilitates faster responses and real-time decision-making while improving adaptability to new, unseen cyber threats. Our evaluation on network traffic datasets demonstrates that the model not only excels in detecting and explaining anomalies but also achieves an overall detection accuracy of 0.97 with the integration of domain knowledge for attack legitimacy. Furthermore, it provides 100% accuracy for threat intelligence based on the MITRE ATT&CK framework, ensuring that security measures are verifiable, actionable, and ultimately strengthen IoT environment defenses by delivering real-time threat intelligence and responses, thus minimizing human response time.}
}
@article{KANG2024249,
title = {The Intelligent Infectious Disease Active Surveillance and early warning system in China: An application of dengue prevention and control},
journal = {Global Transitions},
volume = {6},
pages = {249-255},
year = {2024},
issn = {2589-7918},
doi = {https://doi.org/10.1016/j.glt.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S2589791824000185},
author = {Liangyu Kang and Jian Hu and Kangning Cai and Wenzhan Jing and Min Liu and Wannian Liang},
keywords = {Artificial intelligence, Surveillance, Early warning, Infectious diseases, China},
abstract = {Utilizing advanced information technologies such as big data and artificial intelligence (AI), China has established and implemented the Intelligent Infectious Disease Active Surveillance and Early Warning System. It provides new tools for the surveillance, early warning, and response to infectious diseases, enhancing the timeliness, scientific basis, and efficiency of epidemic control efforts. The system comprises four functional modules including multi-channel active surveillance, intelligent early warning, data-driven risk assessment, and smart emergency response. This paper provides a detailed overview of the structure and functions of the Intelligent Infectious Disease Active Surveillance and Early Warning System in China, with a specific focus on its application in dengue prevention and control in Hainan Province from February to May 2024. Firstly, the system can proactively capture and integrate heterogeneous surveillance data from multiple sources. Based on these multi-channel data, users can select appropriate warning indicators and AI models to automatically trigger early warnings. Using vast amounts of surveillance data, the system can construct machine learning models to accurately assess the transmission risk of infectious diseases. In terms of emergency response, the system offers powerful tools for early diagnosis, smart epidemiological investigation, digital contact tracing, vaccine and drug development, and evaluation of intervention measures. This system facilitates early detection, reporting, and management of outbreaks, serving as a valuable reference for other countries and regions. Nevertheless, continuous efforts are needed to strengthen scientific research and multidisciplinary collaboration, establish reliable data collection mechanisms, enhance continuous model monitoring and adjustments, and leverage the latest large language models. In the future, the system will be further optimized to help control emerging and major infectious diseases more effectively.}
}
@article{RASHIDI2025100687,
title = {Generative Artificial Intelligence in Pathology and Medicine: A Deeper Dive},
journal = {Modern Pathology},
volume = {38},
number = {4},
pages = {100687},
year = {2025},
issn = {0893-3952},
doi = {https://doi.org/10.1016/j.modpat.2024.100687},
url = {https://www.sciencedirect.com/science/article/pii/S0893395224002679},
author = {Hooman H. Rashidi and Joshua Pantanowitz and Alireza Chamanzar and Brandon Fennell and Yanshan Wang and Rama R. Gullapalli and Ahmad Tafti and Mustafa Deebajah and Samer Albahra and Eric Glassy and Matthew G. Hanna and Liron Pantanowitz},
keywords = {ChatGPT, diffusion, generative adversarial network, generative artificial intelligence, generative pretrained transformer, multiagent},
abstract = {This review article builds upon the introductory piece in our 7-part series, delving deeper into the transformative potential of generative artificial intelligence (Gen AI) in pathology and medicine. The article explores the applications of Gen AI models in pathology and medicine, including the use of custom chatbots for diagnostic report generation, synthetic image synthesis for training new models, data set augmentation, hypothetical scenario generation for educational purposes, and the use of multimodal along with multiagent models. This article also provides an overview of the common categories within Gen AI models, discussing open-source and closed-source models, as well as specific examples of popular models such as GPT-4, Llama, Mistral, DALL-E, Stable Diffusion, and their associated frameworks (eg, transformers, generative adversarial networks, diffusion-based neural networks), along with their limitations and challenges, especially within the medical domain. We also review common libraries and tools that are currently deemed necessary to build and integrate such models. Finally, we look to the future, discussing the potential impact of Gen AI on health care, including benefits, challenges, and concerns related to privacy, bias, ethics, application programming interface costs, and security measures.}
}
@article{GUO2024111964,
title = {DP-DDCL: A discriminative prototype with dual decoupled contrast learning method for few-shot object detection},
journal = {Knowledge-Based Systems},
volume = {297},
pages = {111964},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111964},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005987},
author = {Yinsai Guo and Liyan Ma and Xiangfeng Luo and Shaorong Xie},
keywords = {Few-shot object detection, Discriminative prototype, Domain knowledge, Discriminative multimodal cross-attention, Dual decoupled contrast learning},
abstract = {Few-shot object detection (FSOD) can effectively improve object detection performance with limited training data, attracting increasing interest from researchers. Due to the limited sample size, there is often a large variance in the learned features, resulting in deviating from the class center and being confused with other classes. People are often able to accurately identify few-shot objects based on discriminative information. Motivated by it, we propose a discriminative prototype with dual decoupled contrast learning (DP-DDCL) method to address the issue of limited training data in FSOD. By introducing domain knowledge comprising the CLIP and attribute knowledge graph to obtain explicit and implicit semantic and visual information, we construct a novel discriminative prototype that enhances the representation of different classes (especially few-shot samples). Simultaneously, dual decoupled contrast learning consists of the decoupled of positive and negative samples, and the decoupled of discriminative prototype and instance contrast learning is proposed. It makes samples of the same class cluster around their respective class prototype while maintaining a clear semantic boundary between different classes. Extensive experiments demonstrate the efficacy of our method, surpassing the current state-of-the-art in any-shot and all-data splits, with maximum performance gains of up to +9.15% on the standard PASCAL VOC benchmark and +2.5% on the challenging COCO benchmark.}
}
@article{FUCHS2024102735,
title = {Intermediate representations to improve the semantic parsing of building regulations},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102735},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102735},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003835},
author = {Stefan Fuchs and Johannes Dimyadi and Michael Witbrock and Robert Amor},
keywords = {Semantic parsing, Building regulation, Transformer, Intermediate representation, Automated compliance checking},
abstract = {Recent developments show that large transformer-based language models have the capability to generate coherent text and source code in response to user prompts. This capability can be used in the construction domain to interpret building regulations and convert them into a formal representation usable for automated compliance checking. While base-size models can already be taught to perform semantic parsing with decent quality, this paper shows how Intermediate Representations (IRs) can be used to improve the semantic parsing quality. With reversible IRs, the training time was reduced to almost a quarter of the initial duration, and through adding a hierarchical parsing step, improvements of up to 6.6% on F1 scores were reached. Furthermore, intermediate representations provide a novel and interpretable method towards a human-in-the-loop approach for translating building regulations into a formal representation.}
}
@article{FAYE2023102230,
title = {A novel hybrid approach for text encoding: Cognitive Attention To Syntax model to detect online misinformation},
journal = {Data & Knowledge Engineering},
volume = {148},
pages = {102230},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102230},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000903},
author = {Géraud Faye and Wassila Ouerdane and Guillaume Gadek and Souhir Gahbiche and Sylvain Gatepaille},
keywords = {Misinformation detection, Hybrid AI, Neurosymbolism, Natural Language Processing, Classification, Web information systems},
abstract = {Most approaches for text encoding rely on the attention mechanism, at the core of the transformers architecture and large language models. The understanding of this mechanism is still limited and present inconvenients such as lack of interpretability, large requirements of data and low generalization. Based on current understanding of the attention mechanism, we propose CATS (Cognitive Attention To Syntax), a neurosymbolic attention encoding approach based on the syntactic understanding of texts. This approach has on-par to better performance compared to classical attention and displays expected advantages of neurosymbolic AI such as better functioning with little data and better explainability. This layer has been tested on the task of misinformation detection but is general and could be used in any task involving natural language processing.}
}
@article{PURSNANI2023100183,
title = {Performance of ChatGPT on the US fundamentals of engineering exam: Comprehensive assessment of proficiency and potential implications for professional environmental engineering practice},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100183},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000620},
author = {Vinay Pursnani and Yusuf Sermet and Musa Kurt and Ibrahim Demir},
keywords = {ChatGPT, Fundamentals of engineering exam, AI in education, Prompt modification techniques, Large language models (LLMs), Responsible AI integration},
abstract = {In recent years, advancements in artificial intelligence (AI) have led to the development of large language models like GPT-4, demonstrating potential applications in various fields, including education. This study investigates the feasibility and effectiveness of using ChatGPT, a GPT-4 based model, in achieving satisfactory performance on the Fundamentals of Engineering (FE) Environmental Exam. This study further shows a significant improvement in the model's accuracy when answering FE exam questions through noninvasive prompt modifications, substantiating the utility of prompt modification as a viable approach to enhance AI performance in educational contexts. Furthermore, the findings reflect remarkable improvements in mathematical capabilities across successive iterations of ChatGPT models, showcasing their potential in solving complex engineering problems. Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity. By evaluating the performance of ChatGPT in the context of the FE Environmental Exam, this study contributes valuable insights into the potential applications and limitations of large language models in educational settings. As AI continues to evolve, these findings offer a foundation for further research into the responsible and effective integration of AI models across various disciplines, ultimately optimizing the learning experience and improving student outcomes.}
}
@article{BERGOMI2024102924,
title = {Reshaping free-text radiology notes into structured reports with generative question answering transformers},
journal = {Artificial Intelligence in Medicine},
volume = {154},
pages = {102924},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102924},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724001660},
author = {Laura Bergomi and Tommaso M. Buonocore and Paolo Antonazzo and Lorenzo Alberghi and Riccardo Bellazzi and Lorenzo Preda and Chandra Bortolotto and Enea Parimbelli},
keywords = {Natural language processing, Clinical text, Generative artificial intelligence, Radiology, Lymphoma, Biomedical information extraction},
abstract = {Background
Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use. Recently, the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness, and information retrieval. We propose a pipeline to extract information from Italian free-text radiology reports that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma.
Methods
Our work aims to leverage the potential of Natural Language Processing and Transformer-based models to deal with automatic SR registry filling. With the availability of 174 Italian radiology reports, we investigate a rule-free generative Question Answering approach based on the Italian-specific version of T5: IT5. To address information content discrepancies, we focus on the six most frequently filled items in the annotations made on the reports: three categorical (multichoice), one free-text (free-text), and two continuous numerical (factual). In the preprocessing phase, we encode also information that is not supposed to be entered. Two strategies (batch-truncation and ex-post combination) are implemented to comply with the IT5 context length limitations. Performance is evaluated in terms of strict accuracy, f1, and format accuracy, and compared with the widely used GPT-3.5 Large Language Model. Unlike multichoice and factual, free-text answers do not have 1-to-1 correspondence with their reference annotations. For this reason, we collect human-expert feedback on the similarity between medical annotations and generated free-text answers, using a 5-point Likert scale questionnaire (evaluating the criteria of correctness and completeness).
Results
The combination of fine-tuning and batch splitting allows IT5 ex-post combination to achieve notable results in terms of information extraction of different types of structured data, performing on par with GPT-3.5. Human-based assessment scores of free-text answers show a high correlation with the AI performance metrics f1 (Spearman's correlation coefficients>0.5, p-values<0.001) for both IT5 ex-post combination and GPT-3.5. The latter is better at generating plausible human-like statements, even if it systematically provides answers even when they are not supposed to be given.
Conclusions
In our experimental setting, a fine-tuned Transformer-based model with a modest number of parameters (i.e., IT5, 220 M) performs well as a clinical information extraction system for automatic SR registry filling task. It can extract information from more than one place in the report, elaborating it in a manner that complies with the response specifications provided by the SR registry (for multichoice and factual items), or that closely approximates the work of a human-expert (free-text items); with the ability to discern when an answer is supposed to be given or not to a user query.}
}
@article{CHEN2024123478,
title = {GAP: A novel Generative context-Aware Prompt-tuning method for relation extraction},
journal = {Expert Systems with Applications},
volume = {248},
pages = {123478},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123478},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424003439},
author = {Zhenbin Chen and Zhixin Li and Yufei Zeng and Canlong Zhang and Huifang Ma},
keywords = {Relation extraction, Prompt-tuning, Pretrained language model, Few-shot learning, Contrastive learning},
abstract = {Prompt-tuning was proposed to bridge the gap between pretraining and downstream tasks, and it has achieved promising results in Relation Extraction (RE). Although the existing prompt-based RE methods have outperformed the methods based on fine-tuning paradigm, these methods require domain experts to design prompt templates, making them hard to be generalized. In this paper, we propose a Generative context-Aware Prompt-tuning method (GAP) to address these limitations. Our method consists of three crucial modules: (1) a pretrained prompt generator module that extracts or generates the relation triggers from the context and embeds them into the prompt tokens, (2) an in-domain adaptive pretraining module that further trains the Pretrained Language Models (PLMs) to promote the adaptability of the model, and (3) a joint contrastive loss that prevents PLMs from generating unrelated content and optimizes our model more effectively. We observe that the context-enhanced prompt tokens generated by GAP can better guide PLMs to make more accurate predictions. And the in-domain pretraining can effectively inject domain knowledge to enhance the robustness of the model. We conduct experiments on four public RE datasets with supervised and few-shot settings. The experimental results have demonstrated the superiority of GAP over existing benchmark methods and GAP shows remarkable improvements in few-shot settings, with average F1 score enhancements of 3.5%, 2.7%, and 3.4% on the TACRED, TACREV, and Re-TACRED datasets, respectively. Furthermore, GAP still achieved state-of-the-art (SOTA) performance in supervised settings.}
}
@article{WU2024453,
title = {Improving few-shot relation extraction through semantics-guided learning},
journal = {Neural Networks},
volume = {169},
pages = {453-461},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.10.053},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023006196},
author = {Hui Wu and Yuting He and Yidong Chen and Yu Bai and Xiaodong Shi},
keywords = {Few-shot relation extraction, Prototype network, Relation information, Semantics-guided learning, Relation graph learning, Contrastive learning},
abstract = {Few-shot relation extraction (few-shot RE) aims to recognize relations between the entity pair in a given text by utilizing very few annotated instances. As a simple yet efficient approach, prototype network-based methods often directly incorporate relation information to enhance prototype representation or leverage contrastive learning to mitigate prediction confusion. Despite achieving good results, the above methods are still susceptible to false judgments of outlier samples and confusion of similar classes. To address these issues, we propose a novel Semantics-Guided Learning (SemGL) method that more effectively utilizes relation information to enhance both the representations of instances and prototypes for improving the performance of few-shot RE. First, SemGL employs the prompt encoder to encode various prompt templates of instances and relation information and obtains more accurate semantic representations of instances, instance prototypes, and concept prototypes via the prompt enhancement from large language models. Then, SemGL introduces a novel technique called relation graph learning, which leverages concept prototypes to cluster homogeneous instances together, emphasizing relation-specific features of concrete instances. Simultaneously, SemGL employs instance-level contrastive learning between instance prototypes and support instances to distinguish between intra-class instances and inter-class instances to promote shared features among intra-class instances. Additionally, prototype-level contrastive learning leverages concept prototypes to pull closer relation-specific features of the concept prototype and shared features of the instance prototype from the same relation. Finally, SemGL utilizes new relation prototypes that integrate interpretable features of concept prototypes and shared features of instance prototypes for prediction. Experimental results on two publicly available few-shot RE datasets demonstrate the effectiveness and efficiency of SemGL in introducing relation information, with particularly promising results for the domain adaptation challenge task.}
}
@article{YIN2024104731,
title = {Augmenting biomedical named entity recognition with general-domain resources},
journal = {Journal of Biomedical Informatics},
volume = {159},
pages = {104731},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104731},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001497},
author = {Yu Yin and Hyunjae Kim and Xiao Xiao and Chih Hsuan Wei and Jaewoo Kang and Zhiyong Lu and Hua Xu and Meng Fang and Qingyu Chen},
keywords = {Natural Language Processing, Biomedical Named Entity Recognition, Transfer Learning},
abstract = {Objective
Training a neural network-based biomedical named entity recognition (BioNER) model usually requires extensive and costly human annotations. While several studies have employed multi-task learning with multiple BioNER datasets to reduce human effort, this approach does not consistently yield performance improvements and may introduce label ambiguity in different biomedical corpora. We aim to tackle those challenges through transfer learning from easily accessible resources with fewer concept overlaps with biomedical datasets.
Methods
We proposed GERBERA, a simple-yet-effective method that utilized general-domain NER datasets for training. We performed multi-task learning to train a pre-trained biomedical language model with both the target BioNER dataset and the general-domain dataset. Subsequently, we fine-tuned the models specifically for the BioNER dataset.
Results
We systematically evaluated GERBERA on five datasets of eight entity types, collectively consisting of 81,410 instances. Despite using fewer biomedical resources, our models demonstrated superior performance compared to baseline models trained with additional BioNER datasets. Specifically, our models consistently outperformed the baseline models in six out of eight entity types, achieving an average improvement of 0.9% over the best baseline performance across eight entities. Our method was especially effective in amplifying performance on BioNER datasets characterized by limited data, with a 4.7% improvement in F1 scores on the JNLPBA-RNA dataset.
Conclusion
This study introduces a new training method that leverages cost-effective general-domain NER datasets to augment BioNER models. This approach significantly improves BioNER model performance, making it a valuable asset for scenarios with scarce or costly biomedical datasets. We make data, codes, and models publicly available via https://github.com/qingyu-qc/bioner_gerbera.}
}
@incollection{CHANG202319,
title = {Chapter 2 - Data-centric artificial intelligence in health care: progress, shortcomings, and remedies},
editor = {Tung-Hung Su and Jia-Horng Kao},
booktitle = {Artificial Intelligence, Machine Learning, and Deep Learning in Precision Medicine in Liver Diseases},
publisher = {Academic Press},
pages = {19-49},
year = {2023},
isbn = {978-0-323-99136-0},
doi = {https://doi.org/10.1016/B978-0-323-99136-0.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991360000052},
author = {Edward Y. Chang},
keywords = {Artificial intelligence, Data-centric AI, Diagnostic procedure, Generative AI, Health care, KG, KG-GANS, Knowledge-guided, Knowledge-guided GANs, Natural language processing, Pre-trained language model, Prompting},
abstract = {Deep learning owes its success to a large volume of training data, which provide good coverage to all expressions of a semantic. In the medical domain, we desire to have training data that can cover all manifestations of a disease, to train a model that can accurately predict it. Unfortunately, no breakthrough has yet been made in image-based diagnosis because of the lack of high-quality annotated data. This chapter presents the importance of the data-centric approach for learning good representations of a disease. We examine four widely used training data generation and aggregation methods (data augmentation, transfer learning, federated learning, and generative adversarial network [GANs]) and analyze their shortcomings. To remedy such shortcomings, we propose knowledge-guided (KG)-GANs to guide training data generation with domain knowledge. Finally, we discuss our work since 2020 in modeling consciousness to interact with pretrained language models to acquire domain knowledge to regulate KG-GANs.}
}
@article{ZHANG2025129374,
title = {DIRel: Joint relational triple extraction through dual implicit relation},
journal = {Neurocomputing},
volume = {624},
pages = {129374},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129374},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225000463},
author = {Liang Zhang and Nan Zheng},
keywords = {Joint extraction, Dual implicit relation, Knowledge graph},
abstract = {Joint extraction of entities and relations from unstructured texts is essential in information extraction and knowledge graph construction. Existing approaches usually extract entities in one identical relation space to construct relational triples, but ignore the difference between the relation of entity pairs and the relation of token pairs in entity pairs. Therefore, previous joint methods suffer from the problem of coarse-grained relations and redundant information. To address this issue, we propose a novel joint entity and relation extraction model based on dual implicit relation, named DIRel. Specifically, our model consists of an implicit relation predictor and a dual implicit relation tagging scheme. The former predicts all possible implicit relations belonging to each token pair, respectively. The latter ensures an effective decoding process that extracts relational triples according to the implicit relations predicted. With comprehensive experiments on three widely used datasets, we demonstrate that DIRel is more effective and computationally efficient.}
}
@article{CHOU2024104064,
title = {Implicit and explicit commonsense for multi-sentence video captioning},
journal = {Computer Vision and Image Understanding},
volume = {247},
pages = {104064},
year = {2024},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2024.104064},
url = {https://www.sciencedirect.com/science/article/pii/S1077314224001450},
author = {Shih-Han Chou and James J. Little and Leonid Sigal},
keywords = {Instruction generation, Video captioning, Commonsense reasoning},
abstract = {Existing dense or paragraph video captioning approaches rely on holistic representations of videos, possibly coupled with learned object/action representations, to condition hierarchical language decoders. However, they fundamentally lack the commonsense knowledge of the world required to reason about progression of events, causality, and even the function of certain objects within a scene. To address this limitation we propose a novel video captioning Transformer-based model, that takes into account both implicit (visuo-lingual and purely linguistic) and explicit (knowledge-base) commonsense knowledge. We show that these forms of knowledge, in isolation and in combination, enhance the quality of produced captions. Further, inspired by imitation learning, we propose a new task of instruction generation, where the goal is to produce a set of linguistic instructions from a video demonstration of its performance. We formalize the task using the ALFRED dataset generated using an AI2-THOR environment. While instruction generation is conceptually similar to paragraph captioning, it differs in the fact that it exhibits stronger object persistence, as well as spatially-aware and causal sentence structure. We show that our commonsense knowledge enhanced approach produces significant improvements on this task (up to 57% in METEOR and 8.5% in CIDEr), as well as the state-of-the-art result on more traditional video captioning in the ActivityNet Captions dataset.}
}
@article{ABDUL2024108051,
title = {Improving preliminary clinical diagnosis accuracy through knowledge filtering techniques in consultation dialogues},
journal = {Computer Methods and Programs in Biomedicine},
volume = {246},
pages = {108051},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108051},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724000476},
author = {Ashu Abdul and Binghong Chen and Siginamsetty Phani and Jenhui Chen},
keywords = {Disease, Knowledge graph, Natural language processing, Patient syndrome, Preliminary clinical diagnosis, Transformers},
abstract = {Background and Objective
Symptom descriptions by ordinary people are often inaccurate or vague when seeking medical advice, which often leads to inaccurate preliminary clinical diagnoses. To address this issue, we propose a deep learning model named the knowledgeable diagnostic transformer (KDT) for the natural language processing (NLP)-based preliminary clinical diagnoses.
Methods
The KDT extracts symptom-disease relation triples (h,r,t) from patient symptom descriptions by using a proposed bipartite medical knowledge graph (bMKG). To avoid too many relation triples causing the knowledge noise issue, we propose a knowledge inclusion-exclusion approach (KIA) to eliminate undesirable triples (a knowledge filtering layer). Next, we combine token embedding techniques with the transformer model to predict the diseases that patients may encounter.
Results
To train the KDT, a medical diagnosis question-answering dataset (named MDQA dataset) containing large-scale, high-quality questions (patient syndrome description) and answering (diagnosis) corpora with 2.6M entries (1.07GB in size) in Mandarin was built. We also train the KDT with the National Institutes of Health (NIH) English dataset (MedQuAD). The KDT marks a transformative approach by achieving a remarkable accuracy of 99% for different evaluation metrics when compared with the baseline transformers used for the NLP-based preliminary clinical diagnoses approaches.
Conclusions
In essence, our study not only demonstrates the effectiveness of the KDT in enhancing diagnostic precision but also underscores its potential to revolutionize the field of preliminary clinical diagnoses. By harnessing the power of knowledge-based approaches and advanced NLP techniques, we have paved the way for more accurate and reliable diagnoses, ultimately benefiting both healthcare providers and patients. The KDT has the potential to significantly reduce misdiagnoses and improve patient outcomes, marking a pivotal advancement in the realm of medical diagnostics.}
}
@article{CHEN2025107068,
title = {Unified Knowledge-Guided Molecular Graph Encoder with multimodal fusion and multi-task learning},
journal = {Neural Networks},
volume = {184},
pages = {107068},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107068},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024009973},
author = {Mukun Chen and Xiuwen Gong and Shirui Pan and Jia Wu and Fu Lin and Bo Du and Wenbin Hu},
keywords = {Knowledge graphs, Message Passing Neural Networks, Multimodal fusion, Attention mechanism, Molecular modeling},
abstract = {The remarkable success of Graph Neural Networks underscores their formidable capacity to assimilate multimodal inputs, markedly enhancing performance across a broad spectrum of domains. In the context of molecular modeling, considerable efforts have been made to enrich molecular representations by integrating data from diverse aspects. Nevertheless, current methodologies frequently compartmentalize geometric and semantic components, resulting in a fragmented approach that impairs the holistic integration of molecular attributes. This constrained scope limits the generalizability and efficacy of such models in downstream applications. A pivotal challenge lies in harmonizing heterogeneous data sources, particularly in addressing the inherent inconsistencies and sparsity within multimodal molecular datasets. To overcome these limitations, we present the Unified Knowledge-Guided Molecular Graph Encoder (UKGE), a groundbreaking framework that leverages heterogeneous graphs to unify the representation of diverse molecular modalities. Unlike prior methods, UKGE reconciles geometric and semantic features through the use of elemental knowledge graphs (KGs) and meta-path definitions by constructing Unified Molecular Graphs, enabling comprehensive and unified molecular representations. It employs an innovative Meta-Path Aware Message Passing mechanism within its molecular encoder, enhancing the integration of multimodal data. Additionally, a multi-task learning strategy balances data from different modalities, further enriching UKGE’s capability to embed complex biological insights.Empirical evaluations highlight UKGE’s excellence across tasks: DDI prediction achieves 96.91% ACC and 99.14% AUC in warm-start settings, with 83.15% ACC in cold-start scenarios. For CPI prediction, it reaches 0.644 CI on Davis and 0.659 on KIBA. In LBDD, it achieves 99.3% validity, 98.4% uniqueness, and 98.9% novelty, establishing UKGE as a state-of-the-art molecular modeling framework.}
}
@article{LI2024112119,
title = {Multi-aspect Knowledge-enhanced Hypergraph Attention Network for Conversational Recommendation Systems},
journal = {Knowledge-Based Systems},
volume = {299},
pages = {112119},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112119},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124007536},
author = {Xiaokang Li and Yihao Zhang and Yonghao Huang and Kaibei Li and Yunjia Zhang and Xibin Wang},
keywords = {Conversational recommendation systems, Knowledge graph, Hypergraph attention network, Dual attention mechanism},
abstract = {Conversational recommendation systems (CRS) aim to proactively elicit user preferences through multi-turn conversations for item recommendations. However, most existing works focus solely on user’s current conversation information, which fails to capture user implicit preferences comprehensively. Moreover, these approaches primarily center around pairwise relations among data in CRS to enhance item representations, while largely overlooking the complicated relationships in CRS. To address these limitations, we propose a hypergraph-based knowledge-enhanced CRS model namely Multi-aspect Knowledge-enhanced Hypergraph Attention Network for Conversational Recommendation Systems (MKHCR). We construct three hypergraphs based on multiple aspects knowledge to mine high-order relations among data for enhancing user implicit preference representations. Specifically, we build a session hypergraph to capture high-order complicated relations in the historical conversations to explore user implicit preferences. To mitigate the data scarcity issue, we incorporate knowledge graphs and items review information, modeling them within hypergraph structure to learn complicated semantic relationships, thereby enhancing item representations. Moreover, a hypergraph attention network with a dual attention mechanism is proposed to flexibly aggregate important high-order features from these hypergraphs, which contributes to enhance user preference representations for both the recommendation and conversation generation tasks. Extensive experiments on two publicly available CRS datasets validate the effectiveness of our proposed MKHCR model, which exhibits significant improvements across key evaluation metrics, including HR@50, MRR@50, and NDCG@50, achieving enhancements of 6.76%, 9.16%, and 7.92%, respectively.}
}
@article{WANG2023103953,
title = {Are the BERT family zero-shot learners? A study on their potential and limitations},
journal = {Artificial Intelligence},
volume = {322},
pages = {103953},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103953},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000991},
author = {Yue Wang and Lijun Wu and Juntao Li and Xiaobo Liang and Min Zhang},
keywords = {Pre-trained language model, Zero-shot text classification, Prompt-based learning},
abstract = {Starting from the resurgence of deep learning, language models (LMs) have never been so popular. Through simply increasing model scale and data size, large LMs pre-trained with self-supervision objectives demonstrate awe-inspiring results on both task performance and generalization. At the early stage, supervised fine-tuning is indispensable in adapting pre-trained language models (PLMs) to downstream tasks. Later on, the sustained growth of model capacity and data size, as well as newly presented pre-training techniques, make the PLMs perform well under the few-shot setting, especially in the recent paradigm of prompt-based learning. After witnessing the success of PLMs for few-shot tasks, we propose to further study the potential and limitations of PLMs for the zero-shot setting. We utilize 3 models from the most popular BERT family to launch the empirical study on 20 different datasets. We are surprised to find that some simple strategies (without the need of human efforts or unsupervised data) can yield very promising results on a few widely-used datasets, e.g., 88.34%(±0.60) accuracy on the IMDB dataset, and 84.88%(±2.83) accuracy on the Amazon dataset, which outperforms manually created prompts without engineering in achieving much better and stable performance with the accuracy of 74.06%(±13.04), 75.54%(±11.77) for comparison. However, we also observe some limitations of PLMs under the zero-shot setting, particularly for the language understanding tasks (e.g., GLUE, SuperGLUE).2}
}
@article{RIVIERE2024637,
title = {Proceedings from the inaugural Artificial Intelligence in Primary Immune Deficiencies (AIPID) conference},
journal = {Journal of Allergy and Clinical Immunology},
volume = {153},
number = {3},
pages = {637-642},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0091674924000332},
author = {Jacques G. Rivière and Pere {Soler Palacín} and Manish J. Butte},
keywords = {Artificial intelligence, machine learning, large language models, natural language processing, electronic health records, inborn errors of immunity, diagnosis, ethics},
abstract = {Here, we summarize the proceedings of the inaugural Artificial Intelligence in Primary Immune Deficiencies conference, during which experts and advocates gathered to advance research into the applications of artificial intelligence (AI), machine learning, and other computational tools in the diagnosis and management of inborn errors of immunity (IEIs). The conference focused on the key themes of expediting IEI diagnoses, challenges in data collection, roles of natural language processing and large language models in interpreting electronic health records, and ethical considerations in implementation. Innovative AI-based tools trained on electronic health records and claims databases have discovered new patterns of warning signs for IEIs, facilitating faster diagnoses and enhancing patient outcomes. Challenges in training AIs persist on account of data limitations, especially in cases of rare diseases, overlapping phenotypes, and biases inherent in current data sets. Furthermore, experts highlighted the significance of ethical considerations, data protection, and the necessity for open science principles. The conference delved into regulatory frameworks, equity in access, and the imperative for collaborative efforts to overcome these obstacles and harness the transformative potential of AI. Concerted efforts to successfully integrate AI into daily clinical immunology practice are still needed.}
}
@article{WANG2025128829,
title = {Multi-modal soft prompt-tuning for Chinese Clickbait Detection},
journal = {Neurocomputing},
volume = {614},
pages = {128829},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128829},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401600X},
author = {Ye Wang and Yi Zhu and Yun Li and Liting Wei and Yunhao Yuan and Jipeng Qiang},
keywords = {Chinese Clickbait Detection, Soft prompt-tuning, Multi-modal},
abstract = {With the rapid growth of Chinese online services, clickbait has proliferated at an unprecedented rate, designed to manipulate users into clicking for increased traffic or advertising promotion. Such clickbait not only facilitates the spread of fake news and misinformation but also enables click-jacking attacks, redirecting users to deceptive websites that steal personal information. These harmful activities can result in significant losses and serious repercussions. The widespread presence of clickbait underscores both the importance and the challenges of developing effective detection methods. To date, the research paradigm of clickbait detection evolved from deep neural networks to fine-tuned Pre-trained Language Models (PLMs) and, more recently, into prompt-tuning models. However, these methods may suffer two main limitations: (1) they fail to utilize the multi-modal context information in news or posts and explore the higher-level feature representations to enhance the performance of clickbait detection; (2) they largely ignore the diverse range of Chinese expressive forms and neglect the complex semantics and syntactic structures of textual content to assist in learning a better news representation. To overcome these limitations, we proposed a Multi-modal Soft Prompt-tuning Method (MSP) for Chinese Clickbait Detection, which jointly models the textual and image information into a continuous prompt embedding as the input of PLMs. Specifically, firstly, the soft prompt-tuning model including Graph Attention Network and Contrastive Language-Image Pre-training are employed to learn the feature representations of texts and images in news or posts, respectively. Then the obtained text and image representations are re-input into the soft prompt-tuning model with automatic template generation. The extensive experiments on three Chinese clickbait detection datasets demonstrate that our MSP achieved state-of-the-art performance.}
}
@article{SAITO2024125149,
title = {Leveraging multiple behaviors and explicit preferences for job recommendation},
journal = {Expert Systems with Applications},
volume = {258},
pages = {125149},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125149},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424020165},
author = {Yosuke Saito and Kazunari Sugiyama},
keywords = {Job recommendation, Multi-behavior recommendation, Graph representation learning, Metric learning},
abstract = {Numerous job openings have been posted online, indicating the increasing importance of job recommendations. Recently, job seekers often enter their preferences into job search websites to receive some job recommendations that they hope to apply for. To achieve this goal, the following two types of data are available: (1) auxiliary behavior data such as viewing job postings, bookmarking them and (2) explicit preference data such as conditions for a job that each job seeker desires. Although limited research has employed both (1) and (2) simultaneously, no sophisticated job recommendation method leverages multiple types of interactions and explicit preferences to achieve high accuracy. Given this point, we propose a method for job recommendation that employs auxiliary behavior data and each user’s explicit preference data simultaneously. Additionally, our proposed method addresses multiple behavior overlaps and refines the latent representations. Furthermore, the integration method of the latent representations obtained from each of the two modules addresses the consistency of user preferences and the similarity with job postings, enabling a more accurate estimation of user preferences. Experimental results on our dataset constructed from an actual job search website show that our proposed model outperforms the best baseline by 31.4% and 32.8% in terms of Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain@5 (nDCG@5), respectively. We have released our source codes.11https://github.com/saitoxu/JME-ESWA.}
}@article{LI2025308,
title = {Trustworthy AI for human-centric smart manufacturing: A survey},
journal = {Journal of Manufacturing Systems},
volume = {78},
pages = {308-327},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002747},
author = {Dongpeng Li and Shimin Liu and Baicun Wang and Chunyang Yu and Pai Zheng and Weihua Li},
keywords = {Industry 5.0, Human-centric smart manufacturing, Human–machine symbiosis, Trustworthy AI},
abstract = {Human-centric smart manufacturing (HCSM) envisions a symbiotic relationship between humans and machines, leveraging human capability and Artificial Intelligence (AI)’s precision and computational power to achieve mutual enhancement. Trustworthy AI (TAI) is a promising enabler in this transition, ensuring that the integration of AI technologies within manufacturing scenarios is safe, transparent, and participatory. This paper systematically reviews TAI within the context of HCSM by adopting a progressive 3-layer framework. This framework aligns with the developmental stages of HCSM and includes basic safety (protection), advancing to explainability, accountability, and uncertainty awareness (perception), and culminating in continuous updating with human involvement (participation). The review explores the role of TAI across key stages of the product lifecycle, demonstrating how TAI can empower humans and highlighting current advancements while identifying ongoing challenges. The paper concludes by discussing future directions and offering insights for developing TAI-integrated HCSM.}
}
@article{ALI2024100954,
title = {Cognitive systems and interoperability in the enterprise: A systematic literature review},
journal = {Annual Reviews in Control},
volume = {57},
pages = {100954},
year = {2024},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2024.100954},
url = {https://www.sciencedirect.com/science/article/pii/S1367578824000233},
author = {Jana Al Haj Ali and Ben Gaffinet and Hervé Panetto and Yannick Naudet},
keywords = {Cognition, Cognitive systems, Cognitive cyber–physical systems, Cognitive Digital Twin, Cognitive interoperability},
abstract = {The transition from automated processes to mechanisms that manifest intelligence through cognitive abilities such as memorisation, adaptability and decision-making in uncertain contexts, has marked a turning point in the field of industrial systems, particularly in the development of cyber–physical systems and digital twins. This evolution, supported by advances in cognitive science and artificial intelligence, has opened the way to a new era in which systems are able to adapt and evolve autonomously, while offering more intuitive interaction with human users. This article proposes a systematic literature review to gather and analyse current research on Cognitive Cyber–Physical Systems (CCPS), Cognitive Digital Twins (CDT), and cognitive interoperability, which are pivotal in a contemporary Cyber–Physical Enterprise (CPE). From this review, we first seek to understand how cognitive capabilities that are traditionally considered as human traits have been defined and modelled in cyber–physical systems and digital twins in the context of Industry 4.0/5.0, and what cognitive functions they implement. We explore their theoretical foundations, in particular in relation to cognitive psychology and humanities definitions and theories. Then we analyse how interoperability between cognitive systems has been considered, leading to cognitive interoperability, and we highlight the role of knowledge representation and reasoning.}
}
@article{REN2024112595,
title = {Self-labeling in multivariate causality and quantification for adaptive machine learning},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112595},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112595},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124012292},
author = {Yutian Ren and Aaron Haohua Yen and G.P. Li},
keywords = {Adaptive learning, Self-supervised learning, Machine learning, Causality inspired learning, Causal time delay, Noisy label},
abstract = {Adaptive machine learning (ML) aims to allow ML models to adapt to ever-changing environments with potential concept drift after model deployment. Traditionally, adaptive ML requires a new dataset to be manually labeled to tailor deployed models to altered data distributions. Recently, an interactive causality based self-labeling method was proposed to autonomously associate causally related data streams for domain adaptation, showing promising results compared to traditional feature similarity-based semi-supervised learning. Several unanswered research questions remain, including self-labeling’s compatibility with multivariate causality and the quantitative analysis of the auxiliary models used in the self-labeling. The auxiliary models, the interaction time model (ITM) and the effect state detector (ESD), are vital to the success of self-labeling. This paper further develops the self-labeling framework and its theoretical foundations to address these research questions. A framework for the application of self-labeling to multivariate causal graphs is proposed using four basic causal relationships, and the impact of non-ideal ITM and ESD performance is analyzed. A simulated experiment is conducted based on a multivariate causal graph, validating the proposed theory.}
}
@incollection{ZHENG2024,
title = {Machine Learning in Bioinformatics},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00166-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001664},
author = {Huiru Zheng and Jyotsna Talreja Wassan and Haiying Wang},
keywords = {Algorithms, Bioinformatics, Deep learning, Generative AI, Genomics, Machine learning, Natural language processing, Proteomics},
abstract = {The unprecedented growth in scale and type of biological data has attracted the use of machine learning in bioinformatics to build informative models for understanding the underlying biological processes. In this encyclopedia chapter, we aim to provide readers with a general introduction to a few key machine learning models useful in bioinformatics, including the most recently developed techniques of deep neural networks. The chapter discusses how different machine-learning models may be suited to varied biological data. Furthermore, applications and recommendations of machine learning in the field of bioinformatics are highlighted in the chapter.}
}
@article{HASSAN2024128058,
title = {Unfolding Explainable AI for Brain Tumor Segmentation},
journal = {Neurocomputing},
volume = {599},
pages = {128058},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128058},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224008294},
author = {Muhammad Hassan and Ahmed Ameen Fateh and Jieqiong Lin and Yijiang Zhuang and Guisen Lin and Hairui Xiong and Zhou You and Peiwu Qin and Hongwu Zeng},
keywords = {Segmentation, Brain Tumor, Machine Learning, Deep Learning, Explainable AI, Neuro-Symbolic Learning},
abstract = {Brain tumor segmentation (BTS) has been studied from handcrafted engineered features to conventional machine learning (ML) methods, followed by the cutting-edge deep learning approaches. Each recent approach has attempted to overcome the challenges of previous methods and brought conveniences in efficacy, throughput, computation, explainability, investigation, and interpretability. Recently, deep learning (DL) algorithms show excellent performance regarding diverse fields, including image process, computer vision, health analytics, autonomous vehicles, and natural language processes; however, ultimately impediment in making the artificial intelligence explainable and interpretable to clinicians while dealing with critical health informatics and radiomics. Besides the sophisticated deep learning models for brain tumor segmentation, notorious notions like explainability, investigation, trust, and interpretability of DL raised significant concerns for clinicians in their domains. Among many DL methods, the neuro-symbolic learning (NSL) concept has gained more attention as it can contribute to explainable and interpretable AI. In the current study, we survey the prominent approaches, from handcrafted engineering conventional ML to deep learning algorithms, highlight the challenges in DL algorithms, and propose NSL architectures for BTS. Compared to existing surveys, our study not only outlines handcrafted to DL methods for BTS but also proposed explainable and interpretable pipelines appropriate for clinical practices. Our study can better facilitate novice learners in explainable AI and propose efficient, robust, interpretable DL models to facilitate the diagnosis, prognosis, and treatment of BTS.}
}
@article{SANTOSA2024112437,
title = {S3PaR: Section-based Sequential Scientific Paper Recommendation for paper writing assistance},
journal = {Knowledge-Based Systems},
volume = {303},
pages = {112437},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112437},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124010712},
author = {Natasha Christabelle Santosa and Xin Liu and Hyoil Han and Jun Miyazaki},
keywords = {Recommender systems, Sequential recommendation, Dynamic user interest, Scientific paper recommendation, Graph neural networks, Attention mechanisms},
abstract = {A scientific paper recommender system (RS) is very helpful for literature searching in that it (1) helps novice researchers explore their own field and (2) helps experienced researchers explore new fields outside their area of expertise. However, existing RSs usually recommend relevant papers based on users’ static interests, i.e., papers they cited in their past publication(s) or reading histories. In this paper, we propose a novel recommendation task based on users’ dynamic interests during their paper-writing activity. This dynamism is revealed in (for example) the topic shift while writing the Introduction vs. Related Works section. In solving this task, we developed a new pipeline called “Section-based Sequential Scientific Paper Recommendation (S3PaR)”, which recommends papers based on the context of the given user’s currently written paper section. Our experiments demonstrate that this unique task and our proposed pipeline outperform existing standard RS baselines.}
}
@article{PANCHENDRARAJAN2024124097,
title = {Synergizing machine learning & symbolic methods: A survey on hybrid approaches to natural language processing},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124097},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124097},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009631},
author = {Rrubaa Panchendrarajan and Arkaitz Zubiaga},
keywords = {Hybrid NLP, Machine learning, Symbolic methods, Hybrid approaches, Natural language processing},
abstract = {The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning. Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges and future directions, offering a roadmap for future research avenues.}
}
@article{WANG2024123781,
title = {What can rhetoric bring us? Incorporating rhetorical structure into neural related work generation},
journal = {Expert Systems with Applications},
volume = {251},
pages = {123781},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123781},
url = {https://www.sciencedirect.com/science/article/pii/S095741742400647X},
author = {Pancheng Wang and Shasha Li and Jintao Tang and Ting Wang},
keywords = {Natural language processing, Related work generation, Multi-document summarization, Rhetorical structure analysis},
abstract = {The ever-increasing volume of research literature poses challenges for researchers in keeping up with related works in their fields. Automating the generation of related work section holds promise for saving time and effort. However, current models often fall short of producing coherent and logically correct related work with multiple sentences, a phenomenon we refer to as rhetorical structure chaos. Rhetorical structure describes how adjacent spans of units are connected to each other, and logically correct rhetorical structure is essential for a well-structured related work. Hence, to tackle the rhetorical structure chaos issue, this paper explicitly incorporates rhetorical structure information into related work generation. Firstly, we conduct the first rhetorical structure analysis for related work section, which provides insights into understanding the organization and arrangement of contents within related work. Then, based on two preliminary studies on rhetorical structure, we present a novel related work generation model called RSGen, which incorporates rhetorical structure at both the encoding and decoding stages. The encoding stage is facilitated with a rhetorical structure-based graph encoder, while the decoding process is guided by a rhetorical plan — ordered sequence of rhetorical functions of the related work. We conduct extensive experiments on three related work generation datasets to evaluate the performance of our model. The results show that our approach achieves state-of-the-art performance on ROUGE metrics. An ablation study and more analyses further highlight the remarkable efficacy of introducing rhetorical structure into both the encoding and decoding stages.}
}
@article{SUN2025101707,
title = {Knowledge-aware audio-grounded generative slot filling for limited annotated data},
journal = {Computer Speech & Language},
volume = {89},
pages = {101707},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101707},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000901},
author = {Guangzhi Sun and Chao Zhang and Ivan Vulić and Paweł Budzianowski and Philip C. Woodland},
keywords = {Slot filling, Spoken language understanding, Audio-grounding, Contextual biasing, Knowledge base, Generative model, Limited data, Few-shot, Zero-shot},
abstract = {Manually annotating fine-grained slot-value labels for task-oriented dialogue (ToD) systems is an expensive and time-consuming endeavour. This motivates research into slot-filling methods that operate with limited amounts of labelled data. Moreover, the majority of current work on ToD is based solely on text as the input modality, neglecting the additional challenges of imperfect automatic speech recognition (ASR) when working with spoken language. In this work, we propose a Knowledge-Aware Audio-Grounded generative slot filling framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for ToD with speech input. KA2G achieves robust and data-efficient slot filling for speech-based ToD by (1) framing it as a text generation task, (2) grounding text generation additionally in the audio modality, and (3) conditioning on available external knowledge (e.g. a predefined list of possible slot values). We show that combining both modalities within the KA2G framework improves the robustness against ASR errors. Further, the knowledge-aware slot-value generator in KA2G, implemented via a pointer generator mechanism, particularly benefits few-shot and zero-shot learning. Experiments, conducted on the standard speech-based single-turn SLURP dataset and a multi-turn dataset extracted from a commercial ToD system, display strong and consistent gains over prior work, especially in few-shot and zero-shot setups.}
}
@article{KILICOGLU2024104588,
title = {Semantics-enabled biomedical literature analytics},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104588},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104588},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000066},
author = {Halil Kilicoglu and Faezeh Ensan and Bridget McInnes and Lucy Lu Wang}
}
@article{STANCIU2025108526,
title = {Decoding a decade. Trends and evolution in learning analytics: A comprehensive synthesis},
journal = {Computers in Human Behavior},
volume = {165},
pages = {108526},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108526},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224003947},
author = {Ionut Dorin Stanciu and Ángel Hernández-García and Miguel Ángel Conde and Nicolae Nistor},
keywords = {Learning analytics, Topic modeling, Latent Dirichlet Allocation, Gibbs sampling, Machine learning},
abstract = {This article concludes the special issue “Learning Analytics 10 Years After: A Retrospective and Research Agenda” with a summary of the contributed studies. To assess their representativeness of the last decade's learning analytics research, a literature analysis was performed based on topic extraction. Following PRISMA guidelines, 3897 journal articles and conference papers in Learning Analytics were analyzed with Latent Dirichlet Allocation with Gibbs sampling to uncover common topics. Nine primary topics emerged: skills assessment and program evaluation; adoption of learning analytics in higher education; educational tool design and teacher support; student engagement in online courses; predictive modeling in education; technology integration in education; social learning and collaborative knowledge building; data mining in educational research; and online learning environments and student behavior. Time and publication type significantly influenced topic presence. The articles selected for this special issue spanned the most frequent publication themes, demonstrating their representativeness. Furthermore, the review underscores the importance of integrating methodology with educational theory, as highlighted in the authoritative review that opens the issue, paving the way for continued advancements.}
}
@article{ZHIOUA2024e148,
title = {NEW APPROACH OF ARTIFICIAL INTELLIGENCE FOR FERTILITY TREATMENT KNOWLEDGE ACCESS AND DECISION-MAKING},
journal = {Fertility and Sterility},
volume = {122},
number = {4, Supplement },
pages = {e148-e149},
year = {2024},
note = {80th Scientific Congress of the American Society for Reproductive Medicine},
issn = {0015-0282},
doi = {https://doi.org/10.1016/j.fertnstert.2024.07.537},
url = {https://www.sciencedirect.com/science/article/pii/S0015028224011543},
author = {Kais Zhioua and Marouen Braham}
}
@article{THEODOSIOU20243247,
title = {BioTextQuest v2.0: An evolved tool for biomedical literature mining and concept discovery},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3247-3253},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024002757},
author = {Theodosios Theodosiou and Konstantinos Vrettos and Ismini Baltsavia and Fotis Baltoumas and Nikolas Papanikolaou and Andreas Ν. Antonakis and Dimitrios Mossialos and Christos A. Ouzounis and Vasilis J. Promponas and Makrina Karaglani and Ekaterini Chatzaki and Sven Brandau and Georgios A. Pavlopoulos and Evangelos Andreakos and Ioannis Iliopoulos},
keywords = {Biomedical literature mining, Concept discovery},
abstract = {The process of navigating through the landscape of biomedical literature and performing searches or combining them with bioinformatics analyses can be daunting, considering the exponential growth of scientific corpora and the plethora of tools designed to mine PubMed(®) and related repositories. Herein, we present BioTextQuest v2.0, a tool for biomedical literature mining. BioTextQuest v2.0 is an open-source online web portal for document clustering based on sets of selected biomedical terms, offering efficient management of information derived from PubMed abstracts. Employing established machine learning algorithms, the tool facilitates document clustering while allowing users to customize the analysis by selecting terms of interest. BioTextQuest v2.0 streamlines the process of uncovering valuable insights from biomedical research articles, serving as an agent that connects the identification of key terms like genes/proteins, diseases, chemicals, Gene Ontology (GO) terms, functions, and others through named entity recognition, and their application in biological research. Instead of manually sifting through articles, researchers can enter their PubMed-like query and receive extracted information in two user-friendly formats, tables and word clouds, simplifying the comprehension of key findings. The latest update of BioTextQuest leverages the EXTRACT named entity recognition tagger, enhancing its ability to pinpoint various biological entities within text. BioTextQuest v2.0 acts as a research assistant, significantly reducing the time and effort required for researchers to identify and present relevant information from the biomedical literature.}
}
@article{GAN2024102786,
title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102786},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004348},
author = {Lu Gan and Wenbo Chu and Guofa Li and Xiaolin Tang and Keqiang Li},
keywords = {Intelligent transportation systems, Autonomous vehicles, Survey, Large models, Efficient deployment techniques},
abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.}
}
@article{MARINI2024103303,
title = {Multimodal representations of biomedical knowledge from limited training whole slide images and reports using deep learning},
journal = {Medical Image Analysis},
volume = {97},
pages = {103303},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103303},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002287},
author = {Niccolò Marini and Stefano Marchesin and Marek Wodzinski and Alessandro Caputo and Damian Podareanu and Bryan Cardenas Guevara and Svetla Boytcheva and Simona Vatrano and Filippo Fraggetta and Francesco Ciompi and Gianmaria Silvello and Henning Müller and Manfredo Atzori},
keywords = {Computational pathology, Multimodal Learning, Medical ontology, Natural language processing, Colon cancer},
abstract = {The increasing availability of biomedical data creates valuable resources for developing new deep learning algorithms to support experts, especially in domains where collecting large volumes of annotated data is not trivial. Biomedical data include several modalities containing complementary information, such as medical images and reports: images are often large and encode low-level information, while reports include a summarized high-level description of the findings identified within data and often only concerning a small part of the image. However, only a few methods allow to effectively link the visual content of images with the textual content of reports, preventing medical specialists from properly benefitting from the recent opportunities offered by deep learning models. This paper introduces a multimodal architecture creating a robust biomedical data representation encoding fine-grained text representations within image embeddings. The architecture aims to tackle data scarcity (combining supervised and self-supervised learning) and to create multimodal biomedical ontologies. The architecture is trained on over 6,000 colon whole slide Images (WSI), paired with the corresponding report, collected from two digital pathology workflows. The evaluation of the multimodal architecture involves three tasks: WSI classification (on data from pathology workflow and from public repositories), multimodal data retrieval, and linking between textual and visual concepts. Noticeably, the latter two tasks are available by architectural design without further training, showing that the multimodal architecture that can be adopted as a backbone to solve peculiar tasks. The multimodal data representation outperforms the unimodal one on the classification of colon WSIs and allows to halve the data needed to reach accurate performance, reducing the computational power required and thus the carbon footprint. The combination of images and reports exploiting self-supervised algorithms allows to mine databases without needing new annotations provided by experts, extracting new information. In particular, the multimodal visual ontology, linking semantic concepts to images, may pave the way to advancements in medicine and biomedical analysis domains, not limited to histopathology.}
}
@article{KLIMOVA20231,
title = {Strategic Trends in Artificial Intelligence Through Impact of Computational Science: What Young Scientists Should Expect},
journal = {Procedia Computer Science},
volume = {229},
pages = {1-7},
year = {2023},
note = {12th International Young Scientists Conference in Computational Science, YSC2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923019919},
author = {Alexandra Klimova and Denis Nasonov and Alexander Hvatov and Nikolay O. Nikitin and Sergey V. Ivanov and Anna V. Kalyuzhnaya and Alexander Boukhanovsky},
keywords = {Artificial Intelligence, Computational Science, Trends, Impact, Young Scientists},
abstract = {This volume presents selected papers of the 12th Young Scientists Conference in Computational Science (YSC'2023). ITMO University annually organises the event with various academic partners to disseminate current trends in Artificial Intelligence and Computational science among young researchers. In this paper, we present our view on major trends and challenges today in front of scientific and industrial society in this promising area.}
}
@article{LI2025106005,
title = {Towards worker-centric construction scene understanding: Status quo and future directions},
journal = {Automation in Construction},
volume = {171},
pages = {106005},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106005},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525000457},
author = {Huimin Li and Hui Deng and Yichuan Deng},
keywords = {Scene understanding, Construction site, Computer vision, Image captioning, Construction worker},
abstract = {Construction scene understanding is the process of perceiving, analyzing, and interpreting three-dimensional dynamic scenes observed through sensor networks, which is usually real-time. The purpose is to understand the construction scene by analyzing the geometric and semantic features of the objects and their relationships. Construction scene understanding is a basic technology for construction automation and Human-Robot Interaction. Although there are several reviews on this topic, existing reviews do not cover the latest technological advances and lack systematic elaboration for construction scene understanding. Therefore, this paper reviews research on construction scene understanding conducted over the past fifteen years, summarizing five key scene elements: signals, pixels, objects, relationships, and events, along with the richness of semantic information. It also outlines advancements in perception and reasoning abilities in this domain. In addition, this review proposes five research trends and seven future directions to provide some inspiration for researchers.}
}
@article{LIU2024111818,
title = {The fusion of fuzzy theories and natural language processing: A state-of-the-art survey},
journal = {Applied Soft Computing},
volume = {162},
pages = {111818},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111818},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624005921},
author = {Ming Liu and Hongjun Zhang and Zeshui Xu and Kun Ding},
keywords = {Fuzzy theory, Natural language processing, Fusion, Artificial intelligence},
abstract = {Recent years have witnessed a drastic surge in natural language processing (NLP), which is a popular research orientation in artificial intelligence. In contrast to precise numbers, human language is very complex and diverse, with millions of expressions, both spoken and written. It is due to this ambiguity and imprecision that most of the problems in NLP relating to cognition, translation, and understanding are non-trivial. Fuzzy theory, which accepts the fact that ambiguity exists, aims to address and actively quantify conceptual vagueness into messages that can be processed by computers. Following the thread of recent studies, we systematically review the fusion of fuzzy theory and NLP technologies from the aspects of commonly used fuzzy theories in NLP, the NLP tasks fuzzy theories are applied to, the application fields of fusion and the basic paradigms of fusion. Towards the end of this paper, we delineate the constraints and obstacles encountered in current researches, while also endeavoring to suggest avenues for enhancement that may serve as a reference for subsequent scholarly inquiry.}
}
@incollection{HARRER2024345,
title = {Chapter 40 - Artificial intelligence drives the digital transformation of pharma},
editor = {Chayakrit Krittanawong},
booktitle = {Artificial Intelligence in Clinical Practice},
publisher = {Academic Press},
pages = {345-372},
year = {2024},
isbn = {978-0-443-15688-5},
doi = {https://doi.org/10.1016/B978-0-443-15688-5.00049-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443156885000498},
author = {Stefan Harrer and Jeffrey Menard and Michael Rivers and Darren V.S. Green and Joel Karpiak and Jeliazko R. Jeliazkov and Maxim V. Shapovalov and Diego {del Alamo} and Matt C. Sternke},
keywords = {Drug discovery, drug design, drug development, biopharma manufacturing, artificial intelligence, data analytics, pharma, digital health, digital transformation},
abstract = {The evolution of AI has reached a point at which first commoditized analytical tools can be integrated into real-world workflows for assisting human practitioners in the health and life sciences sector by making better and faster decisions. Pharma is embracing this development by exploring the use of AI technology not only for increasing the efficiency and capabilities of drug discovery, design, and development but also for addressing new markets and adopting new business models in digital health. Since 2018 several big pharma enterprises have built substantial in-house capabilities in data science, AI/ML research and development, struck partnerships with big tech, EMR, cloud and analytics vendors, and expanded their data and AI footprint through acquisitions of innovative start- and scale-ups. We shed a light on market and technology drivers for these developments by explaining selected high-impact use cases of AI for the drug development cycle: discovery of new drug targets, custom-design of novel drug compounds, design and execution of more efficient clinical trials, as well as automation and optimization of drug manufacturing, distribution, and marketing channels. Transporting these applications into the digital health sector, we describe why and how pharma explores the use of AI, data, and cloud technology to bring new diagnostic, prognostic, personalized treatment, and prevention schemes for health, disease, and wellness management to patients and caregivers. Focus is given to the following topics: digital therapeutics, digital diagnostics and pathology, telehealth and virtual care, patient monitoring, and interoperable and securely linked electronic health data management platforms. Technology and market trends affecting the near and long-term future of the role of AI as driver of the digital transformation of pharma are provided accompanied by a comprehensive list of technical and strategic articles and market research analyses.}
}
@article{SUTRIAWAN2024200360,
title = {Review of ambiguity problem in text summarization using hybrid ACA and SLR},
journal = {Intelligent Systems with Applications},
volume = {22},
pages = {200360},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200360},
url = {https://www.sciencedirect.com/science/article/pii/S266730532400036X},
author = {Sutriawan Sutriawan and Supriadi Rustad and Guruh Fajar Shidik and Pujiono Pujiono and Muljono Muljono},
keywords = {Systematic_literature_review, Text_summarization, Ambiguity_problem, SLR, Hybrid_ACA_&_SLR},
abstract = {Text summarization is the process of creating a text summary that contains important information from a text document. In recent years, significant progress has been made in the field of text summarization research, along with the challenges that drive research progress in the field at large. The development of textual data has sparked great interest in text summarization research, which is thoroughly reviewed in this survey study. Text summarization research improvements continue to be made to date with various approaches, such as abstractive and extractive. The abstractive approach uses an intermediate representation of the input document to produce a summary that may differ from the original text. The extractive approach means that key sentences are extracted from the source document and combined to form a summary. Despite the various methodologies and approaches recommended, the summaries produced still contain ambiguities that can be interpreted with different meanings, resulting in errors in defining ambiguities, uncertainty in measuring the quality of summaries, difficulty in modeling linguistic context, difficulty in representing semantic meanings, and difficulty in specifying types of ambiguities. This research survey offers a comprehensive exploration of text summarization research, covering challenges, classifications, approaches, preprocessing methods, features, techniques, and evaluation methods, meeting future research needs. The results provide an overview of the state of the art of recent research developments in the topic of ambiguity resolution in text summarization, such as trends in research topics and approaches or techniques used in addressing ambiguity problems in text summarization.}
}
@article{SOUDEEP2024105882,
title = {Enhancing road traffic flow in sustainable cities through transformer models: Advancements and challenges},
journal = {Sustainable Cities and Society},
volume = {116},
pages = {105882},
year = {2024},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2024.105882},
url = {https://www.sciencedirect.com/science/article/pii/S2210670724007066},
author = {Shahriar Soudeep and Most. {Lailun Nahar Aurthy} and Jamin Rahman Jim and M.F. Mridha and Md Mohsin Kabir},
keywords = {Predictive modeling, Transformer models, Sustainable cities, Traffic flow prediction, Urban planning, Traffic management},
abstract = {Efficient traffic flow is crucial for sustainable cities, as it directly impacts energy consumption, pollution levels, and overall quality of life. The integration of superficial intelligence, particularly transformer models, plays a significant role in enhancing the predictive capabilities for traffic management, thereby supporting sustainable urban development. In this survey, we explored the application of transformer models to predict and optimize traffic flow in sustainable cities. These models leverage advanced machine learning to capture intricate spatiotemporal patterns,thereby providing valuable insights for urban planners and traffic management centers. By systematically reviewing the literature, we emphasize the importance of transformer models in urban planning and sustainable resource use. Our study demonstrates how transformer models can learn complex spatiotemporal patterns from traffic data by incorporating both real-time and historical data to enhance prediction accuracy. This improved predictive capability aids the development of smart cities by reducing traffic congestion, facilitating smoother movement for city dwellers and tourists, and ultimately contributing to the sustainability goals of urban areas. This comprehensive review highlights the transformative potential of predictive modeling using transformer models, underscoring their critical role in optimizing urban infrastructure and promoting sustainable city development.}
}
@article{MA2025104156,
title = {A multi-view projection-based object-aware graph network for dense captioning of point clouds},
journal = {Computers & Graphics},
volume = {126},
pages = {104156},
year = {2025},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.104156},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324002917},
author = {Zijing Ma and Zhi Yang and Aihua Mao and Shuyi Wen and Ran Yi and Yongjin Liu},
keywords = {Point clouds, 3D dense captioning, Multimodel, Graph network},
abstract = {3D dense captioning has received increasing attention in the multimodal field of 3D vision and language. This task aims to generate a specific descriptive sentence for each object in the 3D scene, which helps build a semantic understanding of the scene. However, due to inevitable holes in point clouds, there are often incorrect objects in the generated descriptions. Moreover, most existing models use KNN to construct relation graphs, which are not robust and have poor adaptability to different scenes. They cannot represent the relationship between the surrounding objects well. To address these challenges, in this paper, we propose a novel multi-level mixed encoding model for accurate 3D dense captioning of objects in point clouds. To handle holes in point clouds, we extract multi-view projection image features of objects based on our key observation that a hole in an object seldom exists in all projection images from different view angles. Then, the image features are fused with object detection features as the input of subsequent modules. Moreover, we combine KNN and DBSCAN clustering algorithms to construct a graph G and fuse their output features subsequently, which ensures the robustness of the graph structure for accurately describing the relationships between objects. Specifically, DBSCAN clusters are formed based on density, which alleviates the problem of using a fixed K value in KNN. Extensive experiments conducted on ScanRefer and Nr3D datasets demonstrate the effectiveness of our proposed model.}
}
@article{GRENCZUK20245545,
title = {AI-Supported Translation Tools for Legal Texts: A Comparative Analysis},
journal = {Procedia Computer Science},
volume = {246},
pages = {5545-5554},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.707},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027686},
author = {Andrzej Greńczuk and Iwona Chomiak-Orsa and Katarzyna Tryczyńska},
keywords = {Artificial intelligence, legal translation, legal act, machine translation},
abstract = {One of the effects of globalization is the increase in the intensity and importance of international cooperation. The context of internationalization of the functioning of organizations and international contracts has influenced the need to popularize translation services. In the case of everyday language or basic communication processes, the lack of clarity and an appropriate level of quality of translations between any language of the world can cause minor problems and communication problems. However, in the case of contracts, political protocol or legal regulations, the quality of translation processes between languages is very important. Despite the high popularity of IT translation tools, there is still a need for the services of professional, traditional translators, especially when translation processes involve highly specialized vocabulary or highly formalized studies, such as legal regulations. The aim of the article is a comparative analysis of two tools using LLM in the processes of translating legal texts into less popular languages, such as Dutch and Polish. In order to assess the possibility and quality of translation of popular translators such as DeepL and Google Translate, the authors used a scientific experiment in which a sworn translator from Dutch took part, assessing the quality and unambiguity of the translations made by IT tools.}
}
@article{ZHAO2025104044,
title = {Towards human-like questioning: Knowledge base question generation with bias-corrected reinforcement learning from human feedback},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104044},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104044},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004035},
author = {Runhao Zhao and Jiuyang Tang and Weixin Zeng and Yunxiao Guo and Xiang Zhao},
keywords = {Knowledge base question generation, Reinforcement learning from human feedback, Human-like style, Sycophancy and confirmation biases},
abstract = {Knowledge Base Question Generation (KBQG) aims to output natural language questions based on a Knowledge Base (KB) and the target answers. However, existing KBQG solutions neglect the authenticity of the generated questions, and the questions exhibit low diversity and lack of human questioning style. To address this challenge, we for the first time introduce reinforcement learning from human feedback (RLHF) to cope with KBQG and propose a Bias-corrected RLHF framework, i.e., BC-RLHF, to reduce the sycophancy and confirmation biases existing in current RLHF paradigm. Specifically, we begin by training a cold-start question generation model, QG-SFT. We then design a high-quality feedback mechanism to train two models, i.e., the human-like question judge and multi-granular question quality judge, which are adept at evaluating the quality of generated questions. Subsequently, we incorporate these judges and human feedback into the reinforcement learning framework, designing a bias-corrected reinforcement learning model to optimize and train the final question generation model. Consequently, the generated questions exhibit a more human-like style while ensuring fluency and accuracy. Extensive experiments demonstrate the effectiveness of our proposed method in this challenging task, achieving relative performance gains of 24.50%, 4.21% and 2.16% on three KBQG datasets, respectively, outperforming existing state-of-the-art methods.}
}
@article{JURISICA2024102006,
title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {38},
number = {4},
pages = {102006},
year = {2024},
note = {Genetics of Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2024.102006},
url = {https://www.sciencedirect.com/science/article/pii/S1521694224000779},
author = {I Jurisica},
keywords = {Precision medicine, Rheumatoid arthritis, Artificial intelligence, Integrative computational biology},
abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.}
}
@article{CHANG2024108258,
title = {The path from task-specific to general purpose artificial intelligence for medical diagnostics: A bibliometric analysis},
journal = {Computers in Biology and Medicine},
volume = {172},
pages = {108258},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108258},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524003421},
author = {Chuheng Chang and Wen Shi and Youyang Wang and Zhan Zhang and Xiaoming Huang and Yang Jiao},
keywords = {Artificial intelligence, Bibliometric analysis, Diagnostics, Medicine, Network visualization},
abstract = {Artificial intelligence (AI) has revolutionized many fields, and its potential in healthcare has been increasingly recognized. Based on diverse data sources such as imaging, laboratory tests, medical records, and electrophysiological data, diagnostic AI has witnessed rapid development in recent years. A comprehensive understanding of the development status, contributing factors, and their relationships in the application of AI to medical diagnostics is essential to further promote its use in clinical practice. In this study, we conducted a bibliometric analysis to explore the evolution of task-specific to general-purpose AI for medical diagnostics. We used the Web of Science database to search for relevant articles published between 2010 and 2023, and applied VOSviewer, the R package Bibliometrix, and CiteSpace to analyze collaborative networks and keywords. Our analysis revealed that the field of AI in medical diagnostics has experienced rapid growth in recent years, with a focus on tasks such as image analysis, disease prediction, and decision support. Collaborative networks were observed among researchers and institutions, indicating a trend of global cooperation in this field. Additionally, we identified several key factors contributing to the development of AI in medical diagnostics, including data quality, algorithm design, and computational power. Challenges to progress in the field include model explainability, robustness, and equality, which will require multi-stakeholder, interdisciplinary collaboration to tackle. Our study provides a holistic understanding of the path from task-specific, mono-modal AI toward general-purpose, multimodal AI for medical diagnostics. With the continuous improvement of AI technology and the accumulation of medical data, we believe that AI will play a greater role in medical diagnostics in the future.}
}
@article{DEROSARIO2023,
title = {Applications of Natural Language Processing for the Management of Stroke Disorders: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/48693},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000248},
author = {Helios {De Rosario} and Salvador Pitarch-Corresa and Ignacio Pedrosa and Marina Vidal-Pedrós and Beatriz {de Otto-López} and Helena García-Mieres and Lydia Álvarez-Rodríguez},
keywords = {stroke, natural language processing, artificial intelligence, scoping review, scoping, review methods, review methodology, NLP, cardiovascular, machine learning, deep learning},
abstract = {Background
Recent advances in natural language processing (NLP) have heightened the interest of the medical community in its application to health care in general, in particular to stroke, a medical emergency of great impact. In this rapidly evolving context, it is necessary to learn and understand the experience already accumulated by the medical and scientific community.
Objective
The aim of this scoping review was to explore the studies conducted in the last 10 years using NLP to assist the management of stroke emergencies so as to gain insight on the state of the art, its main contexts of application, and the software tools that are used.
Methods
Data were extracted from Scopus and Medline through PubMed, using the keywords “natural language processing” and “stroke.” Primary research questions were related to the phases, contexts, and types of textual data used in the studies. Secondary research questions were related to the numerical and statistical methods and the software used to process the data. The extracted data were structured in tables and their relative frequencies were calculated. The relationships between categories were analyzed through multiple correspondence analysis.
Results
Twenty-nine papers were included in the review, with the majority being cohort studies of ischemic stroke published in the last 2 years. The majority of papers focused on the use of NLP to assist in the diagnostic phase, followed by the outcome prognosis, using text data from diagnostic reports and in many cases annotations on medical images. The most frequent approach was based on general machine learning techniques applied to the results of relatively simple NLP methods with the support of ontologies and standard vocabularies. Although smaller in number, there has been an increasing body of studies using deep learning techniques on numerical and vectorized representations of the texts obtained with more sophisticated NLP tools.
Conclusions
Studies focused on NLP applied to stroke show specific trends that can be compared to the more general application of artificial intelligence to stroke. The purpose of using NLP is often to improve processes in a clinical context rather than to assist in the rehabilitation process. The state of the art in NLP is represented by deep learning architectures, among which Bidirectional Encoder Representations from Transformers has been found to be especially widely used in the medical field in general, and for stroke in particular, with an increasing focus on the processing of annotations on medical images.}
}
@article{DAROCHAFRANCO2024100708,
title = {Managing and controlling digital role-playing game elements: A current state of affairs},
journal = {Entertainment Computing},
volume = {51},
pages = {100708},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000764},
author = {Artur de Oliveira {da Rocha Franco} and Windson Viana {de Carvalho} and José Wellington Franco {da Silva} and José Gilvan Rodrigues Maia and Miguel Franklin {de Castro}},
keywords = {RPG, Artificial intelligence, Procedural content generation},
abstract = {Role-playing games (RPGs) are interactive gaming experiences designed and driven by the intricate development of narratives and characters. However, these games also highlight elements, such as combat, exploration, creativity, and crafting. Game developers helped to shape the market by incorporating RPGs into digital media; however, they suffered restrictions in their game design and storytelling caused by technological limitations. Fortunately, academia has studied these challenges and mitigated these latter issues to better understand and connect digital RPGs with their analog versions. Our research offers a comprehensive overview of studies centered on technologies for managing, generating, and controlling digital RPG elements. We aim to characterize Artificial Intelligence research within this domain and elucidate the diverse techniques employed. In this context, we examined 72 papers about Procedural Content Generation (PCG) in RPG, which were identified by mixing a database search with a snowballing method. In this paper, we provide an overview of PCG techniques investigated within the realm of RPGs and present the trending approaches for future developments in the field.}
}
@article{LIU2024112041,
title = {Event extraction as machine reading comprehension with question-context bridging},
journal = {Knowledge-Based Systems},
volume = {299},
pages = {112041},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112041},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124006750},
author = {Liu Liu and Ming Liu and Shanshan Liu and Kun Ding},
keywords = {Event extraction, Natural language processing, Graph neural network, Machine reading comprehension},
abstract = {Most existing methods regard event extraction as the classification task. They not only heavily rely on named entity recognition, causing error propagation, but are also inefficient in low resource scenarios. To deal with above challenges, we propose an improved machine reading comprehension (MRC) approach, namely MRCBEE. Firstly, a new paradigm is applied to redefine event extraction as MRC task by designing question templates for event detection and argument extraction tasks. Specially, Question-Context Bridging is a new graph structure drawn to reconstruct the semantic relationship between the template and the text, in order to strengthen the prompt role of question templates. Then, a cross-domain attention module is designed to integrate both the semantic feature and global feature of words. A new GNN based on gated mechanism is proposed to capture the global feature and filter the information of invalid neighbors. Finally, the results of our experiments show that MRCBEE achieves better performance than the state-of-the-art methods on ACE2005 and ERE datasets. And it outperforms prior methods in low resource and complex text scenarios.}
}
@article{SUKHOBOKOV2024101279,
title = {A universal knowledge model and cognitive architectures for prototyping AGI},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101279},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000731},
author = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
keywords = {Cognitive architecture, AGI, Metagraph, Archigraph, Universal knowledge model, Machine consciousness, Machine subconsciousness, Machine reflection, Machine worldview},
abstract = {The article identified 56 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a reference cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph model are used, constructed as a development of annotated metagraphs. As other components, the reference cognitive architecture being developed includes following modules: machine consciousness, machine subconsciousness, interaction with the external environment, a goal management, an emotional control, social interaction, reflection, ethics, worldview, learning, monitoring, statement problems, solving problems, self-organization and meta learning. Based on the composition of the proposed reference architecture modules, existing cognitive architectures containing the following modules were analyzed: machine consciousness, machine subconsciousness, reflection, worldview.}
}
@article{PEREZPEREZ2024103960,
title = {Insights into wheat science: A bibliometric review using unsupervised machine learning techniques},
journal = {Journal of Cereal Science},
volume = {118},
pages = {103960},
year = {2024},
issn = {0733-5210},
doi = {https://doi.org/10.1016/j.jcs.2024.103960},
url = {https://www.sciencedirect.com/science/article/pii/S0733521024001188},
author = {Martín Pérez-Pérez and Miguel Ribeiro and Florentino Fdez-Riverola and Gilberto Igrejas},
keywords = {Wheat, Literature analysis, Knowledge extraction, Machine learning, LLM, Clustering},
abstract = {Wheat (Triticum spp.) has been one of the most important cereal crops, serving as a source of protein and energy in the human diet. It remains a vital component of global food security, with extensive scientific literature dedicated to its study, although the large volume of literature often hinders global analysis. In this study, different unsupervised machine learning techniques, such as K-Nearest Neighbors (KNN) and Uniform Manifold Approximation and Projection (UMAP), text mining analyses, including word embeddings and statistical word analysis, and graph analysis methodologies, were applied to gain a deeper understanding of the wheat literature. The proposed bibliometric analysis was conducted and integrated with the Journal Citation Reports (JCR) to identify major wheat research trends in the PubMed literature. This analysis examined the evolution of these trends over time, evaluated the geographical distribution, impact, and research domains, and assessed author collaboration networks and the evolving relevance of different countries. Research on disease resistance, genetic modification, and dietary impact demonstrates a consistent increase in output, while interest in topics related to overcoming salt stress and enhancing animal feed appears to be diminishing. Interestingly, research on wheat germ agglutinin saw a surge in interest during the late 2000s, stabilizing thereafter. These trends underscore the dynamic nature of wheat research, driven by evolving priorities and technological advancements, particularly in genetics and omics tools. Moreover, the increasing significance of China in wheat research, including its size, impact, and networking, alongside longstanding leaders such as the United States, signals a shifting landscape in global wheat research.}
}
@article{YANG2024167263,
title = {Autophagy and machine learning: Unanswered questions},
journal = {Biochimica et Biophysica Acta (BBA) - Molecular Basis of Disease},
volume = {1870},
number = {6},
pages = {167263},
year = {2024},
issn = {0925-4439},
doi = {https://doi.org/10.1016/j.bbadis.2024.167263},
url = {https://www.sciencedirect.com/science/article/pii/S0925443924002527},
author = {Ying Yang and Zhaoying Pan and Jianhui Sun and Joshua Welch and Daniel J. Klionsky},
keywords = {Artificial intelligence, Lysosome, Macroautophagy, Stress},
abstract = {Autophagy is a critical conserved cellular process in maintaining cellular homeostasis by clearing and recycling damaged organelles and intracellular components in lysosomes and vacuoles. Autophagy plays a vital role in cell survival, bioenergetic homeostasis, organism development, and cell death regulation. Malfunctions in autophagy are associated with various human diseases and health disorders, such as cancers and neurodegenerative diseases. Significant effort has been devoted to autophagy-related research in the context of genes, proteins, diagnosis, etc. In recent years, there has been a surge of studies utilizing state of the art machine learning (ML) tools to analyze and understand the roles of autophagy in various biological processes. We taxonomize ML techniques that are applicable in an autophagy context, comprehensively review existing efforts being taken in this direction, and outline principles to consider in a biomedical context. In recognition of recent groundbreaking advances in the deep-learning community, we discuss new opportunities in interdisciplinary collaborations and seek to engage autophagy and computer science researchers to promote autophagy research with joint efforts.}
}
@article{TANG2024110839,
title = {Cyber threat indicators extraction based on contextual knowledge prompt},
journal = {Computer Networks},
volume = {254},
pages = {110839},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110839},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624006716},
author = {Hailiang Tang and Dawei Lin and Wanyu Li and Wenxiao Zhang and Jun Zhao},
keywords = {Cyber threat intelligence, Cyber security, IOC extraction, Interpretability, Social data},
abstract = {Extracting Indicators of Compromise (IOC) from security-related social data (e.g., security blogs, hacker forums) is crucial for predicting cyber risks and mitigating cyber attacks proactively. However, existing IOC extraction approaches suffer from two serious limitations. First, they fail to learn the multiculti-granular and fine-grained IOC features, resulting in high false positives. Second, current methods cannot incorporate symbolic rules and contextual knowledge, resulting in poor interpretability. In this paper, we propose AIIOC, an Accurate and Interpretable I O C extraction model based on contextual knowledge prompts. Particularly, AIIOC first proposes a multi-granularity attention mechanism to learn fine-grained IOC features and boost the accuracy of IOC identification. Additionally, AIIOC designs a novel sequence labeling method that integrates symbolic rules and contextual knowledge prompts, which can encode symbolic rules and contextual semantics of IOC in trainable recurrent neural networks to improve both accuracy and interpretability. Experimental results on two real-world datasets verify that AIIOC outperforms state-of-the-art methods and showcases promising interpretability by incorporating symbolic rules and contextual knowledge prompts.}
}
@article{DU2025102755,
title = {Natural language processing in finance: A survey},
journal = {Information Fusion},
volume = {115},
pages = {102755},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102755},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005335},
author = {Kelvin Du and Yazhi Zhao and Rui Mao and Frank Xing and Erik Cambria},
keywords = {Natural language processing, Finance, Financial sentiment analysis, Financial narrative processing, Financial forecasting, Portfolio management, Question answering, Risk management, Regulatory compliance, ESG and sustainable finance, Explainable AI, Digital assets},
abstract = {This survey presents an in-depth review of the transformative role of Natural Language Processing (NLP) in finance, highlighting its impact on ten major financial applications: (1) financial sentiment analysis, (2) financial narrative processing, (3) financial forecasting, (4) portfolio management, (5) question answering, virtual assistant and chatbot, (6) risk management, (7) regulatory compliance monitoring, (8) Environmental, Social, Governance (ESG) and sustainable finance, (9) explainable artificial intelligence (XAI) in finance and (10) NLP for digital assets. With the integration of vast amounts of unstructured financial data and advanced NLP techniques, the study explores how NLP enables data-driven decision-making and innovation in the financial sector, alongside the limitations and challenges. By providing a comprehensive analysis of NLP applications combining both academic and industrial perspectives, this study postulates the future trends and evolution of financial services. It introduces a unique review framework to understand the interaction of financial data and NLP technologies systematically and outlines the key drivers, transformations, and emerging areas in this field. This survey targets researchers, practitioners, and professionals, aiming to close their knowledge gap by highlighting the significance and future direction of NLP in enhancing financial services.}
}
@article{ISAEE2025121643,
title = {Addressing the challenges of open n-ary relation extraction with a deep learning-driven approach},
journal = {Information Sciences},
volume = {692},
pages = {121643},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121643},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524015573},
author = {Mitra Isaee and Afsaneh Fatemi and Mohammadali Nematbakhsh},
keywords = {Natural language processing, Open relation extraction, Open-domain text, N-ary relation, Entity embedding, SpanBERT},
abstract = {Open relation extraction is a critical task in natural language processing aimed at automatically extracting relations between entities in open-domain corpora. Most existing systems focus on extracting binary relations (relations between two entities) while extracting more complex n-ary relations (involving more than two entities) remains a significant challenge. Additionally, many previous systems rely on hand-crafted patterns and natural language processing tools, which result in error accumulation and reduced accuracy. The current study proposes a novel approach to open n-ary relation extraction that leverages recent advancements in deep learning architectures. This approach addresses the limitations of existing open relation extraction systems, particularly their reliance on hand-crafted patterns and their focus on binary relations. It utilizes SpanBERT to capture relational patterns from text data directly and introduces entity embedding vectors to create distinct representations of entities within sentences. These vectors enhance the proposed system’s understanding of the entities within the input sentence, leading to more accurate relation extraction. Notably, the proposed system in the present study achieves an F1-score of 89.79 and 92.67 on the LSOIE-wiki and OpenIE4 datasets, outperforming the best existing models by over 12% and 10%, respectively. These results highlight the effectiveness of the proposed approach in addressing the challenges of open n-ary relation extraction.}
}
@article{LAVALLE2025102410,
title = {A methodology for the systematic design of storytelling dashboards applied to Industry 4.0},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102410},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102410},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000059},
author = {Ana Lavalle and Alejandro Maté and Maribel Yasmina Santos and Pedro Guimarães and Juan Trujillo and Antonina Santos},
keywords = {Analytical requirements, Big data, Data visualization, Storytelling dashboards, Industrial processes},
abstract = {Dashboards are popular tools for presenting key insights to decision-makers by translating large volumes of data into clear information. However, while individual visualizations may effectively answer specific questions, they often fail to connect in a way that conveys the overall narrative, leaving decision-makers without a cohesive understanding of the area under analysis. This paper presents a novel methodology for the systematic design of holistic dashboards, moving from analytical requirements to storytelling dashboards. Our approach ensures that all visualizations are aligned with the analytical goals of decision-makers. It includes several key steps: capturing analytical requirements through the i* framework; structuring and refining these requirements into a tree model to reflect the decision-maker’s mental analysis; identifying and preparing relevant data; capturing the key concepts and relationships for the composition of the cohesive storytelling dashboard through a novel storytelling conceptual model; finally, implementing and integrating the visualizations into the dashboard, ensuring coherence and alignment with the decision-maker’s needs. Our methodology has been applied in real-world industrial environments. We evaluated its impact through a controlled experiment. The findings show that storytelling dashboards significantly improve data interpretation, reduce misinterpretations, and enhance the overall user experience compared to traditional dashboards.}
}
@article{ZHANG2025128695,
title = {Cross-attention multi-perspective fusion network based fake news censorship},
journal = {Neurocomputing},
volume = {611},
pages = {128695},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128695},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014668},
author = {Weishan Zhang and Mingli Zhang and Zhicheng Bao and Zhenqi Wang},
keywords = {Fake news censorship, Internal perspectives, News environment, Cross-attention, LLMs},
abstract = {Current fake news censorship models mostly use only one single semantic perspective, which contains insufficient information and may result in biases. However, news inherently encompasses multiple perspectives, including internal elements such as semantics, emotion, and style, as well as the news environment which may offer valuable information for news censorship. Combining multiple perspectives helps mitigate the biases and provides comprehensive information for news censorship. Thus, we propose the Cross-Attention Multi-Perspective Fusion Network Based Fake News Censorship (MPFNC). In this model, we design an Internal Multi-Perspective Fusion Network to integrate the internal perspectives of news, including semantic, emotion, and style, and equip it with Cross-Attention to capture the correlations between these perspectives. Additionally, we construct the news environment based on the fuzzy cluster and apply Cross-Attention to uncover associations with the internal perspectives. Compared to the current SOTA models, our model achieves a 3.04% and 3.86% improvement in F1 score on Chinese and English datasets, respectively. On fake news datasets generated by LLMs, our model also outperforms the best-performing LLM by 10% in accuracy.}
}
@article{HU2024103742,
title = {DLRGeoTweet: A comprehensive social media geocoding corpus featuring fine-grained places},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103742},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103742},
url = {https://www.sciencedirect.com/science/article/pii/S030645732400102X},
author = {Xuke Hu and Tobias Elßner and Shiyu Zheng and Helen Ngonidzashe Serere and Jens Kersten and Friederike Klan and Qinjun Qiu},
keywords = {Annotated twitter corpus, Geoparsing, Geocoding, Toponym resolution, Toponym disambiguation, Fine-grained places},
abstract = {Every day, many short text messages on social media are generated in response to real-world events, providing a valuable resource for various domains such as emergency response and traffic management. Since exact coordinates of social media posts are rarely attached by users, accurately recognizing and resolving fine-grained place names, such as home addresses and Points of Interest, from these posts is crucial for understanding the precise locations of critical events, such as rescue requests. This task, known as geoparsing, involves toponym recognition and toponym resolution or geocoding. However, existing social media datasets for evaluating geoparsing approaches often lack sufficient fine-grained place names with associated geo-coordinates or linked to gazetteers, making evaluating, comparing, and training geocoding methods for such locations challenging. Moreover, the absence of supportive annotation tools compounds this challenge. To address these gaps, we implemented a lightweight Python tool leveraging Nominatim. Using this tool, we annotated a comprehensive X (formerly Twitter) geocoding corpus called DLRGeoTweet. The corpus underwent a rigorous cross-validation process to guarantee its quality. This corpus includes a total of 7,364 tweets and 12,510 places, of which 6,012 are fine-grained. It comprises two global datasets encompassing worldwide events and three local datasets related to local events such as the 2017 Hurricane Harvey. The annotation process spanned over ten months and required approximately 1000 person-hours to complete. We then evaluate 15 latest and representative geocoding approaches, including many deep learning-based, on DLRGeoTweet. The results highlight the inherent challenges in resolving fine-grained places accurately. Despite increasing access constraints to Twitter data, our corpus’s focus on short, informal text makes it a valuable resource for geocoding across multiple social media platforms.}
}
@article{FEHER2025109492,
title = {Learning to generate and evaluate fact-checking explanations with transformers},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109492},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109492},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624016506},
author = {Darius Feher and Abdullah Khered and Hao Zhang and Riza Batista-Navarro and Viktor Schlegel},
keywords = {Explainable fact checking, Natural language generation, Automated natural language generation evaluation, Explainability, Fact verification},
abstract = {In an era increasingly dominated by digital platforms, the spread of misinformation poses a significant challenge, highlighting the need for solutions capable of assessing information veracity. Our research contributes to the field of Explainable Artificial Antelligence (XAI) by developing transformer-based fact-checking models that contextualise and justify their decisions by generating human-accessible explanations. Importantly, we also develop models for automatic evaluation of explanations for fact-checking verdicts across different dimensions such as (self)-contradiction, hallucination, convincingness and overall quality. By introducing human-centred evaluation methods and developing specialised datasets, we emphasise the need for aligning Artificial Intelligence (AI)-generated explanations with human judgements. This approach not only advances theoretical knowledge in XAI but also holds practical implications by enhancing the transparency, reliability and users’ trust in AI-driven fact-checking systems. Furthermore, the development of our metric learning models is a first step towards potentially increasing efficiency and reducing reliance on extensive manual assessment. Based on experimental results, our best performing generative model achieved a Recall-Oriented Understudy for Gisting Evaluation-1 (ROUGE-1) score of 47.77 demonstrating superior performance in generating fact-checking explanations, particularly when provided with high-quality evidence. Additionally, the best performing metric learning model showed a moderately strong correlation with human judgements on objective dimensions such as (self)-contradiction and hallucination, achieving a Matthews Correlation Coefficient (MCC) of around 0.7.}
}
@article{THO2024104674,
title = {Improving biomedical Named Entity Recognition with additional external contexts},
journal = {Journal of Biomedical Informatics},
volume = {156},
pages = {104674},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104674},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000923},
author = {Bui Duc Tho and Minh-Tien Nguyen and Dung Tien Le and Lin-Lung Ying and Shumpei Inoue and Tri-Thanh Nguyen},
keywords = {Biomedical Named Entity Recognition, Information extraction, Transformers, External contexts},
abstract = {Objective:
Biomedical Named Entity Recognition (bio NER) is the task of recognizing named entities in biomedical texts. This paper introduces a new model that addresses bio NER by considering additional external contexts. Different from prior methods that mainly use original input sequences for sequence labeling, the model takes into account additional contexts to enhance the representation of entities in the original sequences, since additional contexts can provide enhanced information for the concept explanation of biomedical entities.
Methods:
To exploit an additional context, given an original input sequence, the model first retrieves the relevant sentences from PubMed and then ranks the retrieved sentences to form the contexts. It next combines the context with the original input sequence to form a new enhanced sequence. The original and new enhanced sequences are fed into PubMedBERT for learning feature representation. To obtain more fine-grained features, the model stacks a BiLSTM layer on top of PubMedBERT. The final named entity label prediction is done by using a CRF layer. The model is jointly trained in an end-to-end manner to take advantage of the additional context for NER of the original sequence.
Results:
Experimental results on six biomedical datasets show that the proposed model achieves promising performance compared to strong baselines and confirms the contribution of additional contexts for bio NER.
Conclusion:
The promising results confirm three important points. First, the additional context from PubMed helps to improve the quality of the recognition of biomedical entities. Second, PubMed is more appropriate than the Google search engine for providing relevant information of bio NER. Finally, more relevant sentences from the context are more beneficial than irrelevant ones to provide enhanced information for the original input sequences. The model is flexible to integrate any additional context types for the NER task.}
}
@article{GAO2024723,
title = {Artificial Intelligence in manufacturing: State of the art, perspectives, and future directions},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {723-749},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.04.101},
url = {https://www.sciencedirect.com/science/article/pii/S000785062400115X},
author = {Robert X. Gao and Jörg Krüger and Marion Merklein and Hans-Christian Möhring and József Váncza},
keywords = {Artificial intelligence, Smart manufacturing, Machine learning},
abstract = {Inspired by the natural intelligence of humans and bio-evolution, Artificial Intelligence (AI) has seen accelerated growth since the beginning of the 21st century. Successful AI applications have been broadly reported, with Industry 4.0 providing a thematic platform for AI-related research and development in manufacturing. This paper highlights applications of AI in manufacturing, ranging from production system design and planning to process modeling, optimization, quality assurance, maintenance, automated assembly and disassembly. In addition, the paper presents an overview of representative manufacturing problems and matching AI solutions, and a perspective of future research to leverage AI towards the realization of smart manufacturing.}
}
@article{FRIEDMAN2024385,
title = {Editorial},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {10},
number = {4},
pages = {385-388},
year = {2024},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S240587262400100X},
author = {Ken Friedman and Yongqi Lou and Jin Ma}
}
@article{NICOLSON2023102633,
title = {Improving chest X-ray report generation by leveraging warm starting},
journal = {Artificial Intelligence in Medicine},
volume = {144},
pages = {102633},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102633},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723001471},
author = {Aaron Nicolson and Jason Dowling and Bevan Koopman},
keywords = {Chest X-ray report generation, Image captioning, Multi-modal learning, Warm starting},
abstract = {Automatically generating a report from a patient’s Chest X-rays (CXRs) is a promising solution to reducing clinical workload and improving patient care. However, current CXR report generators—which are predominantly encoder-to-decoder models—lack the diagnostic accuracy to be deployed in a clinical setting. To improve CXR report generation, we investigate warm starting the encoder and decoder with recent open-source computer vision and natural language processing checkpoints, such as the Vision Transformer (ViT) and PubMedBERT. To this end, each checkpoint is evaluated on the MIMIC-CXR and IU X-ray datasets. Our experimental investigation demonstrates that the Convolutional vision Transformer (CvT) ImageNet-21K and the Distilled Generative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for warm starting the encoder and decoder, respectively. Compared to the state-of-the-art (M2 Transformer Progressive), CvT2DistilGPT2 attained an improvement of 8.3% for CE F-1, 1.8% for BLEU-4, 1.6% for ROUGE-L, and 1.0% for METEOR. The reports generated by CvT2DistilGPT2 have a higher similarity to radiologist reports than previous approaches. This indicates that leveraging warm starting improves CXR report generation. Code and checkpoints for CvT2DistilGPT2 are available at https://github.com/aehrc/cvt2distilgpt2.}
}
@article{WEI20243299,
title = {Graph Convolutional Networks Embedding Textual Structure Information for Relation Extraction},
journal = {Computers, Materials and Continua},
volume = {79},
number = {2},
pages = {3299-3314},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.047811},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824002613},
author = {Chuyuan Wei and Jinzhe Li and Zhiyuan Wang and Shanshan Wan and Maozu Guo},
keywords = {Relation extraction, graph convolutional neural networks, dependency tree, dynamic structure attention},
abstract = {Deep neural network-based relational extraction research has made significant progress in recent years, and it provides data support for many natural language processing downstream tasks such as building knowledge graph, sentiment analysis and question-answering systems. However, previous studies ignored much unused structural information in sentences that could enhance the performance of the relation extraction task. Moreover, most existing dependency-based models utilize self-attention to distinguish the importance of context, which hardly deals with multiple-structure information. To efficiently leverage multiple structure information, this paper proposes a dynamic structure attention mechanism model based on textual structure information, which deeply integrates word embedding, named entity recognition labels, part of speech, dependency tree and dependency type into a graph convolutional network. Specifically, our model extracts text features of different structures from the input sentence. Textual Structure information Graph Convolutional Networks employs the dynamic structure attention mechanism to learn multi-structure attention, effectively distinguishing important contextual features in various structural information. In addition, multi-structure weights are carefully designed as a merging mechanism in the different structure attention to dynamically adjust the final attention. This paper combines these features and trains a graph convolutional network for relation extraction. We experiment on supervised relation extraction datasets including SemEval 2010 Task 8, TACRED, TACREV, and Re-TACED, the result significantly outperforms the previous.}
}
@article{BASOLE2024114133,
title = {Complex business ecosystem intelligence using AI-powered visual analytics},
journal = {Decision Support Systems},
volume = {178},
pages = {114133},
year = {2024},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114133},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623002087},
author = {Rahul C. Basole and Hyunwoo Park and C. David Seuss},
keywords = {Business ecosystem, Artificial intelligence, Text mining, Complex networks, Interactive visualization},
abstract = {Business ecosystems are complex, dynamic systems characterized by a multitude of entities, including companies, ventures, and technologies, as well as activities and trends. Understanding the state of business ecosystems is an increasingly critical strategic imperative for many decision makers, but it is a resource-intensive activity as relevant information sources are dispersed, often highly unstructured, and not integrated or curated to deliver actionable insights. In this research, we present the design and implementation of an interactive visual analytic system that integrates artificial intelligence and graph visualization techniques to augment decision makers’ understanding of the complex public narrative associated with business ecosystems entities. Our system is driven by a real-time content engine of 100,000+ global data sources including press releases, news articles, industry reports, analyst blogs in multiple languages organized across several domain-specific repositories. Following a user-specified query, the engine extracts both domain-agnostic and domain-specific entities and concepts for each document in the result set. We then model and visualize the resulting data as a dynamic, multipartite network and implement graph pruning algorithms and interactive data controls to enable users to interactively explore and discover the underlying business ecosystem from multiple perspectives. We illustrate and discuss the value of our system using representative use cases. Our study makes multiple contributions to visual decision support theory and practice, including mining unstructured data, constructing and interacting with knowledge graphs, and designing visual analytic tools for ecosystem intelligence. We conclude the study with implications and future research opportunities.}
}
@article{CHU2025126378,
title = {GeoSMIE: An event extraction framework for Document-Level spatial morphological information extraction},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126378},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126378},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424032457},
author = {Deping Chu and Bo Wan and Huizhu Ni and Hong Li and Zhuo Tan and Yan Dai and Zijing Wan and Tao Tang and Shunping Zhou},
keywords = {Spatial information extraction, Spatial morphological information, Chinese geological text, Event extraction},
abstract = {Spatial morphological information (SMI) in geological texts provides critical insights into the formation, localization, and distribution of geological bodies. However, SMI is often scattered across multiple sentences or coexists in complex forms within the same document, making it challenging to extract using traditional methods. In this paper, we address this gap by formalizing SMI extraction as an event extraction task and proposed a novel Geological body SMI Extraction model, GeoSMIE. Our approach is innovative in two key ways: first, we implement a no-trigger-word annotation strategy to capture both descriptive and digital SMI, ensuring that SMI without explicit morphological triggers is not missed. Second, we design dual graph neural networks (GNNs) to handle long-distance dependencies and complex interactions between scattered arguments across sentences. To validate its effectiveness, we compared GeoSMIE to state-of-the-art models on the constructed dataset. For SMI extraction, GeoSMIE outperformed the optimal baseline by 0.4%, 2.2%, and 1.5% for accuracy, recall, and Micro-F1 score, respectively. This work provides an innovative idea for extracting complex spatial information from geoscience texts.}
}
@article{LIN2025102795,
title = {Has multimodal learning delivered universal intelligence in healthcare? A comprehensive survey},
journal = {Information Fusion},
volume = {116},
pages = {102795},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102795},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005736},
author = {Qika Lin and Yifan Zhu and Xin Mei and Ling Huang and Jingying Ma and Kai He and Zhen Peng and Erik Cambria and Mengling Feng},
keywords = {Multimodal fusion, Intelligent healthcare, Multimodal learning, Foundation model, Medical vision-language},
abstract = {The rapid development of artificial intelligence has constantly reshaped the field of intelligent healthcare and medicine. As a vital technology, multimodal learning has increasingly garnered interest because of data complementarity, comprehensive information fusion, and great application potential. Currently, numerous researchers are dedicating their attention to this field, conducting extensive studies and constructing abundant intelligent systems. Naturally, an open question arises that has multimodal learning delivered universal intelligence in healthcare? To answer this question, we adopt three unique viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey of the current progress of medical multimodal learning from the perspectives of datasets, task-oriented methods, and universal foundation models. Based on them, we further discuss the proposed question from five issues to explore the real impacts of advanced techniques in healthcare, from data and technologies to performance and ethics. The answer is that current technologies have NOT achieved universal intelligence and there remains a significant journey to undertake. Finally, in light of the above reviews and discussions, we point out ten potential directions for exploration to promote multimodal fusion technologies in the domain, towards the goal of universal intelligence in healthcare.}
}
@article{ZHANG2024105632,
title = {Knowledge management for off-site construction},
journal = {Automation in Construction},
volume = {166},
pages = {105632},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105632},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524003686},
author = {Zhen Zhang and Yang Zou and Brian H.W. Guo and Johannes Dimyadi and Roy Davies and Lixin Jiang},
keywords = {Off-site construction (OSC), Knowledge management (KM), Artificial intelligence (AI), Systematic literature review},
abstract = {Off-site construction (OSC) is expected to boost productivity, shorten construction time, and reduce labour and material wastage. Despite these benefits, most OSC projects have not fully achieved these advantages, where a primary obstacle lies in the limited management of OSC knowledge. However, there is still no holistic understanding of the integration of KM in the OSC context. Therefore, this paper explores the latest development in KM for OSC through a systematic literature review using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and template analysis. The review is based on 66 screened and assessed journal articles from all years to 2024 with a particular focus on KM and OSC. Through the quantitative and qualitative analysis, this study groups four main research themes including KM for OSC design, KM for OSC project management, knowledge-based OSC decision-making, and the management of OSC knowledge. The results are discussed to gain a systematic understanding of key OSC knowledge domains, investigate the integration of KM for OSC, and explore future research needs including emerging artificial intelligence (AI) technologies.}
}
@article{RONG2025102819,
title = {Pred-ID: Future event prediction based on event type schema mining by graph induction and deduction},
journal = {Information Fusion},
volume = {117},
pages = {102819},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102819},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524005979},
author = {Huan Rong and Zhongfeng Chen and Zhenyu Lu and Xiao-ke Xu and Kai Huang and Victor S. Sheng},
keywords = {Event intelligence analysis, Event graph mining, Graph generation, Event prediction},
abstract = {In the field of information management, effective event intelligence management is crucial for its development. With the continuous evolution of events, predicting future events has become a key task in information management. Event Prediction aims to predict upcoming events based on given contextual information. This requires modeling events and their relationships in the context to infer the structure of future events. However, the existing event prediction methods ignore that the event graph schema based on core events can provide more knowledge about history and future for event prediction through induction and deduction, so as to achieve accurate event prediction. In addressing this issue, we directed our focus towards Event Schema Induction. Inspired by it, we propose the Pred-ID model, designed to build event evolutionary pattern through Inductive Event Graph Generation, Deductive Event Graph Expansion, and Graph Fusion for Event Prediction. Specifically, in the Inductive Event Graph Generation phase, Pred-ID extracts the event core subgraph and event developmental trends from the instance event graph, learning the global structure and uncovering the main processes of event development. Then, in the Deductive Event Graph Expansion phase, by expanding future event node and stretching the main processes of event development into future directions, Pred-ID obtains deductive results, so as to construct the event evolutionary pattern. Finally, in the Graph Fusion for Event Prediction phase, aligning and merging the event evolutionary pattern with the instance event graph enables collaborative prediction of future events. The experimental results indicate that our proposed Pred-ID achieves optimal performance in event evolutionary pattern generation and event prediction tasks.}
}
@article{ALONSOBARRIUSO2024109028,
title = {Recommendation system of scientific articles from discharge summaries},
journal = {Engineering Applications of Artificial Intelligence},
volume = {136},
pages = {109028},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109028},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624011862},
author = {Adrián {Alonso Barriuso} and Alberto Fernández-Isabel and Isaac {Martín de Diego} and Alfonso Ardoiz and J.F. {J. Viseu Pinheiro}},
keywords = {Medical text processing, Diagnosis retrieval, Computational semantics, Recommendation system, Scientific relevance},
abstract = {Medical professionals are often overwhelmed by the amount of patients they have to care for, leaving little time available to keep up to date in their respective specialities. They usually find it challenging to keep up with the vast amount of medical literature and identify the most relevant articles for their practice, especially those related to their patient’s specific conditions. Therefore, a system that proactively supports healthcare professionals in selecting relevant articles related to the characteristics of the patients is crucial. This paper presents Medical Expert Linguist for Evaluating Nosology and Diagnosis Information (MELENDI) to tackle this issue. It is a recommendation system that effectively and efficiently recommends pertinent medical articles to healthcare professionals based on their patients’ diagnoses. It combines a semantic similarity model generated using the content of discharge summaries, with a relevance estimator produced by analysing scientific publications. To test the system, 1,000,000 abstracts were obtained from PubMed and 10 discharge reports from ’Medical Information Mart for Intensive Care (MIMIC-III) were used. A group of 5 medical specialists has been involved in the system’s evaluation. These evaluations demonstrated good overall performance, supporting the implementation of the system in a real-world environment, such as a hospital information system.}
}
@article{WU2025128999,
title = {MixEI: Mixing explicit and implicit commonsense knowledge in open-domain dialogue response generation},
journal = {Neurocomputing},
volume = {618},
pages = {128999},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128999},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224017703},
author = {Sixing Wu and Jiong Yu and Wei Zhou},
keywords = {Dialogue generation, Commonsense knowledge, Knowledge alignment, Implicit knowledge externalization},
abstract = {The inadequate awareness of real-world knowledge often causes machines to produce generic responses, such as ‘I think so.’, which may bring the degression of user interests. Consequently, enriching knowledge awareness and fabricating informative responses are long-standing challenges in open-domain dialogue systems. Previous studies have shown incorporating everyday commonsense knowledge can significantly enhance the open-domain dialogue response models. Nonetheless, previous works can only use explicit or implicit knowledge. Unlike them, this work presents a novel MixEI to leverage both explicit and implicit commonsense knowledge in dialogue generation. MixEI uses Dual-Way Knowledge Alignment and MixEI-Ranker to retrieve a set of contextually relevant commonsense facts as the explicit background knowledge and identify implicit knowledge labels by selecting clue facts that can tightly connect the dialogue context. MixEI uses BART as the backbone. After jointly encoding the background knowledge and dialogue history, MixEI first tries to externalize the implicit clue knowledge; then, the response decoding can seek information from both explicit and implicit knowledge. Extensive experiments on Chinese Weibo and English Reddit have verified the superior performance of the proposed MixEI-Ranker and MixEI.}
}
@article{VISWESWARAN2024104713,
title = {Fairness and inclusion methods for biomedical informatics research},
journal = {Journal of Biomedical Informatics},
volume = {158},
pages = {104713},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104713},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400131X},
author = {Shyam Visweswaran and Yuan Luo and Mor Peleg}
}
@article{YANG2025,
title = {Robust Automated Harmonization of Heterogeneous Data Through Ensemble Machine Learning: Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/54133},
url = {https://www.sciencedirect.com/science/article/pii/S229196942500016X},
author = {Doris Yang and Doudou Zhou and Steven Cai and Ziming Gan and Michael Pencina and Paul Avillach and Tianxi Cai and Chuan Hong},
keywords = {ensemble learning, semantic learning, distribution learning, variable harmonization, machine learning, cardiovascular health study, intracohort comparison, intercohort comparison, gold standard labels},
abstract = {Background
Cohort studies contain rich clinical data across large and diverse patient populations and are a common source of observational data for clinical research. Because large scale cohort studies are both time and resource intensive, one alternative is to harmonize data from existing cohorts through multicohort studies. However, given differences in variable encoding, accurate variable harmonization is difficult.
Objective
We propose SONAR (Semantic and Distribution-Based Harmonization) as a method for harmonizing variables across cohort studies to facilitate multicohort studies.
Methods
SONAR used semantic learning from variable descriptions and distribution learning from study participant data. Our method learned an embedding vector for each variable and used pairwise cosine similarity to score the similarity between variables. This approach was built off 3 National Institutes of Health cohorts, including the Cardiovascular Health Study, the Multi-Ethnic Study of Atherosclerosis, and the Women’s Health Initiative. We also used gold standard labels to further refine the embeddings in a supervised manner.
Results
The method was evaluated using manually curated gold standard labels from the 3 National Institutes of Health cohorts. We evaluated both the intracohort and intercohort variable harmonization performance. The supervised SONAR method outperformed existing benchmark methods for almost all intracohort and intercohort comparisons using area under the curve and top-k accuracy metrics. Notably, SONAR was able to significantly improve harmonization of concepts that were difficult for existing semantic methods to harmonize.
Conclusions
SONAR achieves accurate variable harmonization within and between cohort studies by harnessing the complementary strengths of semantic learning and variable distribution learning.}
}
@article{EDGELL2024100075,
title = {A monstrous matter: The three faces of artificial creativity},
journal = {Journal of Creativity},
volume = {34},
number = {1},
pages = {100075},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100075},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000013},
author = {Robert A. Edgell},
keywords = {Artificial intelligence, Creativity, Matters of concern, Trust, Creative value, Creative personal identity},
abstract = {Through a focus on artificial creativity (AC), creativity and innovation researchers, practitioners, and educators are beginning to demystify the phenomenon's liminality by exploring and contesting the potential affordances, constraints, and pitfalls brought about by the deployment of powerful AI models for creative endeavors. For the creativity community, AC as a sociotechnical network has become a deeply consternating and contested monster. Given the recency of AC, there has been little theorizing yet. My critical self-reflection paper seeks to understand the community's concerns and, thereby, to discern theoretical insights that conceptually contribute towards a theory of AC. Drawing on autoethnography, I identified three distinct perceived matters of concern represented by anthropomorphic personalities or faces of AC: Trickster, Surveyor, and Harbinger. The findings reveal that the Trickster is the most monstrous and disconcerting face of AC. It may be prankish or deceptive, but can also be beneficent and supportive. While the Surveyor provides surveillance, measurement, and calculation, the Harbinger announces competing future visions, one of utopian hope and the other of dystopian despair. I conclude by discussing the implications of three underlying theoretical variables: trust, creative value, and creative personal identity.}
}
@article{CAO2025102403,
title = {Textual data augmentation using generative approaches - Impact on named entity recognition tasks},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102403},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102403},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001277},
author = {Danrun Cao and Nicolas Béchet and Pierre-François Marteau and Oussama Ahmia},
keywords = {Data augmentation, Named entity recognition, Feature engineering, Word embeddings, Model robustness, Generative model, Call for tenders},
abstract = {Industrial applications of Named Entity Recognition (NER) are usually confronted with small and imbalanced corpora. This could harm the performance of trained and finetuned recognition models, especially when they encounter unknown data. In this study we develop three generation-based data enrichment approaches, in order to increase the number of examples of underrepresented entities. We compare the impact of enriched corpora on NER models, using both non-contextual (fastText) and contextual (Bert-like) embedding models to provide discriminant features to a biLSTM-CRF used as an entity classifier. The approach is evaluated on a contract renewal detection task applied to a corpus of calls for tenders. The results show that the proposed data enrichment procedure effectively improves the NER model’s effectiveness when applied on both known and unknown data.}
}
@article{MAI2025104368,
title = {Towards the next generation of Geospatial Artificial Intelligence},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {136},
pages = {104368},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104368},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225000159},
author = {Gengchen Mai and Yiqun Xie and Xiaowei Jia and Ni Lao and Jinmeng Rao and Qing Zhu and Zeping Liu and Yao-Yi Chiang and Junfeng Jiao},
keywords = {Geospatial Artificial Intelligence, Heterogeneity-aware GeoAI, Knowledge-Guided GeoAI, Spatial representation learning, Geo-Foundation Models, Fairness-aware GeoAI, Privacy-aware GeoAI, Interpretable and explainable GeoAI},
abstract = {Geospatial Artificial Intelligence (GeoAI), as the integration of geospatial studies and AI, has become one of the fastest-developing research directions in spatial data science and geography. This rapid change in the field calls for a deeper understanding of the recent developments and envision where the field is going in the near future. In this work, we provide a quantitative analysis of the GeoAI literature from the spatial, temporal, and semantic aspects. We briefly discuss the history of AI and GeoAI by highlighting some pioneering work. Then we discuss the current landscape of GeoAI by selecting five representative subdomains including remote sensing, urban computing, Earth system science, cartography, and geospatial semantics. Finally, we highlight several unique future research directions of GeoAI which are classified into two groups: GeoAI method development challenges and GeoAI Ethics challenges. Topics include heterogeneity-aware GeoAI, knowledge-guided GeoAI, spatial representation learning, geo-foundation models, fairness-aware GeoAI, privacy-aware GeoAI, as well as interpretable and explainable GeoAI. We hope our review of GeoAI’s past, present, and future is comprehensive and can enlighten the next generation of GeoAI research.}
}
@article{ALONSO2025100509,
title = {A novel approach for job matching and skill recommendation using transformers and the O*NET database},
journal = {Big Data Research},
volume = {39},
pages = {100509},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000048},
author = {Rubén Alonso and Danilo Dessí and Antonello Meloni and Diego {Reforgiato Recupero}},
keywords = {Information extraction, Transformers, Online enrolling process, Natural language processing, Course recommendation},
abstract = {Today we have tons of information posted on the web every day regarding job supply and demand which has heavily affected the job market. The online enrolling process has thus become efficient for applicants as it allows them to present their resumes using the Internet and, as such, simultaneously to numerous organizations. Online systems such as Monster.com, OfferZen, and LinkedIn contain millions of job offers and resumes of potential candidates leaving to companies with the hard task to face an enormous amount of data to manage to select the most suitable applicant. The task of assessing the resumes of candidates and providing automatic recommendations on which one suits a particular position best has, therefore, become essential to speed up the hiring process. Similarly, it is important to help applicants to quickly find a job appropriate to their skills and provide recommendations about what they need to master to become eligible for certain jobs. Our approach lies in this context and proposes a new method to identify skills from candidates' resumes and match resumes with job descriptions. We employed the O*NET database entities related to different skills and abilities required by different jobs; moreover, we leveraged deep learning technologies to compute the semantic similarity between O*NET entities and part of text extracted from candidates' resumes. The ultimate goal is to identify the most suitable job for a certain resume according to the information there contained. We have defined two scenarios: i) given a resume, identify the top O*NET occupations with the highest match with the resume, ii) given a candidate's resume and a set of job descriptions, identify which one of the input jobs is the most suitable for the candidate. The evaluation that has been carried out indicates that the proposed approach outperforms the baselines in the two scenarios. Finally, we provide a use case for candidates where it is possible to recommend courses with the goal to fill certain skills and make them qualified for a certain job.}
}
@article{TRAJANOV2023714,
title = {Review of Natural Language Processing in Pharmacology},
journal = {Pharmacological Reviews},
volume = {75},
number = {4},
pages = {714-738},
year = {2023},
issn = {0031-6997},
doi = {https://doi.org/10.1124/pharmrev.122.000715},
url = {https://www.sciencedirect.com/science/article/pii/S0031699724007762},
author = {Dimitar Trajanov and Vangel Trajkovski and Makedonka Dimitrieva and Jovana Dobreva and Milos Jovanovik and Matej Klemen and Aleš Žagar and Marko Robnik-Šikonja},
abstract = {Natural language processing (NLP) is an area of artificial intelligence that applies information technologies to process the human language, understand it to a certain degree, and use it in various applications. This area has rapidly developed in the past few years and now employs modern variants of deep neural networks to extract relevant patterns from large text corpora. The main objective of this work is to survey the recent use of NLP in the field of pharmacology. As our work shows, NLP is a highly relevant information extraction and processing approach for pharmacology. It has been used extensively, from intelligent searches through thousands of medical documents to finding traces of adversarial drug interactions in social media. We split our coverage into five categories to survey modern NLP: methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries. We split each of the five categories into appropriate subcategories, describe their main properties and ideas, and summarize them in a tabular form. The resulting survey presents a comprehensive overview of the area, useful to practitioners and interested observers.
Significance Statement
The main objective of this work is to survey the recent use of NLP in the field of pharmacology in order to provide a comprehensive overview of the current state in the area after the rapid developments that occurred in the past few years. The resulting survey will be useful to practitioners and interested observers in the domain.}
}
@article{HUANG2025100526,
title = {Generative spatial artificial intelligence for sustainable smart cities: A pioneering large flow model for urban digital twin},
journal = {Environmental Science and Ecotechnology},
volume = {24},
pages = {100526},
year = {2025},
issn = {2666-4984},
doi = {https://doi.org/10.1016/j.ese.2025.100526},
url = {https://www.sciencedirect.com/science/article/pii/S2666498425000043},
author = {Jeffrey Huang and Simon Elias Bibri and Paul Keel},
keywords = {Sustainable smart cities, Generative artificial intelligence, Generative spatial artificial intelligence, Foundation models, Large flow model, Urban digital twin, Urban planning and design},
abstract = {Rapid urbanization, alongside escalating resource depletion and ecological degradation, underscores the critical need for innovative urban development solutions. In response, sustainable smart cities are increasingly turning to cutting-edge technologies—such as Generative Artificial Intelligence (GenAI), Foundation Models (FMs), and Urban Digital Twin (UDT) frameworks—to transform urban planning and design practices. These transformative tools provide advanced capabilities to analyze complex urban systems, optimize resource management, and enable evidence-based decision-making. Despite recent progress, research on integrating GenAI and FMs into UDT frameworks remains scant, leaving gaps in our ability to capture complex urban flows and multimodal dynamics essential to achieving environmental sustainability goals. Moreover, the lack of a robust theoretical foundation and real-world operationalization of these tools hampers comprehensive modeling and practical adoption. This study introduces a pioneering Large Flow Model (LFM), grounded in a robust foundational framework and designed with GenAI capabilities. It is specifically tailored for integration into UDT systems to enhance predictive analytics, adaptive learning, and complex data management functionalities. To validate its applicability and relevance, the Blue City Project in Lausanne City is examined as a case study, showcasing the ability of the LFM to effectively model and analyze urban flows—namely mobility, goods, energy, waste, materials, and biodiversity—critical to advancing environmental sustainability. This study highlights how the LFM addresses the spatial challenges inherent in current UDT frameworks. The LFM demonstrates its novelty in comprehensive urban modeling and analysis by completing impartial city data, estimating flow data in new locations, predicting the evolution of flow data, and offering a holistic understanding of urban dynamics and their interconnections. The model enhances decision-making processes, supports evidence-based planning and design, fosters integrated development strategies, and enables the development of more efficient, resilient, and sustainable urban environments. This research advances both the theoretical and practical dimensions of AI-driven, environmentally sustainable urban development by operationalizing GenAI and FMs within UDT frameworks. It provides sophisticated tools and valuable insights for urban planners, designers, policymakers, and researchers to address the complexities of modern cities and accelerate the transition towards sustainable urban futures.}
}
@article{EGUIA2024,
title = {Clinical Decision Support and Natural Language Processing in Medicine: Systematic Literature Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/55315},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006149},
author = {Hans Eguia and Carlos Luis Sánchez-Bocanegra and Franco Vinciarelli and Fernando Alvarez-Lopez and Francesc Saigí-Rubió},
keywords = {artificial intelligence, AI, natural language processing, clinical decision support system, CDSS, health recommender system, clinical information extraction, electronic health record, systematic literature review, patient, treatment, diagnosis, health workers},
abstract = {Background
Ensuring access to accurate and verified information is essential for effective patient treatment and diagnosis. Although health workers rely on the internet for clinical data, there is a need for a more streamlined approach.
Objective
This systematic review aims to assess the current state of artificial intelligence (AI) and natural language processing (NLP) techniques in health care to identify their potential use in electronic health records and automated information searches.
Methods
A search was conducted in the PubMed, Embase, ScienceDirect, Scopus, and Web of Science online databases for articles published between January 2000 and April 2023. The only inclusion criteria were (1) original research articles and studies on the application of AI-based medical clinical decision support using NLP techniques and (2) publications in English. A Critical Appraisal Skills Programme tool was used to assess the quality of the studies.
Results
The search yielded 707 articles, from which 26 studies were included (24 original articles and 2 systematic reviews). Of the evaluated articles, 21 (81%) explained the use of NLP as a source of data collection, 18 (69%) used electronic health records as a data source, and a further 8 (31%) were based on clinical data. Only 5 (19%) of the articles showed the use of combined strategies for NLP to obtain clinical data. In total, 16 (62%) articles presented stand-alone data review algorithms. Other studies (n=9, 35%) showed that the clinical decision support system alternative was also a way of displaying the information obtained for immediate clinical use.
Conclusions
The use of NLP engines can effectively improve clinical decision systems’ accuracy, while biphasic tools combining AI algorithms and human criteria may optimize clinical diagnosis and treatment flows.
Trial Registration
PROSPERO CRD42022373386; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=373386}
}
@article{YOUSIF2023100372,
title = {Exploring deep learning approaches for video captioning: A comprehensive review},
journal = {e-Prime - Advances in Electrical Engineering, Electronics and Energy},
volume = {6},
pages = {100372},
year = {2023},
issn = {2772-6711},
doi = {https://doi.org/10.1016/j.prime.2023.100372},
url = {https://www.sciencedirect.com/science/article/pii/S277267112300267X},
author = {Adel Jalal Yousif and Mohammed H. Al-Jammas},
keywords = {Evaluation metrics, Video captioning, Video description, Computer vision, Deep learning},
abstract = {While humans can easily describe visual data at varying levels of detail, the same task presents a significant challenge for machines. This challenge becomes even more complex when dealing with video data. The process of understanding a video and generating descriptive text for it is known as video captioning. Video captioning requires not only understanding the visual content but also producing human-like descriptions that accurately capture its semantics. Achieving this level of understanding requires the collaborative efforts of both the computer vision and natural language processing research communities. The captions produced through video captioning serve as valuable resources that can be further leveraged for various applications such as video search, accessibility for visually impaired people, and human-robot interaction. Deep learning strategies have emerged as powerful tools in addressing the complexities of video captioning. By leveraging large scale annotated video caption datasets and sophisticated neural network architectures, deep learning approaches have made significant advances in this challenging task. In the existing literature, numerous techniques, benchmark datasets, and evaluation metrics have been developed, emphasizing the necessity for a comprehensive examination to concentrate research efforts in this rapidly evolving field. This paper provides a survey of deep learning based methods for video captioning, highlighting their key components, challenges, and recent advancements.}
}
@article{DELIMA2025100810,
title = {An AI-powered approach to the semiotic reconstruction of narratives},
journal = {Entertainment Computing},
volume = {52},
pages = {100810},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100810},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001782},
author = {Edirlei Soares {de Lima} and Margot M.E. Neggers and Bruno Feijó and Marco A. Casanova and Antonio L. Furtado},
keywords = {Computational narratology, Interactive story composition, Deconstruction, Semiotic relations, Semiotic reconstruction, Book narratives, Movie narratives, Storyboards, Artificial intelligence, Intelligent agents},
abstract = {This article presents a novel and highly interactive process to generate natural language narratives based on our ongoing work on semiotic relations, providing four criteria for composing new narratives from existing stories. The wide applicability of this semiotic reconstruction process is suggested by a reputed literary scholar’s deconstructive claim that new narratives can often be shown to be a tissue of previous narratives. Along, respectively, three semiotic axes – syntagmatic, paradigmatic, and meronymic – existing stories can yield new stories by the combination, imitation, or expansion of an iconic scene; lastly, a new story may emerge through reversal via an antithetic consideration, i.e., through the adoption of opposite values. Targeting casual users, we present a fully operational prototype with a simple and user-friendly interface that incorporates an AI agent, namely ChatGPT. The prototype, in a coauthor capacity, generates context-compatible sequences of events in storyboard format using backward-chaining abductive reasoning (employing Stable Diffusion to draw scene illustrations), conforming as much as possible to the user’s authorial instructions. The extensive repertoire of book and movie summaries available to the AI agent obviates the need to manually supply laborious and error-prone context specifications. A user study was conducted to evaluate user experience and satisfaction with the generated narratives. The preliminary findings suggest that our approach has the potential to enhance story quality while offering a positive user experience.}
}
@article{HERR2024100341,
title = {Estimating prevalence of rare genetic disease diagnoses using electronic health records in a children’s hospital},
journal = {Human Genetics and Genomics Advances},
volume = {5},
number = {4},
pages = {100341},
year = {2024},
issn = {2666-2477},
doi = {https://doi.org/10.1016/j.xhgg.2024.100341},
url = {https://www.sciencedirect.com/science/article/pii/S2666247724000812},
author = {Kate Herr and Peixin Lu and Kessi Diamreyan and Huan Xu and Eneida Mendonca and K. Nicole Weaver and Jing Chen},
keywords = {rare genetic diseases, natural language processing, bioinformatics, Orphanet, electronic health record, genetic testing},
abstract = {Summary
Rare genetic diseases (RGDs) affect a significant number of individuals, particularly in pediatric populations. This study investigates the efficacy of identifying RGD diagnoses through electronic health records (EHRs) and natural language processing (NLP) tools, and analyzes the prevalence of identified RGDs for potential underdiagnosis at Cincinnati Children’s Hospital Medical Center (CCHMC). EHR data from 659,139 pediatric patients at CCHMC were utilized. Diagnoses corresponding to RGDs in Orphanet were identified using rule-based and machine learning-based NLP methods. Manual evaluation assessed the precision of the NLP strategies, with 100 diagnosis descriptions reviewed for each method. The rule-based method achieved a precision of 97.5% (95% CI: 91.5%, 99.4%), while the machine-learning-based method had a precision of 73.5% (95% CI: 63.6%, 81.6%). A manual chart review of 70 randomly selected patients with RGD diagnoses confirmed the diagnoses in 90.3% (95% CI: 82.0%, 95.2%) of cases. A total of 37,326 pediatric patients were identified with 977 RGD diagnoses based on the rule-based method, resulting in a prevalence of 5.66% in this population. While a majority of the disorders showed a higher prevalence at CCHMC compared with Orphanet, some diseases, such as 1p36 deletion syndrome, indicated potential underdiagnosis. Analyses further uncovered disparities in RGD prevalence and age of diagnosis across gender and racial groups. This study demonstrates the utility of employing EHR data with NLP tools to systematically investigate RGD diagnoses in large cohorts. The identified disparities underscore the need for enhanced approaches to guarantee timely and accurate diagnosis and management of pediatric RGDs.}
}
@article{TAO2024127795,
title = {KFEX-N : A table-text data question-answering model based on knowledge-fusion encoder and EX-N tree decoder},
journal = {Neurocomputing},
volume = {593},
pages = {127795},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127795},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224005666},
author = {Ye Tao and Jiawang Liu and Hui Li and Wenqian Cao and Xiugong Qin and Yunlong Tian and Yongjie Du},
keywords = {Question answering, Tabular and textual, Numerical reasoning, Domain knowledge, Natural language processing},
abstract = {Answering questions about hybrid data combining tables and text is challenging. Recent research has employed encoder-tree decoder frameworks to simulate the reasoning process of arithmetic expressions for generating answers. However, this approach overlooks the inherent diversity of expressions; there might be multiple valid reasoning paths, leading to a decrease in the accuracy of inferred expression trees. Moreover, encoders, lacking rich domain knowledge, struggle to capture deep relationships between questions and supporting evidence; this limitation results in models making errors when selecting operation units. In this paper, we propose a Knowledge-Fusion encoder and EX-N tree decoder table-text data question-answering model(KFEX-N). During the encoding process, the integration of traditional encoders with cross-fusion encoders forms a knowledge-fusion encoder, endowing the model with rich domain knowledge and enhancing its understanding of the operational units required to answer questions. Additionally, we propose an EX-N tree decoder. It reduces the diversity of inference paths through a constrained structure and mitigates the occurrence of final answer errors resulting from decoding errors. We validate our model using publicly available Table-Text QA datasets (TAT-QA and Fin-QA) and achieve state-of-the-art performance.}
}
@article{ZABALALOPEZ2024226,
title = {A survey of data-centric technologies supporting decision-making before deploying military assets},
journal = {Defence Technology},
volume = {42},
pages = {226-246},
year = {2024},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2024.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S221491472400182X},
author = {Alexandra Zabala-López and Mario Linares-Vásquez and Sonia Haiduc and Yezid Donoso},
keywords = {Data-centric technologies, Military, Analytics, Machine learning, Data science, Artificial intelligence},
abstract = {In a time characterized by the availability of vast amounts of data, the effective utilization of information is critical for timely decision-making in military operations. However, processing large amounts of data requires computational resources and time. Therefore, decision makers have used data-centric technologies to take advantage of public and private data sources to support military operations. This survey explores the integration and application of data-centric technologies, such as data analytics, data science, and machine learning, to optimize decision-making workflows within military contexts supporting the deployment of military assets and resources. To address the information gap, this article presents a literature review, specifically a survey. Our survey examines the use of the mentioned technologies to process and analyze information that contributes to the phases of situational awareness, and planning in military environments. We then introduce a taxonomy of the approaches associated with implementing these technologies in military scenarios. Furthermore, we discuss relevant factors for the seamless integration of data-centric technologies into military decision-making processes, and reveal the importance of specialized personnel, architectures, and cybersecurity issues in the task of developing prototypes and models. The findings of this paper aim to provide valuable insights for military institutions, offering a deeper understanding of the use of data-centric technologies as innovative practices to enhance the effectiveness of military decision-making.}
}
@article{TRAPPEY2021120511,
title = {An intelligent patent recommender adopting machine learning approach for natural language processing: A case study for smart machinery technology mining},
journal = {Technological Forecasting and Social Change},
volume = {164},
pages = {120511},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120511},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520313378},
author = {Amy Trappey and Charles V. Trappey and Alex Hsieh},
keywords = {Natural language processing, Patent recommendation, Word embedding, Technology mining and trend analysis},
abstract = {Recommendation systems are widely applied in many fields, such as online customized product searches and customer-centric advertisements. This research develops the methodology for a patent recommender to discover semantically relevant patents for further technology mining and trend analysis. The proposed recommender adopts machine learning (ML) algorithms for natural language processing (NLP) to represent patent documents in vector space and to enable semantic analyses of the patent documents. The ML approach of neural network (NN) language models, trained by domain patent documents (text) as a training set, convert patent documents into vectors and, thus, can identify semantically similar patents using document similarity measures. In particular, the proposed recommender is deployed to in-depth case studies for advanced patent recommendations. The case domain of smart machinery is used to better enable smart manufacturing by incorporating innovative technologies, such as intelligent sensors, intelligent controllers, and intelligent decision making. The research uses six sub-domains in smart machinery technologies as the case studies to verify the superior accuracy and efficacy of the recommender system and methodologies.}
}
@article{CUI2025126128,
title = {Research on the mechanism of organizing and managing mainstream integrated media information resources in the era of big data},
journal = {Expert Systems with Applications},
volume = {266},
pages = {126128},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126128},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029956},
author = {Jindong Cui and Chenyu Li and Chenrui Bao and Guoli Qu},
keywords = {Mainstream integrated media, Information organization, Management Mechanism, Big data},
abstract = {As mainstream integrated media assumes increasing importance in guiding public opinion, achieving efficient organization and management of its information resources within the context of big data and the information economy has become a fundamental cornerstone for its development and value creation. This paper focuses on the content and dissemination characteristics of mainstream integrated media information, constructs a model for its organization and management, and examines various aspects such as decentralized information collection, multimodal resource processing, semantic feature extraction, information unit construction and association, information chain traceability, and simulation results. Additionally, it proposes management strategies and countermeasures from a comprehensive perspective of the entire information organization chain. The developed organization and management mechanism aligns with the evolving requirements of mainstream integrated media in the big data era, providing a foundation for the deep utilization and enhancement of information value.}
}
@article{WANG2025102968,
title = {A homogeneous multimodality sentence representation for relation extraction},
journal = {Information Fusion},
volume = {118},
pages = {102968},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102968},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000417},
author = {Kai Wang and Yanping Chen and WeiZhe Yang and Yongbin Qin},
keywords = {Natural language processing, Relation extraction, Homogeneous multimodality, Semantic structures},
abstract = {Deep neural networks enable a sentence to be transformed into different multimodalities such as a token sequence representation (a one-dimensional semantic representation) or a semantic plane (a two-dimensional semantic representation). Sequence representation has the advantage of learning sequential dependencies of a sentence. Semantic plane is built by organizing all spans of a sentence, which is effective in resolving complicated sentence semantic structures. The two representations are derived from a homogeneous resource (the same sentence), but they are separately used in related works. In this paper, a homogeneous multimodality sentence representation is proposed to make full use of semantic information in a sentence. We construct a homomodality model, which is composed of three components: a sequential encoder to generate sequential modality, a plane encoder to build plane modality, and a multimodality fusion component aligning homogeneous multimodalities for learning a multi-granularity semantic representation. Our model is evaluated on four public datasets to support the relation extraction task. Compared with related works, it achieves state-of-the-art performance on all datasets. Analytical experiments show that fusing homogeneous multimodalities is effective in making full use of sentence information for advancing the discriminability of a deep neural network.}
}
@article{LIU2024509,
title = {Integration of data science with product design towards data-driven design},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {509-532},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0007850624001252},
author = {Ang Liu and Stephen Lu and Fei Tao and Nabil Anwer},
keywords = {Product design, Data science, Data-driven design},
abstract = {This paper aims to investigate the scientific integration of data science with product design towards data-driven design (D3). Data science has potential to facilitate design decision-making through insight extraction, predictive analytics, and automatic decisions. A systematic scoping review is conduced to converge various D3 applications in four dimensions: the design dimension about design operations, the data dimension about popular data sources and common data-related challenges, the method dimension about the methodological foundations, and the social/ethical dimension about social/ethical considerations and implications. Based on the state-of-the-art, this paper also highlights potential future research avenues in this dynamic field.}
}
@article{BRANNSTROM2024101203,
title = {A formal understanding of computational empathy in interactive agents},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101203},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101203},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001377},
author = {Andreas Brännström and Joel Wester and Juan Carlos Nieves},
keywords = {Computational empathy, Conversational agents, Human–agent interaction, Knowledge engineering},
abstract = {Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agents’ empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definition—an ontology—of empathy is developed. We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactions—be it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors.}
}
@article{RAJENDRAN2024100913,
title = {Learning across diverse biomedical data modalities and cohorts: Challenges and opportunities for innovation},
journal = {Patterns},
volume = {5},
number = {2},
pages = {100913},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100913},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923003227},
author = {Suraj Rajendran and Weishen Pan and Mert R. Sabuncu and Yong Chen and Jiayu Zhou and Fei Wang},
abstract = {Summary
In healthcare, machine learning (ML) shows significant potential to augment patient care, improve population health, and streamline healthcare workflows. Realizing its full potential is, however, often hampered by concerns about data privacy, diversity in data sources, and suboptimal utilization of different data modalities. This review studies the utility of cross-cohort cross-category (C4) integration in such contexts: the process of combining information from diverse datasets distributed across distinct, secure sites. We argue that C4 approaches could pave the way for ML models that are both holistic and widely applicable. This paper provides a comprehensive overview of C4 in health care, including its present stage, potential opportunities, and associated challenges.}
}
@article{WANG2025103085,
title = {Implementation path and reference model for Multilateral Data Circulation System (MDCS) in Datacentric Product-Service System (DPSS): from an industrial practice survey},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103085},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007365},
author = {Chengjun Wang and Xinguo Ming and Xinming Gao and Xianyu Zhang},
keywords = {Datacentric product-service systems, Data circulation, Data sovereignty, Data privacy},
abstract = {With the digital transformation of enterprises and the development of digital infrastructure (smart sensors, 5G/6G, IoT, Industrial Internet, etc.), large amounts of data are generated in various stages of the product life cycle. The value of data in the Product-Service System is becoming prominent. However, through literature review and industrial practice survey, it has been observed that there is a lack of systematic investigation into the processes of data circulation and utilization within PSS. Additionally, within the existing research on data circulation, scholars focus on partial points such as data privacy computing, data sharing and data transaction, lacking an overall reference model for the data circulation in the Product-Service System and the path of implementing a multilateral data circulation platform in the industry. This paper aims to use the industrial practice survey method, based on the literature review, to propose the Datacentric Product-Service System (DPSS) for the first time, and study the main processes of data circulation in the DPSS. The study of the reference model and industrial implementation path of the multilateral data circulation system that meets the industry’s needs in the Datacentric Product-Service System. It provides a reference for the government and industry to design, implement and regulate the domain data circulation platform. In addition, the proposed data circulation system reference model and implementation path can enhance the value symbiosis among enterprises and increase industry benefits.}
}
@article{ZHANG2025104048,
title = {Graph structure prefix injection transformer for multi-modal entity alignment},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104048},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104048},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004072},
author = {Yan Zhang and Xiangyu Luo and Jing Hu and Miao Zhang and Kui Xiao and Zhifei Li},
keywords = {Multi-modal knowledge graphs, Multi-modal entity alignment, Contrastive learning},
abstract = {Multi-modal entity alignment aims to integrate corresponding entities across different MMKGs. However, previous studies have not adequately considered the impact of graph structural heterogeneity on EA tasks. Different MMKGs typically exhibit variations in graph structural features, leading to distinct structural representations of the same entity relationships. Additionally, the topological structure of the graph also differs. To tackle these challenges, we introduce GSIEA, the MMEA framework that integrates structural prefix injection and modality fusion. Different from other methods that directly fuse structural data with multi-modal features to perform the alignment task, GSIEA separately processes structural data and multi-modal data such as images and attributes, incorporating a prefix injection interaction module within a multi-head attention mechanism to optimize the utilization of multi-modal information and minimize the impact of graph structural differences. Additionally, GSIEA employs a convolutional enhancement module to extract fine-grained multi-modal features and computes cross-modal weights to achieve feature fusion. We conduct experimental evaluations on two public datasets, containing 12,846 and 11,199 entity pairs, respectively, demonstrating that GSIEA outperforms baseline models, with an average improvement of 3.26% in MRR and a maximum gain of 12.5%. Furthermore, the average improvement in Hits@1 is 4.96%, with a maximum increase of 16.92%. The code of our model is stored at https://github.com/HubuKG/GSIEA.}
}
@article{YU2025104074,
title = {Exploring long- and short-term knowledge state graph representations with adaptive fusion for knowledge tracing},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104074},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104074},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000160},
author = {Ganfeng Yu and Zhiwen Xie and Guangyou Zhou and Zhuo Zhao and Jimmy Xiangji Huang},
keywords = {Knowledge tracing, Knowledge states, Graph representation learning, Temporal graph prediction, Educational AI},
abstract = {Knowledge Tracing (KT) is an important research area in online education that focuses on predicting future academic performance based on students’ historical exercise records. The key to solving the KT problem lies in assessing students’ knowledge states through their responses to concept-related exercises. However, analyzing exercise records from a single perspective does not provide a comprehensive model of student knowledge. The truth is that students’ knowledge states often exhibit long- and short-term phenomena, corresponding to long-term knowledge systems and short-term real-time learning, both of which are closely related to learning quality and preferences. Existing studies have often neglected the learning preferences implied by long-term knowledge states and their impact on student performance. Therefore, we introduce a hybrid knowledge tracing model that utilizes both long- and short-term knowledge state representations (L-SKSKT). It enhances KT by fusing these two types of knowledge state representations and measuring their impact on learning quality. L-SKSKT includes a graph construction method designed to model students’ long- and short-term knowledge states. In addition, L-SKSKT incorporates a knowledge state graph embedding model that can effectively capture long- and short-term dependencies, generating corresponding knowledge state representations. Furthermore, we propose a fusion mechanism to integrate these representations and trace their impact on learning outcomes. Extensive empirical results on four benchmark datasets show that our approach achieves the best performance for KT, and beats various strong baselines with a large margin.}
}
@article{2024iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {244},
pages = {iii-v},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(24)03028-X},
url = {https://www.sciencedirect.com/science/article/pii/S187705092403028X}
}
@incollection{TOZZI2024355,
title = {4.15 - Advanced Mechanics of Hard Tissue Using Imaging-Based Measurements and Artificial Intelligence},
editor = {Vadim Silberschmidt},
booktitle = {Comprehensive Mechanics of Materials (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {355-380},
year = {2024},
isbn = {978-0-323-90647-0},
doi = {https://doi.org/10.1016/B978-0-323-90646-3.00046-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323906463000460},
author = {Gianluca Tozzi and Markus J. Buehler},
keywords = {Bone, Computed tomography, Deep learning, Diffusion models., Digital volume correlation, Hard tissue, Imaging, Mechanics, Neural networks, Tooth, X-ray},
abstract = {This chapter focuses on the use of advanced imaging-based techniques to assess the complex mechanical patterns of hard tissue, with a specific focus on bone and tooth. Hard tissues are complex hierarchical materials, requiring constant technological advancement to capture their structure-function relationship at different dimensional levels. Experimental techniques such as digital volume correlation, mainly based on X-ray tomography, greatly contributed to deepen understanding of hard tissues local deformation with investigations ranging from clinical computed tomography (CT) to nanoCT. In recent years, the advent of artificial intelligence proposed novel methodologies encompassing image classification and imaging-based mechanical predictions, which have the potential to revolutionize the field of hard tissue mechanics. These approaches and their integration are illustrated with various examples.}
}
@article{XIE2025128888,
title = {MAGO: Multi-Knowledge Aware and Global Strategy Sequence Optimizing Network for Emotional Support Conversation},
journal = {Neurocomputing},
volume = {618},
pages = {128888},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128888},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401659X},
author = {Qijun Xie and Wei Peng},
keywords = {Emotional Support Conversation, Global strategy selection, Multi-Knowledge Aware Integrator, Response generation},
abstract = {Emotional Support Conversation (ESConv) task aims to mitigate the emotional distress of help-seekers by providing supportive responses, which is facilitated by two main sub-tasks: selecting an appropriate supportive strategy and generating effective supportive responses to the help-seeker. However, the existing methodologies exhibit two primary shortcomings. Firstly, they primarily utilize commonsense knowledge such as using the COMET, ignoring the conceptual facts which can enhance the user-utterance understanding and improve the supportive response generation quality. Secondly, they rely excessively on dialogue history or user emotional feedbacks for strategy selection, neglecting the broader characteristics and interrelationships among various strategies globally. In this paper, we introduce the Multi-Knowledge Aware and Global Strategy Sequence Optimizing Network (MAGO). MAGO employs a Multi-Knowledge Aware Integrator to enrich the context understanding and improve response generation from the perspective of commonsense knowledge and conceptual facts. Additionally, MAGO globally incorporates the estimation of optimal strategy tag sequence by using a Strategy-Constrained Conditional Random Field (SCRF). Experimental results demonstrate MAGO’s superior performance in both strategy prediction (significantly increasing accuracy by 18%) and response generation, and analyses illustrate the importance of selecting strategies globally.}
}
@article{DOLHA20243246,
title = {Experiments with natural language queries on RDF vs. XML-serialized BPMN diagrams},
journal = {Procedia Computer Science},
volume = {246},
pages = {3246-3255},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.315},
url = {https://www.sciencedirect.com/science/article/pii/S187705092402338X},
author = {Damaris Naomi Dolha and Robert Andrei Buchmann},
keywords = {BPMN, RDF, prompt engineering, process queries},
abstract = {Our study reports a comparative analysis of natural language interactions with BPMN models, specifically contrasting semantic graphs generated from diagrams by the RDF export of Bee-Up, against the traditional standard BPMN 2.0 XML export from standard-compliant tools (in our case, SAP Signavio Process Transformation Suite). Utilizing varied prompt engineering techniques, the study evaluates the efficacy of GPT-based services from OpenAI in interpreting the nuanced semantic network structures of BPMN-as-RDF and the standard control flow hierarchical decomposition of BPMN-as-XML. Although image-based multi-modal interpretation of BPMN diagrams is also available in such services, our work is motivated by the fact that most BPM systems deliver structured serializations and not images through their APIs; moreover, any data stored in diagrams cannot be scrutinized by computer vision capabilities, being set as annotations in most tools. By exploring both the challenges and effectiveness of utilizing natural language in interacting with BPMN models, the analysis underscores the ability of RDF to mediate semantic richness and open-ended extension of procedural knowledge compared to the closed-world of the XML interchange schemas. Diagrammatic environments are thus encouraged to pursue this as a potential convergence between different means of knowledge representation.}
}
@article{XIAO2024105874,
title = {Automated daily report generation from construction videos using ChatGPT and computer vision},
journal = {Automation in Construction},
volume = {168},
pages = {105874},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105874},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006101},
author = {Bo Xiao and Yifan Wang and Yongpan Zhang and Chen Chen and Amos Darko},
keywords = {Construction daily report generation, Computer vision, ChatGPT, Construction management, Project documentation},
abstract = {Daily reports are important in construction management, informing project teams about status, enabling timely resolutions of delays and budget issues, and serving as official records for disputes and litigation. However, current practices are manual and time-consuming, requiring engineers to physically visit sites for observations. To fill this gap, this paper proposes an automated framework to generate daily construction reports from on-site videos by integrating ChatGPT and computer vision (CV)-based methods. The framework utilizes CV methods to analyze video footage and extract relevant productivity and activity information, which is then fed into ChatGPT using proper prompts to generate daily reports. A web application is developed to implement and validate the framework on a real construction site in Hong Kong, generating daily reports over a month. This research enhances construction management by significantly reducing documentation efforts through generative artificial intelligence, with potential applications in jobsite safety management, quality reporting, and stakeholder communication.}
}
@article{WU2024715,
title = {A review of deep learning methods for ligand based drug virtual screening},
journal = {Fundamental Research},
volume = {4},
number = {4},
pages = {715-737},
year = {2024},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2024.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S2667325824001043},
author = {Hongjie Wu and Junkai Liu and Runhua Zhang and Yaoyao Lu and Guozeng Cui and Zhiming Cui and Yijie Ding},
keywords = {Virtual screening, Deep learning, Drug discovery, Drug-target interaction, Drug-target affinity},
abstract = {Drug discovery is costly and time consuming, and modern drug discovery endeavors are progressively reliant on computational methodologies, aiming to mitigate temporal and financial expenditures associated with the process. In particular, the time required for vaccine and drug discovery is prolonged during emergency situations such as the coronavirus 2019 pandemic. Recently, the performance of deep learning methods in drug virtual screening has been particularly prominent. It has become a concern for researchers how to summarize the existing deep learning in drug virtual screening, select different models for different drug screening problems, exploit the advantages of deep learning models, and further improve the capability of deep learning in drug virtual screening. This review first introduces the basic concepts of drug virtual screening, common datasets, and data representation methods. Then, large numbers of common deep learning methods for drug virtual screening are compared and analyzed. In addition, a dataset of different sizes is constructed independently to evaluate the performance of each deep learning model for the difficult problem of large-scale ligand virtual screening. Finally, the existing challenges and future directions in the field of virtual screening are presented.}
}
@article{SHAMAY2025906,
title = {Mastering the complexities of cancer nanomedicine with text mining, AI and automation},
journal = {Journal of Controlled Release},
volume = {379},
pages = {906-919},
year = {2025},
issn = {0168-3659},
doi = {https://doi.org/10.1016/j.jconrel.2025.01.057},
url = {https://www.sciencedirect.com/science/article/pii/S0168365925000665},
author = {Yosi Shamay},
abstract = {In this contribution to the Orations - New Horizons of the Journal of Controlled Release, I present a personal perspective on the complexities of cancer nanomedicine and the approaches to master them. This oration draws mainly from my lab's journey to explore three transformative approaches to master complexities in the field: (1) leveraging text mining to construct dynamic knowledge bases for hypothesis generation in cell-specific drug delivery, (2) introducing the concept of meta-synergy to further optimize and classify multi-drug combinations across dimensions such as chemical loading, pharmacodynamics, and pharmacokinetics (3) utilizing automation to accelerate nanoparticle discovery with advanced screening methodologies such as aggregation-induced emission (AIE). I argue that by embracing complexity in nanomedicine, we can manifest new therapeutic possibilities, paving the way for more effective, precise, and adaptive treatment strategies.}
}
@incollection{GREENBERG2025349,
title = {15 - Ethics for artificial agents: Toward commensurate capability and self-regulation},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {349-371},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000018},
author = {Ariel M. Greenberg},
keywords = {Advance directive, Artificial intelligence, Autonomous systems, Design policy, Duties, Machine ethics, Moral judgment and decision-making},
abstract = {In this chapter, we tackle the charge to make artificial agent self-regulation on par with their expanding capabilities. We begin by offering research and development schemes focused on installing in machines the perception, knowledge, and reasoning required to support ethics sensitivity and to produce agentic behavior compliant with the principles of nonmaleficence, beneficence, and responsibility. Next, we review evolutionary, psychological, and neuroscientific accounts of the phenomenology and emergence of moral judgment and decision-making that speak to the natural coupling of capability and self-regulation. Finally, we discuss sweeping themes of endogeneity, generality, mentalization, legibility, and duty responsiveness meant to guide how we ought to design an appropriate balance of capability and self-regulation in artificial agents.}
}
@article{MAGNUS2024167,
title = {Towards a GPT-Based Lean Manufacturing Consultant for Manufacturing Optimization},
journal = {Procedia CIRP},
volume = {130},
pages = {167-176},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.072},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012253},
author = {Christian S. Magnus and Moritz Venschott},
keywords = {generative pre-trained transformer, deep learning, smart factory, process optimization},
abstract = {The ever-present competition between industrial companies, both domestically and globally, puts manufacturing companies under constant pressure to optimize. Lean Manufacturing approaches have proven their worth in this environment and continue to hold great promise in the age of digitalization in Smart Factories. However, due to a lack of internal expertise and human resources, very few companies succeed in realizing the hidden potential across all processes. Additional expertise and human resources are often bought in from consultants. Generative Pre-Trained Transformers (GPTs) are a class of deep learning models that process and generate text using text-to-text conversational interfaces. They use large amounts of pre-existing text data to learn complex patterns, semantic relationships, and contextual nuances. Simple conversations with ChatGPT show that it already has some knowledge about Lean Manufacturing. Further investigations are necessary to check, if it is capable to help manufacturing companies with Value Stream Analyses, Line Balancing, etc. to create conversational systems that guide users through the necessary analyses and optimization steps. This paper presents the results of a study on the applicability of OpenAI‘s ChatGPT-4 as a substitute for external consulting services in the described area. It shows exemplary use-cases in manufacturing, the structure and results of an experimental study, and the challenges to be solved when using ChatGPT-4 as Lean Manufacturing consultant.}
}
@article{BUNNE20247045,
title = {How to build the virtual cell with artificial intelligence: Priorities and opportunities},
journal = {Cell},
volume = {187},
number = {25},
pages = {7045-7063},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424013321},
author = {Charlotte Bunne and Yusuf Roohani and Yanay Rosen and Ankit Gupta and Xikun Zhang and Marcel Roed and Theo Alexandrov and Mohammed AlQuraishi and Patricia Brennan and Daniel B. Burkhardt and Andrea Califano and Jonah Cool and Abby F. Dernburg and Kirsty Ewing and Emily B. Fox and Matthias Haury and Amy E. Herr and Eric Horvitz and Patrick D. Hsu and Viren Jain and Gregory R. Johnson and Thomas Kalil and David R. Kelley and Shana O. Kelley and Anna Kreshuk and Tim Mitchison and Stephani Otte and Jay Shendure and Nicholas J. Sofroniew and Fabian Theis and Christina V. Theodoris and Srigokul Upadhyayula and Marc Valer and Bo Wang and Eric Xing and Serena Yeung-Levy and Marinka Zitnik and Theofanis Karaletsos and Aviv Regev and Emma Lundberg and Jure Leskovec and Stephen R. Quake},
keywords = {cell biology, AI, ML, virtual cell},
abstract = {Summary
Cells are essential to understanding health and disease, yet traditional models fall short of modeling and simulating their function and behavior. Advances in AI and omics offer groundbreaking opportunities to create an AI virtual cell (AIVC), a multi-scale, multi-modal large-neural-network-based model that can represent and simulate the behavior of molecules, cells, and tissues across diverse states. This Perspective provides a vision on their design and how collaborative efforts to build AIVCs will transform biological research by allowing high-fidelity simulations, accelerating discoveries, and guiding experimental studies, offering new opportunities for understanding cellular functions and fostering interdisciplinary collaborations in open science.}
}
@article{LI2024100266,
title = {A systematic review of the first year of publications on ChatGPT and language education: Examining research on ChatGPT’s use in language learning and teaching},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100266},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100266},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000699},
author = {Belle Li and Victoria L. Lowell and Chaoran Wang and Xiangning Li},
keywords = {Systematic review, Artificial intelligence (AI), ChatGPT, Language learning},
abstract = {This systematic review aims to explore published research on the use of ChatGPT in language learning between November 2022 and November 2023, outlining the types of papers, methodologies adopted, publishing journals, major research trends, topics of interest, and existing gaps demanding attention. The PRISMA framework was utilized to capture the latest published articles, selecting 36 articles that met the inclusion criteria. Findings extracted from this review include (1) authors worldwide contribute to this topic, with Asia and North America leading; (2) the wide distribution across various journals underscores the interdisciplinary nature of this research topic, such as computer science, psychology, linguistics, education, and other social sciences; (3) empirical research dominates the literature that is published, with the majority focusing on higher education and ethical considerations. Other findings include that ChatGPT plays multifaceted roles, supporting self-directed language learning, content generation, and teacher workflows. Research gaps include the need for diversified scopes, longitudinal studies, exploration of stakeholders’ perceptions, and assessments of feedback quality.}
}
@article{OWEN2024,
title = {AI for Analyzing Mental Health Disorders Among Social Media Users: Quarter-Century Narrative Review of Progress and Challenges},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/59225},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124007933},
author = {David Owen and Amy J Lynham and Sophie E Smart and Antonio F Pardiñas and Jose {Camacho Collados}},
keywords = {mental health, depression, anxiety, schizophrenia, social media, natural language processing, narrative review},
abstract = {Background
Mental health disorders are currently the main contributor to poor quality of life and years lived with disability. Symptoms common to many mental health disorders lead to impairments or changes in the use of language, which are observable in the routine use of social media. Detection of these linguistic cues has been explored throughout the last quarter century, but interest and methodological development have burgeoned following the COVID-19 pandemic. The next decade may see the development of reliable methods for predicting mental health status using social media data. This might have implications for clinical practice and public health policy, particularly in the context of early intervention in mental health care.
Objective
This study aims to examine the state of the art in methods for predicting mental health statuses of social media users. Our focus is the development of artificial intelligence–driven methods, particularly natural language processing, for analyzing large volumes of written text. This study details constraints affecting research in this area. These include the dearth of high-quality public datasets for methodological benchmarking and the need to adopt ethical and privacy frameworks acknowledging the stigma experienced by those with a mental illness.
Methods
A Google Scholar search yielded peer-reviewed articles dated between 1999 and 2024. We manually grouped the articles by 4 primary areas of interest: datasets on social media and mental health, methods for predicting mental health status, longitudinal analyses of mental health, and ethical aspects of the data and analysis of mental health. Selected articles from these groups formed our narrative review.
Results
Larger datasets with precise dates of participants’ diagnoses are needed to support the development of methods for predicting mental health status, particularly in severe disorders such as schizophrenia. Inviting users to donate their social media data for research purposes could help overcome widespread ethical and privacy concerns. In any event, multimodal methods for predicting mental health status appear likely to provide advancements that may not be achievable using natural language processing alone.
Conclusions
Multimodal methods for predicting mental health status from voice, image, and video-based social media data need to be further developed before they may be considered for adoption in health care, medical support, or as consumer-facing products. Such methods are likely to garner greater public confidence in their efficacy than those that rely on text alone. To achieve this, more high-quality social media datasets need to be made available and privacy concerns regarding the use of these data must be formally addressed. A social media platform feature that invites users to share their data upon publication is a possible solution. Finally, a review of literature studying the effects of social media use on a user’s depression and anxiety is merited.}
}
@article{REALENOSEI2024103264,
title = {From vision to text: A comprehensive review of natural image captioning in medical diagnosis and radiology report generation},
journal = {Medical Image Analysis},
volume = {97},
pages = {103264},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103264},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524001890},
author = {Gabriel Reale-Nosei and Elvira Amador-Domínguez and Emilio Serrano},
keywords = {Medical image captioning, Natural image captioning, Diagnostic captioning, Radiology report generation, Survey, State-of-the-art review},
abstract = {Natural Image Captioning (NIC) is an interdisciplinary research area that lies within the intersection of Computer Vision (CV) and Natural Language Processing (NLP). Several works have been presented on the subject, ranging from the early template-based approaches to the more recent deep learning-based methods. This paper conducts a survey in the area of NIC, especially focusing on its applications for Medical Image Captioning (MIC) and Diagnostic Captioning (DC) in the field of radiology. A review of the state-of-the-art is conducted summarizing key research works in NIC and DC to provide a wide overview on the subject. These works include existing NIC and MIC models, datasets, evaluation metrics, and previous reviews in the specialized literature. The revised work is thoroughly analyzed and discussed, highlighting the limitations of existing approaches and their potential implications in real clinical practice. Similarly, future potential research lines are outlined on the basis of the detected limitations.}
}
@article{AMOR2024102764,
title = {Digital regulatory compliance checking for the construction industry},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102764},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102764},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004129},
author = {Robert Amor and Bimal Kumar and Richard Watson}
}
@article{2024iii,
title = {Contents},
journal = {Procedia Computer Science},
volume = {246},
pages = {iii-xxxiv},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/S1877-0509(24)03167-3},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924031673}
}
@article{VALCAMONICO2025110834,
title = {A systematic procedure for the analysis of maintenance reports based on a taxonomy and BERT attention mechanism},
journal = {Reliability Engineering & System Safety},
volume = {257},
pages = {110834},
year = {2025},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2025.110834},
url = {https://www.sciencedirect.com/science/article/pii/S0951832025000377},
author = {Dario Valcamonico and Piero Baraldi and July Bias Macêdo and Márcio Das Chagas Moura and Jonathan Brown and Stéphane Gauthier and Enrico Zio},
keywords = {Maintenance, Natural Language Processing, BERT, DBSCAN, Freight transport trains},
abstract = {This work proposes a systematic procedure for analyzing maintenance reports to support maintenance decision-making for a fleet of similar systems. The proposed procedure allows achieving three objectives: (1) grouping maintenance interventions, (2) identifying common characteristics in the maintenance interventions, and (3) recognizing occurrences of rare events of maintenance intervention. Specifically, the attention mechanism of Bidirectional Encoder Representation from Transformer (BERT) and the Density Based Spatial Clustering Applications with Noise (DBSCAN) methods are combined to group maintenance interventions according to their similarity of stated features. A taxonomy of the words used in the textual reports to state the maintenance interventions is developed to systematically identify common features of the clusters, such as the involved components, their working state, the occurred failures or malfunctions, the performed maintenance actions and the personnel that has performed the intervention. The proposed procedure is applied to a repository of reports of maintenance interventions performed on mechanical and electric components of traction systems of a fleet of trains. The obtained results show that it can effectively support decision-making on the maintenance of traction systems.}
}
@article{EVANS20241509,
title = {Developments and applications of the OPTIMADE API for materials discovery, design, and data exchange††Electronic supplementary information (ESI) available: Copy of Table 1 with web links. See DOI: https://doi.org/10.1039/d4dd00039k},
journal = {Digital Discovery},
volume = {3},
number = {8},
pages = {1509-1533},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00039k},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001219},
author = {Matthew L. Evans and Johan Bergsma and Andrius Merkys and Casper W. Andersen and Oskar B. Andersson and Daniel Beltrán and Evgeny Blokhin and Tara M. Boland and Rubén {Castañeda Balderas} and Kamal Choudhary and Alberto {Díaz Díaz} and Rodrigo {Domínguez García} and Hagen Eckert and Kristjan Eimre and María Elena {Fuentes Montero} and Adam M. Krajewski and Jens Jørgen Mortensen and José Manuel {Nápoles Duarte} and Jacob Pietryga and Ji Qi and Felipe de Jesús {Trejo Carrillo} and Antanas Vaitkus and Jusong Yu and Adam Zettel and Pedro Baptista {de Castro} and Johan Carlsson and Tiago F. T. Cerqueira and Simon Divilov and Hamidreza Hajiyani and Felix Hanke and Kevin Jose and Corey Oses and Janosh Riebesell and Jonathan Schmidt and Donald Winston and Christen Xie and Xiaoyu Yang and Sara Bonella and Silvana Botti and Stefano Curtarolo and Claudia Draxl and Luis Edmundo {Fuentes Cobas} and Adam Hospital and Zi-Kui Liu and Miguel A. L. Marques and Nicola Marzari and Andrew J. Morris and Shyue Ping Ong and Modesto Orozco and Kristin A. Persson and Kristian S. Thygesen and Chris Wolverton and Markus Scheidgen and Cormac Toher and Gareth J. Conduit and Giovanni Pizzi and Saulius Gražulis and Gian-Marco Rignanese and Rickard Armiento},
abstract = {The Open Databases Integration for Materials Design (OPTIMADE) application programming interface (API) empowers users with holistic access to a growing federation of databases, enhancing the accessibility and discoverability of materials and chemical data. Since the first release of the OPTIMADE specification (v1.0), the API has undergone significant development, leading to the v1.2 release, and has underpinned multiple scientific studies. In this work, we highlight the latest features of the API format, accompanying software tools, and provide an update on the implementation of OPTIMADE in contributing materials databases. We end by providing several use cases that demonstrate the utility of the OPTIMADE API in materials research that continue to drive its ongoing development.}
}
@article{MOKOS2025112231,
title = {Model-based safety analysis of requirement specifications},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112231},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112231},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002759},
author = {Konstantinos Mokos and Panagiotis Katsaros and Preben Bohn},
keywords = {Model-based design, Requirements formalization, Ontology-based specification, Formal verification, Safety analysis},
abstract = {Model-based design primarily aims to establish a communication framework throughout a system’s design. Moreover, models with formal semantics allow verification based on rigorous methods, including the analysis of system safety. However, building formal models is a tedious manual process and cannot be easily applied to real problems. A key gap that hinders automation of model development is that there is no systematic way to connect system requirements with the activity of model-based design. In this article, we introduce a workflow to tackle this gap and ultimately automate the analysis of system safety using formal methods. We extend our previous work on boilerplate-based specification of system requirements with ontological semantics towards specifying FDIR (Failure, Detection, Isolation, Recovery) requirements. The workflow is centered around the automated generation of a model skeleton in SLIM, a component-based formal modeling language, from a set of ontology-based requirement specifications. Our approach has been implemented into a dedicated tool, which not only provides visualization of the ontology relations, but also supports traceability of the analysis findings back to the requirements specification. Finally, we provide results on the safety analysis of a real star-tracker system based on a SLIM model derived by minimally changing the auto-generated model skeleton.}
}
@article{SARKER2024101110,
title = {Multi-aspect rule-based AI: Methods, taxonomy, challenges and directions towards automation, intelligence and transparent cybersecurity modeling for critical infrastructures},
journal = {Internet of Things},
volume = {25},
pages = {101110},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101110},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524000520},
author = {Iqbal H. Sarker and Helge Janicke and Mohamed Amine Ferrag and Alsharif Abuadbba},
keywords = {Cybersecurity, Rule-based Modeling, Explainable AI, Responsible AI, Machine learning, Security data analytics, Knowledge discovery, Data-driven decision-making, Automation, Intelligence, Transparency, Trustworthiness, Critical infrastructures},
abstract = {Critical infrastructure (CI) typically refers to the essential physical and virtual systems, assets, and services that are vital for the functioning and well-being of a society, economy, or nation. However, the rapid proliferation and dynamism of today’s cyber threats in digital environments may disrupt CI functionalities, which would have a debilitating impact on public safety, economic stability, and national security. This has led to much interest in effective cybersecurity solutions regarding automation and intelligent decision-making, where AI-based modeling is potentially significant. In this paper, we take into account “Rule-based AI” rather than other black-box solutions since model transparency, i.e., human interpretation, explainability, and trustworthiness in decision-making, is an essential factor, particularly in cybersecurity application areas. This article provides an in-depth study on multi-aspect rule based AI modeling considering human interpretable decisions as well as security automation and intelligence for CI. We also provide a taxonomy of rule generation methods by taking into account not only knowledge-driven approaches based on human expertise but also data-driven approaches, i.e., extracting insights or useful knowledge from data, and their hybridization. This understanding can help security analysts and professionals comprehend how systems work, identify potential threats and anomalies, and make better decisions in various real-world application areas. We also cover how these techniques can address diverse cybersecurity concerns such as threat detection, mitigation, prediction, diagnosis for root cause findings, and so on in different CI sectors, such as energy, defence, transport, health, water, agriculture, etc. We conclude this paper with a list of identified issues and opportunities for future research, as well as their potential solution directions for how researchers and professionals might tackle future generation cybersecurity modeling in this emerging area of study.}
}
@article{NAITHANI2025102686,
title = {Editorial overview: Genome studies and molecular genetics 2024},
journal = {Current Opinion in Plant Biology},
volume = {83},
pages = {102686},
year = {2025},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2024.102686},
url = {https://www.sciencedirect.com/science/article/pii/S1369526624001778},
author = {Sushma Naithani and Leena Tripathi}
}
@article{DENG2025105224,
title = {Does ChatGPT enhance student learning? A systematic review and meta-analysis of experimental studies},
journal = {Computers & Education},
volume = {227},
pages = {105224},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105224},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524002380},
author = {Ruiqi Deng and Maoli Jiang and Xinlu Yu and Yuyan Lu and Shasha Liu},
keywords = {Teaching/learning strategies, Improve classroom teaching, Elementary education, Secondary education, Post-secondary education},
abstract = {Chat Generative Pre-Trained Transformer (ChatGPT) has generated excitement and concern in education. While cross-sectional studies have highlighted correlations between ChatGPT use and learning performance, they fall short of establishing causality. This review examines experimental studies on ChatGPT's impact on student learning to address this gap. A comprehensive search across five databases identified 69 articles published between 2022 and 2024 for analysis. The findings reveal that ChatGPT interventions are predominantly implemented at the university level, cover various subject areas focusing on language education, are integrated into classroom environments as part of regular educational practices, and primarily involve direct student use of ChatGPT. Overall, ChatGPT improves academic performance, affective-motivational states, and higher-order thinking propensities; it reduces mental effort and has no significant effect on self-efficacy. However, methodological limitations, such as the lack of power analysis and concerns regarding post-intervention assessments, warrant cautious interpretation of results. This review presents four propositions from the findings: (1) distinguish between the quality of ChatGPT outputs and the positive effects of interventions on academic performance by shifting from well-defined problems in post-intervention assessments to more complex, project-based assessments that require skill demonstration, adopting proctored assessments, or incorporating metrics such as originality alongside quality; (2) evaluate long-term impacts to determine whether the positive effects on affective-motivational states are sustained or merely owing to novelty effect; (3) prioritise objective measures to complement subjective assessments of higher-order thinking; and (4) use power analysis to determine adequate sample sizes to avoid Type II errors and provide reliable effect size estimates. This review provides valuable insights for researchers, instructors, and policymakers evaluating the effectiveness of generative AI integration in educational practice.}
}
@article{LUO2025125827,
title = {ChatGPT based contrastive learning for radiology report summarization},
journal = {Expert Systems with Applications},
volume = {267},
pages = {125827},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125827},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424026940},
author = {Zhenjie Luo and Zuowei Jiang and Mingyang Wang and Xiaoyan Cai and Dehong Gao and Libin Yang},
keywords = {Text summarization, Radiology reports, ChatGPT, Contrastive learning},
abstract = {Automatically Impression Generation (AIG) can conclude essential information of the “Findings” section, thus facilitating more effective communication between radiographers and physicians. Different from general abstractive summarization, AIG is more challenging for data-driven neural models. This may be due to two critical issues: the serious data bias and the shallow and rough use of available data. To alleviate data bias problem and make best use of available data, we propose a novel ChatGPT based Contrastive Learning (CCL) approach. Specifically, CCL progressively learns to generate impressions to address the above problems. Firstly, we input “findings” and its target “impression” into T5 for fine-tuning, namely WarmUp; Secondly, we propose CCL to alleviate exposure bias by adding its self-generation as negative samples, and treat all negative samples differently according to their sample quality. This can make full use of the available limited data; Thirdly, we select the non-dominant data in data bias by assessing model and input them into ChatGPT for data augmentation. By iteratively doing the second step and third step, CCL can gradually improve its summarization performance on radiology reports. Obtained results on the public OpenI and MIMIC-CXR datasets show effectiveness of our CCL.22https://github.com/jzw1234/Chataug-CCL.}
}
@article{TRAPPEY201819,
title = {Construction and validation of an ontology-based technology function matrix: Technology mining of cyber physical system patent portfolios},
journal = {World Patent Information},
volume = {55},
pages = {19-24},
year = {2018},
note = {Advanced Analytics of Intellectual Property Information for TechMining},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0172219018300139},
author = {Amy J.C. Trappey and Charles V. Trappey and Usharani Hareesh Govindarajan and Allen C.C. Jhuang},
keywords = {Technology function matrix (TFM), Cyber Physical System (CPS), Patent analysis, Patent portfolio},
abstract = {This research develops a computer-supported ontology-based Technology Function Matrix (TFM) construction method, called eTFM, as an approach to reduce technology mining man-power and enhance the accuracy and consistency of patent analysis results. The paper addresses a rarely discussed issue of the TFM validation. The proposed validation approach compares the TFMs construction based on both on the domain ontology and the International Patent Classification (IPC) classes. The research demonstrates the methodology's practical applications using the patent analysis case of cyber physical system (CPS), an essential core technology enabling advanced manufacturing and Industry 4.0.}
}
@article{BALASUBRAMANIAN2025100545,
title = {A cognitive platform for collecting cyber threat intelligence and real-time detection using cloud computing},
journal = {Decision Analytics Journal},
volume = {14},
pages = {100545},
year = {2025},
issn = {2772-6622},
doi = {https://doi.org/10.1016/j.dajour.2025.100545},
url = {https://www.sciencedirect.com/science/article/pii/S2772662225000013},
author = {Prasasthy Balasubramanian and Sadaf Nazari and Danial Khosh Kholgh and Alireza Mahmoodi and Justin Seby and Panos Kostakos},
keywords = {Cyber threat intelligence, Machine learning operations, Classification, Indicators of compromise, Bidirectional encoder representations from transformers (BERT), Longformer},
abstract = {The extraction of cyber threat intelligence (CTI) from open sources is a rapidly expanding defensive strategy that enhances the resilience of both Information Technology (IT) and Operational Technology (OT) environments against large-scale cyber-attacks. However, for most organizations, collecting actionable CTI remains both a technical bottleneck and a black box. While previous research has focused on improving individual components of the extraction process, the community lacks open-source platforms for deploying streaming CTI data pipelines in the wild. This study proposes an efficient platform capable of processing compute-intensive data pipelines, based on cloud computing, for real-time detection, collection, and sharing of CTI from various online sources. We developed a prototype platform (TSTEM) with a containerized microservice architecture that uses Tweepy, Scrapy, Terraform, Elasticsearch, Logstash, and Kibana (ELK), Kafka, and Machine Learning Operations (MLOps) to autonomously search, extract, and index indicators of compromise (IOCs) in the wild. Moreover, the provisioning, monitoring, and management of the platform are achieved through infrastructure as code (IaC). Custom focus-crawlers collect web content, processed by a first-level classifier to identify potential IOCs. Relevant content advances to a second level for further examination. State-of-the-art natural language processing (NLP) models are used for classification and entity extraction, enhancing the IOC extraction methodology. Our results indicate these models exhibit high accuracy (exceeding 98%) in classification and extraction tasks, achieving this performance within less than a minute. The system’s effectiveness is due to a finely-tuned IOC extraction method that operates at multiple stages, ensuring precise identification with low false positives.}
}
@article{GALDO2024195,
title = {Artificial intelligence in paediatrics: Current events and challenges},
journal = {Anales de Pediatría (English Edition)},
volume = {100},
number = {3},
pages = {195-201},
year = {2024},
issn = {2341-2879},
doi = {https://doi.org/10.1016/j.anpede.2024.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S2341287924000383},
author = {Brais Galdo and Carla Pazos and Jerónimo Pardo and Alfonso Solar and Daniel Llamas and Enrique Fernández-Blanco and Alejandro Pazos},
keywords = {Artificial intelligence, 7P medicine, Machine learning, Paediatrics, Personalized medicine, Inteligencia artificial, Medicina de las 7P, Aprendizaje máquina, Pediatría, Medicina personalizada},
abstract = {This article examines the use of artificial intelligence (AI) in the field of paediatric care within the framework of the 7P medicine model (Predictive, Preventive, Personalized, Precise, Participatory, Peripheral and Polyprofessional). It highlights various applications of AI in the diagnosis, treatment and management of paediatric diseases as well as the role of AI in prevention and in the efficient management of health care resources and the resulting impact on the sustainability of public health systems. Successful cases of the application of AI in the paediatric care setting are presented, placing emphasis on the need to move towards a 7P health care model. Artificial intelligence is revolutionizing society at large and has a great potential for significantly improving paediatric care.
Resumen
Se examina el uso de la inteligencia artificial (IA) en el campo de la atención a la salud pediátrica dentro del marco de la "Medicina de las 7P" (Predictiva, Preventiva, Personalizada, Precisa, Participativa, Periférica y Poliprofesional). Se destacan diversas aplicaciones de la IA en el diagnóstico, el tratamiento y el control de enfermedades pediátricas, así como su papel en la prevención y en la gestión eficiente de los recursos médicos con su repercusión en la sostenibilidad de los sistemas públicos de salud. Se presentan casos de éxito de la aplicación de la IA en el ámbito pediátrico y se hace un gran énfasis en la necesidad de caminar hacia la Medicina de las 7P. La IA está revolucionando la sociedad en general ofreciendo un gran potencial para mejorar significativamente el cuidado de la salud en pediatría.}
}
@article{ZHANG20243652,
title = {Foundation model for generalist remote sensing intelligence: Potentials and prospects},
journal = {Science Bulletin},
volume = {69},
number = {23},
pages = {3652-3656},
year = {2024},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2024.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S2095927324006510},
author = {Mi Zhang and Bingnan Yang and Xiangyun Hu and Jianya Gong and Zuxun Zhang}
}@article{DOSSANTOS2024108422,
title = {Machine learning applied to digital phenotyping: A systematic literature review and taxonomy},
journal = {Computers in Human Behavior},
volume = {161},
pages = {108422},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108422},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224002905},
author = {Marília Pit {dos Santos} and Wesllei Felipe Heckler and Rodrigo Simon Bavaresco and Jorge Luis Victória Barbosa},
keywords = {Systematic literature review, Digital phenotyping, Digital phenotype, Machine learning},
abstract = {Health conditions, encompassing both physical and mental aspects, hold an influence that extends beyond the individual. These conditions affect personal well-being, relationships, and financial stability. Innovative strategies in healthcare, such as digital phenotyping, are strategic to mitigate these impacts. By merging diverse data sources, digital phenotyping seeks a comprehensive understanding of health, well-being, and behavioral conditions. Machine learning can enhance the analysis of these data, improving the comprehension of health and well-being. Therefore, this paper presents a systematic literature review on machine learning and digital phenotyping, examining the research field by filtering 2,860 articles from eleven databases published up to November 2023. The analysis focused on 124 articles to answer six research questions addressing machine learning techniques, data, devices, ontologies, and research challenges. This work presents a taxonomy for mapping explored areas in digital phenotyping and another for organizing machine learning techniques used in digital phenotyping research. The review found increased publications in 2023, indicating a growing interest in the field. The main challenges arise from the studies’ small participant samples and imbalanced datasets, limiting the generalizability of the results to broader populations and the choice of ML methods. Furthermore, the reliance on self-reported data can introduce potential inaccuracies due to recall and reporting biases. Beyond self-reports, authors explored different data types, including physiological, clinical, contextual, smartphone-based, and multimedia. Despite using video recordings in controlled experiments, studies have yet to investigate this method within intelligent environments. Researchers also analyzed neurophysiological phenotypes, suggesting the potential for interventions based on these characteristics.}
}
@article{LUO2024102678,
title = {Learning multimodal adaptive relation graph and action boost memory for visual navigation},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102678},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102678},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003264},
author = {Jian Luo and Bo Cai and Yaoxiang Yu and Aihua Ke and Kang Zhou and Jian Zhang},
keywords = {Action boost memory, Knowledge graph, Reinforcement learning, Visual navigation, Visual transformer network},
abstract = {The task of visual navigation (VN) is steering the agent find target object only using visual perceptions. Previous works largely exploit multimodal information (e.g. visual and training memory) to improve the environmental perception ability, while making less effort to leverage interchange information. Besides, multimodal fusion tends to ignore the data dependencies (prefer a part of the modal data) as well as the supervision of the action. In this work, we present a novel multimodal graph learning (MGL) structure for VN, which consists of three parts. (1) the multimodal fusion exploits the rich information across spatial, RGB, and depth information about objects’ place, as well as semantic information about their categories, (2) adaptive relation graph (ARG) is dynamically built using object detectors, which encodes multimodal fusion and adapt to a novel environment. It embeds its navigation history and other useful task-oriented structural information, thus make the agent own the association ability and make advisable informed decisions and (3) action boost module (ABM) aims to assist the agent make intelligent decisions, which predicts more accurate action using beneficial training experience. Our agent can foresight what the goal state may look like and how to get closer towards that state. These combinations of the “what” and the “how” allow the agent to navigate to the target object effectively. We validate our approach on the AI2-THOR dataset. It reports 24.2% and 23.7% increase in SPL(Success weighted by Per Length) and SR(Success Rate) compared with baselines, respectively. Code and datasets can be found in https://github.com/luosword/ABM_VN.}
}
@article{LE2025167680,
title = {An in-depth review of AI-powered advancements in cancer drug discovery},
journal = {Biochimica et Biophysica Acta (BBA) - Molecular Basis of Disease},
volume = {1871},
number = {3},
pages = {167680},
year = {2025},
issn = {0925-4439},
doi = {https://doi.org/10.1016/j.bbadis.2025.167680},
url = {https://www.sciencedirect.com/science/article/pii/S0925443925000250},
author = {Minh Huu Nhat Le and Phat Ky Nguyen and Thi Phuong Trang Nguyen and Hien Quang Nguyen and Dao Ngoc Hien Tam and Han Hong Huynh and Phat Kim Huynh and Nguyen Quoc Khanh Le},
keywords = {AI-driven drug discovery, cancer genomics, Computational drug design, Bioinformatics, Precision oncology, cancer therapeutics},
abstract = {The convergence of artificial intelligence (AI) and genomics is redefining cancer drug discovery by facilitating the development of personalized and effective therapies. This review examines the transformative role of AI technologies, including deep learning and advanced data analytics, in accelerating key stages of the drug discovery process: target identification, drug design, clinical trial optimization, and drug response prediction. Cutting-edge tools such as DrugnomeAI and PandaOmics have made substantial contributions to therapeutic target identification, while AI's predictive capabilities are driving personalized treatment strategies. Additionally, advancements like AlphaFold highlight AI's capacity to address intricate challenges in drug development. However, the field faces significant challenges, including the management of large-scale genomic datasets and ethical concerns surrounding AI deployment in healthcare. This review underscores the promise of data-centric AI approaches and emphasizes the necessity of continued innovation and interdisciplinary collaboration. Together, AI and genomics are charting a path toward more precise, efficient, and transformative cancer therapeutics.}
}
@article{BARNES20242072,
title = {This month in The Journal},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {10},
pages = {2072-2073},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2024.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0002929724003380},
author = {Alyson B. Barnes and Kylee L. Spencer}
}
@article{ORDONEZ2024102346,
title = {Data engineering and modeling for artificial intelligence},
journal = {Data & Knowledge Engineering},
volume = {153},
pages = {102346},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102346},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000703},
author = {Carlos Ordonez and Wojciech Macyna and Ladjel Bellatreche}
}
@article{HEYDER2023101772,
title = {Ethical management of human-AI interaction: Theory development review},
journal = {The Journal of Strategic Information Systems},
volume = {32},
number = {3},
pages = {101772},
year = {2023},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2023.101772},
url = {https://www.sciencedirect.com/science/article/pii/S0963868723000185},
author = {Teresa Heyder and Nina Passlack and Oliver Posegga},
keywords = {Artificial intelligence, Ethics, Human-AI interaction, Theoretical review, Sociomateriality},
abstract = {AI-based technologies have changed the nature of the symbiosis between humans and AI, and so strategic management of human-AI interaction in organizations requires deeper ethical considerations. Aligning AI with human values requires a systematic understanding of the ethical management of human-AI interaction. We conduct a theoretical review, from a sociotechnical perspective, and analyze ethical management of human-AI interaction through the lens of sociomateriality. Our systematic approach helps explain and clarify the interdependencies between two ethical perspectives – duty and virtue ethics – in sociotechnical systems. We also provide a theoretical framework that leads to seven avenues for future research.}
}
@article{BACK202423,
title = {Accelerated chemical science with AI},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {23-33},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00213f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000858},
author = {Seoin Back and Alán Aspuru-Guzik and Michele Ceriotti and Ganna Gryn'ova and Bartosz Grzybowski and Geun Ho Gu and Jason Hein and Kedar Hippalgaonkar and Rodrigo Hormázabal and Yousung Jung and Seonah Kim and Woo Youn Kim and Seyed Mohamad Moosavi and Juhwan Noh and Changyoung Park and Joshua Schrier and Philippe Schwaller and Koji Tsuda and Tejs Vegge and O. Anatole {von Lilienfeld} and Aron Walsh},
abstract = {In light of the pressing need for practical materials and molecular solutions to renewable energy and health problems, to name just two examples, one wonders how to accelerate research and development in the chemical sciences, so as to address the time it takes to bring materials from initial discovery to commercialization. Artificial intelligence (AI)-based techniques, in particular, are having a transformative and accelerating impact on many if not most, technological domains. To shed light on these questions, the authors and participants gathered in person for the ASLLA Symposium on the theme of ‘Accelerated Chemical Science with AI’ at Gangneung, Republic of Korea. We present the findings, ideas, comments, and often contentious opinions expressed during four panel discussions related to the respective general topics: ‘Data’, ‘New applications’, ‘Machine learning algorithms’, and ‘Education’. All discussions were recorded, transcribed into text using Open AI's Whisper, and summarized using LG AI Research's EXAONE LLM, followed by revision by all authors. For the broader benefit of current researchers, educators in higher education, and academic bodies such as associations, publishers, librarians, and companies, we provide chemistry-specific recommendations and summarize the resulting conclusions.}
}
@incollection{LAMURIAS2024,
title = {Text Mining for Bioinformatics Using Biomedical Literature},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000178},
author = {Andre Lamurias and Diana F. Sousa and Francisco M. Couto},
keywords = {Biomedical literature, Distant supervision, Event extraction, Machine Learning, Named entity recognition, Normalization, Relation extraction, Text mining},
abstract = {Biomedical literature is a large and rich source of information for various applications. Text mining tools aim at extracting information from the literature in an efficient manner since processing scientific texts is a complex task given the formal and highly specialized language. Text mining tools tackle these challenges using different approaches, such as rule-based methods and machine learning algorithms including deep learning. This document overviews the current biomedical text mining tools by describing their approaches, tasks (e.g., Named Entity Recognition, Relation Extraction, Event Extraction, Question Answering), available corpora, toolkits and applications, and community challenges.}
}
@article{SONG202416844,
title = {AI empowering traditional Chinese medicine?},
journal = {Chemical Science},
volume = {15},
number = {41},
pages = {16844-16886},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc04107k},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024015359},
author = {Zhilin Song and Guanxing Chen and Calvin Yu-Chian Chen},
abstract = {For centuries, Traditional Chinese Medicine (TCM) has been a prominent treatment method in China, incorporating acupuncture, herbal remedies, massage, and dietary therapy to promote holistic health and healing. TCM has played a major role in drug discovery, with over 60% of small-molecule drugs approved by the FDA from 1981 to 2019 being derived from natural products. However, TCM modernization faces challenges such as data standardization and the complexity of TCM formulations. The establishment of comprehensive TCM databases has significantly improved the efficiency and accuracy of TCM research, enabling easier access to information on TCM ingredients and encouraging interdisciplinary collaborations. These databases have revolutionized TCM research, facilitating advancements in TCM modernization and patient care. In addition, advancements in AI algorithms and database data quality have accelerated progress in AI for TCM. The application of AI in TCM encompasses a wide range of areas, including herbal screening and new drug discovery, diagnostic and treatment principles, pharmacological mechanisms, network pharmacology, and the incorporation of innovative AI technologies. AI also has the potential to enable personalized medicine by identifying patterns and correlations in patient data, leading to more accurate diagnoses and tailored treatments. The potential benefits of AI for TCM are vast and diverse, promising continued progress and innovation in the field.}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{LIU2025125701,
title = {A cascaded retrieval-while-reasoning multi-document comprehension framework with incremental attention for medical question answering},
journal = {Expert Systems with Applications},
volume = {265},
pages = {125701},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125701},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424025685},
author = {Jiandong Liu and Jianfeng Ren and Ruibin Bai and Zibo Zhang and Zheng Lu},
keywords = {Medical QA, Multi-document MRC, Cascaded network, Incremental attention, Retrieval-while-reasoning},
abstract = {Clinical Machine Reading Comprehension (MRC) is challenging due to the need for medical expertise and comprehensive reasoning chains for diagnosis. This paper introduces a novel cascade retrieval-while-reasoning framework for clinical MRC that incrementally retrieves and processes multiple supporting documents to effectively address the complexities of answering medical questions. The proposed cascade system is designed in such a way that easier questions are processed by shallower network layers with fewer documents, while more difficult ones are handled by deeper layers with more documents. In the proposed system, a retriever is designed to provide knowledge documents incrementally according to the comprehended difficulty level of each question, which interacts with the reader via query updating for each retrieval and modifies its search direction to better exploit the knowledge bank. To handle the supporting information from incrementally retrieved documents, a progressive attention mechanism is designed to extract cross-document features for better reasoning. The attentional information from multiple supporting documents is then aggregated for the final decision. The proposed method is compared with state-of-the-art models for medical MRC tasks on a large medical QA dataset. Experimental results show that the proposed model effectively combines multiple knowledge documents to solve challenging real-world clinical diagnosis problems. It significantly outperforms the previously best-performing model by 1.84%, reaching an accuracy of 63.00%.}
}
@article{GRAVES2024100051,
title = {Modeling morality and spirituality in artificial chaplains},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100051},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100051},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000112},
author = {Mark Graves}
}
@article{HAN2024121052,
title = {LegalAsst: Human-centered and AI-empowered machine to enhance court productivity and legal assistance},
journal = {Information Sciences},
volume = {679},
pages = {121052},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121052},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524009666},
author = {Wenjuan Han and Jiaxin Shen and Yanyao Liu and Zhan Shi and Jinan Xu and Fangxu Hu and Hao Chen and Yan Gong and Xueli Yu and Huaqing Wang and Zhijing Liu and Yajie Yang and Tianshui Shi and Mengyao Ge},
keywords = {Tools to assist in the trial, Explainable process, Multi-level inference, Traceable decision, Controllable judgment, Artificial intelligence technology},
abstract = {We propose autonomous software (namely, LegalAsst ) as a step toward an AI-empowered but human-centered machine focused on enhancing court productivity and legal assistance. LegalAsst aims to provide explainable, traceable, and controllable legal assistance and references for lawyers, judges, government officials, and the general public. To achieve this goal, it collates, processes, distills, and visualizes the whole judgment procedure. It streamlines and semi-automates the judgment procedure through case analysis, legislation analysis, and judicial decision-making. Specifically, to make laws and cases easier to navigate and understand, we incorporate structured representations to perform them. Then based on structured representations, we take a step further by introducing a decision-tree-based judgment, making the entire judging process visible and tractable. Our system not only tracks the procedural aspects of judgments but also incorporates modification capabilities, enabling the consideration of the most up-to-date legislation and societal factors to generate more adaptable judgment outcomes.1}
}
@article{SADLEK2025103956,
title = {Severity-based triage of cybersecurity incidents using kill chain attack graphs},
journal = {Journal of Information Security and Applications},
volume = {89},
pages = {103956},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103956},
url = {https://www.sciencedirect.com/science/article/pii/S2214212624002588},
author = {Lukáš Sadlek and Muhammad Mudassar Yamin and Pavel Čeleda and Basel Katt},
keywords = {Kill chain, Attack graph, Incident severity, Incident triage, MITRE ATT&CK, Cyber crisis},
abstract = {Security teams process a vast number of security events. Their security analysts spend considerable time triaging cybersecurity alerts. Many alerts reveal incidents that must be handled first and escalated to the more experienced staff to allow appropriate responses according to their severity. The current state requires an automated approach, considering contextual relationships among security events, especially detected attack tactics and techniques. In this paper, we propose a new graph-based approach for incident triage. First, it generates a kill chain attack graph from host and network data. Second, it creates sequences of detected alerts that could represent ongoing multi-step cyber attacks and matches them with the attack graph. Last, it assigns severity levels to the created sequences of alerts according to the most advanced kill chain phases that were used and the criticality of assets. We implemented the approach using the MulVAL attack graph generator and generation rules for MITRE ATT&CK techniques. The evaluation was accomplished in a testbed where multi-step attack scenarios were executed. Classification of sequences of alerts based on computed match scores obtained 0.95 area under the receiver operating characteristic curve in a feasible time. Moreover, a threshold exists for classifying 80% of positive sequences correctly and only a small percentage of negative sequences wrongly. Therefore, the approach selects malicious sequences of alerts and significantly improves incident triage.}
}
@article{CAO2024561,
title = {Artificial intelligence in metal forming},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {561-587},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.04.102},
url = {https://www.sciencedirect.com/science/article/pii/S0007850624001161},
author = {Jian Cao and Markus Bambach and Marion Merklein and Mojtaba Mozaffar and Tianju Xue},
keywords = {Artificial intelligence, Machine learning, Material characterization, Process design, Process control},
abstract = {Forming processes are known for their intricacies in prediction and control due to the complex loading conditions and material flow. This paper will first introduce the AI algorithms used or having potential to be used in forming, and then investigate the state-of-the-art advances of AI-based technologies in forming processes with four main pillars of process simulation, process design and optimization, in-situ process control, and qualification and certification of forming processes and formed products. Future directions of AI in forming for both academic research and industrial applications will be proposed to leverage digitalization and data science to explore new solutions in forming processes.}
}
@article{THOMAS20241060,
title = {One-shot relation retrieval in news archives: adapting N-way K-shot relation Classification for efficient knowledge extraction},
journal = {Procedia Computer Science},
volume = {246},
pages = {1060-1069},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.525},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924025705},
author = {Hugo Thomas and Guillaume Gravier and Pascale Sébillot},
keywords = {relation extraction, relation retrieval, few-shot learning, N-way K-shot learning},
abstract = {One-shot relation retrieval is the knowledge extraction task that consists in searching in a textual dataset for all occurrences of a relation of interest, named the source relation, characterized by a single example—a relation being a link between a pair of entities in an utterance. Performing this task on large datasets requires an intelligent system to automate the process, for instance when exploring news archives for press review or business intelligence. We propose a framework that leverages the representation learning capabilities of N-way K-shot models for few-shot relation Classification and extends these models to enable one-shot retrieval with a rejection class. At evaluation time, one-shot relation retrieval is performed in a N-way K-shot setting where 1 of the N ways (or relations) is the source relation and the N-1 others are distractors, i.e., relations modeling a rejection class. We benchmark this framework and investigate the influence of the number and the choice of distractors on the standard TACREV and FewRel datasets. Experimental results demonstrate the effectiveness of our approach to address this highly challenging task, however with high variability primarily induced by the type of the source relation. Experiments also highlight a sound strategy for the choice of distractors—a large number of distractors at an intermediate distance from the embedding of the source relation in the latent space learned by the model—, which provides a competing trade-of between recall and precision. This strategy is globally optimal but can however be surpassed on certain source relations by others, depending on the characteristics of the source relation, paving the way for future work. We finally show the substantial benefit of two-shot retrieval over one-shot retrieval, which sheds light on the design of actual intelligent applications leveraging one- or few-shot relation retrieval.}
}
@article{XIA2025129031,
title = {MPE3: Learning meta-prompt with entity-enhanced semantics for few-shot named entity recognition},
journal = {Neurocomputing},
volume = {620},
pages = {129031},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129031},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224018022},
author = {Yuwei Xia and Zhao Tong and Liang Wang and Qiang Liu and Shu Wu and Xiaoyu Zhang},
keywords = {Information extraction, Few-shot named entity recognition, Prompt-learning, Meta-learning},
abstract = {Recently, prompt-tuning has been proven to be surprisingly effective on few-shot tasks. Intuitively, some studies explore Few-shot Named Entity Recognition (NER) based on prompt-tuning. However, how to properly initialize and effectively learn the prompt under limited training conditions still remains significantly challenging for few-shot NER. To meet these challenges, we propose a novel Meta-Prompt with Entity-Enhanced semantics for Few-shot NER, MPE3 for brevity. Specifically, we first explore the importance of the named entities’ semantics in the few-shot NER task. And we propose to construct prompts with entity-enhanced semantics which contain much useful prior knowledge for identifying named entities. Furthermore, to address the issue of inadequate training more substantially, we aim to train a meta-prompt that can be more effective and adaptive for few-shot NER scenarios. To achieve this, we divide the training data into many source-domain agnostic meta-tasks tailored to the characteristics of the NER problem for training. And we specially design a prompt meta-learner for training these meta-tasks. This training strategy succeeds in guiding prompts to optimize in a better direction for few-shot scenarios by the learned meta-knowledge from each meta-task. We conduct extensive experiments on three NER datasets under two different few-shot settings. Our method outperforms the current state-of-the-art model by 5.60%∼13.34% and 2.41%∼7.34% on average in the two different few-shot settings respectively, which validates the effectiveness and superiority of our model.}
}
@article{MAIER20241623,
title = {Simulation Discovery and Semi-Automatic Scenario Generation for Evaluation of Turbulence in Production Systems},
journal = {Procedia CIRP},
volume = {130},
pages = {1623-1631},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.292},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124014495},
author = {Julian B. Maier and Eduardo Colangelo and Theresa-Franziska Hinrichsen and Dinh Khoi Tran and Hans-Hermann Wiendahl and Marco F. Huber},
keywords = {Resilience, Digital Twins, Model Discovery, Scenario Generation},
abstract = {Production systems have always faced changes and disruptions, which require dynamic decision-making to adjust existing plans to the unfolding reality. The interdependence of highly interconnected supply chain networks further adds to this volatility. Given this complexity, mainly caused by ambiguity and the systems’ dynamic, achieving transparency to make decisions in the context of production planning and control is challenging. Simulation models can help assess the outcome of different scenarios through experiments. However, building simulation models by hand requires extensive manual effort and expert knowledge of simulation tools. Although often partly automated, simulation experiments still require the exertion of simulation engineers to be conducted on a large scale. Moreover, the created models are often static and require additional resources to be updated in order to reflect changes in the physical system. To reduce this effort, the authors propose a concept combining the automatic discovery of simulation models from execution data with the semi-automatic generation of scenarios. This facilitates logistical risk analysis and prediction by evaluating the consequences of possible disruptive events. The concept aims to enable domain experts to use digital twins in large-scale virtual scenario evaluation, which is fundamental for increasing the agility of manufacturing systems by speeding up decision processes.}
}
@article{ARROTTA2025126178,
title = {Multi-subject human activities: A survey of recognition and evaluation methods based on a formal framework},
journal = {Expert Systems with Applications},
volume = {267},
pages = {126178},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126178},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424030458},
author = {Luca Arrotta and Gabriele Civitarese and Xi Chen and Julien Cumin and Claudio Bettini},
keywords = {Multi-subject HAR, Group activity recognition, Human activity recognition, Ambient intelligence},
abstract = {Human Activity Recognition (HAR) in smart environments is a well-explored research domain, given its diverse applications which include healthcare, surveillance, building management, and many more. While the majority of HAR research focuses on recognizing the activities of a single subject, in real-world scenarios smart environments are often populated by multiple subjects that may be engaged in both independent and joint activities. This gives rise to the challenge of Multi-Subject HAR, which is an open and complex problem. This survey paper aims to offer researchers and practitioners a comprehensive analysis of Multi-Subject HAR, encompassing its potential applications, sensing solutions, methods, datasets, evaluation metrics, and ongoing challenges. In addition to presenting the latest research works in this area and identifying open issues, our major contributions consist of a comprehensive problem formalization and a thorough discussion of the evaluation metrics to assess different dimensions of multi-subject HAR systems.}
}
@article{JIAO2025124343,
title = {The secrets to high-level green technology innovation of China's waste power battery recycling enterprises},
journal = {Journal of Environmental Management},
volume = {375},
pages = {124343},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2025.124343},
url = {https://www.sciencedirect.com/science/article/pii/S0301479725003196},
author = {Jianling Jiao and Yuqin Chen and Jingjing Li and Shanlin Yang},
keywords = {Green technology innovation, Waste power battery recycling, PSR, Bayesian network, GPT-4},
abstract = {Green technology innovation (GTI) in China's waste power battery recycling (WPBR) sector is a key driver for sustainable resource management, environmental protection, and economic prosperity. Using the PSR-BN-GPT-4 model and multi-source data, this study explores China's WPBRenterprises' high-level GTI mechanism. The research concludes that (1) Compared to traditional expert knowledge, the Bayesian network model based on GPT-4 exhibits superior causal reasoning capability. (2) The current level of GTI in China's WPBR industry is relatively low, with the probability of high-level GTI being only 19%. (3) Key factors identified include incentives like R&D investment, bottlenecks such as green finance policy tools, and hindrances like government procurement policy tools. (4) “Supporting Infrastructure Policy Tools - Recycling Outlets Number - Market Potential -Green Technology Innovation” and “Green Finance Policy Tools - R&D Investment - Green Technology Innovation” are two critical paths for enhancing the high-level development of GTI in WPBR enterprises. The study offers valuable insights for governmental, industrial, and corporate decision-making regarding GTI in battery recycling.}
}
@article{ZHAO2024102552,
title = {Multimodal Aspect-Based Sentiment Analysis: A survey of tasks, methods, challenges and future directions},
journal = {Information Fusion},
volume = {112},
pages = {102552},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102552},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524003300},
author = {Tianyu Zhao and Ling-ang Meng and Dawei Song},
keywords = {Multimodal sentiment analysis, Multimodal Named Entity Recognition, Multimodal Aspect Based Sentiment Analysis, Multimodal Category Based Sentiment Analysis},
abstract = {With the development of social media, users increasingly tend to express their sentiments (broadly including sentiment polarities, emotions and sarcasm, etc.) associated with fine-grained aspects (e.g., entities) in multimodal content (mostly encompassing images and texts). Consequently, automated recognition of sentiments within multimodal content over different aspects, namely Multimodal Aspect-Based Sentiment Analysis (MABSA), has recently become an emergent research area. This paper assesses the state-of-the-art methods in MABSA based on a systematic taxonomy over different subtasks of MABSA. It compiles advanced models for each task and offers a concise overview of popular datasets and evaluation standards. Finally, we discuss the limitations of current research and highlight promising future research directions.}
}
@article{YU2025126356,
title = {A non-autoregressive Chinese-Braille translation approach with CTC loss optimization},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126356},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126356},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424032238},
author = {Hailong Yu and Wei Su and Yi Yang and Lei liu and Yongna Yuan and Yingchun Xie and Tianyuan Huang},
keywords = {Chinese-Braille translation, Non-autoregressive translation, CTC loss},
abstract = {The rise of Neural Machine Translation (NMT) models opens doors for translating Chinese text into Braille, improving information access for visually impaired individuals. However, current NMT models, often based on encoder–decoder architectures, utilize sequential rather than parallel processing in the decoder. This autoregressive decoding hinders architectures like the Transformer from fully leveraging their training speed advantages during inference. While the Transformer excels in parallel training, its inference time complexity remains O(T2), where T represents sequence length. This bottleneck becomes particularly significant when translating Braille, known for its long character sequences. We propose a non-autoregressive Chinese-to-Braille translation model that solely employs the encoder architecture along with Connectionist Temporal Classification (CTC) loss to generate complete Braille sequences simultaneously. This approach significantly improves inference speed, achieving a substantial acceleration compared to autoregressive models during inference with a time complexity of O(1). Remarkably, alongside increased inference speed, translation accuracy also improves. By incorporating a pre-training technique, our method achieves a remarkable BLEU Score of 95.10% with a limited dataset of only 2k Chinese-Braille training pairs.}
}
@article{YU2024,
title = {Coastal Zone Information Model: A comprehensive architecture for coastal digital twin by integrating data, models, and knowledge},
journal = {Fundamental Research},
year = {2024},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2024.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S2667325824002619},
author = {Zhaoyuan Yu and Pei Du and Lin Yi and Wen Luo and Dongshuang Li and Binru Zhao and Longhui Li and Zhuo Zhang and Jun Zhang and Jiyi Zhang and Wenchao Ma and Changchun Huang and Shuo Li and Xiaolu Yan and Guonian Lv and Linwang Yuan},
keywords = {Coastal zone information model, Coastal digitization, Coastal knowledge cognition, Data and model integration, Coastal digital twin},
abstract = {The coastal zone represents a critical intersection of naturally ecological and socio-economic processes. The abundance of data, models, and knowledge derived from various sources in coastal zones facilitates us to integrate them to better understand the evolution of coastal environments. This paper proposes a comprehensive framework of Coastal Zone Information Model (CZIM) to integrate multi-domain coastal information. The core idea of CZIM is to integrate multi-discipline coastal data, models, and knowledge for standardized governance, so as to carry, express, and apply coastal information by the digital system approaching the coastal digital twin. The CZIM framework includes four aspects: coastal data governance, model integration, knowledge engineering, and system construction. We perform a detailed literature review to illustrate the demands and challenges related to those four. The components of each aspect and their interlinks are introduced subsequently, and the future challenges of constructing coastal digital twins relying on CZIM are discussed. CZIM aims to strengthen the ability to organize, manage and apply refined coastal information to support more efficient, scientific, and intelligent decision-making in response to gradually volatile forces from both human activities and natural events, now and in the future. This paper provides a valuable reference for the next generation of coastal digitization in the target of the coastal digital twin.}
}
@article{MARINO2024101072,
title = {RummaGEO: Automatic mining of human and mouse gene sets from GEO},
journal = {Patterns},
volume = {5},
number = {10},
pages = {101072},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101072},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002319},
author = {Giacomo B. Marino and Daniel J.B. Clarke and Alexander Lachmann and Eden Z. Deng and Avi Ma’ayan},
keywords = {transcriptomics, gene expression, gene set enrichment analysis, data mining, data integration, signature search, LINCS, CFDE, ARCHS4, RNA sequencing},
abstract = {Summary
The Gene Expression Omnibus (GEO) has millions of samples from thousands of studies. While users of GEO can search the metadata describing studies, there is a need for methods to search GEO at the data level. RummaGEO is a gene expression signature search engine for human and mouse RNA sequencing perturbation studies extracted from GEO. To develop RummaGEO, we automatically identified groups of samples and computed differential expressions to extract gene sets from each study. The contents of RummaGEO are served for gene set, PubMed, and metadata search. Next, we analyzed the contents of RummaGEO to identify patterns and perform global analyses. Overall, RummaGEO provides a resource that is enabling users to identify relevant GEO studies based on their own gene expression results. Users of RummaGEO can incorporate RummaGEO into their analysis workflows for integrative analyses and hypothesis generation.}
}
@article{FARAHANI2025126508,
title = {Chart question answering with multimodal graph representation learning and zero-shot classification},
journal = {Expert Systems with Applications},
volume = {270},
pages = {126508},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126508},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001307},
author = {Ali Mazraeh Farahani and Peyman Adibi and Mohammad Saeed Ehsani and Hans-Peter Hutter and Alireza Darvishy},
keywords = {VQA, Chart question answering, Graph neural network, Graph attention, Graph isomorphism, Zero-shot learning},
abstract = {Chart Question Answering (CQA) is a special case of Visual Question Answering (VQA) that aims to generate accurate answers for questions related to charts, graphs, and other data representations. When exploring charts, one often asks complex reasoning questions involving logical and arithmetic operations, as well as referring to visual features. Previous approaches to CQA have applied general VQA methods, but their performance have been poor. In contrast, recent large models have focused on using image-based attention mechanisms for question answering while neglecting the inherent structure of charts. We propose a Graph Neural Network (GNN)-based model for CQA that leverages chart structure and relationships between chart components and question words to improve performance. By incorporating positional and structural encodings, our approach captures spatial relationships and chart topology, enhancing the model’s understanding of visual data. Using Graph Attention Network (GAT) and Graph Isomorphism Network (GIN), we effectively model relationships in graph-based data, offering a resource-efficient solution to CQA given hardware limitations. To address the Out of Vocabulary (OOV) problem, we integrate zero-shot learning (ZSL) for better generalization to unseen words and concepts. Additionally, Hard Negative Mining (HNM) enhances robustness by training the model to distinguish closely related answers, improving performance on complex and underrepresented question types. We have conducted intensive experiments on three publicly available challenging datasets (FigureQA, DVQA, and PlotQA), and analyzed different aspects of the proposed model. We demonstrate that our proposed approach surpasses competitive methods, particularly excelling in addressing structural questions, as expected from a graph-based topology capturing approach. Furthermore, we have conducted a series of examinations on various aspects of our proposed method by comprehensively analyzing the components within the answering pipeline. Additionally, we critically evaluated methods that utilize a classification approach for answer generation and demonstrate why their reported high accuracies are not necessarily reasonable. The resulting performance of the proposed method on the benchmarks are quite promising.}
}
@article{YANG2025107117,
title = {A discrete convolutional network for entity relation extraction},
journal = {Neural Networks},
volume = {184},
pages = {107117},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107117},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024010463},
author = {Weizhe Yang and Yongbin Qin and Kai Wang and Ying Hu and Ruizhang Huang and Yanping Chen},
keywords = {Relation extraction, Discrete convolution, Semantic structure, Deep learning, Natural language processing},
abstract = {Relation extraction independently verifies all entity pairs in a sentence to identify predefined relationships between named entities. Because these entity pairs share the same contextual features of a sentence, they lead to a complicated semantic structure. To distinguish semantic expressions between relation instances, manually designed rules or elaborate deep architectures are usually applied to learn task-relevant representations. In this paper, a discrete convolutional network is proposed to incorporate discrete linguistic interactions and deep feature weighting. This network applies a discretization strategy to fix parameters of convolutional kernels into ternary values. Then, these discretized kernels are used to learn discrete semantic structures from vectorized token representations. Our approach leverages the ability of discrete CNNs to capture discrete linguistic patterns of a sentence, thereby maintaining model expressiveness and improving performance in the relation extraction task. Furthermore, our method has the advantages of reducing the overfitting problem caused by depending on prior knowledge and decreasing the computational complexity by reducing the number of trainable parameters. Our model is evaluated on five widely used benchmark datasets. It achieves state-of-the-art performance, outperforming all compared related works. Experimental results also demonstrate that, compared with traditional CNN networks, it achieves an average improvement of 14.66% in F1-score and accelerates training by an average of 17.46%, highlighting the efficiency and effectiveness of our model in the relation extraction task.}
}
@article{FORNARI2025101477,
title = {Digital Twins of Business Processes: A Research Manifesto},
journal = {Internet of Things},
volume = {30},
pages = {101477},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101477},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524004189},
author = {Fabrizio Fornari and Ivan Compagnucci and Massimo {Callisto De Donato} and Yannis Bertrand and Harry H. Beyel and Emilio Carrión and Marco Franceschetti and Wolfgang Groher and Joscha Grüger and Emre Kilic and Agnes Koschmider and Francesco Leotta and Chiao-Yun Li and Giovani Lugaresi and Lukas Malburg and Juergen Mangler and Massimo Mecella and Oscar Pastor and Uwe Riss and Ronny Seiger and Estefania Serral and Victoria Torres and Pedro Valderas},
keywords = {Digital Twin, Business process, Internet of Things},
abstract = {Modern organizations necessitate continuous business processes improvement to maintain efficiency, adaptability, and competitiveness. In the last few years, the Internet of Things, via the deployment of sensors and actuators, has heavily been adopted in organizational and industrial settings to monitor and automatize physical processes influencing and enhancing how people and organizations work. Such advancements are now pushed forward by the rise of the Digital Twin paradigm applied to organizational processes. Advanced ways of managing and maintaining business processes come within reach as there is a Digital Twin of a business process - a virtual replica with real-time capabilities of a real process occurring in an organization. Combining business process models with real-time data and simulation capabilities promises to provide a new way to guide day-to-day organization activities. However, integrating Digital Twins and business processes is a non-trivial task, presenting numerous challenges and ambiguities. This manifesto paper aims to contribute to the current state of the art by clarifying the relationship between business processes and Digital Twins, identifying ongoing research and open challenges, thereby shedding light on and driving future exploration of this innovative interplay.}
}
@article{ZHENG2025,
title = {Machine Memory Intelligence: Inspired by Human Memory Mechanisms},
journal = {Engineering},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S2095809925000293},
author = {Qinghua Zheng and Huan Liu and Xiaoqing Zhang and Caixia Yan and Xiangyong Cao and Tieliang Gong and Yong-Jin Liu and Bin Shi and Zhen Peng and Xiaocen Fan and Ying Cai and Jun Liu},
keywords = {Machine memory intelligence, Neural mechanism, Associative representation, Continual learning, Collaborative reasoning},
abstract = {Large models, exemplified by ChatGPT, have reached the pinnacle of contemporary artificial intelligence (AI). However, they are plagued by three inherent drawbacks: excessive training data and computing power consumption, susceptibility to catastrophic forgetting, and a deficiency in logical reasoning capabilities within black-box models. To address these challenges, we draw insights from human memory mechanisms to introduce “machine memory,” which we define as a storage structure formed by encoding external information into a machine-representable and computable format. Centered on machine memory, we propose the brand-new machine memory intelligence (M2I) framework, which encompasses representation, learning, and reasoning modules and loops. We explore the key issues and recent advances in the four core aspects of M2I, including neural mechanisms, associative representation, continual learning, and collaborative reasoning within machine memory. M2I aims to liberate machine intelligence from the confines of data-centric neural networks and fundamentally break through the limitations of existing large models, driving a qualitative leap from weak to strong AI.}
}
@article{LEEWIS2025107627,
title = {Improving operational decision-making through decision mining - utilizing method engineering for the creation of a decision mining method},
journal = {Information and Software Technology},
volume = {179},
pages = {107627},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107627},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002325},
author = {Sam Leewis and Koen Smit and Bas {van den Boom} and Johan Versendaal},
keywords = {Decision making, Decision mining, Method engineering, Decision mining method, Systematic literature review},
abstract = {Context
This study addresses the challenge of enhancing the efficiency and agility of decision support software supporting both operational decision-making and software production teams developing decision support software. It centers on creating a method that assists in mining decisions, checking decisions on conformance, and improving decisions, which supports software production teams in developing decision support software.
Objective
The primary objective is to develop an explicit, clear, and structured approach for discovering, checking, and improving decisions using decision support software. The study aims to create a blueprint for software production teams to develop Decision Mining (DM) software, in line with recent advancements in the field. Additionally, it seeks to provide a consolidated, methodical overview of activities and deliverables in the DM research field.
Method
The research employs method engineering principles to construct a method for DM that leverages the existing body of knowledge by utilizing a Systematic Literature Review (SLR). The study focuses on developing individual building blocks and method fragments incorporated into seven DM scenarios.
Results
The study led to the creation of a Decision Mining Method (DMM), which includes 138 method fragments grouped into eleven categories. These fragments were systematically merged to form a comprehensive DMM. The method encapsulates the complexity of DM and provides practical applicability in real-world scenarios, highlighted by the identification of seven distinct scenarios in DM phases. The study also conducted the first SLR in the DM field, providing a comprehensive overview of current practices and outcomes.
Conclusion
The study helps in advancing the DM field by creating a structured approach and a comprehensive method for DM, aligning with recent developments in the field. It successfully aggregated the fragmented DM domain into a cohesive methodological overview, crucial for future research. The study also lays out a detailed agenda for future research, focusing on expanding and validating the DMM, incorporating cross-disciplinary insights, and addressing the challenges in machine learning within DM. The future research directions aim to refine and broaden the applicability of the DMM, ensuring its effectiveness in diverse practical contexts and contributing to a more holistic and comprehensive approach to decision mining.}
}
@article{GAN2024110794,
title = {A survey of dialogic emotion analysis: Developments, approaches and perspectives},
journal = {Pattern Recognition},
volume = {156},
pages = {110794},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110794},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324005454},
author = {Chenquan Gan and Jiahao Zheng and Qingyi Zhu and Yang Cao and Ye Zhu},
keywords = {Dialogic emotion analysis, Natural language process, Dialogic artificial intelligence},
abstract = {Dialogic emotion analysis is an emerging and important research field in natural language processing. It aims to understand and process emotions in various forms of dialogue, such as human-human conversations, human–machine interactions, and chatbot responses. However, dialogic emotion analysis faces many challenges, such as the diversity of dialogue genres, the complexity of emotional expressions, and the difficulty of capturing the emotional needs of dialogue participants. Moreover, the current dialogue systems lack the ability to analyze emotions effectively and appropriately in different dialogue contexts. Therefore, a comprehensive review of the existing research on dialogic emotion analysis is needed. This survey aims to review dialogic emotion analysis methods based on natural language processing from 2017 to 2024. The review process follows the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). We summarize the research methods and emphasize their main research contributions. In addition, we also discuss current research trends and possible future research directions, as well as the impact of personal traits on emotions and potential ethical issues.}
}
@article{GHIO2024102687,
title = {Democratizing academic research with Artificial Intelligence: The misleading case of language},
journal = {Critical Perspectives on Accounting},
volume = {98},
pages = {102687},
year = {2024},
issn = {1045-2354},
doi = {https://doi.org/10.1016/j.cpa.2023.102687},
url = {https://www.sciencedirect.com/science/article/pii/S1045235423001430},
author = {Alessandro Ghio},
keywords = {Artificial Intelligence, ChatGPT, Communication model, Language, Posthuman, Technology, Translation},
abstract = {This essay questions the use of Artificial Intelligence (AI) models like ChatGPT to enable academics to work in multiple languages. ChatGPT has the potential to dismantle the dominance of English in research communication. Adapting Te Eni's model of communication complexity, I explore the implications of using ChatGPT for non-native English speakers in the development, inputs, process, and impact of research communication. I then relate these technological changes to broader reflections on the relationship between machines and humans and the implications for the future of academic research. I argue that far from democratizing research communication, the proliferation of AI models like ChatGPT is creating new power imbalances and hegemonic positions that raise important ethical concerns for the academic community.}
}
@article{CASHEEKAR2024100632,
title = {A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions},
journal = {Computer Science Review},
volume = {52},
pages = {100632},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100632},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000169},
author = {Avyay Casheekar and Archit Lahiri and Kanishk Rath and Kaushik Sanjay Prabhakar and Kathiravan Srinivasan},
keywords = {Computational intelligence, Artificial intelligence, Chatbots, Conversational agents, ChatGPT},
abstract = {This review paper offers an in-depth analysis of AI-powered virtual conversational agents, specifically focusing on OpenAI’s ChatGPT. The main contributions of this paper are threefold: (i) an exhaustive review of prior literature on chatbots, (ii) a background of chatbots including existing chatbots/conversational agents like ChatGPT, and (iii) a UI/UX design analysis of prominent chatbots. Another contribution of this review is the comprehensive exploration of ChatGPT’s applications across a multitude of sectors, including education, business, public health, and more. This review highlights the transformative potential of ChatGPT, despite the challenges it faces such as hallucination, biases in training data, jailbreaks, and anonymous data collection. The review paper then presents a comprehensive survey of prior literature reviews on chatbots, identifying gaps in the prior work and highlighting the need for further research in areas such as chatbot evaluation, user experience, and ethical considerations. The paper also provides a detailed analysis of the UI/UX design of prominent chatbots, including their conversational flow, visual design, and user engagement. The paper also identifies key future research directions, including mitigating language bias, enhancing ethical decision-making capabilities, improving user interaction and personalization, and developing robust governance frameworks. By solving these issues, we can ensure that AI chatbots like ChatGPT are used responsibly and effectively across a broad variety of applications. This review will be a valuable resource for researchers and practitioners in understanding the current state and future potential of AI chatbots like ChatGPT.}
}
@article{LI2025129232,
title = {Event-level supervised contrastive learning with back-translation augmentation for event causality identification},
journal = {Neurocomputing},
volume = {621},
pages = {129232},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129232},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224020034},
author = {Shunhang Li and Gang Zhou and Jing Chen and Yepeng Sun and Ningbo Huang and Sisi Peng},
keywords = {Event causality identification, Complex causality reasoning, Back-translation, Supervised contrastive learning, Sampling strategy},
abstract = {How to identify event causality from natural language texts is becoming a heated research topic due to the significant role played by causality in AI domain. However, the limited scale and complex causality expressions of existing labeled datasets still impede better event causality identification (ECI) performance, especially in sentence-level ECI. Here, this study proposes the event-level supervised contrastive learning with back-translation augmentation method (BT-ESupCL) to address the aforementioned two obstacles simultaneously. Specifically, an efficient compound back-translation augmentation method with three parallel strategies is designed to mitigate data scarcity accompanied by more expression diversity and lower computational cost. To validly model the intricate causality expressions (in terms of both event semantics and casual structures) and then train a robust model, BT-ESupCL develops novel sampling strategies to construct contrastive pairs with more causality interactions. And, a fine-tuned supervised contrastive loss is devised to cope with imbalanced class distributions and data noise. Comprehensive experiments and analyses are implemented to verify the efficacy of our method. The results demonstrate that our approach achieves more promising performance than prior methods with 1.3% and 2.2% F1-score improvements on EventStoryLine Corpus and CausalTimeBank respectively, and helps to gain more diverse samples, more robustness to noise.}
}
@article{SUN2025129035,
title = {Global Span Semantic Dependency Awareness and Filtering Network for nested named entity recognition},
journal = {Neurocomputing},
volume = {617},
pages = {129035},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129035},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401806X},
author = {Yunlei Sun and Xiaoyang Wang and Haosheng Wu and Miao Hu},
keywords = {Named entity recognition, Span-based methods, Semantic dependency},
abstract = {Span-based methods for nested named entity recognition (NER) are effective in handling the complexities of nested entities with hierarchical structures. However, these methods often overlook valid semantic dependencies among global spans, resulting in a partial loss of semantic information. To address this issue, we propose the Global Span Semantic Dependency Awareness and Filtering Network (GSSDAF). Our model begins with BERT for initial sentence encoding. Following this, a span semantic representation matrix is generated using a multi-head biaffine attention mechanism. We introduce the Global Span Dependency Awareness (GSDA) module to capture valid semantic dependencies among all spans, and the Local Span Dependency Enhancement (LSDE) module to selectively enhance key local dependencies. The enhanced span semantic representation matrix is then decoded to classify the spans. We evaluated our model on seven public datasets. Experimental results demonstrate that our model effectively handles nested NER, achieving higher F1 scores compared to baselines. Ablation experiments confirm the effectiveness of each module. Further analysis indicates that our model can learn valid semantic dependencies between global spans, significantly improving the accuracy of nested entity recognition. Our code is available at https://github.com/Shaun-Wong/GSSDAF.}
}
@article{ASIF2024483,
title = {Robotic disassembly for end-of-life products focusing on task and motion planning: A comprehensive survey},
journal = {Journal of Manufacturing Systems},
volume = {77},
pages = {483-524},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524002127},
author = {Mohammed Eesa Asif and Alireza Rastegarpanah and Rustam Stolkin},
keywords = {Electric vehicles, Lithium-ion batteries, Robotic disassembly, Recycling, Circular Economy, Task and motion planning},
abstract = {The rise of mass production and the resulting accumulation of end-of-life (EoL) products present a growing challenge in waste management and highlight the need for efficient resource recovery. In response to this challenge, robotic disassembly has emerged as a vital tool for the circular economy. Combining accuracy, adaptability, and the potential for handling hazardous materials offers a sustainable solution for dismantling complex EoL objects. This comprehensive survey delves into the motivations for robotic disassembly and the pivotal role of task and motion planning (TAMP) in optimising disassembly processes. It analyses the evolution of disassembly strategies, from conventional methods to those driven by cutting-edge artificial intelligence (AI) techniques, for the future of waste management. Additionally, the survey explores several case study applications, focusing on the disassembly of EV lithium-ion batteries. It highlights how TAMP and AI integration can bolster adaptability, safety, and informed decision-making within real-world disassembly challenges. Finally, the review examines promising future research directions in robotics that hold the potential to advance further improvement in robotic disassembly to increase sustainability and the responsible management of EoL products.}
}
@article{WANG2024109588,
title = {Digital evolution and twin miracle of sugarcane breeding},
journal = {Field Crops Research},
volume = {318},
pages = {109588},
year = {2024},
issn = {0378-4290},
doi = {https://doi.org/10.1016/j.fcr.2024.109588},
url = {https://www.sciencedirect.com/science/article/pii/S0378429024003411},
author = {Xiaoding Wang and Qibin Wu and Haitao Zeng and Xu Yang and Xuechao Yang and Xun Yi and Ibrahim Khalil and Youxiong Que},
keywords = {Sugarcane breeding, Smart breeding, Artificial intelligence, Blockchain, Human-Cyber-Physical System, Digital twin},
abstract = {Context
Sugarcane, as an important economic crop, faces challenges such as long breeding cycles, low genetic improvement efficiency, and complex breeding operations.
Method
In order to address these challenges and improve the economic benefits of sugarcane breeding, this paper proposes an innovative smart sugarcane breeding system driven by artificial intelligence (AI), blockchain and digital twin technologies.
Results
The system integrates these technologies within a Human-Cyber-Physical System framework to offer a more efficient, secure, and smart strategy for sugarcane breeding. Firstly, AI processes extensive genetic and phenotypic data to enable precise prediction and optimization of sugarcane traits, resulting in shortened breeding cycles and enhanced efficiency and accuracy in selecting elite sugarcane varieties. Secondly, blockchain technology ensures the security and traceability of breeding data, enhancing the reliability and integrity of the breeding process. Thirdly, digital twin technology enables the real-time circulation of lifelike representations of real-world data among breeding-related workers. The system architecture consists of three layers: a physical layer for data collection, a cyber layer responsible for data analysis, storage and circulation managed by AI, blockchain and digital twin, and a human layer comprised of breeders and stakeholders. This multi-layered approach allows for sophisticated interaction and collaboration between the physical and digital realms, enhancing decision-making and breeding outcomes.
Conclusion
Taken together, the system utilizes AI, blockchain, and digital twin technologies to support sugarcane breeding, offering a promising solution to overcome the limitations of traditional methods and establish a more sustainable and profitable sugarcane breeding system.}
}
@article{BUDUR2024112243,
title = {Building efficient and effective OpenQA systems for low-resource languages},
journal = {Knowledge-Based Systems},
volume = {302},
pages = {112243},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112243},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124008773},
author = {Emrah Budur and Rıza Özçelik and Dilara Soylu and Omar Khattab and Tunga Güngör and Christopher Potts},
keywords = {Question answering, Open domain question answering, OpenQA, Low-resource languages, Machine translation},
abstract = {Question answering (QA) is the task of answering questions posed in natural language with free-form natural language answers extracted from a given passage. In the OpenQA variant, only a question text is given, and the system must retrieve relevant passages from an unstructured knowledge source and use them to provide answers, which is the case in the mainstream QA systems on the Web. QA systems currently are mostly limited to the English language due to the lack of large-scale labeled QA datasets in non-English languages. In this paper, we show that effective, low-cost OpenQA systems can be developed for low-resource contexts. The key ingredients are (1) weak supervision using machine-translated labeled datasets and (2) a relevant unstructured knowledge source in the target language context. Furthermore, we show that only a few hundred gold assessment examples are needed to reliably evaluate these systems. We apply our method to Turkish as a challenging case study, since English and Turkish are typologically very distinct and Turkish has limited resources for QA. We present SQuAD-TR, a machine translation of SQuAD2.0, and we build our OpenQA system by adapting ColBERT-QA and retraining it over Turkish resources and SQuAD-TR using two versions of Wikipedia dumps spanning two years. We obtain a performance improvement of 24–32% in the Exact Match (EM) score and 22–29% in the F1 score compared to the BM25-based and DPR-based baseline QA reader models. Our results show that SQuAD-TR makes OpenQA feasible for Turkish, which we hope encourages researchers to build OpenQA systems in other low-resource languages. We make all the code, models, and the dataset publicly available at https://github.com/boun-tabi/SQuAD-TR.}
}
@article{WANG2025103029,
title = {EDDINet: Enhancing drug–drug interaction prediction via information flow and consensus constrained multi-graph contrastive learning},
journal = {Artificial Intelligence in Medicine},
volume = {159},
pages = {103029},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.103029},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724002719},
author = {Hong Wang and Luhe Zhuang and Yijie Ding and Prayag Tiwari and Cheng Liang},
keywords = {DDI prediction, Information flow, Multi-graph, Consensus regularization, Contrastive learning},
abstract = {Predicting drug–drug interactions (DDIs) is crucial for understanding and preventing adverse drug reactions (ADRs). However, most existing methods inadequately explore the interactive information between drugs in a self-supervised manner, limiting our comprehension of drug–drug associations. This paper introduces EDDINet: Enhancing Drug-Drug Interaction Prediction via Information Flow and Consensus-Constrained Multi-Graph Contrastive Learning for precise DDI prediction. We first present a cross-modal information-flow mechanism to integrate diverse drug features, enriching the structural insights conveyed by the drug feature vector. Next, we employ contrastive learning to filter various biological networks, enhancing the model’s robustness. Additionally, we propose a consensus regularization framework that collaboratively trains multi-view models, producing high-quality drug representations. To unify drug representations derived from different biological information, we utilize an attention mechanism for DDI prediction. Extensive experiments demonstrate that EDDINet surpasses state-of-the-art unsupervised models and outperforms some supervised baseline models in DDI prediction tasks. Our approach shows significant advantages and holds promising potential for advancing DDI research and improving drug safety assessments. Our codes are available at: https://github.com/95LY/EDDINet_code.}
}
@incollection{ZUCCO2024,
title = {Deep Learning Methods in NLP},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00249-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002499},
author = {Chiara Zucco},
keywords = {Machine learning, Ensemble learning, Bagging, Random forest, Decision tree, Bias-variance decomposition, Resampling},
abstract = {Deep learning encompasses a series of algorithms that utilize multiple layers of nonlinear transformations to model complex tasks with increasing levels of abstraction. Over the past decades, deep learning has transformed traditional NLP, shifting from manual feature engineering to automated representation learning, allowing for more sophisticated and effective models. This chapter focuses on the impact of deep learning architectures on natural language processing (NLP) and text mining, particularly in the context of bioinformatics. We will explore the evolution of deep learning architectures, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and transformer-based models, and how these methods have been applied to solve specific challenges in text mining for bioinformatics, such as Named Entity Recognition (NER), Relation Extraction, and Topic Modeling. Ultimately, this chapter aims to provide a comprehensive overview of how deep learning has advanced NLP applications in bioinformatics.}
}
@article{BLAUDINDETHE2023103772,
title = {Transforming drug discovery with a high-throughput AI-powered platform: A 5-year experience with Patrimony},
journal = {Drug Discovery Today},
volume = {28},
number = {11},
pages = {103772},
year = {2023},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2023.103772},
url = {https://www.sciencedirect.com/science/article/pii/S135964462300288X},
author = {François-Xavier {Blaudin de Thé} and Claire Baudier and Renan {Andrade Pereira} and Céline Lefebvre and Philippe Moingeon},
keywords = {artificial intelligence, computational precision medicine, data integration, drug discovery, industrialization, target identification},
abstract = {High-throughput computational platforms are being established to accelerate drug discovery. Servier launched the Patrimony platform to harness computational sciences and artificial intelligence (AI) to integrate massive multimodal data from internal and external sources. Patrimony has enabled researchers to prioritize therapeutic targets based on a deep understanding of the pathophysiology of immuno-inflammatory diseases. Herein, we share our experience regarding main challenges and critical success factors faced when industrializing the platform and broadening its applications to neurological diseases. We emphasize the importance of integrating such platforms in an end-to-end drug discovery process and engaging human experts early on to ensure a transforming impact.}
}
@incollection{2024301,
title = {Index},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {301-306},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.20001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964200011}
}
@article{JIN2025112750,
title = {Multi-LoRA continual learning based instruction tuning framework for universal information extraction},
journal = {Knowledge-Based Systems},
volume = {308},
pages = {112750},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112750},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013844},
author = {Yu Jin and Jie Liu and Shaowei Chen},
keywords = {Multi-LoRA continual learning, Instruction tuning, Universal information extraction},
abstract = {Universal information extraction (Universal IE) aims to develop one model capable of solving multiple IE target tasks. Previous works have enhanced extraction performance of target tasks through auxiliary tasks. However, there are still limitations in terms of learning strategies. From one aspect, joint learning-based universal IE approaches, which simply mix auxiliary tasks with target tasks, fail to enable the model to master basic knowledge from auxiliary tasks before learning target tasks. From another aspect, continual learning-based universal IE approaches, which sequentially update all the model parameters on auxiliary tasks and target tasks, tend to cause catastrophic forgetting. In this study, we design a multi-LoRA continual learning-based instruction fine-tuning framework for universal IE. Specifically, we design unique LoRA modules for learning auxiliary tasks and target tasks. We first freeze pre-trained weights and update additional parameters on auxiliary tasks through one LoRA module. Subsequently, we keep the weights frozen and further adjust parameters through another LoRA module to adapt the model to the target tasks. Finally, we merge the frozen weights with learned weights, thereby enabling the model to better leverage the acquired abilities during the inference phase. Therefore, our model masters basic extraction abilities before learning target tasks and does not forget this basic knowledge during the target learning process. Moreover, we regard extraction, classification, and recognition as basic abilities and further design auxiliary tasks based on these basic abilities. Experimental results on 37 datasets across 3 tasks show that our approach reaches state-of-the-art performance.}
}
@article{OPDAHL2023102182,
title = {Trustworthy journalism through AI},
journal = {Data & Knowledge Engineering},
volume = {146},
pages = {102182},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102182},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X23000423},
author = {Andreas L Opdahl and Bjørnar Tessem and Duc-Tien Dang-Nguyen and Enrico Motta and Vinay Setty and Eivind Throndsen and Are Tverberg and Christoph Trattner},
keywords = {Artificial Intelligence, Journalism, News Production, Trustworthiness},
abstract = {Quality journalism has become more important than ever due to the need for quality and trustworthy media outlets that can provide accurate information to the public and help to address and counterbalance the wide and rapid spread of disinformation. At the same time, quality journalism is under pressure due to loss of revenue and competition from alternative information providers. This vision paper discusses how recent advances in Artificial Intelligence (AI), and in Machine Learning (ML) in particular, can be harnessed to support efficient production of high-quality journalism. From a news consumer perspective, the key parameter here concerns the degree of trust that is engendered by quality news production. For this reason, the paper will discuss how AI techniques can be applied to all aspects of news, at all stages of its production cycle, to increase trust.}
}
@article{LU2024,
title = {AI: Bridging Ancient Wisdom and Modern Innovation in Traditional Chinese Medicine},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/58491},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000747},
author = {Linken Lu and Tangsheng Lu and Chunyu Tian and Xiujun Zhang},
keywords = {traditional Chinese medicine, TCM, artificial intelligence, AI, diagnosis},
abstract = {The pursuit of groundbreaking health care innovations has led to the convergence of artificial intelligence (AI) and traditional Chinese medicine (TCM), thus marking a new frontier that demonstrates the promise of combining the advantages of ancient healing practices with cutting-edge advancements in modern technology. TCM, which is a holistic medical system with >2000 years of empirical support, uses unique diagnostic methods such as inspection, auscultation and olfaction, inquiry, and palpation. AI is the simulation of human intelligence processes by machines, especially via computer systems. TCM is experience oriented, holistic, and subjective, and its combination with AI has beneficial effects, which presumably arises from the perspectives of diagnostic accuracy, treatment efficacy, and prognostic veracity. The role of AI in TCM is highlighted by its use in diagnostics, with machine learning enhancing the precision of treatment through complex pattern recognition. This is exemplified by the greater accuracy of TCM syndrome differentiation via tongue images that are analyzed by AI. However, integrating AI into TCM also presents multifaceted challenges, such as data quality and ethical issues; thus, a unified strategy, such as the use of standardized data sets, is required to improve AI understanding and application of TCM principles. The evolution of TCM through the integration of AI is a key factor for elucidating new horizons in health care. As research continues to evolve, it is imperative that technologists and TCM practitioners collaborate to drive innovative solutions that push the boundaries of medical science and honor the profound legacy of TCM. We can chart a future course wherein AI-augmented TCM practices contribute to more systematic, effective, and accessible health care systems for all individuals.}
}
@article{LEE2025115039,
title = {Metadata schema for virtual building models in digital twins: VB schema implemented in GPT-based applications},
journal = {Energy and Buildings},
volume = {327},
pages = {115039},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115039},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011551},
author = {Jeyoon Lee and Sungmin Yoon},
keywords = {GPT, Virtual models, Virtual buildings, Ontology, HVAC, Digital twins, Model metadata, Operation and maintenance (O&M), Built environments},
abstract = {A virtual building model (VBM) is a virtual entity that represents the physical behavior of a target building mathematically within a digital twin environment. The creation and synchronization of a VBM are achieved by utilizing various interrelated virtual sub-models, including behavior, correction, and distance models. To achieve continuous digital twinning, it is essential to manage the VBM with virtual sub-models. However, existing metadata schemas have limitations in describing VBMs representing operational building behaviors within the concept of building digital twins (DTs). Therefore, this study proposes a novel metadata schema, termed the virtual building model metadata schema (VB schema), to represent and manage VBMs in DT-built environments. The VB schema is established according to the mathematical and semantic ontology of the in-situ modeling and calibration approach for constructing and correcting virtual models during building operations, and it is linked to physical entities, data, and applications within DTs. Specifically, it involves: (1) determining classes for operational data and virtual models; (2) establishing relationships for interactions between model and data entities, between model classes, between model and physical entities, and between model and applications; (3) defining properties for each class of models; and (4) extending into the exiting metadata schema of Brick. To demonstrate the proposed VB schema, a virtual model describing supply pressure behaviors in a central heating system was developed and represented using the VB schema for DT-enabled building operations. Additionally, the VB schema was utilized for implementing generative pre-trained transformer (GPT)-based DT applications, which highlights its benefits in enhancing ontology comprehension of DTs in the context of VBMs, improving autonomous problem-solving capabilities in real building systems, and providing better interpretation of application results compared to cases where only the Brick schema was used. The VB schema is expected to enable continuous and autonomous in-situ management of VBMs for intelligent building services within the DT.}
}
@article{HU2025332,
title = {An overview of fake news detection: From a new perspective},
journal = {Fundamental Research},
volume = {5},
number = {1},
pages = {332-346},
year = {2025},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2024.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S2667325824000414},
author = {Bo Hu and Zhendong Mao and Yongdong Zhang},
keywords = {Fake news detection, Social media, Intentional creation, Heteromorphic transmission, Controversial reception},
abstract = {With the rapid development and popularization of Internet technology, the propagation and diffusion of information become much easier and faster. While making life more convenient, the Internet also promotes the wide spread of fake news, which will have a great negative impact on countries, societies, and individuals. Therefore, a lot of research efforts have been made to combat fake news. Fake news detection is typically a classification problem aiming at verifying the veracity of news contents, which may include texts, images and videos. This article provides a comprehensive survey of fake news detection. We first summarize three intrinsic characteristics of fake news by analyzing its entire diffusion process, namely intentional creation, heteromorphic transmission, and controversial reception. The first refers to why users publish fake news, the second denotes how fake news propagates and distributes, and the last means what viewpoints different users may hold for fake news. We then discuss existing fake news detection approaches according to these characteristics. Thus, this review will enable readers to better understand this field from a new perspective. We finally discuss the trends of technological advances in this field and also outline some potential directions for future research.}
}
@article{PANESAR202320,
title = {Natural language processing-driven framework for the early detection of language and cognitive decline},
journal = {Language and Health},
volume = {1},
number = {2},
pages = {20-35},
year = {2023},
issn = {2949-9038},
doi = {https://doi.org/10.1016/j.laheal.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2949903823000337},
author = {Kulvinder Panesar and María Beatriz {Pérez Cabello de Alba}},
keywords = {Language production, Memory concerns, Pre-screening model, Role and reference grammar, Speech assessment, Natural language processing},
abstract = {Natural Language Processing (NLP) technology has the potential to provide a non-invasive, cost-effective method using a timely intervention for detecting early-stage language and cognitive decline in individuals concerned about their memory. The proposed pre-screening language and cognition assessment model (PST-LCAM) is based on the functional linguistic model Role and Reference Grammar (RRG) to analyse and represent the structure and meaning of utterances, via a set of language production and cognition parameters. The model is trained on a DementiaBank dataset with markers of cognitive decline aligned to the global deterioration scale (GDS). A hybrid approach of qualitative linguistic analysis and assessment is applied, which includes the mapping of participants´ tasks of speech utterances and words to RRG phenomena. It uses a metric-based scoring with resulting quantitative scores and qualitative indicators as pre-screening results. This model is to be deployed in a user-centred conversational assessment platform.}
}
@article{PENG2025128724,
title = {Easy and effective! Data augmentation for knowledge-aware dialogue generation via multi-perspective sentences interaction},
journal = {Neurocomputing},
volume = {614},
pages = {128724},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128724},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014954},
author = {Sisi Peng and Dan Qu and Wenlin Zhang and Hao Zhang and Shunhang Li and Minchen Xu},
keywords = {Data augmentation, Knowledge-aware, Dialogue generation, Sentence interaction},
abstract = {In recent years, knowledge-based dialogue generation has garnered significant attention due to its capacity to produce informative and coherent responses through the integration of external knowledge into models. However, obtaining high-quality knowledge that aligns with the dialogue content poses a considerable challenge, necessitating substantial time and resources. To tackle the issue of limited dialogue data, a majority of research endeavors concentrate on data augmentation to augment the volume of training data. Regrettably, these methods overlook knowledge augmentation, leading to a restricted diversity in input data and yielding enhancements solely in specific metrics. Real-world conversations exhibit a spectrum of characteristics, including repetitions, reversals, and interruptions, demanding a heightened level of data diversity. In this study, we introduce a straightforward yet effective data augmentation technique known as Multi-perspective Sentence Interaction to bolster the connections among sentences from varied viewpoints. Through an examination of target responses from multiple dialogue perspectives, we enhance our comprehension of the relationships between dialogue sentences, thereby facilitating the expansion of knowledge-based dialogue data. Through experiments conducted on various knowledge-based dialogue datasets and utilizing different models, our findings illustrate a notable enhancement in the quality of model generation facilitated by our method. Specifically, we observed a 3.5% enhancement in reply accuracy and a 0.1506 increase in diversity (DIST-2). Moreover, there was a substantial improvement in knowledge selection accuracy by 19.04% and a reduction in model perplexity by 31.48%.}
}
@incollection{2025201,
title = {Index},
editor = {Faadiel Essop},
booktitle = {Truth Unveiled},
publisher = {Academic Press},
pages = {201-205},
year = {2025},
series = {Fundamentals of Physiology},
isbn = {978-0-443-23655-6},
doi = {https://doi.org/10.1016/B978-0-443-23655-6.09993-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443236556099937}
}
@article{ZHUANG2025113029,
title = {DyCR-Net: A dynamic context-aware routing network for multi-modal sarcasm detection in conversation},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {113029},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113029},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000772},
author = {Xingjie Zhuang and Zhixin Li and Fengling Zhou and Jingliang Gu and Canlong Zhang and Huifang Ma},
keywords = {Multimodal emotion recognition, Sarcasm detection, Multimedia content understanding, Multi-party conversations},
abstract = {Sarcasm is frequently used as a rhetorical device in daily life, where speakers express criticism, mockery, or irony by saying the opposite of what they mean or making statements that contrast with reality. In everyday conversations, humans typically rely on context to detect sarcastic intent, interpreting the background of the dialogue, perceiving emotions conveyed through tone of voice, and decoding non-verbal cues from body language and facial expressions. Most existing studies focus on constructing multimodal representations and identifying inconsistencies between modalities as indicators of sarcasm. However, the dynamic shifts in modality focus are critical for understanding complex real-world sarcastic scenarios. To address this, we propose a Dynamic Context-Aware Routing Network (DyCR-Net), which leverages multi-granularity cues from various modalities and constructs cross-modal routing networks (Text–Video, Text–Audio, and Video–Audio) that prioritize different networks depending on the sarcastic scenario. The weights of the routing networks are dynamically adjusted to better capture the required sarcastic information. Notably, our framework outperforms current state-of-the-art methods across multiple benchmark datasets.}
}
@article{YANG2025110149,
title = {Integrating prompt techniques and multi-similarity matching for named entity recognition in low-resource settings},
journal = {Engineering Applications of Artificial Intelligence},
volume = {144},
pages = {110149},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110149},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625001496},
author = {Jun Yang and Liguo Yao and Taihua Zhang and Chieh-Yuan Tsai and Yao Lu and Mingming Shen},
keywords = {Few-shot learning, Named entity recognition, Metric learning},
abstract = {Few-shot Named Entity Recognition (few-shot NER) is a technique that effectively trains models with limited annotated data, aiming to address the issue of low accuracy in traditional named entity recognition tasks due to sparse data. Existing methods often pay little attention to specific entity information when constructing prompts, which can challenge the model's understanding of semantic relationships between labels and entities. Metric learning methods rely on a single similarity metric, potentially limiting their ability to capture complex relationships between support and query sets. To address these issues, this paper proposes a novel method named Integrating Prompt Techniques and Multi-Similarity Matching for Named Entity Recognition in Low-Resource Settings (ProMSM). This method combines entity-guided masked prompt techniques with a multi-similarity fusion mechanism. The entity-guided masked prompt aims to fully utilize entity information from the support set or source domain training set. It integrates labels and their associated entity instances into input sequences, providing rich contextual information to enhance the model's accurate understanding of semantic relationships between labels and entities. The multi-similarity fusion technique integrates traditional dot product similarity with context-based custom attention scores, enhancing the precision of relationship measurement between tokens in the support and query sets. Finally, ProMSM was evaluated on five widely used public few-shot datasets under cross-label space and domain transfer settings. Extensive experimental results demonstrate that ProMSM outperforms previous few-shot NER methods.}
}
@article{ANDRIES2023100176,
title = {Alexa doesn't have that many feelings: Children's understanding of AI through interactions with smart speakers in their homes},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100176},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100176},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000553},
author = {Valentina Andries and Judy Robertson},
keywords = {And phrases: education, Conversational assistants, Smart speakers, AI education, Child-computer interaction, Trust, Anthropomorphism, LLM},
abstract = {As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google Home, have become commonly embedded in households, many children now routinely interact with Artificial Intelligence (AI) systems. It is important to research children's experiences with consumer devices which use AI techniques because these shape their understanding of AI and its capabilities. We conducted a mixed-methods study (questionnaires and interviews) with primary-school children aged 6–11 in Scotland to establish children's understanding of how voice-based CAs work, how they perceive their cognitive abilities, agency and other human-like qualities, their awareness and trust of privacy aspects when using CAs and what they perceive as appropriate verbal interactions with CAs. Most children overestimated the CAs' intelligence and were uncertain about the systems' feelings or agency. They also lacked accurate understanding of data privacy and security aspects, and believed it was wrong to be rude to conversational assistants. Exploring children's current understanding of AI-supported technology has educational implications; such findings will enable educators to develop appropriate materials to address the pressing need for AI literacy.}
}
@article{PHANUSUPAWIMOL2025101099,
title = {Innovation through intelligent computer-aided formulation design},
journal = {Current Opinion in Chemical Engineering},
volume = {47},
pages = {101099},
year = {2025},
issn = {2211-3398},
doi = {https://doi.org/10.1016/j.coche.2025.101099},
url = {https://www.sciencedirect.com/science/article/pii/S2211339825000103},
author = {Thunyaras Phanusupawimol and Kris Prasopsanti and Naz P Taskiran and Venkat Venkatasubramanian and Rafiqul Gani},
abstract = {This perspective paper presents a focused review of a selected topic of chemical-based products, namely, formulations. As formulations cover a wide range of chemical-based products, we highlight opportunities for innovation in three types of formulations — liquid blends, which are mixtures of chemicals that are in the liquid state at standard conditions; liquid formulations, which are mixtures of chemicals that may exist in different states but the final product is a single-phase liquid; and emulsions, which are also mixtures of chemicals that may exist in different states, but the final product is in the form of an emulsion. In each case, we discuss aspects of design, analysis, and innovation together with issues and challenges that could be tackled to find better and more sustainable products. In particular, the potential of hybrid artificial intelligence augmented computer-aided techniques that can aid in the design, analysis, and innovation of formulations is highlighted.}
}
@article{LIU2024103979,
title = {Multi-level bioinformatics resources support drug target discovery of protein–protein interactions},
journal = {Drug Discovery Today},
volume = {29},
number = {5},
pages = {103979},
year = {2024},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.103979},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624001041},
author = {Jia-Xin Liu and Xiao Zhang and Yuan-Qin Huang and Ge-Fei Hao and Guang-Fu Yang},
keywords = {, protein–protein interactions, drug targets, network, binding site},
abstract = {Drug discovery often begins with a new target. Protein–protein interactions (PPIs) are crucial to multitudinous cellular processes and offer a promising avenue for drug-target discovery. PPIs are characterized by multi-level complexity: at the protein level, interaction networks can be used to identify potential targets, whereas at the residue level, the details of the interactions of individual PPIs can be used to examine a target’s druggability. Much great progress has been made in target discovery through multi-level PPI-related computational approaches, but these resources have not been fully discussed. Here, we systematically survey bioinformatics tools for identifying and assessing potential drug targets, examining their characteristics, limitations and applications. This work will aid the integration of the broader protein-to-network context with the analysis of detailed binding mechanisms to support the discovery of drug targets.}
}
@article{NEUMANN2024101493,
title = {Cross-domain dynamic vocabulary in metrological use cases: Linkage, automatization and implementation},
journal = {Measurement: Sensors},
pages = {101493},
year = {2024},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101493},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424004690},
author = {Julia Neumann},
keywords = {Controlled vocabulary, Semantic representation form, Semantics, Thesaurus, Dynamic digital processes},
abstract = {This paper with the title “Cross-Domain Dynamic Vocabulary in Metrological Use Cases: Linkage, Automatization and Implementation” describes concepts of controlled vocabulary in digital metrological workflows. Controlled vocabulary usually either focuses on flexibility at the loss of control or it represents a strict structure at the loss of flexibility. This paper discusses an approach which is called dynamic vocabulary. This concept enables the metrological workflow to maintain a high level of flexibility and to have control over the contents. The five levels of digitalization are used as a reference source to explain the idea of dynamic controlled vocabulary. A concept for domain linkage is described afterwards. To showcase this concept, the representation form of a thesaurus is used. The final topics of the paper consider automatization processes for dynamic vocabulary implementations. It also provides an outlook of the importance for the interaction with artificial intelligence and/or machine learning processes.}
}
@article{MONTEJORAEZ2024100654,
title = {A survey on detecting mental disorders with natural language processing: Literature review, trends and challenges},
journal = {Computer Science Review},
volume = {53},
pages = {100654},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100654},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000388},
author = {Arturo Montejo-Ráez and M. Dolores Molina-González and Salud María Jiménez-Zafra and Miguel Ángel García-Cumbreras and Luis Joaquín García-López},
keywords = {Mental disorders detection, Natural language processing, Machine learning, Survey},
abstract = {For years, the scientific community has researched monitoring approaches for the detection of certain mental disorders and risky behaviors, like depression, eating disorders, gambling, and suicidal ideation among others, in order to activate prevention or mitigation strategies and, in severe cases, clinical treatment. Natural Language Processing is one of the most active disciplines dealing with the automatic detection of mental disorders. This paper offers a comprehensive and extensive review of research works on Natural Language Processing applied to the identification of some mental disorders. To this end, we have identified from a literature review, which are the main types of features used to represent the texts, the machine learning algorithms that are preferred or the most targeted social media platforms, among other aspects. Besides, the paper reports on scientific forums and projects focused on the automatic detection of these problems over the most popular social networks. Thus, this compilation provides a broad view of the matter, summarizing main strategies, and significant findings, but, also, recognizing some of the weaknesses in the research works published so far, serving as clues for future research.}
}
@article{MUYAMA2024104746,
title = {Machine learning approaches for the discovery of clinical pathways from patient data: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {160},
pages = {104746},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104746},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001643},
author = {Lillian Muyama and Antoine Neuraz and Adrien Coulet},
keywords = {Clinical pathway, Machine learning, Data-driven approach, Patient data},
abstract = {Background:
Clinical pathways are sequences of events followed during the clinical care of a group of patients who meet pre-defined criteria. They have many applications ranging from healthcare evaluation and optimization to clinical decision support. These pathways can be discovered from existing healthcare data, in particular with machine learning which is a family of methods used to learn patterns from data. This review provides a comprehensive overview of the literature concerning the use of machine learning methods for clinical pathway discovery from patient data.
Methods:
Guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method , we conducted a systematic review of the existing literature. We searched 6 databases, i.e., ACM Digital Library, ScienceDirect, Web of Science, PubMed, IEEE Xplore, and Scopus spanning from January 2004 to December 2023 using search terms pertinent to clinical pathways and their development. Subsequently, the retrieved papers were analyzed to assess their relevance to the scope of this study.
Results:
In total, 131 papers that met the specified inclusion criteria were identified. These papers expressed diverse motivations behind data-driven clinical pathway discovery ranging from knowledge discovery to conformance checking with established clinical guidelines (derived from existing literature and clinical experts). Notably, the predominant methods employed (67.2%, n=88) involved unsupervised machine learning techniques, such as clustering and process mining.
Conclusions:
Relevant clinical pathways can be discovered from patient data using machine learning methods, with the desirable potential to aid clinical decision-making in healthcare. However, to reach this objective, the methods used to discover pathways should be reproducible, and rigorous performance evaluation by clinical experts needs to be conducted for validation.}
}
@article{AZAD2024103000,
title = {Advances in medical image analysis with vision Transformers: A comprehensive review},
journal = {Medical Image Analysis},
volume = {91},
pages = {103000},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.103000},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002608},
author = {Reza Azad and Amirhossein Kazerouni and Moein Heidari and Ehsan Khodapanah Aghdam and Amirali Molaei and Yiwei Jia and Abin Jose and Rijo Roy and Dorit Merhof},
keywords = {Transformers, Medical image analysis, Vision transformers, Deep neural networks},
abstract = {The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.}
}
@article{ZHENG2025106824,
title = {Personalized multi-head self-attention network for news recommendation},
journal = {Neural Networks},
volume = {181},
pages = {106824},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106824},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024007482},
author = {Cong Zheng and Yixuan Song},
keywords = {News recommendation, Natural language processing, Multi-head self-attention, Neural networks, Embedding},
abstract = {With the rapid explosion of online news and user population, personalized news recommender systems have proved to be efficient ways of alleviating information overload problems by suggesting information which attracts users in line with their tastes. Exploring relationships among words and news is critical to structurally model users’ latent tastes including interested domains, while selecting informative words and news can directly reflect users’ interests. Most of the current studies do not provide an effective framework that combines distilling users’ interested latent spaces and explicit points systematically. Moreover, introducing more advanced techniques to merely chase accuracy has become a universal phenomenon. In this study, we design a Personalized Multi-Head Self-Attention Network (PMSN) for news recommendation, which combines multi-head self-attention network with personalized attention mechanism from both word and news levels. Multi-head self-attention mechanism is used to model interactions among words and news, exploring latent interests. Personalized attention mechanism is applied by embedding users’ IDs to highlight informative words and news, which can enhance the interpretability of personalization. Comprehensive experiments conducted using two real-world datasets demonstrate that PMSN efficiently outperforms state-of-the-art methods in terms of recommendation accuracy, without complicated structure design and exhausted even external resources consumption. Furthermore, visualized case study validates that attention mechanism indeed increases the interpretability.}
}
@article{RAHHAL2024124101,
title = {Data science for job market analysis: A survey on applications and techniques},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124101},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124101},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009679},
author = {Ibrahim Rahhal and Ismail Kassou and Mounir Ghogho},
keywords = {Labor market analytics, Job market needs, Data science, Job title classification, Skill identification, Natural language processing},
abstract = {The job market is evolving continuously due to changes in economic landscapes, technological improvements, and skill requirements. In the era of digitalization, a wealth of data is becoming available, opening up new opportunities for labor market analysis. Many stakeholders can make informed decisions if they benefit from accurate and timely insights about the job market. However, traditional data sources and methods used for labor market analysis often fall short of capturing the diversity and trends of the evolving job market. Recently, researchers started exploring various data sources by leveraging data science techniques, which makes information extraction achievable. This survey reviews recent research published between 2015 and 2022 on labor market analytics through data science techniques and discusses future research directions. 101 primary studies were classified and evaluated to identify the data sources utilized for job market analysis; the skill extraction methods and their type; the occupation and sector identification methods; and the application of the study conducted. Finally, we explore potential avenues for future research in this area.}
}
@article{YUAN2024102616,
title = {Toward dynamic rehabilitation management: A novel smart product-service system development approach based on fine-tuned large vision model and Fuzzy-Dematel},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102616},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102616},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002647},
author = {Wenyu Yuan and Hua Zhao and Xiongjie Yang and Ting Han and Danni Chang},
keywords = {Smart PSS, Fine-tuned large model, Personalized service, Fuzzy DEMATEL, Rehabilitation management},
abstract = {Nowadays, transformative technologies such as artificial intelligence, big data, and cloud computing are significantly influencing and reshaping the daily lives of individuals. Guided by the overarching concept of digital transformation, data-driven Smart Product-Service Systems (SPSS) have emerged, prompting scholars to investigate development approaches tailored to diverse data sources. However, the current approaches employed in the construction of SPSS exhibit limited capability in processing vast amounts of user-generated unstructured data. The relationship between big data intelligence and personalized services remains undisclosed. Moreover, the current focus of SPSS orientation predominantly addresses end consumers or manufacturers, with inadequate attention given to dynamic collaborative models that involve multiple stakeholders. These gaps are particularly conspicuous in complex industries such as rehabilitation management. To tackle these challenges, this study introduces a novel SPSS development approach that integrates a large vision model and the fuzzy-DEMATEL method. Specifically, a data-driven predictive assessment module was proposed, which constructs a medical image dataset and trains a rehabilitation predictive assessment model based on the transformer architecture. Secondly, personalized intervention services were generated, involving the representation of system elements, configuration, and optimization of service parameters. The fuzzy-DEMATEL method is mainly used for the initialization of service parameters. Then, interactive feedback is integrated into rehabilitation exercises for achieving continuous rehabilitation evaluation and service improvement. To validate the proposed approach, a FPRM-SPSS case was implemented, and it shows that the predictive assessment model achieved a high level of accuracy when applied to the clinical dataset constructed in this study, and the system was evaluated with high scores in user satisfaction.}
}
@article{SCHRIJVER2024200340,
title = {Automobile insurance fraud detection using data mining: A systematic literature review},
journal = {Intelligent Systems with Applications},
volume = {21},
pages = {200340},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200340},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000164},
author = {Gilian Schrijver and Dipti K. Sarmah and Mohammed El-hajj},
keywords = {Systematic literature review, Automobile insurance, Insurance fraud, Fraud detection, Data mining, Machine learning},
abstract = {Insurance is a pivotal element in modern society, but insurers face a persistent challenge from fraudulent behaviour performed by policyholders. This behaviour could be detrimental to both insurance companies and their honest customers, but the intricate nature of insurance fraud severely complicates its efficient, automated detection. This study surveys fifty recent publications on automobile insurance fraud detection, published between January 2019 and March 2023, and presents both the most commonly used data sets and methods for resampling and detection, as well as interesting, novel approaches. The study adopts the highly-cited Systematic Literature Review (SLR) methodology for software engineering research proposed by Kitchenham and Charters and collected studies from four online databases. The findings indicate limited public availability of automobile insurance fraud data sets. In terms of detection methods, the prevailing approach involves supervised machine learning methods that utilise structured, intrinsic features of claims or policies and that lack consideration of an example-dependent cost of misclassification. However, alternative techniques are also explored, including the use of graph-based methods, unstructured textual data, and cost-sensitive classifiers. The most common resampling approach was found to be oversampling. This SLR has identified commonly used methods in recent automobile insurance fraud detection research, and interesting directions for future research. It adds value over a related review by also including studies published from 2021 onward, and by detailing the used methodology. Limitations of this SLR include its restriction to a small number of considered publication years and limited validation of choices made during the process.}
}
@article{LI2025126077,
title = {An improved two-stage zero-shot relation triplet extraction model with hybrid cross-entropy loss and discriminative reranking},
journal = {Expert Systems with Applications},
volume = {265},
pages = {126077},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126077},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424029440},
author = {Diyou Li and Lijuan Zhang and Juncheng Zhou and Jie Huang and Neal Xiong and Lei Zhang and Jian Wan},
keywords = {Zero-shot relation triplet extraction, Hybrid cross-entropy loss function, Discriminative reranking training},
abstract = {Zero-shot relation triplet extraction (ZeroRTE) aims to extract relation triplets from unstructured text under zero-shot conditions, where the relation sets in the training and testing stages are disjoint. However, most of the existing ZeroRTE models do not fully utilize the generation ability of the model and lack of precise measurement for the sorting of triplets. To this end, we propose a novel ZeroRTE model based on two training stages in this paper, including a generative training stage and a discriminative training stage. In the generative training stage, our model designs the hybrid cross-entropy loss function, which combines the forward cross-entropy loss function with the reverse cross-entropy loss function to improve the generation quality of relation triplets. In the discriminative training stage, our model integrates a reranking task to enhance the sorting accuracy of our model for candidate triplets. We evaluate the proposed model on two ZeroRTE datasets (FewRel and Wiki-ZSL), and relevant experimental results fully demonstrate the effectiveness of our method.}
}
@article{WATKINS2025108638,
title = {Neuradicon: Operational representation learning of neuroimaging reports},
journal = {Computer Methods and Programs in Biomedicine},
pages = {108638},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108638},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725000550},
author = {Henry Watkins and Robert Gray and Adam Julius and Yee-Haur Mah and James Teo and Walter H.L. Pinaya and Paul Wright and Ashwani Jha and Holger Engleitner and Jorge Cardoso and Sebastien Ourselin and Geraint Rees and Rolf Jaeger and Parashkev Nachev},
keywords = {Natural language processing, Neurology, Neuroradiology, Artificial intelligence},
abstract = {Background and Objective:
Radiological reports typically summarize the content and interpretation of imaging studies in unstructured form that precludes quantitative analysis. This limits the monitoring of radiological services to throughput undifferentiated by content, impeding specific, targeted operational optimization. Here we present Neuradicon, a natural language processing (NLP) framework for quantitative analysis of neuroradiological reports.
Methods:
Our framework is a hybrid of rule-based and machine-learning models to represent neurological reports in succinct, quantitative form optimally suited to operational guidance. These include probabilistic models for text classification and tagging tasks, alongside auto-encoders for learning latent representations and statistical mapping of the latent space.
Results:
We demonstrate the application of Neuradicon to operational phenotyping of a corpus of 336,569 reports, and report excellent generalizability across time and two independent healthcare institutions. In particular, we report pathology classification metrics with f1-scores of 0.96 on prospective data, and semantic means of interrogating the phenotypes surfaced via latent space representations.
Conclusion:
Neuradicon allows the segmentation, analysis, classification, representation and interrogation of neuroradiological reports structure and content. It offers a blueprint for the extraction of rich, quantitative, actionable signals from unstructured text data in an operational context.}
}
@article{DAI2024104744,
title = {MultiADE: A Multi-domain benchmark for Adverse Drug Event extraction},
journal = {Journal of Biomedical Informatics},
volume = {160},
pages = {104744},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104744},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400162X},
author = {Xiang Dai and Sarvnaz Karimi and Abeed Sarker and Ben Hachey and Cecile Paris},
keywords = {Adverse drug event, Drug safety, Natural language processing, Information extraction, Named entity recognition},
abstract = {Objective:
Active adverse event surveillance monitors Adverse Drug Events (ADE) from different data sources, such as electronic health records, medical literature, social media and search engine logs. Over the years, many datasets have been created, and shared tasks have been organised to facilitate active adverse event surveillance. However, most – if not all – datasets or shared tasks focus on extracting ADEs from a particular type of text. Domain generalisation – the ability of a machine learning model to perform well on new, unseen domains (text types) – is under-explored. Given the rapid advancements in natural language processing, one unanswered question is how far we are from having a single ADE extraction model that is effective on various types of text, such as scientific literature and social media posts.
Methods:
We contribute to answering this question by building a multi-domain benchmark for adverse drug event extraction, which we named MultiADE. The new benchmark comprises several existing datasets sampled from different text types and our newly created dataset—CADECv2, which is an extension of CADEC (Karimi et al., 2015), covering online posts regarding more diverse drugs than CADEC. Our new dataset is carefully annotated by human annotators following detailed annotation guidelines.
Conclusion:
Our benchmark results show that the generalisation of the trained models is far from perfect, making it infeasible to be deployed to process different types of text. In addition, although intermediate transfer learning is a promising approach to utilising existing resources, further investigation is needed on methods of domain adaptation, particularly cost-effective methods to select useful training instances. The newly created CADECv2 and the scripts for building the benchmark are publicly available at CSIRO’s Data Portal (https://data.csiro.au/collection/csiro:62387). These resources enable the research community to further information extraction, leading to more effective active adverse drug event surveillance.}
}
@article{ZHANG2024104676,
title = {Location-enhanced syntactic knowledge for biomedical relation extraction},
journal = {Journal of Biomedical Informatics},
volume = {156},
pages = {104676},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104676},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000947},
author = {Yan Zhang and Zhihao Yang and Yumeng Yang and Hongfei Lin and Jian Wang},
keywords = {Biomedical relation extraction, Syntactic knowledge, Position information},
abstract = {Biomedical relation extraction has long been considered a challenging task due to the specialization and complexity of biomedical texts. Syntactic knowledge has been widely employed in existing research to enhance relation extraction, providing guidance for the semantic understanding and text representation of models. However, the utilization of syntactic knowledge in most studies is not exhaustive, and there is often a lack of fine-grained noise reduction, leading to confusion in relation classification. In this paper, we propose an attention generator that comprehensively considers both syntactic dependency type information and syntactic position information to distinguish the importance of different dependency connections. Additionally, we integrate positional information, dependency type information, and word representations together to introduce location-enhanced syntactic knowledge for guiding our biomedical relation extraction. Experimental results on three widely used English benchmark datasets in the biomedical domain consistently outperform a range of baseline models, demonstrating that our approach not only makes full use of syntactic knowledge but also effectively reduces the impact of noisy words.}
}
@article{WEN2024100906,
title = {LATTE: Label-efficient incident phenotyping from longitudinal electronic health records},
journal = {Patterns},
volume = {5},
number = {1},
pages = {100906},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100906},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923003136},
author = {Jun Wen and Jue Hou and Clara-Lea Bonzel and Yihan Zhao and Victor M. Castro and Vivian S. Gainer and Dana Weisenfeld and Tianrun Cai and Yuk-Lam Ho and Vidul A. Panickan and Lauren Costa and Chuan Hong and J. Michael Gaziano and Katherine P. Liao and Junwei Lu and Kelly Cho and Tianxi Cai},
abstract = {Summary
Electronic health record (EHR) data are increasingly used to support real-world evidence studies but are limited by the lack of precise timings of clinical events. Here, we propose a label-efficient incident phenotyping (LATTE) algorithm to accurately annotate the timing of clinical events from longitudinal EHR data. By leveraging the pre-trained semantic embeddings, LATTE selects predictive features and compresses their information into longitudinal visit embeddings through visit attention learning. LATTE models the sequential dependency between the target event and visit embeddings to derive the timings. To improve label efficiency, LATTE constructs longitudinal silver-standard labels from unlabeled patients to perform semi-supervised training. LATTE is evaluated on the onset of type 2 diabetes, heart failure, and relapses of multiple sclerosis. LATTE consistently achieves substantial improvements over benchmark methods while providing high prediction interpretability. The event timings are shown to help discover risk factors of heart failure among patients with rheumatoid arthritis.}
}
@article{CHEN2025102969,
title = {RK-VQA: Rational knowledge-aware fusion-in-decoder for knowledge-based visual question answering},
journal = {Information Fusion},
volume = {118},
pages = {102969},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102969},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000429},
author = {Weipeng Chen and Xu Huang and Zifeng Liu and Jin Liu and Lan Yo},
keywords = {Knowledge-based VQA, Knowledge retrieval, Fusion-in-decoder},
abstract = {Knowledge-based Visual Question Answering (KB-VQA) expands traditional VQA by utilizing world knowledge from external sources when the image alone is insufficient to infer a correct answer. Existing methods face challenges due to low recall rates, limiting the ability to gather essential information for accurate answers. While increasing the amount of retrieved knowledge entries can enhance recall, it often introduces irrelevant information, adversely impairing model performance. To overcome these challenges, we propose RK-VQA, which comprises two components: First, a zero-shot weighted hybrid knowledge retrieval method that integrates local and global visual features with textual features from image–question pairs, enhancing the quality of knowledge retrieval and improving recall rates. Second, a rational knowledge-aware Fusion-in-Decoder architecture enhances answer generation by focusing on rational knowledge and reducing the influence of irrelevant information. Specifically, we develop a rational module to extract rational features, subsequently utilized to prioritize pertinent information via a novel rational knowledge-aware attention mechanism. We evaluate our RK-VQA on the OK-VQA, which is the largest knowledge-based VQA dataset. The results demonstrate that RK-VQA achieves significant results, recording an accuracy of 64.11%, surpassing the previous best result by 2.03%.}
}
@article{QIU2024102776,
title = {Artificial intelligence for drug discovery and development in Alzheimer's disease},
journal = {Current Opinion in Structural Biology},
volume = {85},
pages = {102776},
year = {2024},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2024.102776},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X24000034},
author = {Yunguang Qiu and Feixiong Cheng},
abstract = {The complex molecular mechanism and pathophysiology of Alzheimer's disease (AD) limits the development of effective therapeutics or prevention strategies. Artificial Intelligence (AI)-guided drug discovery combined with genetics/multi-omics (genomics, epigenomics, transcriptomics, proteomics, and metabolomics) analysis contributes to the understanding of the pathophysiology and precision medicine of the disease, including AD and AD-related dementia. In this review, we summarize the AI-driven methodologies for AD-agnostic drug discovery and development, including de novo drug design, virtual screening, and prediction of drug-target interactions, all of which have shown potentials. In particular, AI-based drug repurposing emerges as a compelling strategy to identify new indications for existing drugs for AD. We provide several emerging AD targets from human genetics and multi-omics findings and highlight recent AI-based technologies and their applications in drug discovery using AD as a prototypical example. In closing, we discuss future challenges and directions in AI-based drug discovery for AD and other neurodegenerative diseases.}
}
@article{WANG2024101144,
title = {Elucidating the role of artificial intelligence in drug development from the perspective of drug-target interactions},
journal = {Journal of Pharmaceutical Analysis},
pages = {101144},
year = {2024},
issn = {2095-1779},
doi = {https://doi.org/10.1016/j.jpha.2024.101144},
url = {https://www.sciencedirect.com/science/article/pii/S2095177924002417},
author = {Boyang Wang and Tingyu Zhang and Qingyuan Liu and Chayanis Sutcharitchan and Ziyi Zhou and Dingfan Zhang and Shao Li},
keywords = {Artificial intelligence, Drug-target interactions, Deep learning, Machine learning, Drug combination, Network pharmacology},
abstract = {Drug development remains a critical issue in the field of biomedicine. With the rapid advancement of information technologies such as artificial intelligence (AI) and the advent of the big data era, AI-assisted drug development has become a new trend, particularly in predicting drug-target associations. To address the challenge of drug-target prediction, AI-driven models have emerged as powerful tools, offering innovative solutions by effectively extracting features from complex biological data; accurately modeling molecular interactions; and precisely predicting potential drug-target outcomes. Traditional machine learning, network-based, and advanced deep learning architectures such as convolutional neural networks (CNNs), graph convolutional networks (GCNs), and transformers each play a pivotal role. This review systematically compiles and evaluates AI algorithms for drug- and drug combination-target predictions, highlighting their theoretical frameworks, strengths, and limitations. CNNs effectively identify spatial patterns and molecular features critical for drug-target interactions. GCNs provide deep insights into molecular interactions via relational data, whereas transformers increase prediction accuracy by capturing complex dependencies within biological sequences. Network-based models offer a systematic perspective by integrating diverse data sources, and traditional machine learning efficiently handles large datasets to improve overall predictive accuracy. Collectively, these AI-driven methods are transforming drug-target predictions and advancing the development of personalized therapy. This review summarizes the application of AI in drug development, particularly in drug-target prediction, and offers recommendations on models and algorithms for researchers engaged in biomedical research. It also provides typical cases to better illustrate how AI can further accelerate development in the fields of biomedicine and drug discovery.}
}
@article{AGRAWAL2024122470,
title = {Revolutionizing subjective assessments: A three-pronged comprehensive approach with NLP and deep learning},
journal = {Expert Systems with Applications},
volume = {239},
pages = {122470},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122470},
url = {https://www.sciencedirect.com/science/article/pii/S095741742302972X},
author = {Raghav Agrawal and Harshit Mishra and Ilanthenral Kandasamy and Shrishail Ravi Terni and Vasantha W.B.},
keywords = {Deep Neural Networks (DNN), Natural Language Processing (NLP), Question answering, Yet Another Keyword Extractor (YAKE), KeyBERT, Simple Contrastive Sentence Embedding Framework (simCSE), Camembert, Sentence Bidirectional Encoder Representations from Transformers (SBERT)},
abstract = {The enhanced answer evaluation system is a cutting-edge automated tool that evaluates subjective answers in various contexts, such as educational assessments, surveys, and feedback forms. The proposed system leverages Natural Language Processing (NLP) and deep learning techniques to analyse subjective answers and provide evaluation scores with precision. Students’ answers are evaluated based on various criteria, such as keywords, context, relevance, coherence, and similarity. This paper introduces an architecture for a subjective answer evaluator using three main aspects: detection of keywords, similarity matrix, and presence of named entities. It combines the three aspects and provides a final score. It provides a standardized mechanism to score a given user answer compared to the particular model answer without human prejudice. This research aims to transcend traditional methodologies that predominantly utilize keyword or keyphrase scoring (text-based similarity) to determine the final score of an answer without delving into its technical intricacies. The semantic similarity (vector-based) employs vector data representations for score calculation. This approach necessitates partitioning data into multiple vectors for a comprehensive analysis. While text similarity is effective for short answers, its efficacy diminishes as the length of the answer increases. Therefore, this study emphasizes the critical role of similarity scoring and Named Entity Recognition (NER) scoring in evaluating more extended responses based on the stsb-en-main dataset (short answers) and a custom dataset with 190 records. This research reveals its remarkable performance, which excels through a dynamic three-pronged approach: keyword scoring, semantic similarity, and NER scoring with models like Yet Another Keyword Extractor (YAKE), SimCSE and Camembert. These three independent components synergize to produce unmatched results, establishing a new standard in the field. This enhancement led to Root Mean Square Error (RMSE) scores of 0.031 (optimized error rate) and an impressive 71%+ accuracy for our comprehensive system. This achievement surpasses existing works, which typically reached accuracies ranging between 40%–60% for long answers.}
}
@article{WALEK2025125816,
title = {A text-based recommender system for recommending relevant news articles},
journal = {Expert Systems with Applications},
volume = {266},
pages = {125816},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125816},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424026836},
author = {Bogdan Walek and Patrik Müller},
keywords = {Recommender system, Text-based recommender system, News recommender system, Word2Vec, Doc2Vec, TF-IDF},
abstract = {Despite recent advances in the related fields and the growing popularity of AI-based tools, small businesses, and public institutions still face challenges when implementing recommendation systems to increase profits and provide personalised services. This paper provides a comprehensive overview of state-of-the-art systems and a case study of a recommender system for a small, low-budget project, battling with several constraints: the use of uncommon natural language, the use of raw, unlabelled textual data, thus using mainly techniques coming from the field of data mining and text mining rather than relying on supervised learning methods. Another constraint was the reliance on the smaller dataset and the limited resources available for the development. This led us to explore a range of content-based methods, including the utilisation of the similarity measures applied to word embeddings derived from the Word2Vec and Doc2Vec shallow neural network models, as well as the TF-–IDF method. Additionally, a topic modelling approach utilising the Latent Dirichlet Allocation was used, as well as collaborative filtering methods, such as the algorithm using the Singular Value Decomposition method, and hybrid methods integrating the fuzzy inference and fuzzy expert systems. Some of the obstacles identified were subsequently demonstrated to be too challenging for the development of accurate recommendations. However, the item-to-item similarity solutions, which were primarily content-based, yielded satisfactory results when the threshold of 75% of the average precision of recommendations assessed by users was exceeded. One such solution was the one that used the Word2Vec-based model, which had been trained on the parameters obtained from the word similarities and analogies tests. Furthermore, an overview of alternative techniques and methodologies is provided.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{HOU2025128667,
title = {HiNER: Hierarchical feature fusion for Chinese named entity recognition},
journal = {Neurocomputing},
volume = {611},
pages = {128667},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128667},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014383},
author = {Shuxiang Hou and Yurong Qian and Jiaying Chen and Jigui Zhao and Huiyong Lv and Jiyuan Zhang and Hongyong Leng and Mengnan Ma},
keywords = {Chinese named entity recognition, Hierarchical feature fusion, Transformer, Semantic enhancement},
abstract = {Named Entity Recognition (NER) aims to extract structured entity information from unstructured textual data by identifying entity boundaries and categories. Chinese NER is more challenging than that of English due to the complex structure and ambiguous word boundaries, as well as nested and discontinuous occurrences of entities. Previous Chinese NER methods are limited by their character-based approach and dependence on external lexical information, which is often non-contextualized, leading to the introduction of noise and potentially compromising model performance. This paper proposes a novel Chinese NER model, HiNER, which leverages external semantic enhancement and hierarchical attention fusion. Specifically, we initially formulate the Chinese NER as a character–character relation classification task, thoroughly taking into account the cases of nested and discontinuous entities. Then, by incorporating syntactic information, we develop a Triformer module that is used to better integrate Chinese character, lexical, and syntactic embeddings, carefully considering the impact of external semantic enhancement on the original text embeddings and reducing extrinsic information interference to some extent. In addition, through the fusion of local and global attention mechanisms, the representation of character–character relationships is enhanced, allowing for the effective capture of semantic features at various hierarchical levels within the Chinese context. We conduct extensive experiments on seven Chinese NER datasets, and the results indicate that the HiNER model achieves state-of-the-art (SOTA) performance. The outcomes also confirm that external semantic enhancement and hierarchical attention fusion can provide better assistance in accomplishing the Chinese NER task.}
}
@article{LEE2024124786,
title = {ESC-ZSAR: Expanded Semantics from Categories with Cross-Attention for Zero-Shot Action Recognition},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124786},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124786},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424016531},
author = {Jeong-Cheol Lee and Dong-Gyu Lee},
keywords = {Zero-shot action recognition, Cross-attention, Semantics expansion},
abstract = {Zero-shot action recognition endeavors to identify novel action categories not encountered during training by aligning a joint semantic space. However, despite advancements, zero-shot action recognition still needs to grapple with the inadequate semantic representation of seen data, hindering the transfer of diverse action videos. This study introduces a novel framework combining video, optical flow, and expanded label description via a cross-attention mechanism. This integration facilitates the capture of low and high-level motion dynamics, effectively bridging the domain gap between the video and text modalities. The proposed approach of generating expanded label descriptions efficiently enhances semantic information, thus ameliorating zero-shot transferability and providing a comprehensive grasp of semantics and motion. The temporal shuffle and alignment module is designed to enhance the generalization ability of image sequences by capturing discriminative high-level motions through frame sorting. The efficacy of the proposed method is validated through extensive experiments on three benchmark datasets, namely Kinetic-600, UCF-101, and HMDB-51. Notably, our model achieves state-of-the-art results in the zero-shot action recognition task.}
}
@article{YU2024105901,
title = {Future considerations for the Human Affectome: Reply to commentaries},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {167},
pages = {105901},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105901},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424003701},
author = {Alessandra N.C. Yu and Leroy Lowe and Daniela Schiller}
}
@article{EID2024202,
title = {A-MASA: Arabic Multi-Domain Aspect-Based Sentiment Analysis Datasets},
journal = {Procedia Computer Science},
volume = {244},
pages = {202-211},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.193},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029946},
author = {Yomna Eid and Hala Zayed and Walaa Medhat},
keywords = {Aspects Extraction, Arabic Aspect-Based Sentiment Analysis, Natural language processing, Datasets},
abstract = {The rapid growth of Natural Language Processing (NLP) applications has gained interest in various languages, including Arabic. One critical NLP task is Aspect-Based Sentiment Analysis (ABSA). ABSA involves identifying the sentiment expressed toward specific aspects or attributes of entities mentioned in a piece of text, rather than determining the overall sentiment. Despite the increased use of Semitic languages like Arabic in NLP over the past decade, there remains a lack of resources and datasets in Arabic, particularly in dialectical Arabic. Additionally, existing ABSA datasets are often limited to a single domain, which does not support all types of ABSA, such as multi-domain ABSA, nor do they allow for testing a model's generalization capabilities. This study addresses these limitations by constructing multi-domain ABSA datasets in Arabic for the tasks of aspect extraction and aspect polarity detection. The datasets include both real and synthetic data, covering dialectical and modern standard Arabic, and comprising 6,500 records across five domains. These datasets can be utilized in single-domain, cross-domain, and multi-domain experiments for ABSA. Furthermore, we evaluated the constructed datasets using various transformer-based models and assessed the impact of generating synthetic data on the diversity and balance of the data, as well as on model performance. The results reported in this research demonstrate that the data is valid for use in ABSA tasks. Moreover, the models used for evaluation achieved competitive or superior results compared to existing models on various datasets. Additionally, the annotation guidelines we created are presented as recommended practices applicable to different tasks. All datasets used in this study are publicly available for research purposes.}
}
@article{BRAGAZZI2023,
title = {The Impact of Generative Conversational Artificial Intelligence on the Lesbian, Gay, Bisexual, Transgender, and Queer Community: Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/52091},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123009330},
author = {Nicola Luigi Bragazzi and Andrea Crapanzano and Manlio Converti and Riccardo Zerbetto and Rola Khamisy-Farah},
keywords = {generative conversational artificial intelligence, chatbot, lesbian, gay, bisexual, transgender, and queer community, LGBTQ, scoping review, mobile phone},
abstract = {Background
Despite recent significant strides toward acceptance, inclusion, and equality, members of the lesbian, gay, bisexual, transgender, and queer (LGBTQ) community still face alarming mental health disparities, being almost 3 times more likely to experience depression, anxiety, and suicidal thoughts than their heterosexual counterparts. These unique psychological challenges are due to discrimination, stigmatization, and identity-related struggles and can potentially benefit from generative conversational artificial intelligence (AI). As the latest advancement in AI, conversational agents and chatbots can imitate human conversation and support mental health, fostering diversity and inclusivity, combating stigma, and countering discrimination. In contrast, if not properly designed, they can perpetuate exclusion and inequities.
Objective
This study aims to examine the impact of generative conversational AI on the LGBTQ community.
Methods
This study was designed as a scoping review. Four electronic scholarly databases (Scopus, Embase, Web of Science, and MEDLINE via PubMed) and gray literature (Google Scholar) were consulted from inception without any language restrictions. Original studies focusing on the LGBTQ community or counselors working with this community exposed to chatbots and AI-enhanced internet-based platforms and exploring the feasibility, acceptance, or effectiveness of AI-enhanced tools were deemed eligible. The findings were reported in accordance with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews).
Results
Seven applications (HIVST-Chatbot, TelePrEP Navigator, Amanda Selfie, Crisis Contact Simulator, REALbot, Tough Talks, and Queer AI) were included and reviewed. The chatbots and internet-based assistants identified served various purposes: (1) to identify LGBTQ individuals at risk of suicide or contracting HIV or other sexually transmitted infections, (2) to provide resources to LGBTQ youth from underserved areas, (3) facilitate HIV status disclosure to sex partners, and (4) develop training role-play personas encompassing the diverse experiences and intersecting identities of LGBTQ youth to educate counselors. The use of generative conversational AI for the LGBTQ community is still in its early stages. Initial studies have found that deploying chatbots is feasible and well received, with high ratings for usability and user satisfaction. However, there is room for improvement in terms of the content provided and making conversations more engaging and interactive. Many of these studies used small sample sizes and short-term interventions measuring limited outcomes.
Conclusions
Generative conversational AI holds promise, but further development and formal evaluation are needed, including studies with larger samples, longer interventions, and randomized trials to compare different content, delivery methods, and dissemination platforms. In addition, a focus on engagement with behavioral objectives is essential to advance this field. The findings have broad practical implications, highlighting that AI’s impact spans various aspects of people’s lives. Assessing AI’s impact on diverse communities and adopting diversity-aware and intersectional approaches can help shape AI’s positive impact on society as a whole.}
}
@article{BILAL2025,
title = {NLP for Analyzing Electronic Health Records and Clinical Notes in Cancer Research: A Review},
journal = {Journal of Pain and Symptom Management},
year = {2025},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2025.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0885392425000375},
author = {Muhammad Bilal and Ameer Hamza and Nadia Malik},
keywords = {Natural language processing, Electronic health records, Clinical notes, Cancer, Information extraction, Text classification},
abstract = {This review examines the application of natural language processing (NLP) techniques in cancer research using electronic health records (EHRs) and clinical notes. It addresses gaps in existing literature by providing a broader perspective than previous studies focused on specific cancer types or applications. A comprehensive literature search in the Scopus database identified 94 relevant studies published between 2019 and 2024. The analysis revealed a growing trend in NLP applications for cancer research, with information extraction (47 studies) and text classification (40 studies) emerging as predominant NLP tasks, followed by named entity recognition (7 studies). Among cancer types, breast, lung, and colorectal cancers were found to be the most studied. A significant shift from rule-based and traditional machine learning approaches to advanced deep learning techniques and transformer-based models was observed. It was found that dataset sizes used in existing studies varied widely, ranging from small, manually annotated datasets to large-scale EHRs. The review highlighted key challenges, including the limited generalizability of proposed solutions and the need for improved integration into clinical workflows. While NLP techniques show significant potential in analyzing EHRs and clinical notes for cancer research, future work should focus on improving model generalizability, enhancing robustness in handling complex clinical language, and expanding applications to understudied cancer types. The integration of NLP tools into palliative medicine and addressing ethical considerations remain crucial for utilizing the full potential of NLP in enhancing cancer diagnosis, treatment, and patient outcomes. This review provides valuable insights into the current state and future directions of NLP applications in cancer research.}
}
@article{BOSL2025101480,
title = {Dynamical measures of developing neuroelectric fields in emerging consciousness},
journal = {Current Opinion in Behavioral Sciences},
volume = {61},
pages = {101480},
year = {2025},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101480},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624001311},
author = {William J Bosl and Jenny R {Capua Shenkar}},
abstract = {Human consciousness emerges over time. From the moment of conception, a process of neurodevelopment and complexification begins, generating and supporting a neuroelectric field that can be quantified by computational methods from dynamical systems theory. In the early embryo, genetically driven cellular processes are mediated by endogenous electromagnetic fields and intrinsic electrical fields produced by migrating neurons. In the ambient cellular environment, these interactions influence each other, impacting neural migration. The emergence of Theory of Mind, often considered a hallmark of conscious awareness, is accompanied by increasing neural connectivity, neuroelectric field complexity, and more integrated information processing. Neurodegeneration in old age and the often-associated decline in conscious awareness correlate closely with changes in the dynamical complexity of the neuroelectric field. Monitoring trajectories of the neuroelectric field and its complexity changes through the lifespan presents a developmental perspective and empirical correlation for studying the emergence and decline of human consciousness.}
}
@article{KAPUSTINA2024100072,
title = {User-friendly and industry-integrated AI for medicinal chemists and pharmaceuticals},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100072},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000307},
author = {Olga Kapustina and Polina Burmakina and Nina Gubina and Nikita Serov and Vladimir Vinogradov},
keywords = {Machine Learning, Medicinal Chemistry, Pharmaceutics, Data-Driven Drug Discovery},
abstract = {Artificial intelligence has brought crucial changes to the whole field of natural sciences. Myriads of machine learning algorithms have been developed to facilitate the work of experimental scientists. Molecular property prediction and drug synthesis planning become routine tasks. Moreover, inverse design of compounds with tunable properties as well as on-the-fly autonomous process optimization and chemical space exploration became possible in silico. Affordable robotic platforms exist able to perform thousands of experiments every day, analyzing the results and tuning the protocols. Despite this, most of these developments get trapped at the stage of code or overlooked, limiting their use by experimental scientists. Meanwhile, visibility and the number of user-friendly tools and technologies available to date is too low to compensate for this fact, rendering the development of novel therapeutic compounds inefficient. In this Review, we set the goal to bridge the gap between modern technologies and experimental scientists to improve drug development efficacy. Here we survey advanced and easy-to-use technologies able to help medical chemists at every stage of their research, including those integrated in technological processes during COVID-19 pandemic motivated by the need for fast yet precise solutions. Moreover, we review how these technologies are integrated by industry and clinics to streamline drug development and production. These technologies already transform the current paradigm of scientific thinking and revolutionize not only medicinal chemistry, but the whole field of natural sciences.}
}
@article{LIU2023104294,
title = {A survey of Semantic Reasoning frameworks for robotic systems},
journal = {Robotics and Autonomous Systems},
volume = {159},
pages = {104294},
year = {2023},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104294},
url = {https://www.sciencedirect.com/science/article/pii/S092188902200183X},
author = {Weiyu Liu and Angel Daruna and Maithili Patel and Kartik Ramachandruni and Sonia Chernova},
keywords = {Semantic reasoning, Robotics, Knowledge bases},
abstract = {Robots are increasingly transitioning from specialized, single-task machines to general-purpose systems that operate in diverse and dynamic environments. To address the challenges associated with operation in real-world domains, robots must effectively generalize knowledge, learn, and be transparent in their decision making. This survey examines Semantic Reasoning techniques for robotic systems, which enable robots to encode and use semantic knowledge, including concepts, facts, ideas, and beliefs about the world. Continually perceiving, understanding, and generalizing semantic knowledge allows a robot to identify the meaningful patterns shared between problems and environments, and therefore more effectively perform a wide range of real-world tasks. We identify the three common components that make up a computational Semantic Reasoning Framework: knowledge sources, computational frameworks, and world representations. We analyze the existing implementations and the key characteristics of these components, highlight the many interactions that occur between them, and examine their integration for solving robotic tasks related to five aspects of the world, including objects, spaces, agents, tasks, and actions. By analyzing the computational formulation and underlying mechanisms of existing methods, we provide a unified view of the wide range of semantic reasoning techniques and identify open areas for future research.}
}
@article{HU2025126318,
title = {A traditional Chinese medicine prescription recommendation model based on contrastive pre-training and hierarchical structure network},
journal = {Expert Systems with Applications},
volume = {268},
pages = {126318},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126318},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031853},
author = {Hailong Hu and Yaqian Li and Zeyu Zheng and Wenjun Hu and Riyang Lin and Yanlei Kang},
keywords = {TCM prescription recommendation, Contrastive pre-training, Hierarchical structure network, Network pharmacology},
abstract = {Traditional Chinese Medicine (TCM) prescriptions are personalized treatment plans crafted by Chinese practitioners based on TCM principles and clinical insights, tailored through the examination of patient symptoms, physical constitution, and other relevant data. However, the efficacy of existing TCM prescription recommendation models is often hampered by data scarcity, disparities in node popularity, and challenges in interpreting the recommended prescriptions, leading to outcomes that may need more convincing accuracy and interpretability. This paper introduces a TCM prescription recommendation model utilizing contrastive pre-training and a hierarchical structure network. By fitting node features via multi-view contrastive pre-training, this approach alleviates the issue of data sparsity. It further integrates linked features within homogeneous networks at a granular level. Moreover, a hierarchical structural network focuses on less popular nodes, enriching the representations of symptoms and herbal features. During the analysis of the results, an interpretability analysis of the recommended TCM prescriptions is performed using the network pharmacology method. The performance of our model surpasses the compared methods in the comparison. Compared to the best model, our model shows improvements on both datasets. In Dataset1, Precision@20, Recall@20, and F1-score@20 increase by 2.14%, 5.51%, and 3.07%, respectively. In Dataset2, Precision@20, Recall@20, and F1-score@20 rise by 1.50%, 1.64%, and 1.51%, respectively. The herbal prescription recommendation model in this study enhances the accuracy of herbal recommendations. It not only provides new insights for TCM clinical practice but also promotes the modernization and innovative development of TCM diagnosis and treatment.}
}
@article{PICCOLI2024101835,
title = {Digital transformation requires digital resource primacy: Clarification and future research directions},
journal = {The Journal of Strategic Information Systems},
volume = {33},
number = {2},
pages = {101835},
year = {2024},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2024.101835},
url = {https://www.sciencedirect.com/science/article/pii/S0963868724000179},
author = {Gabriele Piccoli and Varun Grover and Joaquin Rodriguez},
keywords = {Digital transformation, IT-enabled transformation, Digital ontology, Digital organization, Digital resources},
abstract = {Responding to recent calls, this essay offers a commentary on the framing and definition of organizational digital transformation. We focus on the unique ontology of digital transformation and delineate it from neighboring concepts.Our contention is that, despite its volume, current research remains unclear about how the digital transformation of organizations differs from their IT-enabled transformation. We advocate definitional precision to foster knowledge accumulation and to enable scholars to pursue important research questions that are unique to digital transformation. Our perspective, grounded in the notion of digital resources, defines digital transformation as the metamorphosis of an IT-enabled organization into a digital organization – one with a specific digital architecture and design principles.A key departure from previous conceptualization is that we characterize digital transformation as a change in digital technology architecture rather than a change from digital technology use. Our paper achieves the following: describes the constructs underpinning this formulation, digital resources and digital organization; justifies their use; and describes what research directions the new perspective promotes. With sound definitions of key constructs, Information Systems scholars have the unprecedented opportunity to lead the way in digital “x” research, making our discipline the reference point for the burgeoning “digital research” literature in related business fields.}
}
@article{GATTIGLIA2025225,
title = {Managing Artificial Intelligence in Archeology. An overview},
journal = {Journal of Cultural Heritage},
volume = {71},
pages = {225-233},
year = {2025},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2024.11.020},
url = {https://www.sciencedirect.com/science/article/pii/S1296207424002516},
author = {Gabriele Gattiglia},
keywords = {Archaeology, Artificial intelligence, Big Data, Theory, Ethics},
abstract = {The integration of AI in archaeology poses several risks due to the oversimplification of complex archaeological data for computational ease. This reductionist approach fosters a deterministic view, treating provisional classifications as definitive truths and influencing subsequent interpretations. The reliance on legacy data and Big Data for AI training risks perpetuating outdated ideas and frameworks. As AI expands from automating tasks to interpreting and creating reconstructions, archaeologists must adopt a critical approach to avoid biased and harmful outputs. The deterministic view of AI hinders informed debate. Archaeologists should engage in discussions that address the classificatory, and ethical aspects as well as the materiality of AI. The accumulation of data in AI mimics storytelling but lacks the interpretative depth needed to understand historical human perspectives. Developing theories and narrative practices is essential to making archaeological data meaningful. The shift from a representational to a co-creative view of data is necessary to understand its re-use and the power dynamics involved. Finally, to normalise AI in archaeology, a critical and sceptical approach is needed to integrate AI into the real world and understand its implications and ethical considerations.}
}
@article{LIN2023102611,
title = {Medical visual question answering: A survey},
journal = {Artificial Intelligence in Medicine},
volume = {143},
pages = {102611},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102611},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723001252},
author = {Zhihong Lin and Donghao Zhang and Qingyi Tao and Danli Shi and Gholamreza Haffari and Qi Wu and Mingguang He and Zongyuan Ge},
keywords = {Visual question answering, Medical image interpretation, Computer vision, Natural language processing},
abstract = {Medical Visual Question Answering (VQA) is a combination of medical artificial intelligence and popular VQA challenges. Given a medical image and a clinically relevant question in natural language, the medical VQA system is expected to predict a plausible and convincing answer. Although the general-domain VQA has been extensively studied, the medical VQA still needs specific investigation and exploration due to its task features. In the first part of this survey, we collect and discuss the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature. In the second part, we review the approaches used in medical VQA tasks. We summarize and discuss their techniques, innovations, and potential improvements. In the last part, we analyze some medical-specific challenges for the field and discuss future research directions. Our goal is to provide comprehensive and helpful information for researchers interested in the medical visual question answering field and encourage them to conduct further research in this field.}
}
@article{ZHANG2024123938,
title = {Medical chief complaint classification with hierarchical structure of label descriptions},
journal = {Expert Systems with Applications},
volume = {252},
pages = {123938},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123938},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424008042},
author = {Zibo Zhang and Zheng Lu and Jiandong Liu and Ruibin Bai},
keywords = {Deep learning, Text classification, Chief complaint, Hierarchical label},
abstract = {With rapid growth of online healthcare systems, chief complaint classification plays an important role in areas such as triage or doctor recommendation. Existing medical text classification techniques such as rule-based or learning-based methods fail to effectively utilize the inherent hierarchical structure of label descriptions that contain strong domain knowledge. In this paper, we propose a novel text classification framework for chief complaint by embedding both input text and hierarchical structure of label descriptions based on deep neural networks. The proposed framework makes use of not only three branches (i.e. chief complaint branch, main-category branch, and sub-category branch) with a Sequence Information Encoder to encode semantics from chief complaint and hierarchical structure of label descriptions but also a Hierarchical Relational Network with Attention module to capture complex relationships among them focusing on informative words with attentional scores. We evaluate our framework on two public medical datasets with label descriptions extracted from medical books and websites. Experimental results show that the proposed method outperforms other baseline techniques by a significant margin. The source code of our framework is available at ANONYMISED.}
}
@article{PENG202412,
title = {The Tong Test: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social Interactions},
journal = {Engineering},
volume = {34},
pages = {12-22},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2023.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S209580992300293X},
author = {Yujia Peng and Jiaheng Han and Zhenliang Zhang and Lifeng Fan and Tengyu Liu and Siyuan Qi and Xue Feng and Yuxi Ma and Yizhou Wang and Song-Chun Zhu},
keywords = {Artificial general intelligence, Artificial intelligence benchmark, Artificial intelligence evaluation, Embodied artificial intelligence, Value alignment, Turing test, Causality},
abstract = {The release of the generative pre-trained transformer (GPT) series has brought artificial general intelligence (AGI) to the forefront of the artificial intelligence (AI) field once again. However, the questions of how to define and evaluate AGI remain unclear. This perspective article proposes that the evaluation of AGI should be rooted in dynamic embodied physical and social interactions (DEPSI). More specifically, we propose five critical characteristics to be considered as AGI benchmarks and suggest the Tong test as an AGI evaluation system. The Tong test describes a value- and ability-oriented testing system that delineates five levels of AGI milestones through a virtual environment with DEPSI, allowing for infinite task generation. We contrast the Tong test with classical AI testing systems in terms of various aspects and propose a systematic evaluation system to promote standardized, quantitative, and objective benchmarks and evaluation of AGI.}
}
@article{WANG2024105442,
title = {Transformer-based deep learning model for the diagnosis of suspected lung cancer in primary care based on electronic health record data},
journal = {eBioMedicine},
volume = {110},
pages = {105442},
year = {2024},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2024.105442},
url = {https://www.sciencedirect.com/science/article/pii/S235239642400478X},
author = {Lan Wang and Yonghua Yin and Ben Glampson and Robert Peach and Mauricio Barahona and Brendan C. Delaney and Erik K. Mayer},
keywords = {Deep learning, Transformers, Machine learning, Cancer prediction, Primary care, Artificial intelligence},
abstract = {Summary
Background
Due to its late stage of diagnosis lung cancer is the commonest cause of death from cancer in the UK. Existing epidemiological risk models in clinical usage, which have Positive Predictive Values (PPV) of less than 10%, do not consider the temporal relations expressed in sequential electronic health record (EHR) data. We aimed to build a model for lung cancer early detection in primary care using machine learning with deep ‘transformer’ models on EHR data to learn from these complex sequential ‘care pathways’.
Methods
We split the Whole Systems Integrated Care (WSIC) dataset into 70% training and 30% validation. Within the training set we created a case–control study with lung cancer cases and control cases of ‘other’ cancers or respiratory conditions or ‘other’ non cancer conditions. Based on 3,303,992 patients from January 1981 to December 2020 there were 11,847 lung cancer cases. 5789 cases and 7240 controls were used for training and 50,000 randomly selected patients out of the whole validation population of 368,906 for validation. GP EHR data going back three years from the date of diagnosis less the most recent one months were semantically pre-processed by mapping from more than 30,000 terms to 450. Model building was performed using ALBERT with a Logistic Regression Classifier (LRC) head. Clustering was explored using k-means. An additional regression model alone was built on the pre-processed data as a comparator.
Findings
Our model achieved an AUROC of 0.924 (95% CI 0.921–0.927) with a PPV of 3.6% (95% CI 3.5–3.7) and Sensitivity of 86.6% (95% CI 85.3–87.8) based on the three year's data prior to diagnosis less the immediate month before index diagnosis. The comparator regression model achieved a PPV of 3.1% (95% CI 3.0–3.1) and AUROC of 0.887 (95% CI 0.884–0.889). We interpreted our model using cluster analysis and have identified six groups of patients exhibiting similar lung cancer progression patterns and clinical investigation patterns.
Interpretation
Capturing temporal sequencing between cancer and non-cancer pathways to diagnosis enables much more accurate models. Future work will focus on external dataset validation and integration into GP clinical systems for evaluation.
Funding
Cancer Research UK.}
}
@article{FAN2024112718,
title = {A confidence-based knowledge integration framework for cross-domain table question answering},
journal = {Knowledge-Based Systems},
volume = {306},
pages = {112718},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112718},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013522},
author = {Yuankai Fan and Tonghui Ren and Can Huang and Beini Zheng and Yinan Jing and Zhenying He and Jinbao Li and Jianxin Li},
keywords = {Table question answering, Natural language interfaces, Sequence-to-sequence model, Confidence estimation, Similarity ranking},
abstract = {Recent advancements in TableQA leverage sequence-to-sequence (Seq2seq) deep learning models to accurately respond to natural language queries. These models achieve this by converting the queries into SQL queries, using information drawn from one or more tables. However, Seq2seq models often produce uncertain (low-confidence) predictions when distributing probability mass across multiple outputs during a decoding step, frequently yielding translation errors. To tackle this problem, we present Ckif, a confidence-based knowledge integration framework that uses a two-stage deep-learning-based ranking technique to mitigate the low-confidence problem commonly associated with Seq2seq models for TableQA. The core idea of Ckif is to introduce a flexible framework that seamlessly integrates with any existing Seq2seq translation models to enhance their performance. Specifically, by inspecting the probability values in each decoding step, Ckif first masks out each low-confidence prediction from the predicted outcome of an underlying Seq2seq model. Subsequently, Ckif integrates prior knowledge of query language to generalize masked-out queries, enabling the generation of all possible queries and their corresponding NL expressions. Finally, a two-stage deep-learning ranking approach is developed to evaluate the semantic similarity of NL expressions to a given NL question, hence determining the best-matching result. Extensive experiments are conducted to investigate Ckif by applying it to five state-of-the-art Seq2seq models using a widely used public benchmark. The experimental results indicate that Ckif consistently enhances the performance of all the Seq2seq models, demonstrating its effectiveness for better supporting TableQA.}
}
@article{MOLENAAR2025107648,
title = {Concept definition review: A method for studying terminology in software engineering},
journal = {Information and Software Technology},
volume = {180},
pages = {107648},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107648},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924002532},
author = {Sabine Molenaar and Nikita {van den Berg} and Fabiano Dalpiaz and Sjaak Brinkkemper},
keywords = {Literature review, Research method, Concept definition, Software engineering, Requirements engineering},
abstract = {Context:
In scientific domains, definitions provide a precise description of fundamental concepts. Although the debate within the philosophy of computer science regarding the scientific nature of software engineering (SE) is inconclusive, SE researchers have laid down important steps toward treating SE as a scientific paradigm.
Objective:
We aim to support precise and effective communication among SE researchers and practitioners by providing a systematic process for the identification and analysis of definitions, in order to support the selection of a suitable definition for a certain use case.
Method:
Inspired by methods for the planning and execution of systematic literature reviews, we construct a method that is specific for concept definition reviews (CDRs). These reviews are performed whenever a research team wishes to obtain a detailed understanding of an SE concept that may have been characterized by dozens, if not hundreds, definitions.
Results:
We built our method via two design science iterations. The first one focused on the concept feature and resulted in the definitive version of the CDR method presented in this paper. We then applied the revised method to two, related concepts: quality requirement and non-functional requirement. Besides showing the applicability of the CDR method, our results include findings regarding the characteristics and evolution of the terms.
Conclusions:
The two applications of the CDR method highlight the existence and citation of hundreds of definitions, many of which are nearly (but not exactly) identical. We put forward our method for other researchers to shed light on the key terminology in other sub-fields of SE.}
}
@article{CHEN2025104085,
title = {Improving cross-document event coreference resolution by discourse coherence and structure},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104085},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104085},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000275},
author = {Xinyu Chen and Peifeng Li and Qiaoming Zhu},
keywords = {Event coreference resolution, Discourse coherence, Discourse structure},
abstract = {Cross-Document Event Coreference Resolution (CD-ECR) is to identify and cluster together event mentions that occur across multiple documents. Existing methods exhibit two limitations: (1) In contrast to within-document event mentions, which are linked by rich, coherent contexts, cross-document event mentions lack such contexts, posing a challenging for the model to understand the relation between two event mentions in different documents. (2) The lack of coherent textual information between cross-document event mentions lead to the inability to capture their global information, which is important to mine long-distance interactions between them. To tackle these issues, we propose a novel discourse coherence enhancement mechanism and introduce discourse structure to improve cross-document event coreference resolution. Specifically, we first introduce a new task: Event-oriented cross-document coherence enhancement (ECD-CoE), which selects coherent sentences that form a coherent text for two cross-document event mentions. Second, we represent the coherent text as a tree structure with rhetorical relation information between textual units. We then obtain the global interaction information of event mentions from the tree structures and finally resolve coreferent events. Experimental results on both the ECB+ and GVC datasets indicate that our proposed method outperforms several state-of-the-art baselines.}
}
@article{CIMMINO2025104282,
title = {Open Digital Rights Enforcement framework (ODRE): From descriptive to enforceable policies},
journal = {Computers & Security},
volume = {150},
pages = {104282},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104282},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005881},
author = {Andrea Cimmino and Juan Cano-Benito and Raúl García-Castro},
keywords = {Open digital rights language, Privacy policies, ODRL enforcement},
abstract = {From centralised platforms to decentralised ecosystems, like Data Spaces, sharing data has become a paramount challenge. For this reason, the definition of data usage policies has become crucial in these domains, highlighting the necessity of effective policy enforcement mechanisms. The Open Digital Rights Language (ODRL) is a W3C standard ontology designed to describe data usage policies, however, it lacks built-in enforcement capabilities, limiting its practical application. This paper introduces the Open Digital Rights Enforcement (ODRE) framework, whose goal is to provide ODRL with enforcement capabilities. The ODRE framework proposes a novel approach to express ODRL policies that integrates the descriptive ontology terms of ODRL with other languages that allow behaviour specification, such as dynamic data handling or function evaluation. The framework includes an enforcement algorithm for ODRL policies and two open-source implementations in Python and Java. The ODRE framework is also designed to support future extensions of ODRL to specific domain scenarios. In addition, current limitations of ODRE, ODRL, and current challenges are reported. Finally, to demonstrate the enforcement capabilities of the implementations, their performance, and their extensibility features, several experiments have been carried out with positive results.}
}
@article{TSIRMPAS2024108231,
title = {Neural natural language processing for long texts: A survey on classification and summarization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108231},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108231},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003890},
author = {Dimitrios Tsirmpas and Ioannis Gkionis and Georgios Th. Papadopoulos and Ioannis Mademlis},
keywords = {Natural language processing, Long document, Document classification, Document summarization, Sentiment analysis, Deep neural networks},
abstract = {The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded online renders automated understanding of lengthy texts a critical issue. Relevant applications include automated Web mining, legal document review, medical records analysis, financial reports analysis, contract management, environmental impact assessment, news aggregation, etc. Despite the relatively recent development of efficient algorithms for analyzing long documents, practical tools in this field are currently flourishing. This article serves as an entry point into this dynamic domain and aims to achieve two objectives. First of all, it provides an introductory overview of the relevant neural building blocks, serving as a concise tutorial for the field. Secondly, it offers a brief examination of the current state-of-the-art in two key long document analysis tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Consequently, this article presents an introductory exploration of document-level analysis, addressing the primary challenges, concerns, and existing solutions. Finally, it offers a concise definition of “long text/document”, presents an original overarching taxonomy of common deep neural methods for long document analysis and lists publicly available annotated datasets that can facilitate further research in this area.}
}
@article{RAVAUT2024,
title = {Targeting COVID-19 and Human Resources for Health News Information Extraction: Algorithm Development and Validation},
journal = {JMIR AI},
volume = {3},
year = {2024},
issn = {2817-1705},
doi = {https://doi.org/10.2196/55059},
url = {https://www.sciencedirect.com/science/article/pii/S2817170524000620},
author = {Mathieu Ravaut and Ruochen Zhao and Duy Phung and Vicky Mengqi Qin and Dusan Milovanovic and Anita Pienkowska and Iva Bojic and Josip Car and Shafiq Joty},
keywords = {COVID-19, SARS-CoV-2, summary, summarize, news articles, deep learning, classification, summarization, machine learning, extract, extraction, news, media, NLP, natural language processing},
abstract = {Background
Global pandemics like COVID-19 put a high amount of strain on health care systems and health workers worldwide. These crises generate a vast amount of news information published online across the globe. This extensive corpus of articles has the potential to provide valuable insights into the nature of ongoing events and guide interventions and policies. However, the sheer volume of information is beyond the capacity of human experts to process and analyze effectively.
Objective
The aim of this study was to explore how natural language processing (NLP) can be leveraged to build a system that allows for quick analysis of a high volume of news articles. Along with this, the objective was to create a workflow comprising human-computer symbiosis to derive valuable insights to support health workforce strategic policy dialogue, advocacy, and decision-making.
Methods
We conducted a review of open-source news coverage from January 2020 to June 2022 on COVID-19 and its impacts on the health workforce from the World Health Organization (WHO) Epidemic Intelligence from Open Sources (EIOS) by synergizing NLP models, including classification and extractive summarization, and human-generated analyses. Our DeepCovid system was trained on 2.8 million news articles in English from more than 3000 internet sources across hundreds of jurisdictions.
Results
Rules-based classification with hand-designed rules narrowed the data set to 8508 articles with high relevancy confirmed in the human-led evaluation. DeepCovid’s automated information targeting component reached a very strong binary classification performance of 98.98 for the area under the receiver operating characteristic curve (ROC-AUC) and 47.21 for the area under the precision recall curve (PR-AUC). Its information extraction component attained good performance in automatic extractive summarization with a mean Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score of 47.76. DeepCovid’s final summaries were used by human experts to write reports on the COVID-19 pandemic.
Conclusions
It is feasible to synergize high-performing NLP models and human-generated analyses to benefit open-source health workforce intelligence. The DeepCovid approach can contribute to an agile and timely global view, providing complementary information to scientific literature.}
}
@article{WANG20241915,
title = {Vocabulary Matters: An Annotation Pipeline and Four Deep Learning Algorithms for Enzyme Named Entity Recognition},
journal = {Journal of Proteome Research},
volume = {23},
number = {6},
pages = {1915-1925},
year = {2024},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.3c00367},
url = {https://www.sciencedirect.com/science/article/pii/S1535390724002981},
author = {Meiqi Wang and Avish Vijayaraghavan and Tim Beck and Joram M. Posma},
keywords = {biomedical natural language processing, deep learning, named entity recognition},
abstract = {Enzymes are indispensable in many biological processes, and with biomedical literature growing exponentially, effective literature review becomes increasingly challenging. Natural language processing methods offer solutions to streamline this process. This study aims to develop an annotated enzyme corpus for training and evaluating enzyme named entity recognition (NER) models. A novel pipeline, combining dictionary matching and rule-based keyword searching, automatically annotated enzyme entities in >4800 full-text publications. Four deep learning NER models were created with different vocabularies (BioBERT/SciBERT) and architectures (BiLSTM/transformer) and evaluated on 526 manually annotated full-text publications. The annotation pipeline achieved an F1-score of 0.86 (precision = 1.00, recall = 0.76), surpassed by fine-tuned transformers for F1-score (BioBERT: 0.89, SciBERT: 0.88) and recall (0.86) with BiLSTM models having higher precision (0.94) than transformers (0.92). The annotation pipeline runs in seconds on standard laptops with almost perfect precision, but was outperformed by fine-tuned transformers in terms of F1-score and recall, demonstrating generalizability beyond the training data. In comparison, SciBERT-based models exhibited higher precision, and BioBERT-based models exhibited higher recall, highlighting the importance of vocabulary and architecture. These models, representing the first enzyme NER algorithms, enable more effective enzyme text mining and information extraction. Codes for automated annotation and model generation are available from https://github.com/omicsNLP/enzymeNER and https://zenodo.org/doi/10.5281/zenodo.10581586.
}
}
@article{KONG2025101749,
title = {TR-Net: Token Relation Inspired Table Filling Network for Joint Entity and Relation Extraction},
journal = {Computer Speech & Language},
volume = {90},
pages = {101749},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101749},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824001323},
author = {Yongle Kong and Zhihao Yang and Zeyuan Ding and Wenfei Liu and Shiqi Zhang and Jianan Xu and Hongfei Lin},
keywords = {Token relation, Table filling, Joint entity and relation extraction},
abstract = {Recently, table filling models have achieved promising performance in jointly extracting relation triplets from complex sentences, leveraging their inherent structural advantage of delineating entities and relations as table cells. Nonetheless, these models predominantly concentrate on the cells corresponding to entity pairs within the predicted tables, neglecting the interrelations among other token pairs. This oversight can potentially lead to the exclusion of essential token information. To address these challenges, we introduce the Token Relation-Inspired Network (TR-Net), a novel framework for the joint extraction of entities and relations. It encompasses a token relation generator that adaptively constructs a token relation table, concentrating on the prominent token cells. Moreover, it also uses a structure-enhanced encoder that integrates the structural and sequential data of sentences via a highway gate mechanism. Our experimental analysis demonstrates that TR-Net delivers considerable enhancements and achieves state-of-the-art performance on four public datasets.}
}
@article{2025100419,
title = {Pathology Visions 2024 Overview},
journal = {Journal of Pathology Informatics},
pages = {100419},
year = {2025},
issn = {2153-3539},
doi = {https://doi.org/10.1016/j.jpi.2025.100419},
url = {https://www.sciencedirect.com/science/article/pii/S215335392500001X}
}
@article{UPADHYAY2024100088,
title = {A comprehensive survey on answer generation methods using NLP},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100088},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100088},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000360},
author = {Prashant Upadhyay and Rishabh Agarwal and Sumeet Dhiman and Abhinav Sarkar and Saumya Chaturvedi},
keywords = {Question-answering systems, Natural language processing, Question analysis, Answer extraction, Information retrieval},
abstract = {Recent advancements in question-answering systems have significantly enhanced the capability of computers to understand and respond to queries in natural language. This paper presents a comprehensive review of the evolution of question answering systems, with a focus on the developments over the last few years. We examine the foundational aspects of a question answering framework, including question analysis, answer extraction, and passage retrieval. Additionally, we delve into the challenges that question answering systems encounter, such as the intricacies of question processing, the necessity of contextual data sources, and the complexities involved in real-time question answering. Our study categorizes existing question answering systems based on the types of questions they address, the nature of the answers they produce, and the various approaches employed to generate these answers. We also explore the distinctions between opinion-based, extraction-based, retrieval-based, and generative answer generation. The classification provides insight into the strengths and limitations of each method, paving the way for future innovations in the field. This review aims to offer a clear understanding of the current state of question answering systems and to identify the scaling needed to meet the rising expectations and demands of users for coherent and accurate automated responses in natural language.}
}
@article{MANTE20243051,
title = {SeqImprove: Machine-Learning-Assisted Curation of Genetic Circuit Sequence Information},
journal = {ACS Synthetic Biology},
volume = {13},
number = {9},
pages = {3051-3055},
year = {2024},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.4c00392},
url = {https://www.sciencedirect.com/science/article/pii/S2161506324002341},
author = {Jeanet Mante and Zach Sents and Duncan Britt and William Mo and Chunxiao Liao and Ryan Greer and Chris J. Myers},
keywords = {named entity recognition, named entity normalization, SBOL, ontologies, machine learning},
abstract = {The progress and utility of synthetic biology is currently hindered by the lengthy process of studying literature and replicating poorly documented work. Reconstruction of crucial design information through post hoc curation is highly noisy and error-prone. To combat this, author participation during the curation process is crucial. To encourage author participation without overburdening them, an ML-assisted curation tool called SeqImprove has been developed. Using named entity recognition, called entity normalization, and sequence matching, SeqImprove creates machine-accessible sequence data and metadata annotations, which authors can then review and edit before submitting a final sequence file. SeqImprove makes it easier for authors to submit sequence data that is FAIR (findable, accessible, interoperable, and reusable).
}
}@article{WANG2024123736,
title = {An exploration method for technology forecasting that combines link prediction with graph embedding: A case study on blockchain},
journal = {Technological Forecasting and Social Change},
volume = {208},
pages = {123736},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123736},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524005341},
author = {Liang Wang and Munan Li},
keywords = {Technology forecasting, Topic recognition, Link prediction, Graph representation learning, Emerging technology, Blockchain},
abstract = {To keep pace with the latest technological changes and advancements, predicting future technological trends and topics has become a critical approach for high-tech companies and policy-making institutions. In this paper, we proposed an explorative method that integrates link prediction and Node2Vec graph embedding to predict future technology topics using co-occurrence data from patent keywords. Specifically, this method collects and preprocesses patent datasets, constructs network graphs that depict relationships among different technology topics, and builds a supervised link prediction model based on the time series of the graph to identify future technology graphs. Furthermore, node2vec graph embedding is conducted to obtain node vector representations, and then the clustering algorithms can be improved to identify the relevant topics, which could be interpreted as future technology. A case study on blockchain is conducted to validate the feasibility and practicality of the method to demonstrate the application of the method. Through the comparison of machine learning methods, we selected the Random Forest (RF) model, which presents the highest accuracy, for our experiments. The results show that the proposed method can be used to effectively visualize potential future topics related to a specific technology. Compared to traditional methods such as Latent Dirichlet Allocation (LDA), our method can identify more unique and differentiated technological topics, significantly reducing topic overlap. Additionally, the reported method can illustrate the internal relationships of topics through subgraphs, helping readers better understand the core concepts of each topic and vividly displaying the structure and composition of the topics. Furthermore, the proposed method can also depict potential relationships between different technology topics, which can facilitate the visualization of new directions of research and development.}
}
@article{LUH2025104287,
title = {Gamifying information security: Adversarial risk exploration for IT/OT infrastructures},
journal = {Computers & Security},
volume = {151},
pages = {104287},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104287},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005935},
author = {Robert Luh and Sebastian Eresheim and Paul Tavolato and Thomas Petelin and Simon Gmeiner and Andreas Holzinger and Sebastian Schrittwieser},
keywords = {Hacking, Security game, Model, Gamification},
abstract = {Today’s interconnected IT and OT infrastructure faces an array of cyber threats from diverse actors with varying motivations and capabilities. The increasing complexity of exposed systems, coupled with adversaries’ sophisticated technical arsenals, poses significant challenges for organizations seeking to defend against these attacks. Understanding the relationship between specific attack techniques and effective technical, organizational and human-centric mitigation measures remains elusive, as does grasping the underlying principles of information security and how they may be applied to cyber defense. In response to these challenges, we propose a gamified metamodel that combines well-established frameworks, including MITRE ATT&CK, D3FEND, CAPEC, and the NIST SP 800-53 security standard. The programmatic implementation of the model, “PenQuest”, combines elements of game theory with cybersecurity concepts to enhance risk assessment and training for IT practitioners and security engineers. In PenQuest, participants engage in a digital battle — attackers attempt to compromise an abstracted IT infrastructure, while defenders work to prevent or mitigate the threat. Bot opponents and the technical foundation for reinforcement learning enable future automated strategy inference. This paper provides an in-depth exploration of the metamodel, the game’s components and features built to translate cybersecurity principles into strategy game rules, and the technical implementation of a mature, ready-to-use education and risk exploration solution. Future work will focus on further improving the attack likelihood and detection chance algorithms for seamless risk assessment.}
}
@article{WANG2024127709,
title = {Triple alignment-enhanced complex question answering over knowledge bases},
journal = {Neurocomputing},
volume = {588},
pages = {127709},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127709},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224004806},
author = {Dong Wang and Sihang Zhou and Jian Huang and Xiangrong Ni},
keywords = {Program induction, Complex question answering, Triple alignment, Function misalignment, Argument ambiguity},
abstract = {Program induction is a crucial paradigm for complex question answering over knowledge bases. In the existing learning framework, the predicted program is required to strictly align (word-by-word) with a gold program, which could cause over penalization for minor deviations. Meanwhile, due to the existence of synonyms, program induction question answering often fails to retrieve answer from the knowledge base because individual function argument cannot perfectly replicate the target argument. To address above misalignment problems, we propose a triple alignment-enhanced complex question answering (TACQA) method by incorporating global token alignment, function alignment, and argument alignment. First, apart from the classical global token alignment, the predicted functions are extracted and aligned separately with the gold functions, enabling efficient learning of implicit structural information related to the query framework of program from input questions. Second, an argument alignment is introduced to correct the ambiguous function arguments, which enhances the disambiguation processing efficiency of multi-argument by optimizing candidate pool construction and similarity calculation. The experiments on KQA Pro show that our method consistently outperforms the SOTA methods, demonstrating the effectiveness of triple alignment processing mechanism for simultaneously addressing function misalignment and argument ambiguity in program induction and further improving the model performance.}
}
@article{VODYAHO2024350,
title = {Continuous agile cyber–physical systems architectures based on digital twins},
journal = {Future Generation Computer Systems},
volume = {153},
pages = {350-359},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23004326},
author = {Alexander Vodyaho and Nataly Zhukova and Radhakrishnan Delhibabu and Alexey Subbotin},
keywords = {Compute continuum, Digital twin, Digital twin networks, Digital threads, Model synthesis, Continuous architecture, Agile architecture, cyber-physical system},
abstract = {Modern cyber-physical systems, for the most part, are large-scale multilevel heterogeneous distributed systems that integrate subsystems of different kinds and are built on the Internet of Things platforms, where system structure and behavior are not constant. Managing such systems and keeping them in working condition throughout their lifetime is a difficult task. The proposed article discusses one of the possible approaches to solving this problem, based on the use of well-known continuous and agile architecture paradigms. However, there are currently no effective mechanisms for implementing these paradigms. The proposed article suggests a new approach to implementing continuous agile architectures by utilizing digital twins and proposes a reference architecture for a run-time dynamic digital twin. This method is unique because it builds a series of dynamic digital twins that model the system in real time, utilizing data about system events. Build the first models using the models used in earlier stages of the system lifecycle. This gives the following opportunities: i) a way to use dynamic digital twins to implement the continuous agile architecture paradigm; ii) a generalized three-level model of the life cycle of the continuous agile architecture; iii) a reference architecture for dynamic digital twins; and iv) a set of models that are all about using dynamic digital twins. The suggested approach enables the management of heterogeneous multilevel cyber-physical systems with variable structure and behavior variability.}
}
@article{SAADY2025103066,
title = {Implementation of artificial intelligence approaches in oncology clinical trials: A systematic review},
journal = {Artificial Intelligence in Medicine},
volume = {161},
pages = {103066},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103066},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000016},
author = {Marwa Saady and Mahmoud Eissa and Ahmed S. Yacoub and Ahmed B. Hamed and Hassan Mohamed El-Said Azzazy},
keywords = {Artificial intelligence, Machine learning, Deep learning, Oncology clinical trials},
abstract = {Introduction
There is a growing interest in leveraging artificial intelligence (AI) technologies to enhance various aspects of clinical trials. The goal of this systematic review is to assess the impact of implementing AI approaches on different aspects of oncology clinical trials.
Methods
Pertinent keywords were used to find relevant articles published in PubMed, Scopus, and Google Scholar databases, which described the clinical application of AI approaches. A quality evaluation utilizing a customized checklist specifically adapted was conducted. This study is registered with PROSPERO (CRD42024537153).
Results
Out of the identified 2833 studies, 72 studies satisfied the inclusion criteria. Clinical Trial Enrollment & Eligibility were among the most commonly studied clinical trial aspects with 30 papers. The prediction of outcomes was covered in 25 studies of which 15 addressed the prediction of patients' survival and 10 addressed the prediction of drug outcomes. The trial design was studied in 10 articles. Three studies addressed each of the personalized treatments and decision-making, while one addressed data management. The results demonstrate using AI in cancer clinical trials has the potential to increase clinical trial enrollment, predict clinical outcomes, improve trial design, enhance personalized treatments, and increase concordance in decision-making. Additionally, automating some areas and tasks, clinical trials were made more efficient, and human error was minimized. Nevertheless, concerns and restrictions related to the application of AI in clinical studies are also noted.
Conclusion
AI tools have the potential to revolutionize the design, enrollment rate, and outcome prediction of oncology clinical trials.}
}
@article{TU2024965,
title = {Architecture for data-centric and semantic-enhanced industrial metaverse: Bridging physical factories and virtual landscape},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {965-979},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001134},
author = {Xinyi Tu and Riku Ala-Laurinaho and Chao Yang and Juuso Autiosalo and Kari Tammi},
keywords = {Industrial metaverse, Virtual–physical continuum, Digital twins, Extended reality, Architecture, Industry 5.0},
abstract = {The metaverse paradigm has recently captured increasing scholarly and industrial attention, particularly within the scope of human-centric Industry 5.0. In this context, the metaverse promises a transformative confluence of the physical and digital realms, offering unparalleled avenues for human augmentation in industrial applications. Yet, while several conceptual metaverse architectures and illustrative case studies have emerged, they scarcely delve deep into the nuanced practice of cultivating the industrial metaverse for factory-scale applications. Addressing this research gap, this work introduces a novel architecture for a data-centric and semantic-enhanced industrial metaverse. The architecture intricately weaves the physical factory domain with the metaverse, fortified by a suite of ten modules, facilitating data flow and knowledge synchronization with the integration of digital twins and semantic models. The practical application and relevance of this architecture are further accentuated through a case study focused on in-plant material flow tracking. Emerging results underline that our architecture encapsulates the essential components for constructing a factory-scale industrial metaverse. Future research will be geared towards a comprehensive validation of the proposed metaverse architecture, culminating in tangible implementations across diverse industrial contexts.}
}
@article{SHI2024121488,
title = {Integrity verification for scientific papers: The first exploration of the text},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121488},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121488},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019905},
author = {Xiang Shi and Yinpeng Liu and Jiawei Liu and Qikai Cheng and Wei Lu},
keywords = {Integrity verification, Scientific paper, Named entity recognition, Joint model},
abstract = {Scientific papers, as pivotal tools for academic communication, should be articulated with clarity and precision to ensure the effective conveyance of scholarly ideas and to prevent reader confusion. Yet, many such papers conspicuously lack in-depth research, and their core content is often ambiguously presented. This pattern poses a significant impediment to the progressive evolution of science and technology. While numerous researchers have recognized this widespread challenge, a holistic theoretical or methodological solution remains elusive in the academic realm. To bridge this gap, we introduce the INTEGrity vERification (INTEGER) task. This task aids researchers in assessing the integrity of their papers by verifying the clarity of each knowledge unit. To implement this task on text, we propose a multi-task learning model that utilizes the Tucker decomposition and span-level attention mechanism to identify terms and their integrity precisely. More specifically, to provide insights into the INTEGER task and validate the effectiveness of the proposed model, we collect 8076 sentences and construct three new datasets containing various types of terms and descriptions in different domains. Extensive experimental results show that our proposed model has an average performance improvement of 1.1% F1 over the three datasets compared to a series of state-of-the-art baseline methods.}
}
@article{PU2024128442,
title = {Graph neural network based intelligent tutoring system: A survey},
journal = {Neurocomputing},
volume = {610},
pages = {128442},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128442},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401213X},
author = {Juhua Pu and Shufei Li and Meng Guo and Xi Chen and Zhang Xiong},
keywords = {Graph neural network, Intelligent tutoring system, Personalized learning, Representation Learning, Overview},
abstract = {Online education is developing rapidly driven by artificial intelligence technology. The massive learning resources lead to information overload and low resource utilization. Intelligent tutoring system (ITS) plays a vital role in the education platform, providing personalized learning services for students. The data obtained from the online education platform has complex correlations, which can be potentially transformed into multi-level graph structures. In recent years, graph neural networks (GNNs) have been tried to be introduced into intelligent learning services due to their superior performance in processing graph-structured data. This paper aims to provide researchers and engineers with a general overview of modeling processes and techniques for intelligent learning services based on GNNs. Through a careful review of the advanced models published between 2019 and 2023, existing research primarily focuses on four detailed areas within the smart services scenario. The GNN models involved are systematically classified, and the principles, pioneers and variants of various models are summarized in detail. Simultaneously, this paper analyzes the applications, the specific problems to be solved, and the technologies and innovations of graph-based models in the four key areas. In addition, we examine the commonly used datasets and evaluation metrics in the field of education. Finally, the current challenges and future development trends are summarized to provide comprehensive and in-depth guidance for research in related fields.}
}
@article{ALTURAYEIF2025125525,
title = {EASE: An enhanced active learning framework for aspect-based sentiment analysis based on sample diversity and data augmentation},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125525},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125525},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424023923},
author = {Nouf Alturayeif and Irfan Ahmad},
keywords = {Aspect-based sentiment analysis, Active learning, Deep learning},
abstract = {Aspect-Based Sentiment Analysis (ABSA) has received considerable attention in recent studies. Powerful pre-trained models were proposed which can be fine-tuned for many Natural Language Processing (NLP) tasks, including ABSA. However, fine-tuning these models needs a relatively large amount of labeled data. In this research, we propose EASE; an active learning framework to minimize manual labeling effort. We extend the active learning technique by incorporating the concept of sample diversity where similar samples are not selected for labeling. Furthermore, we maximize the utility of these samples by incorporating data augmentation. EASE was evaluated on three benchmark ABSA datasets from three different domains. The results show that the reduction of the number of needed labeled samples ranges from 88% to 94% among the three datasets while maintaining accuracy. Our results show that active learning is an effective approach to reduce manual labeling effort while maintaining comparable performance. Moreover, it is possible to reduce the number of labeled data even further by incorporating sample diversity and data augmentation while maintaining performance.}
}
@incollection{2024281,
title = {Nomenclature},
editor = {Shufei Li and Pai Zheng and Lihui Wang},
booktitle = {Proactive Human-Robot Collaboration Toward Human-Centric Smart Manufacturing},
publisher = {Elsevier},
pages = {281-283},
year = {2024},
isbn = {978-0-443-13943-7},
doi = {https://doi.org/10.1016/B978-0-44-313943-7.00020-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044313943700020X}
}
@article{XI2025301846,
title = {Towards a joint semantic analysis in mobile forensics environments},
journal = {Forensic Science International: Digital Investigation},
volume = {52},
pages = {301846},
year = {2025},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2024.301846},
url = {https://www.sciencedirect.com/science/article/pii/S2666281724001732},
author = {Jian Xi and Melanie Siegel and Dirk Labudde and Michael Spranger},
keywords = {Semantic analysis, Mobile forensics, Topic modeling, Natural language processing, Multimodal machine learning, Communication analysis, Text mining, Semantic network},
abstract = {In recent years, mobile devices have become the dominant communication medium in our daily lives. This trend is also evident in the planning, arranging, and committing of criminal activities, particularly in organized crime. Accordingly, mobile devices have become an essential source of evidence for data analysts or investigators, especially in Law Enforcement Agencies (LEAs). However, communication via mobile devices generates vast amounts of data, rendering manual analysis impractical and resulting in growing backlogs of evidence awaiting analysis process, which can take months to years, thereby hindering investigations and trials. The automatic analysis of textual chat messages falls short because communication is not limited to the single modality, such as text, but instead spans multiple modalities, including voice messages, pictures, videos, and sometimes various messengers (channels). These modalities frequently overlap or interchange within the same communication, further complicating the analysis process. To achieve a correct and comprehensive understanding of such communication, it is essential to consider all modalities and channels through a consistent joint semantic analysis. This paper introduces a novel mobile forensics approach that enables efficient assessment of mobile data without losing semantic consistency by unifying semantic concepts across different modalities and channels. Additionally, a knowledge-guided topic modeling approach is proposed, integrating expertise into the investigation process to effectively examine large volumes of noisy mobile data. In this way, investigators can quickly identify evidentiary parts of the communication and completely facilitate reconstructing the course of events.}
}
@article{ABADIE2024123202,
title = {A shared journey: Experiential perspective and empirical evidence of virtual social robot ChatGPT's priori acceptance},
journal = {Technological Forecasting and Social Change},
volume = {201},
pages = {123202},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.123202},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523008879},
author = {Amelie Abadie and Soumyadeb Chowdhury and Sachin Kumar Mangla},
keywords = {Social robot, ChatGPT, Priori acceptance, UTAUT, Consumer value creation framework, Managerial usage intention},
abstract = {Due to recent technological advancements, social robots are becoming increasingly prevalent in the consumer space. ChatGPT, a virtual social robot, has captured significant attention from the mass media and academic practitioners alike since its release in November 2022. This attention arises from its remarkable capabilities, as well as potential challenges it poses to society and various business sectors. In light of these developments, we developed a theoretical model based on the Unified Theory of Acceptance and Use of Technology and a consumer value typology centred around consumer experiences to examine the influence of experiential factors on the intention to use ChatGPT and subsequently collaborating with it for co-creating content among business managers. To test this model, we conducted a survey of 195 business managers in the UK and employed partial PLS-structural equation modelling for analysis. Our findings indicate that the efficiency, excellence, meaningfulness of recommendations, and conversational ability of ChatGPT will influence the behavioural intention to use it during the priori acceptance stage. Based on these findings, we suggest that organisations should thoughtfully consider and strategize the deployment of ChatGPT applications to ensure their acceptance, eventual adoption, and subsequent collaboration between ChatGPT and managers for content creation or problem-solving.}
}
@incollection{2023243,
title = {Index},
editor = {Venu Govindaraju and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {48},
pages = {243-252},
year = {2023},
booktitle = {Deep Learning},
issn = {0169-7161},
doi = {https://doi.org/10.1016/S0169-7161(23)00019-6},
url = {https://www.sciencedirect.com/science/article/pii/S0169716123000196}
}
@article{ZHANG2024101738,
title = {Integrating multi-omics to unravel host-microbiome interactions in inflammatory bowel disease},
journal = {Cell Reports Medicine},
volume = {5},
number = {9},
pages = {101738},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101738},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124004683},
author = {Yiran Zhang and John P. Thomas and Tamas Korcsmaros and Lejla Gul},
abstract = {Summary
The gut microbiome is crucial for nutrient metabolism, immune regulation, and intestinal homeostasis with changes in its composition linked to complex diseases like inflammatory bowel disease (IBD). Although the precise host-microbial mechanisms in disease pathogenesis remain unclear, high-throughput sequencing have opened new ways to unravel the role of interspecies interactions in IBD. Systems biology—a holistic computational framework for modeling complex biological systems—is critical for leveraging multi-omics datasets to identify disease mechanisms. This review highlights the significance of multi-omics data in IBD research and provides an overview of state-of-the-art systems biology resources and computational tools for data integration. We explore gaps, challenges, and future directions in the research field aiming to uncover novel biomarkers and therapeutic targets, ultimately advancing personalized treatment strategies. While focusing on IBD, the proposed approaches are applicable for other complex diseases, like cancer, and neurodegenerative diseases, where the microbiome has also been implicated.}
}
@article{MAKAROV2024108632,
title = {Good machine learning practices: Learnings from the modern pharmaceutical discovery enterprise},
journal = {Computers in Biology and Medicine},
volume = {177},
pages = {108632},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108632},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007170},
author = {Vladimir Makarov and Christophe Chabbert and Elina Koletou and Fotis Psomopoulos and Natalja Kurbatova and Samuel Ramirez and Chas Nelson and Prashant Natarajan and Bikalpa Neupane},
keywords = {Artificial intelligence, Machine learning, Pharmaceutical, Drug discovery, Best practice, Life sciences},
abstract = {Machine Learning (ML) and Artificial Intelligence (AI) have become an integral part of the drug discovery and development value chain. Many teams in the pharmaceutical industry nevertheless report the challenges associated with the timely, cost effective and meaningful delivery of ML and AI powered solutions for their scientists. We sought to better understand what these challenges were and how to overcome them by performing an industry wide assessment of the practices in AI and Machine Learning. Here we report results of the systematic business analysis of the personas in the modern pharmaceutical discovery enterprise in relation to their work with the AI and ML technologies. We identify 23 common business problems that individuals in these roles face when they encounter AI and ML technologies at work, and describe best practices (Good Machine Learning Practices) that address these issues.}
}
@article{JIN2023114038,
title = {Building a deep learning-based QA system from a CQA dataset},
journal = {Decision Support Systems},
volume = {175},
pages = {114038},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2023.114038},
url = {https://www.sciencedirect.com/science/article/pii/S0167923623001136},
author = {Sol Jin and Xu Lian and Hanearl Jung and Jinsoo Park and Jihae Suh},
keywords = {Question answering (QA) system, Community question answering (CQA), BERT, T5},
abstract = {A man-made machine-reading comprehension (MRC) dataset is necessary to train the answer extraction part of existing Question Answering (QA) systems. However, a high-quality and well-structured dataset with question-paragraph-answer pairs is not usually found in the real world. Furthermore, updating or building an MRC dataset is a challenging and costly affair. To address these shortcomings, we propose a QA system that uses a large-scale English Community Question Answering (CQA) dataset (i.e., Stack Exchange) composed of 3,081,834 question-answer pairs. The QA system adopts a classifier-retriever-summarizer structure design. The question classifier and the answer retriever part are based on a Bidirectional Encoder Representations from Transformers (BERT) Natural Language Processing (NLP) model by Google, and the summarizer part introduces a deep learning-based Text-to-Text Transfer Transformer (T5) model to summarize the long answers. We instantiated the proposed QA system with 140 topics from the CQA dataset (including topics such as biology, law, politics, etc.) and conducted human and automatic evaluations. Our system presented encouraging results, considering that it provides high-quality answers to the questions in the test set and satisfied the requirements to develop a QA system without MRC datasets. Our results show the potential of building automatic and high-performance QA systems without being limited by man-made datasets, a significant step forward in the research of open-domain or specific-domain QA systems.}
}
@article{COBANAJ2024113504,
title = {Advancing equitable and personalized cancer care: Novel applications and priorities of artificial intelligence for fairness and inclusivity in the patient care workflow},
journal = {European Journal of Cancer},
volume = {198},
pages = {113504},
year = {2024},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2023.113504},
url = {https://www.sciencedirect.com/science/article/pii/S0959804923008067},
author = {Marisa Cobanaj and Chiara Corti and Edward C. Dee and Lucas McCullum and Laura Boldrini and Ilana Schlam and Sara M. Tolaney and Leo A. Celi and Giuseppe Curigliano and Carmen Criscitiello},
keywords = {Artificial intelligence, Bias, Decision support, Equity, Inclusivity, Precision medicine},
abstract = {Patient care workflows are highly multimodal and intertwined: the intersection of data outputs provided from different disciplines and in different formats remains one of the main challenges of modern oncology. Artificial Intelligence (AI) has the potential to revolutionize the current clinical practice of oncology owing to advancements in digitalization, database expansion, computational technologies, and algorithmic innovations that facilitate discernment of complex relationships in multimodal data. Within oncology, radiation therapy (RT) represents an increasingly complex working procedure, involving many labor-intensive and operator-dependent tasks. In this context, AI has gained momentum as a powerful tool to standardize treatment performance and reduce inter-observer variability in a time-efficient manner. This review explores the hurdles associated with the development, implementation, and maintenance of AI platforms and highlights current measures in place to address them. In examining AI’s role in oncology workflows, we underscore that a thorough and critical consideration of these challenges is the only way to ensure equitable and unbiased care delivery, ultimately serving patients’ survival and quality of life.}
}
@article{SINGHAL2023765,
title = {Opportunities and challenges for biomarker discovery using electronic health record data},
journal = {Trends in Molecular Medicine},
volume = {29},
number = {9},
pages = {765-776},
year = {2023},
issn = {1471-4914},
doi = {https://doi.org/10.1016/j.molmed.2023.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1471491423001417},
author = {P. Singhal and A.L.M. Tan and T.G. Drivas and K.B. Johnson and M.D. Ritchie and B.K. Beaulieu-Jones},
keywords = {electronic health records, biomarker discovery, phenotyping, precision medicine},
abstract = {Electronic health records (EHRs) have become increasingly relied upon as a source for biomedical research. One important research application of EHRs is the identification of biomarkers associated with specific patient states, especially within complex conditions. However, using EHRs for biomarker identification can be challenging because the EHR was not designed with research as the primary focus. Despite this challenge, the EHR offers huge potential for biomarker discovery research to transform our understanding of disease etiology and treatment and generate biological insights informing precision medicine initiatives. This review paper provides an in-depth analysis of how EHR data is currently used for phenotyping and identifying molecular biomarkers, current challenges and limitations, and strategies we can take to mitigate challenges going forward.}
}
@article{JIAO20241,
title = {A Comprehensive Survey on Deep Learning Multi-Modal Fusion: Methods, Technologies and Applications},
journal = {Computers, Materials and Continua},
volume = {80},
number = {1},
pages = {1-35},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.053204},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824005216},
author = {Tianzhe Jiao and Chaopeng Guo and Xiaoyue Feng and Yuming Chen and Jie Song},
keywords = {Multi-modal fusion, representation, translation, alignment, deep learning, comparative analysis},
abstract = {Multi-modal fusion technology gradually become a fundamental task in many fields, such as autonomous driving, smart healthcare, sentiment analysis, and human-computer interaction. It is rapidly becoming the dominant research due to its powerful perception and judgment capabilities. Under complex scenes, multi-modal fusion technology utilizes the complementary characteristics of multiple data streams to fuse different data types and achieve more accurate predictions. However, achieving outstanding performance is challenging because of equipment performance limitations, missing information, and data noise. This paper comprehensively reviews existing methods based on multi-modal fusion techniques and completes a detailed and in-depth analysis. According to the data fusion stage, multi-modal fusion has four primary methods: early fusion, deep fusion, late fusion, and hybrid fusion. The paper surveys the three major multi-modal fusion technologies that can significantly enhance the effect of data fusion and further explore the applications of multi-modal fusion technology in various fields. Finally, it discusses the challenges and explores potential research opportunities. Multi-modal tasks still need intensive study because of data heterogeneity and quality. Preserving complementary information and eliminating redundant information between modalities is critical in multi-modal technology. Invalid data fusion methods may introduce extra noise and lead to worse results. This paper provides a comprehensive and detailed summary in response to these challenges.}
}
@article{CHAKRABORTY2024102295,
title = {The changing scenario of drug discovery using AI to deep learning: Recent advancement, success stories, collaborations, and challenges},
journal = {Molecular Therapy - Nucleic Acids},
volume = {35},
number = {3},
pages = {102295},
year = {2024},
issn = {2162-2531},
doi = {https://doi.org/10.1016/j.omtn.2024.102295},
url = {https://www.sciencedirect.com/science/article/pii/S2162253124001823},
author = {Chiranjib Chakraborty and Manojit Bhattacharya and Sang-Soo Lee and Zhi-Hong Wen and Yi-Hao Lo},
keywords = {MT: Bioinformatics, drug molecules, artificial intelligence, drug discovery, deep learning, pharmaceutics},
abstract = {Due to the transformation of artificial intelligence (AI) tools and technologies, AI-driven drug discovery has come to the forefront. It reduces the time and expenditure. Due to these advantages, pharmaceutical industries are concentrating on AI-driven drug discovery. Several drug molecules have been discovered using AI-based techniques and tools, and several newly AI-discovered drug molecules have already entered clinical trials. In this review, we first present the data and their resources in the pharmaceutical sector for AI-driven drug discovery and illustrated some significant algorithms or techniques used for AI and ML which are used in this field. We gave an overview of the deep neural network (NN) models and compared them with artificial NNs. Then, we illustrate the recent advancement of the landscape of drug discovery using AI to deep learning, such as the identification of drug targets, prediction of their structure, estimation of drug-target interaction, estimation of drug-target binding affinity, design of de novo drug, prediction of drug toxicity, estimation of absorption, distribution, metabolism, excretion, toxicity; and estimation of drug-drug interaction. Moreover, we highlighted the success stories of AI-driven drug discovery and discussed several collaboration and the challenges in this area. The discussions in the article will enrich the pharmaceutical industry.}
}
@article{BAO2024102854,
title = {Platform service portfolio management (PSPM) of social digitalization platform for cloud-based collaborative product development ecosystem: A structural approach},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102854},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102854},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624005020},
author = {Yuguang Bao and Xianyu Zhang and Zhihua Chen and Tongtong Zhou and Xinguo Ming},
keywords = {Social digitalization platform, Platform service portfolio management, Business-technology-system alignment, System analysis, Heterogeneous causal correlations, HINGS-TOPSIS method},
abstract = {In the context of digital transformation, social digitalization platform (SDP) emerges and play an important and irreplaceable role in the whole industrial ecosystems. The main value creation purpose of SDP is to develop general technical service portfolios and customized solutions. For SDPs, platform service portfolio management (PSPM) is a really difficult problem. The decision must dynamically respond to the complex external environments of business requirement, technology evolution, and system innovation. The traditional experience-driven decision-making process confuses various factors together, and it is difficult to produce scientific decisions continuously and effectively. To help these platform-kind firms make wise decision, we put forward a novel and practical decision-making methodological framework for PSPM issues. Firstly, a general system architecture model, namely Business-Digitalization-System (BDS) model, is proposed based on system engineering, which helps platform service component identification more refined. Next, the PSPM problem is formalized and modelled as a causal relation graph-based intertwined system analysis procedure. A novel platform service portfolio evaluation method is developed to obtain the priority of the identified service alternatives. The approach can effectively handle the component system importance and its heterogeneous causal interdependencies simultaneously. Meanwhile, the combinatorial manipulation of subjective judgements and objective measure improves the reasonability and efficiency within a multi-stakeholder group. Finally, a case study in the constructive industry is presented and the results of method comparisons show the feasibility and advantages of the methodology.}
}
@article{GAO2024188,
title = {Hypergraph Computation},
journal = {Engineering},
volume = {40},
pages = {188-201},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924002510},
author = {Yue Gao and Shuyi Ji and Xiangmin Han and Qionghai Dai},
keywords = {High-order correlation, Hypergraph structure modeling, Hypergraph semantic computing, Efficient hypergraph computing, Hypergraph computation framework},
abstract = {Practical real-world scenarios such as the Internet, social networks, and biological networks present the challenges of data scarcity and complex correlations, which limit the applications of artificial intelligence. The graph structure is a typical tool used to formulate such correlations, it is incapable of modeling high-order correlations among different objects in systems; thus, the graph structure cannot fully convey the intricate correlations among objects. Confronted with the aforementioned two challenges, hypergraph computation models high-order correlations among data, knowledge, and rules through hyperedges and leverages these high-order correlations to enhance the data. Additionally, hypergraph computation achieves collaborative computation using data and high-order correlations, thereby offering greater modeling flexibility. In particular, we introduce three types of hypergraph computation methods: ① hypergraph structure modeling, ② hypergraph semantic computing, and ③ efficient hypergraph computing. We then specify how to adopt hypergraph computation in practice by focusing on specific tasks such as three-dimensional (3D) object recognition, revealing that hypergraph computation can reduce the data requirement by 80% while achieving comparable performance or improve the performance by 52% given the same data, compared with a traditional data-based method. A comprehensive overview of the applications of hypergraph computation in diverse domains, such as intelligent medicine and computer vision, is also provided. Finally, we introduce an open-source deep learning library, DeepHypergraph (DHG), which can serve as a tool for the practical usage of hypergraph computation.}
}
@article{ZHANG2025104065,
title = {Empowering crisis information extraction through actionability event schemata and domain-adaptive pre-training},
journal = {Information & Management},
volume = {62},
number = {1},
pages = {104065},
year = {2025},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2024.104065},
url = {https://www.sciencedirect.com/science/article/pii/S0378720624001472},
author = {Yuhao Zhang and Siaw Ling Lo and Phyo Yi Win Myint},
keywords = {Actionability extraction, Social media crisis detection, Multi-task learning, Domain-adaptive pre-training},
abstract = {One of the persistent challenges in crisis detection is inferring actionable information to support emergency response. Existing methods focus on situational awareness but often lack actionable insights. This study proposes a holistic approach to implementing an actionability extraction system on social media, including requirement gathering, selection of machine learning tasks, data preparation, and integration with existing resources, providing guidance for governments, civil services, emergency workers, and researchers on supplementing existing channels with actionable information from social media. Our solution leverages an actionability schema and domain-adaptive pre-training, improving upon the state-of-the-art model by 5.5 % and 10.1 % in micro and macro F1 scores.}
}
@article{MORENOBAREA2025109576,
title = {Named entity recognition for de-identifying Spanish electronic health records},
journal = {Computers in Biology and Medicine},
volume = {185},
pages = {109576},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109576},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524016615},
author = {Francisco J. Moreno-Barea and Guillermo López-García and Héctor Mesa and Nuria Ribelles and Emilio Alba and José M. Jerez and Francisco J. Veredas},
keywords = {Named entity recognition, Natural language processing, De-identification, Electronic health records, Spanish},
abstract = {Background and objectives:
There is an increasing and renewed interest in Electronic Health Records (EHRs) as a substantial information source for clinical decision making. Consequently, automatic de-identification of EHRs is an indispensable task, since their dissociation from personal data is a necessary prerequisite for their dissemination. Nevertheless, the bulk of prior research in this domain has been conducted using English EHRs, given the limited availability of annotated corpora in other languages, including Spanish.
Methods:
In this study, the automatic de-identification of medical documents in Spanish was explored. A private corpus comprising 599 genuine clinical cases was annotated with eight different categories of protected health information. The prediction problem was approached as a named entity recognition task and two deep learning-based methodologies were developed. The first strategy was based on recurrent neural networks (RNN) and the second, an end-to-end approach, was based on Transformers. In addition, we have implemented a procedure to expand the amount of texts employed for model training.
Results:
Our findings demonstrate that Transformers surpass RNNs in the de-identification of clinical data in Spanish. Particularly noteworthy is the excellent performance of the XLM-RoBERTa large Transformer, achieving a rigorous strict-match micro-average of 0.946 for precision, 0.954 for recall, and an F1 score of 0.95 when applied to the amplified version of the corpus. Furthermore, a web-based application has been created to assist specialized clinicians in de-identifying EHRs through the aid of the implemented models.
Conclusion:
The study’s conclusions showcase the practical applicability of the state-of-the-art Transformers models for precise de-identification of clinical notes in real-world medical settings in Spanish, with the potential to improve performance if continual pre-training strategies are implemented.}
}
@article{ZHANG2024103812,
title = {Chinese nested entity recognition method for the finance domain based on heterogeneous graph network},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103812},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103812},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001717},
author = {Han Zhang and Yiping Dang and Yazhou Zhang and Siyuan Liang and Junxiu Liu and Lixia Ji},
keywords = {Chinese finance domain, Nested named entity recognition, Heterogeneous graphs, Expert knowledge},
abstract = {In the finance domain, nested named entities recognition has become a hot topic in named entity recognition tasks. Traditional nested entity recognition methods easily ignore the dependency relationships between entities, and these methods are mostly suitable for English general domain. Therefore, we propose a Chinese nested entity recognition method for the finance domain based on heterogeneous graph network(HGFNER). This method consists of two parts: the boundary division model of candidate entities and the internal relationship graph model of candidate entities. First, the boundary division model of candidate entities that introduces expert knowledge is used to partition the flat entities contained in the text and segment the text to address issues such as long entity boundaries and strong domain features in the Chinese finance domain. Then, by using heterogeneous graphs to represent the internal structure of entities from both spatial and syntactic dependencies to achieve the goal of learning dependency relationships between entities from multiple perspectives. Meanwhile, so as not to affect the operational efficiency of the model, we also propose a fast matching algorithm DAAC_BM for n-gram sequences in domain dictionaries to solve the problems of memory overflow and space waste faced by multi-pattern fast matching algorithms in Chinese matching. In addition, we propose a Chinese nested entity dataset CFNE for the financial field, which, as far as we know, is the first publicly available annotated dataset in the field. HGFNER achieves state-of-the-art macro-F1 value on CFNE, reaching 86.41%.}
}
@article{XU2025102721,
title = {Interpretability research of deep learning: A literature survey},
journal = {Information Fusion},
volume = {115},
pages = {102721},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102721},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004998},
author = {Biao Xu and Guanci Yang},
keywords = {Deep learning, Interpretability, Active explanations, Passive explanations, Explainable artificial intelligence},
abstract = {Deep learning (DL) has been widely used in various fields. However, its black-box nature limits people's understanding and trust in its decision-making process. Therefore, it becomes crucial to research the DL interpretability, which can elucidate the model's decision-making processes and behaviors. This review provides an overview of the current status of interpretability research. First, the DL's typical models, principles, and applications are introduced. Then, the definition and significance of interpretability are clarified. Subsequently, some typical interpretability algorithms are introduced into four groups: active, passive, supplementary, and integrated explanations. After that, several evaluation indicators for interpretability are briefly described, and the relationship between interpretability and model performance is explored. Next, the specific applications of some interpretability methods/models in actual scenarios are introduced. Finally, the interpretability research challenges and future development directions are discussed.}
}
@article{SHEN2024111304,
title = {Unsupervised multilingual machine translation with pretrained cross-lingual encoders},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111304},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111304},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123010523},
author = {Yingli Shen and Wei Bao and Ge Gao and Maoke Zhou and Xiaobing Zhao},
keywords = {Unsupervised learning, Multilingual machine translation, Cross-lingual encoders},
abstract = {Multilingual Neural Machine Translation (MNMT) has recently made great progress in training models that can translate between multiple languages. However, MNMT faces a significant challenge due to the lack of sufficient parallel corpora for all language pairs. Unsupervised machine translation methods, which utilize monolingual data, have emerged as a solution to this challenge. In this paper, we propose a method that leverages cross-lingual encoders, such as XLM-R, in an unsupervised manner (i.e., using monolingual data and bilingual dictionaries) to train a MNMT model. Our method initializes the MNMT model with a pre-trained cross-lingual encoder and employs two levels of alignment to further align the representation space in MNMT model. Experimental results demonstrate that our method outperforms strong baseline systems and exhibits robust domain and language transfer capabilities while preserving the performance of the original pre-trained encoder on other downstream tasks.}
}
@article{SUN2024102823,
title = {A label information fused medical image report generation framework},
journal = {Artificial Intelligence in Medicine},
volume = {150},
pages = {102823},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102823},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000654},
author = {Shuifa Sun and Zhoujunsen Mei and Xiaolong Li and Tinglong Tang and Zhanglin Su and Yirong Wu},
keywords = {Medical image, Text generation, Attention mechanism, Multi-modal feature fusion, Feature extraction},
abstract = {Medical imaging is an important tool for clinical diagnosis. Nevertheless, it is very time-consuming and error-prone for physicians to prepare imaging diagnosis reports. Therefore, it is necessary to develop some methods to generate medical imaging reports automatically. Currently, the task of medical imaging report generation is challenging in at least two aspects: (1) medical images are very similar to each other. The differences between normal and abnormal images and between different abnormal images are usually trivial; (2) unrelated or incorrect keywords describing abnormal findings in the generated reports lead to mis-communications. In this paper, we propose a medical image report generation framework composed of four modules, including a Transformer encoder, a MIX-MLP multi-label classification network, a co-attention mechanism (CAM) based semantic and visual feature fusion, and a hierarchical LSTM decoder. The Transformer encoder can be used to learn long-range dependencies between images and labels, effectively extract visual and semantic features of images, and establish long-term dependent relationships between visual and semantic information to accurately extract abnormal features from images. The MIX-MLP multi-label classification network, the co-attention mechanism and the hierarchical LSTM network can better identify abnormalities, achieving visual and text alignment fusion and multi-label diagnostic classification to better facilitate report generation. The results of the experiments performed on two widely used radiology report datasets, IU X-RAY and MIMIC-CXR, show that our proposed framework outperforms current report generation models in terms of both natural linguistic generation metrics and clinical efficacy assessment metrics. The code of this work is available online at https://github.com/watersunhznu/LIFMRG.}
}
@article{ZHANG2024121211,
title = {Multi-information interaction graph neural network for joint entity and relation extraction},
journal = {Expert Systems with Applications},
volume = {235},
pages = {121211},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121211},
url = {https://www.sciencedirect.com/science/article/pii/S095741742301713X},
author = {Yini Zhang and Yuxuan Zhang and Zijing Wang and Huanchun Peng and Yongsheng Yang and Yuanxiang Li},
keywords = {Joint entity and relation extraction, Graph neural network, Transformer, Overlapping triplets, Distant supervision},
abstract = {Overlap situation where different triplets share entities or relations is a common challenge in joint entity and relation extraction task. On the one hand, there is strong correlation between overlapping triplets. On the other hand, most of the existing large-scale training data come from distant supervision, which introduces incomplete annotations. These practical problems make the information interaction between triplets particularly important. However, there are two problems with the existing methods: (i) the neglect of information interaction between different triplets; (ii) the limited information utilization caused by the specific decoding order. To solve the above problems, we decompose decoding and information interaction. Specifically, entity and relation proposals are obtained by a proposal generator, then a multi-information interaction graph neural network with parallel decoder is proposed to complete the joint extraction task. In this way, the inherent decoding order is broken to achieve the purpose of fully exploiting multi-information interaction across triplets and within triplets. Experimental results show that our proposed model outperforms previous work, especially in the case of incomplete annotations.}
}
@article{AALEALI2024100049,
title = {Machine learning advancements in organic synthesis: A focused exploration of artificial intelligence applications in chemistry},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {1},
pages = {100049},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100049},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000071},
author = {Rizvi Syed {Aal E Ali} and Jiaolong Meng and Muhammad Ehtisham Ibraheem Khan and Xuefeng Jiang},
keywords = {Artificial intelligence, Chemical selectivity, Retrosynthesis prediction, Catalyst design, Material design},
abstract = {Artificial intelligence (AI) is driving a revolution in chemistry, reshaping the landscape of molecular design. This review explores AI’s pivotal roles in the field of organic synthesis applications. AI accurately predicts reaction outcomes, controls chemical selectivity, simplifies synthesis planning, accelerates catalyst discovery, and fuels material innovation and so on. It seamlessly integrates data-driven algorithms with chemical intuition to redefine molecular design. As AI chemistry advances, it promises accelerated research, sustainability, and innovative solutions to chemistry’s pressing challenges. The fusion of AI and chemistry is poised to shape the field’s future profoundly, offering new horizons in precision and efficiency. This review encapsulates the transformation of AI in chemistry, marking a pivotal moment where algorithms and data converge to revolutionize the world of molecules.}
}
@article{JIA2025126130,
title = {Joint entity and relation extraction with table filling based on graph convolutional Networks},
journal = {Expert Systems with Applications},
volume = {266},
pages = {126130},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126130},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402997X},
author = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
keywords = {Graph convolutional networks, Graph construction, Joint entity and relation extraction, Table filling},
abstract = {Information extraction involves extracting structured information from text, which can be categorized into named entity recognition (NER) and relation extraction (RE). Recent successful works address the dependency between NER and RE using a filling table approach, but they overlook the long-distance dependencies between cells in the table, which may not adequately understand the semantics between distant elements in the text. To address this limitation, we introduce an innovative approach known as Table Filling with Graph Convolutional Networks (TableF-GCN) for joint NER and RE. First, TableF-GCN constructs a graph that captures the interaction between cells within the table. Additionally, it leverages GCNs to encode the long-distance dependency within the graph through multi-hop propagation, thereby enabling the acquisition of local and global contextualized embeddings for each node in the graph. Experimental results are conducted on joint NER and RE extraction, ablation study, hyper parameter setting, and a case study on three widely utilized datasets. The comparative analysis demonstrates TableF-GCN outperforms the baselines in recall, but not necessarily in precision. Fortunately, when consolidating the outcomes from the ConLL04 and ADE datasets, TableF-GCN showcases overall improvement through statistical analysis.}
}
@article{WANG2025101600,
title = {Integrating artificial intelligence in energy transition: A comprehensive review},
journal = {Energy Strategy Reviews},
volume = {57},
pages = {101600},
year = {2025},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2024.101600},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X24003092},
author = {Qiang Wang and Yuanfan Li and Rongrong Li},
keywords = {Artificial intelligence, Energy transition, Clean energy supply, Demand-side management, Technological innovation, Smart grids},
abstract = {The global energy transition, driven by the imperative to mitigate climate change, demands innovative solutions to address the technical, economic, and social challenges of decarbonization. Artificial intelligence (AI) has emerged as a transformative technology in this domain, offering tools to enhance each link in the energy system. This comprehensive review examines the current state of AI applications across key energy transition domains, including renewable energy deployment, energy efficiency, grid stability, and smart grid integration. The study identifies the pivotal role of AI in accelerating the adoption of intermittent renewable energy sources like solar and wind, managing demand-side dynamics with advanced forecasting and optimization, and enabling energy storage and distribution innovations such as vehicle-to-grid systems and hybrid energy solutions. It also highlights the potential of AI to advance energy system stability, address cybersecurity risks, and promote equitable and sustainable energy systems. Despite these advancements, challenges remain, including data quality and accessibility, system interoperability, scalability, and concerns regarding privacy and ethics. By synthesizing recent research and practical case studies, this paper provides insights into the opportunities and limitations of AI-driven energy transformation and offers strategic recommendations to guide future research, development, and policy-making. This review highlights that AI is not just a tool but a transformative catalyst, reshaping global energy systems into equitable, resilient, and sustainable frameworks, essential for achieving a net-zero future.}
}
@article{DIKMEN2025104251,
title = {Automated construction contract analysis for risk and responsibility assessment using natural language processing and machine learning},
journal = {Computers in Industry},
volume = {166},
pages = {104251},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104251},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000168},
author = {Irem Dikmen and Gorkem Eken and Huseyin Erol and M. Talat Birgonul},
keywords = {Automated contract review, Natural Language Processing (NLP), Machine Learning (ML), Artificial Intelligence (AI), Text classification, Construction risk management},
abstract = {Construction contracts contain critical risk-related information that requires in-depth examination, yet tight schedules for bidding limit the possibility of comprehensive review of extensive documents manually. This research aims to develop models for automating the review of construction contracts to extract information on risk and responsibility that will provide inputs for risk management plans. Models were trained on 2268 sentences from International Federation of Consulting Engineers templates and tested on an actual construction project contract containing 1217 sentences. A taxonomy classified sentences into Heading, Definition, Obligation, Risk, and Right categories with related parties of Contractor, Employer, and Shared. Twelve models employing diverse Natural Language Processing vectorization techniques and Machine Learning algorithms were implemented and benchmarked based on accuracy and F1 score. Binary classification of sentence types and an ensemble method integrating top models were further applied to improve performance. The best model achieved 89 % accuracy for sentence types and 83 % for related parties, demonstrating the capabilities of automated contract review for identification of risk and responsibilities. Adopting the proposed approach can significantly expedite contract reviews to support risk management activities, bid preparation processes and prevent disputes caused by overlooking risks and responsibilities.}
}
@article{MOSCATO2024112682,
title = {ALDANER: Active Learning based Data Augmentation for Named Entity Recognition},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112682},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112682},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013169},
author = {Vincenzo Moscato and Marco Postiglione and Giancarlo Sperlì and Andrea Vignali},
keywords = {Data augmentation, Named Entity Recognition, Active Learning},
abstract = {Training Named Entity Recognition (NER) models typically necessitates the use of extensively annotated datasets. This requirement presents a significant challenge due to the labor-intensive and costly nature of manual annotation, especially in specialized domains such as medicine and finance. To address data scarcity, two strategies have emerged as effective: (1) Active Learning (AL), which autonomously identifies samples that would most enhance model performance if annotated, and (2) data augmentation, which automatically generates new samples. However, while AL reduces human effort, it does not eliminate it entirely, and data augmentation often leads to incomplete and noisy annotations, presenting new hurdles in NER model training. In this study, we integrate AL principles into a data augmentation framework, named Active Learning-based Data Augmentation for NER (ALDANER), to prioritize the selection of informative samples from an augmented pool and mitigate the impact of noisy annotations. Our experiments across various benchmark datasets and few-shot scenarios demonstrate that our approach surpasses several data augmentation baselines, offering insights into promising avenues for future research.}
}
@article{DING2024123308,
title = {Popularity prediction with semantic retrieval for news recommendation},
journal = {Expert Systems with Applications},
volume = {247},
pages = {123308},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123308},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424001738},
author = {Yuhan Ding and Bang Wang and Xiangyang Cui and Minghua Xu},
keywords = {News recommendation, Popularity, Semantic retrieval, Hypergraph},
abstract = {News recommendation (NR) is critical for helping users to navigate the vast amount of information on online news platforms. However, the key challenges of tackling the cold-start problem, comprehensively modeling user interests and accurately encoding news semantics in NR have not yet been effectively addressed by existing methods. In this paper, we propose to alleviate the cold-start problem by exploiting news popularity. We also argue that the browsing history of a user can be organized as a personalized hypergraph to augment user interest modeling. Furthermore, we suggest enriching the representation of a candidate news article by retrieving its similar news. Our proposed NR method, named Popularity Prediction with Semantic Retrieval (PPSR), features a retrieval-driven popularity predictor that leverages click records to predict news popularity by incorporating semantic retrieval. It also comprises a hypergraph-based user encoder that mines the rich and diverse relatedness among news in a user’s browsing history. In addition, it designs a semantic-enhanced news encoder that retrieves and utilizes similar news in the training set to enrich semantic encoding for candidate news. Experiments on real-world datasets show that our PPSR can outperform the state-of-the-art methods in terms of higher recommendation accuracy and can well mitigate both the user-level and news-level cold-start problems.}
}
@article{MA2025112700,
title = {Semantic-based topic model for public opinion analysis in sudden-onset disasters},
journal = {Applied Soft Computing},
volume = {170},
pages = {112700},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112700},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625000110},
author = {Yulong Ma and Xinsheng Zhang and Runzhou Wang},
keywords = {Sudden-onset disaster, Topic model, Latent Dirichlet allocation, Distributional semantics},
abstract = {Sudden-onset disasters have put forward more stringent requirements for the government to carry out public opinion analysis work. However, most existing topic models ignore the contextual semantics of disaster texts, and fail to balance the robustness and the training cost. To address these issues, a neural clustering topic model is proposed in this work. The topic probability distribution of the LDA model is integrated with the distribution semantic vector generated by a lite BERT. The fused vectors are reconstructed by a nonlinear manifold learning algorithm, and re-clustered into topics by a mini-batch based k-means++ algorithm. Compared to state-of-the-art models on three sudden-onset disaster datasets, the proposed model shows an increase of 1.79 % in average topic coherence and 33.87 % in topic diversity. Meanwhile, the inference time is reduced by 84.09 % on average. The visual study of the latent process of the proposed model reflects that its ability to compact intra-cluster vector distances and sparse inter-cluster vector distances is the potential reason for its better performance. It can be considered that the application of the proposed model can help the government enhance its ability to manage negative public opinions in sudden-onset disasters.}
}
@incollection{2024377,
title = {Index},
editor = {Vijai Singh},
series = {Progress in Molecular Biology and Translational Science},
publisher = {Academic Press},
volume = {205},
pages = {377-388},
year = {2024},
booktitle = {New Approach for Drug Repurposing Part A},
issn = {1877-1173},
doi = {https://doi.org/10.1016/S1877-1173(24)00111-X},
url = {https://www.sciencedirect.com/science/article/pii/S187711732400111X}
}
@article{JIANG2025104278,
title = {Textual adversarial attacks in cybersecurity named entity recognition},
journal = {Computers & Security},
volume = {150},
pages = {104278},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104278},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005844},
author = {Tian Jiang and Yunqi Liu and Xiaohui Cui},
keywords = {Cyber Threat Intelligence, Named Entity Recognition, Fine-tuned models, Adversarial examples, Word substitution, Adversarial detection},
abstract = {In the cybersecurity domain, Cyber Threat Intelligence (CTI) includes procedures that lead to textual reports and different types of pieces of information and evidence on cyber threats. To better understand the behaviors of attackers and construct attack graphs, identifying attack-relevant entities in diverse CTI texts precisely and efficiently becomes more important, and Named Entity Recognition (NER) models can help extract entities automatically. However, such fine-tuned models are usually vulnerable to adversarial attacks. In this paper, we first construct an attack framework that can explore textual adversarial attacks in the cybersecurity NER task by generating adversarial CTI texts. Then, we analyze the most important parts of speech (POSs) from the perspective of grammar, and propose a word-substitution-based attack method. To confront adversarial attacks, we also introduce a method to detect potential adversarial examples. Experimental results show that cybersecurity NER models are also vulnerable to adversarial attacks. Among all attack methods, our method can generate adversarial texts that keep a balanced performance in several aspects. Furthermore, adversarial examples generated by all attack methods perform well in the study of transferability, and they can help improve the robustness of NER models through adversarial training. On the defense side, our detection method is simple but effective against multiple types of textual adversarial attacks.}
}
@article{TCHOUKA2024200416,
title = {Differentially private de-identifying textual medical document is compliant with challenging NLP analyses: Example of privacy-preserving ICD-10 code association},
journal = {Intelligent Systems with Applications},
volume = {23},
pages = {200416},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200416},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324000905},
author = {Yakini Tchouka and Jean-François Couchot and David Laiymani and Philippe Selles and Azzedine Rahmani},
keywords = {De-identification, Clinical data, Local differential privacy, Metric-privacy, Natural language processing, ICD-10 code association, Machine learning},
abstract = {Medical research plays a crucial role within scientific research. Technological advancements, especially those related to the rise of machine learning, pave the way for the exploration of medical issues that were once beyond reach. Unstructured textual data, such as correspondence between doctors, operative reports, etc., often serve as a starting point for many medical applications. However, for obvious privacy reasons, researchers do not legally have the right to access these documents as long as they contain sensitive data, as defined by regulations like GDPR (General Data Protection Regulation) or HIPAA (Health Insurance Portability and Accountability Act). De-identification, meaning the detection, removal or substitution of all sensitive information, is therefore a necessary step to facilitate the sharing of these data between the medical field and research. Over the past decade, various approaches have been proposed to de-identify medical textual data. However, while entity detection is a well-known task in the natural language processing field, it presents some specific challenges in the medical context. Moreover, existing substitution methods proposed in the literature often pay little attention to the medical relevance of de-identified data or are not very resilient to attacks. This paper addresses these challenges. Firstly, an efficient system for detecting sensitive entities in French medical data and then accurately substitute them was implemented. Secondly, robust strategies for generating substitutes that incorporate the medical utility of the data were provided, thereby minimizing the difference in utility between the original and de-identified data, and that mathematically ensure privacy protection. Thirdly, the utility of the de-identification system in a context of ICD-10 code association was evaluated. Finally, various systems developed to tackle ICD-10 code association were presented while providing a state-of-the-art model in French.}
}
@article{GUO2025107024,
title = {Counterfactual learning for higher-order relation prediction in heterogeneous information networks},
journal = {Neural Networks},
volume = {183},
pages = {107024},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.107024},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024009535},
author = {Xuan Guo and Jie Li and Pengfei Jiao and Wang Zhang and Tianpeng Li and Wenjun Wang},
keywords = {Heterogeneous information networks, Higher-order relation prediction, Counterfactual learning, Network representation learning},
abstract = {Heterogeneous Information Networks (HINs) play a crucial role in modeling complex social systems, where predicting missing links/relations is a significant task. Existing methods primarily focus on pairwise relations, but real-world scenarios often involve multi-entity interactions. For example, in academic collaboration networks, an interaction occurs between a paper, a conference, and multiple authors. These higher-order relations are prevalent but have been underexplored. Moreover, existing methods often neglect the causal relationship between the global graph structure and the state of relations, limiting their ability to capture the fundamental factors driving relation prediction. In this paper, we propose HINCHOR, an end-to-end model for higher-order relation prediction in HINs. HINCHOR introduces a higher-order structure encoder to capture multi-entity proximity information. Then, it focuses on a counterfactual question: “If the global graph structure were different, would the higher-order relation change?” By presenting a counterfactual data augmentation module, HINCHOR utilizes global structure information to generate counterfactual relations. Through counterfactual learning, HINCHOR estimates causal effects while predicting higher-order relations. The experimental results on four constructed benchmark datasets show that HINCHOR outperforms existing state-of-the-art methods.}
}
@article{WANG2024103785,
title = {DCTM: Dual Contrastive Topic Model for identifiable topic extraction},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103785},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103785},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001456},
author = {Rui Wang and Peng Ren and Xing Liu and Shuyu Chang and Haiping Huang},
keywords = {Contrastive learning, Neural-based topic model, Topic modeling},
abstract = {The recent advanced Contrastive Neural Topic Model (CNTM) was proposed to tackle topic collapse through document-level contrastive learning. However, limited by its usage of the Logistic-Normal prior in topic space and document level contrastive learning, it is less capable of disentangling semantically similar topics. To address the limitation, we propose a novel Dual Contrastive Topic Model (DCTM) that utilizes the Dirichlet prior to capture interpretable patterns. Besides, it incorporates dual (document-level and topic-level) contrastive learning on the topic distribution matrix which helps generate discriminative topic representations and mine identifiable topics. Our proposed DCTM outperforms the state-of-the-art neural topic models in terms of topic coherence and diversity, which is verified by extensive experimentation on three publicly available text corpora. In detail, the proposed DCTM surpasses baselines on almost all the used topic coherence metrics (CP, CA, NPMI for 20Newsgroups, CP, CA, NPMI and UCI for Grolier and DBPedia), and it also obtains higher topic diversity with 1 datasets respectively. Moreover, when performing text clustering, DCTM also achieves significant improvements, with observed increases of more than 1% (20Newsgroups) and 6% (DBPedia) in accuracy.}
}
@article{JIA2024111545,
title = {Document-level relation extraction with global and path dependencies},
journal = {Knowledge-Based Systems},
volume = {289},
pages = {111545},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111545},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001801},
author = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
keywords = {Relation extraction, Global dependency, Multi-hop path, Path representation},
abstract = {Document-level relation extraction (RE) focuses on extracting relations for each entity pair in the same sentence or across different sentences of a document. Several existing methodologies aim to capture the intricate interactions among entities across a document by constructing diverse document graphs. However, these graphs frequently cannot sufficiently model the intricate global interactions and concurrent explicit path reasoning. Therefore, we introduce a distinctive graph-based model designed to assimilate global and path dependencies within a document for document-level RE, termed graph-based global and path dependencies (GGP). Specifically, the global dependency component captures interactions between mentions, entities, sentences and, the document through two interconnected graphs: the mention-level graph and the entity-level graph (ELG). To integrate relevant paths essential for the designated entity pair, the path dependency component consolidates information from various multi-hop paths of the target entity pair through an attention mechanism on the ELG. In addition, we devised an innovative method for learning path representation, which encapsulates relations and intermediate entities within the multi-hop path in the ELG. Comprehensive experiments conducted on standard document-level RE and CDR datasets reveal the following key findings: (i) GGP achieves an Ign F1 score of 59.98%, surpassing baselines by 0.61% on the test set; and (ii) the integration of various features derived from entities, sentences, documents, and paths enhances GGP's performance in document-level RE.}
}
@incollection{2025251,
title = {Index},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {251-254},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.09991-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962099915}
}
@incollection{2025425,
title = {Index},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {425-430},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.20001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460200011}
}
@article{FARISCO2024106714,
title = {Is artificial consciousness achievable? Lessons from the human brain},
journal = {Neural Networks},
volume = {180},
pages = {106714},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106714},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006385},
author = {Michele Farisco and Kathinka Evers and Jean-Pierre Changeux},
keywords = {Brain, Consciousness, Artificial intelligence, Neuromorphic computing, Robotics, Cognition, Neuroscience},
abstract = {We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model or as a benchmark. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of human-like conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (i.e., structural and architectural) and extrinsic (i.e., related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make human-like conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it cannot be theoretically excluded that AI research can develop partial or potentially alternative forms of consciousness that are qualitatively different from the human form, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word “consciousness” for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify which level and/or type of consciousness AI research aims to develop, as well as what would be common versus differ in AI conscious processing compared to human conscious experience.}
}
@article{LONGO202364,
title = {A framework for cognitive chatbots based on abductive–deductive inference},
journal = {Cognitive Systems Research},
volume = {81},
pages = {64-79},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000359},
author = {Carmelo Fabio Longo and Paolo Marco Riela and Daniele Francesco Santamaria and Corrado Santoro and Antonio Lieto},
keywords = {Chatbot, Question answering, Artificial intelligence, First-order logic, Cognitive architectures, Meta-reasoning},
abstract = {This paper presents a framework based on natural language processing and first-order logic aiming at instantiating cognitive chatbots. The proposed framework leverages two types of knowledge bases interacting with each other in a meta-reasoning process. The first one is devoted to the reactive interactions within the environment, while the second one to conceptual reasoning. The latter exploits a combination of axioms represented with rich semantics and abduction as pre-stage of deduction, dealing also with some of the state-of-the-art issues in the natural language ontology domain. As a case study, a Telegram chatbot system has been implemented, supported by a module which automatically transforms polar and wh-questions into one or more likely assertions, so as to infer Boolean values or snippets with variable length as factoid answer. The conceptual knowledge base is organized in two layers, representing both long- and short-term memory. The knowledge transition between the two layers is achieved by leveraging both a greedy algorithm and the engine’s features of a NoSQL database, with promising timing performance if compared with the adoption of a single layer. Furthermore, the implemented chatbot only requires the knowledge base in natural language sentences, avoiding any script updates or code refactoring when new knowledge has to income. The framework has been also evaluated as cognitive system by taking into account the state-of-the art criteria: the results show that AD-Caspar is an interesting starting point for the design of psychologically inspired cognitive systems, endowed of functional features and integrating different types of perception.}
}
@incollection{LEE202513,
title = {Chapter 2 - What if we have Metaverse GPT? Content singularity and human-Metaverse interaction in the AIGC era},
editor = {Chang S. Nam and Donggil Song and Heejin Jeong},
booktitle = {Human-Centered Metaverse},
publisher = {Morgan Kaufmann},
pages = {13-28},
year = {2025},
isbn = {978-0-443-21996-2},
doi = {https://doi.org/10.1016/B978-0-443-21996-2.00015-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443219962000152},
author = {Lik-Hang Lee and Peng Yuan Zhou and Chaoning Zhang and Simo Hosio},
keywords = {AI-generated content (AIGC), Metaverse, Human-technology interaction, Mixed reality, Content singularity},
abstract = {The global Metaverse development faces a “cooldown moment,” while the academia and industry attention moves from the Metaverse to AI-generated content (AIGC) in 2023. Nonetheless, the current discussion rarely considers the connection between AIGC and the Metaverse. We can imagine the Metaverse, i.e., immersive cyberspace, as the black void of space where AIGC can offer content and facilitate diverse user needs. As such, this article argues that AIGC can be a vital technological enabler for the Metaverse. The article first provides a retrospect of recent history's major pitfall of the Metaverse applications. Second, we discuss from a user-centric perspective how the Metaverse development will likely accelerate with AIGC. Next, the article conjectures future scenarios that leverage the combination of the Metaverse and AIGC. Finally, we present and advocate for an AI-generated Metaverse (AIGM) framework for energizing the creation of Metaverse content in the AIGC era.}
}
@article{WU2024103772,
title = {Fuser: An enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103772},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103772},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001328},
author = {Fan Wu and Bin Gao and Xiaoou Pan and Linlin Li and Yujiao Ma and Shutian Liu and Zhengjun Liu},
keywords = {Hateful memes detection, Multimodal fusion, Congruent reinforced perceptron, Main semantic, Auxiliary context},
abstract = {As a multimodal form of hate speech on social media, hateful memes are more aggressive and cryptic threats to the real life of humans. Automatic detection of hateful memes is crucial, but the images and texts in most memes are only weakly consistent or even irrelevant. Although existing works have achieved the initial goal of detecting hateful memes with pre-trained models, they are limited to monolithic inference methods while ignoring the semantic differences between multimodal representations. To strengthen the comprehension and reasoning of the hidden meaning behind the memes by combining real-world knowledge, we propose an enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection. Inspired by the human cognitive mechanism, we first divide the extracted multisource representations into main semantics and auxiliary contexts based on their strength and relevance, and then precode them into lightly correlated embeddings with unified spatial dimensions via a novel prefix uniform layer, respectively. To jointly learn the intrinsic correlation between primary and secondary semantics, a congruent reinforced perceptron with brain-like perceptual integration is designed to seamlessly fuse multimodal representations in a shared latent space while maintaining the feature integrity in the sub-fusion space, thereby implicitly reasoning about the subtle metaphors behind the memes. Extensive experiments on four benchmark datasets fully demonstrate the effectiveness and superiority of our architecture compared with previous state-of-the-art methods.}
}
@article{SUN2025130361,
title = {Deep reinforcement learning-based influence maximization for heterogeneous hypergraphs},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {660},
pages = {130361},
year = {2025},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2025.130361},
url = {https://www.sciencedirect.com/science/article/pii/S0378437125000135},
author = {Yanhao Sun and Jie Wu and Nuan Song and Tianwei Lin and Longxiang Li and Dong Li},
keywords = {Influence maximization, Diffusion model, Heterogeneous hypergraphs, Deep reinforcement learning},
abstract = {In the field of social computing, understanding and optimizing information dissemination within social networks is crucial. To effectively model the complex higher-order relationships that exist in these networks, hypergraphs are employed. Current research predominantly centers on homogeneous hypergraphs, which encapsulate only a single type of relationship. However, real-world interactions often encompass more intricate and diverse higher-order relationships, which can be captured well by heterogeneous hypergraphs. This paper delves into the challenge of influence maximization within such heterogeneous hypergraphs. Specifically, we first introduce HLabel-LT, an innovative diffusion model designed for heterogeneous hypergraphs, which integrates multiple relationship types to enhance simulation accuracy. Furthermore, we propose a novel seed nodes selection framework, Heterogeneous Influence Maximization with Deep Reinforcement Learning (HIMH-DRL), which harnesses the unique adaptive trial-and-error learning capabilities of deep reinforcement learning to optimize influence spread in heterogeneous hypergraphs. To facilitate the training demands of this framework, we also present a new hypergraph generation model, HyperFF-Label, used to create artificially synthesized heterogeneous hypergraphs. Extensive experiments across various real-world datasets demonstrate that our methodology not only broadens the understanding of hypergraph dynamics but also markedly enhances the efficiency and effectiveness of influence propagation compared to traditional methods. This study underscores the potential of exploiting heterogeneity in hypergraphs to devise more effective information dissemination strategies in social networks.}
}
@incollection{MARTINIS2024,
title = {Natural Language Processing Approaches in Bioinformatics},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00179-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001792},
author = {Maria Chiara Martinis and Zucco Chiara},
keywords = {Active learning, Biomedical natural language processing, Semantical analysis and Syntactical analysis},
abstract = {In this article, we provide an overview of the natural language processing and its bioinformatics applications. We describe the historical evolution of NLP, and summarize the common NLP sub-problems in the field, as well as their research progress in the biomedical domain. In addition, we discuss an advanced topic of applying active learning methods to the NLP systems, to solve the practical issue of lacking training data in the field.}
}
@article{SONG2024100268,
title = {IDL-LTSOJ: Research and implementation of an intelligent online judge system utilizing DNN for defect localization},
journal = {High-Confidence Computing},
pages = {100268},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100268},
url = {https://www.sciencedirect.com/science/article/pii/S2667295224000710},
author = {Lihua Song and Ying Han and Yufei Guo and Chenying Cai},
keywords = {Online Judge (OJ) system, Fine-grained defect localization, Deep neural network, Task scheduling},
abstract = {The evolution of artificial intelligence has thrust the Online Judge (OJ) systems into the forefront of research, particularly within programming education, with a focus on enhancing performance and efficiency. Addressing the shortcomings of the current OJ systems in coarse defect localization granularity and heavy task scheduling architecture, this paper introduces an innovative Integrated Intelligent Defect Localization and Lightweight Task Scheduling Online Judge (IDL-LTSOJ) system. Firstly, to achieve token-level fine-grained defect localization, a Deep Fine-Grained Defect Localization (Deep-FGDL) deep neural network model is developed. By integrating Bidirectional Long Short-Term Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU), this model extracts fine-grained information from the abstract syntax tree (AST) of code, enabling more accurate defect localization. Subsequently, we propose a lightweight task scheduling architecture to tackle issues, such as limited concurrency in task evaluation and high equipment costs. This architecture integrates a Kafka messaging system with an optimized task distribution strategy to enable concurrent execution of evaluation tasks, substantially enhancing system evaluation efficiency. The experimental results demonstrate that the Deep-FGDL model improves the accuracy by 35.9% in the Top-20 rank compared to traditional machine learning benchmark methods for fine-grained defect localization tasks. Moreover, the lightweight task scheduling strategy notably reduces response time by nearly 6000ms when handling 120 task volumes, which represents a significant improvement in evaluation efficiency over centralized evaluation methods.}
}
@article{HAIR2025115047,
title = {Connecting the dots in neuroscience research: The future of evidence synthesis},
journal = {Experimental Neurology},
volume = {384},
pages = {115047},
year = {2025},
issn = {0014-4886},
doi = {https://doi.org/10.1016/j.expneurol.2024.115047},
url = {https://www.sciencedirect.com/science/article/pii/S001448862400373X},
author = {Kaitlyn Hair and María Arroyo-Araujo and Sofija Vojvodic and Maria Economou and Charis Wong and Francesca Tinsdeall and Sean Smith and Torsten Rackoll and Emily S. Sena and Sarah K. McCann},
keywords = {Systematic review, Evidence synthesis, Evidence ecosystem, Artificial intelligence, Research improvement, Data sharing, Meta-analysis},
abstract = {Making progress in neuroscience research involves learning from existing data. In this perspective piece, we explore the potential of a data-driven evidence ecosystem to connect all primary data streams, and synthesis efforts to inform evidence-based research and translational success from bench to bedside. To enable this transformation, we set out how we can produce evidence designed with evidence curation in mind. All data should be findable, understandable, and easily synthesisable, using a combination of human and machine effort. This will require shifts in research culture and tailored infrastructure to support rapid dissemination, data sharing, and transparency. We also discuss improvements in the way we can synthesise evidence to better inform primary research, including the potential of emerging technologies, big-data approaches, and breaking down research silos. Through a case study in stroke research, one of the most well-established areas for synthesis efforts, we demonstrate the progress in implementing elements of this ecosystem, with an emphasis on the need for coordinated efforts between laboratory researchers and synthesists.}
}
@article{LIU2024e32093,
title = {GlyReShot: A glyph-aware model with label refinement for few-shot Chinese agricultural named entity recognition},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32093},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32093},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024081246},
author = {Haitao Liu and Jihua Song and Weiming Peng},
keywords = {Agricultural named entity recognition, Few-shot learning, Glyph features, Agricultural text mining},
abstract = {Chinese agricultural named entity recognition (NER) has been studied with supervised learning for many years. However, considering the scarcity of public datasets in the agricultural domain, exploring this task in the few-shot scenario is more practical for real-world demands. In this paper, we propose a novel model named GlyReShot, integrating the knowledge of Chinese character glyph into few-shot NER models. Although the utilization of glyph has been proven successful in supervised models, two challenges still persist in the few-shot setting, i.e., how to obtain glyph representations and when to integrate them into the few-shot model. GlyReShot handles the two challenges by introducing a lightweight glyph representation obtaining module and a training-free label refinement strategy. Specifically, the glyph representations are generated based on the descriptive sentences by filling the predefined template. As most steps come before training, this module aligns well with the few-shot setting. Furthermore, by computing the confidence values for draft predictions, the refinement strategy selectively utilizes the glyph information only when the confidence values are relatively low, thus mitigating the influence of noise. Finally, we annotate a new agricultural NER dataset and the experimental results demonstrate effectiveness of GlyReShot for few-shot Chinese agricultural NER.}
}
@article{YIN2024108548,
title = {Enhancing bibliographic reference parsing with contrastive learning and prompt learning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108548},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108548},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624007061},
author = {Zhen Yin and Shenghua Wang},
keywords = {Contrastive learning, Prompt learning, Information extraction, Bibliographic reference},
abstract = {Bibliographic references, typically comprising author names, journal titles, paper titles, and publication dates, play a vital role in academic research. Accurately identifying these structured pieces of information from references is a crucial step in developing intelligent bibliographic management systems. However, existing methods often rely on extensive high-quality training data. To mitigate the reliance on extensive training data, we propose a method that integrates prompt learning and contrastive learning for extracting structured information from bibliographic references, named CONT_Prompt_ParseRef. This approach aims to utilize contrastive learning to deepen the understanding of different metadata label types and employ prompt learning to provide specific guidelines for processing and recognition. We constructed a dataset comprising 12,000 samples, available in both Chinese and English versions. The experimental results on this bilingual dataset demonstrate the model's superior performance over existing techniques. Notably, CONT_Prompt_ParseRef shows remarkable robustness in low-resource environments, particularly in scenarios with limited training data, both contrastive and prompt learning play pivotal roles in label extraction from bibliographic references. The ablation study illustrates that omitting either component leads to a decline in performance, with contrastive learning being slightly more influential.}
}
@article{CHEN2024,
title = {Digital Information Ecosystems in Modern Care Coordination and Patient Care Pathways and the Challenges and Opportunities for AI Solutions},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/60258},
url = {https://www.sciencedirect.com/science/article/pii/S143888712400894X},
author = {You Chen and Christoph U Lehmann and Bradley Malin},
keywords = {patient care pathway, care journey, care coordination, digital information ecosystem, digital technologies, artificial intelligence, information interoperability, information silos, workload, information retrieval, care transitions, patient-reported outcome measures, clinical workflow, usability, user experience workflow, health care information systems, networks of health care professionals, patient information flow},
abstract = {The integration of digital technologies into health care has significantly enhanced the efficiency and effectiveness of care coordination. Our perspective paper explores the digital information ecosystems in modern care coordination, focusing on the processes of information generation, updating, transmission, and exchange along a patient’s care pathway. We identify several challenges within this ecosystem, including interoperability issues, information silos, hard-to-map patient care journeys, increased workload on health care professionals, coordination and communication gaps, and compliance with privacy regulations. These challenges are often associated with inefficiencies and diminished care quality. We also examine how emerging artificial intelligence (AI) tools have the potential to enhance the management of patient information flow. Specifically, AI can boost interoperability across diverse health systems; optimize and monitor patient care pathways; improve information retrieval and care transitions; humanize health care by integrating patients’ desired outcomes and patient-reported outcome measures; and optimize clinical workflows, resource allocation, and digital tool usability and user experiences. By strategically leveraging AI, health care systems can establish a more robust and responsive digital information ecosystem, improving care coordination and patient outcomes. This perspective underscores the importance of continued research and investment in AI technologies in patient care pathways. We advocate for a thoughtful integration of AI into health care practices to fully realize its potential in revolutionizing care coordination.}
}
@article{LUO2024124631,
title = {Graph Contrastive Topic Model},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124631},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124631},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424014982},
author = {Zheheng Luo and Lei Liu and Sophia Ananiadou and Qianqian Xie},
keywords = {Neural topic modelling, Contrastive learning, Graph neural networks, Document representations},
abstract = {Contrastive learning has recently been introduced into neural topic models (NTMs) to improve latent semantic discovery, but existing methods suffer from the sample bias problem owing to word frequency-based sampling strategy, which may result in false negative samples with similar semantics to the prototypes. We propose the novel graph contrastive neural topic model (GCTM), based on the graph-based sampling strategy, guided by the in-depth correlation and irrelevance information among documents and words. We model the input document as the document word bipartite graph (DWBG) and construct positive and negative word co-occurrence graphs (WCGs), to capture in-depth semantic correlation and irrelevance among words. Based on the DWBG and WCGs, we design the document-word information propagation (DWIP) process to perform the edge perturbation of DWBG, based on multi-hop correlations/irrelevance among documents and words. This yields the desired negative and positive samples, which are utilized for GCL together with the prototypes to improve learning document topic representations and latent topics. Experiments on several benchmark datasets demonstrate the effectiveness of our method for topic coherence and document representation learning compared with existing state-of-the-art methods.}
}
@article{FRICONNET202542,
title = {Phenomenal consciousness is alien to us: SETI and the fermi paradox seen through the prism of illusionism and attention schema theory},
journal = {Acta Astronautica},
volume = {226},
pages = {42-49},
year = {2025},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2024.10.023},
url = {https://www.sciencedirect.com/science/article/pii/S0094576524005976},
author = {Guillaume Friconnet},
keywords = {Extra-terrestrial intelligence, Fermi paradox, Consciousness, Illusionism, Attention schema theory},
abstract = {Illusionism is an eliminativist position about qualia stating that phenomenal consciousness is nothing more than an introspective illusion. The attention schema theory (AST) relates this philosophical stance to a large body of experimental data and states that phenomenal consciousness arises from an internal model of attention control. In this paper, I intend to show that AST and illusionism have significant implications both in the search for extra-terrestrial intelligence and in the explanation of Fermi paradox. Firstly, on the basis of findings concerning the evolutionary history of phenomenal consciousness on Earth, I argue that extraterrestrial biological life is likely to experience phenomenality. In the second part, I set AST in the context of a post-biological universe, where artificial intelligence (AI) is the dominant form of intelligence. I argue that phenomenal consciousness is probably present in these entities, and that they could even be super-conscious. Finally, I show that because phenomenality grounds value, illusionism has profound revisionary consequences in the field of ethics. This reconsideration of the justifiability of our values paves the way to AI misalignment and may be the source of neocatastrophic scenarios that explain to Fermi paradox.}
}
@article{DARVISHI2024104967,
title = {Impact of AI assistance on student agency},
journal = {Computers & Education},
volume = {210},
pages = {104967},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104967},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523002440},
author = {Ali Darvishi and Hassan Khosravi and Shazia Sadiq and Dragan Gašević and George Siemens},
keywords = {AI in education, Student agency, Peer feedback, Educational technology},
abstract = {AI-powered learning technologies are increasingly being used to automate and scaffold learning activities (e.g., personalised reminders for completing tasks, automated real-time feedback for improving writing, or recommendations for when and what to study). While the prevailing view is that these technologies generally have a positive effect on student learning, their impact on students’ agency and ability to self-regulate their learning is under-explored. Do students learn from the regular, detailed and personalised feedback provided by AI systems, and will they continue to exhibit similar behaviour in the absence of assistance? Or do they instead continue to rely on AI assistance without learning from it? To contribute to filling this research gap, we conducted a randomised controlled experiment that explored the impact of AI assistance on student agency in the context of peer feedback. With 1625 students across 10 courses, an experiment was conducted using peer review. During the initial four-week period, students were guided by AI features that utilised techniques such as rule-based suggestion detection, semantic similarity, and comparison with previous comments made by the reviewer to enhance their submissions if the feedback provided was deemed insufficiently detailed or general in nature. Over the following four weeks, students were divided into four different groups: control (AI) received prompts, (NR) received no prompts, (SR) received self-monitoring checklists in place of AI prompts, and (SAI) had access to both AI prompts and self-monitoring checklists. Results of the experiment suggest that students tended to rely on rather than learn from AI assistance. If AI assistance was removed, self-regulated strategies could help fill the gap but were not as effective as AI assistance. Results also showed that hybrid human-AI approaches that complement AI assistance with self-regulated strategies (SAI) were not more effective than AI assistance on its own. We conclude by discussing the broader benefits, challenges and implications of relying on AI assistance in relation to student agency in a world where we learn, live and work with AI.}
}
@article{FAROOQ2024891,
title = {Artificial intelligence in plant breeding},
journal = {Trends in Genetics},
volume = {40},
number = {10},
pages = {891-908},
year = {2024},
issn = {0168-9525},
doi = {https://doi.org/10.1016/j.tig.2024.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0168952524001677},
author = {Muhammad Amjad Farooq and Shang Gao and Muhammad Adeel Hassan and Zhangping Huang and Awais Rasheed and Sarah Hearne and Boddupalli Prasanna and Xinhai Li and Huihui Li},
keywords = {artificial intelligence, plant breeding, genetic gain, big data, deep learning},
abstract = {Harnessing cutting-edge technologies to enhance crop productivity is a pivotal goal in modern plant breeding. Artificial intelligence (AI) is renowned for its prowess in big data analysis and pattern recognition, and is revolutionizing numerous scientific domains including plant breeding. We explore the wider potential of AI tools in various facets of breeding, including data collection, unlocking genetic diversity within genebanks, and bridging the genotype–phenotype gap to facilitate crop breeding. This will enable the development of crop cultivars tailored to the projected future environments. Moreover, AI tools also hold promise for refining crop traits by improving the precision of gene-editing systems and predicting the potential effects of gene variants on plant phenotypes. Leveraging AI-enabled precision breeding can augment the efficiency of breeding programs and holds promise for optimizing cropping systems at the grassroots level. This entails identifying optimal inter-cropping and crop-rotation models to enhance agricultural sustainability and productivity in the field.}
}
@article{SANEMETERIODELAPARTE2024101030,
title = {Spatio-temporal semantic data management systems for IoT in agriculture 5.0: Challenges and future directions},
journal = {Internet of Things},
volume = {25},
pages = {101030},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.101030},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523003530},
author = {Mario {San Emeterio de la Parte} and José-Fernán Martínez-Ortega and Pedro Castillejo and Néstor Lucas-Martínez},
keywords = {Data science, Internet of Things (IoT), Big data, Agriculture, Spatio-temporal semantic data management system (STSDMS)},
abstract = {The Agri-Food sector is in a stressful situation due to the high demand for food from the growing population around the world. The agricultural sector is facing a challenging situation; it must increase production and reduce its impact on the environment by appropriately allocating resources, adapting to climate change, and avoiding food waste. Agriculture 5.0, as the fifth agricultural evolution, aims to offer a perfect symbiosis between agriculture, advanced technologies, and sustainability. The most advanced technologies in automation, monitoring, and decision support are driven by the collection and processing of large volumes of agricultural data, such as weather information, farm machinery, soil and crop conditions, and marketing demand for higher profits. Taking advantage of the technological paradigm of the Internet of Things, agricultural data provides information on spatial, temporal, and semantic dimensions. Spatio-temporal semantic data management systems have become the cornerstone for the achievement of Agriculture 5.0 through advanced Internet of Things technologies. This paper aims to review the current literature on spatio-temporal semantic data management systems for Agriculture 5.0. This paper uses a systematic literature review technique to study eleven representative spatio-temporal semantic data management systems. A comprehensive evaluation of the aspects of interoperability, accessibility, scalability, real-time operation capability, etc. is carried out. Based on the evaluation results, future challenges are detected and development trends and possible improvements are proposed for future research. Finally, a distributed architecture capable of satisfying the above needs and challenges is proposed. The paper aims to inspire further research and development efforts to improve the efficiency, accessibility, and performance of spatio-temporal semantic data management systems.}
}
@article{YANG2025129089,
title = {Few-shot cyberviolence intent classification with Meta-learning AutoEncoder based on adversarial domain adaptation},
journal = {Neurocomputing},
volume = {620},
pages = {129089},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.129089},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224018605},
author = {Shun Yang and YaJun Du and ShangYi Du and XianYong Li and XiaoLiang Chen and YanLi Li and ChunZhi Xie and Jia Liu},
keywords = {Few-shot learning, Meta-learning, Intent classification, Cyberviolence, AutoEncoder},
abstract = {The phenomenon of cyberviolence has become a critical issue in online security, drawing attention from various stakeholders. A major shortcoming in the previous works is the limitation of using simple methods like ”yes” or ”no” to evaluate cyberviolence utterances, which can significantly restrict netizens’ free speech. Therefore, we provide a novel strategy for detecting cyberviolence utterances based on the user’s real intent. Fine-grained cyberviolence intents are complex, leading to texts that share similar syntactic structures and semantics but differ in intent category. The previous method did not consider this issue. To address this, in this paper, we propose a Meta-learning AutoEncoder (MetaAE) based on adversarial domain adaptation. The goal is to comprehend and learn the inherent logical rules and important semantic knowledge of cyberviolence utterances, specifically targeting fine-grained cyberviolence intent problems. Specifically, we use the autoencoder structure to help the model implement self-supervised learning. This enables the model to comprehend the inherent logical structure of texts with different intent categories and helps the model learn important semantic knowledge of the text during the encoder compression process. At the same time, to solve the problem of overfitting in small samples and multidomain cyberviolence utterance, we introduce domain adversarial learning to align domain features and enhance model robustness. Experimental results on both a real cyberviolence intent classification dataset and a public dataset demonstrate significant improvements. On 5-way 1-shot and 5-shot Chinese and English cyberviolence datasets, MetaAE improved the accuracy by approximately 7.23%, 8.27%, 7.22%, and 5%, respectively. In the public dataset, MetaAE improved accuracy by approximately 2.53% on 5-way 5-shot.11Our code is available at https://github.com/YS19999/Meta-learning-AutoEncoder.}
}
@article{CALISKAN20234895,
title = {Metadata integrity in bioinformatics: Bridging the gap between data and knowledge},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {4895-4913},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023003616},
author = {Aylin Caliskan and Seema Dangwal and Thomas Dandekar},
keywords = {Meta-data, Error, Annotation, Error-transfer, Wrong labelling, Patient data, Control group, Tools overview},
abstract = {In the fast-evolving landscape of biomedical research, the emergence of big data has presented researchers with extraordinary opportunities to explore biological complexities. In biomedical research, big data imply also a big responsibility. This is not only due to genomics data being sensitive information but also due to genomics data being shared and re-analysed among the scientific community. This saves valuable resources and can even help to find new insights in silico. To fully use these opportunities, detailed and correct metadata are imperative. This includes not only the availability of metadata but also their correctness. Metadata integrity serves as a fundamental determinant of research credibility, supporting the reliability and reproducibility of data-driven findings. Ensuring metadata availability, curation, and accuracy are therefore essential for bioinformatic research. Not only must metadata be readily available, but they must also be meticulously curated and ideally error-free. Motivated by an accidental discovery of a critical metadata error in patient data published in two high-impact journals, we aim to raise awareness for the need of correct, complete, and curated metadata. We describe how the metadata error was found, addressed, and present examples for metadata-related challenges in omics research, along with supporting measures, including tools for checking metadata and software to facilitate various steps from data analysis to published research.}
}
@incollection{WOLLOWSKI202515,
title = {2 - Toward a new foundation for AI},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {15-39},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044329246000002X},
author = {Michael Wollowski},
keywords = {Autonomy, Convolutional neural networks as pattern recognizers, Foundation of artificial intelligence, Neural networks, Pattern recognition, Predicted impact of artificial intelligence, Transformers as pattern recognizers},
abstract = {We argue that artificial neural networks (NNs) should be the foundation of AI. We reach back to some early writings on AI in which the earlier authors argued for such an approach. We explain that modern NNs, in particular convolutional neural networks (CNNs), and transformer architectures satisfy the requirements laid out by early workers and critics of AI. We review current practice of teaching AI and contrast those to current successes in NNs and their projected future. We explain limitations of current NNs and highlight approaches designed to overcome them. We show in some detail how current NN architectures suit the desiderata posited by early researchers in the field. We end the chapter by cautioning the desire to build autonomous systems and recommend an approach that ensures that such machines are beneficial.}
}
@article{MARTINSON20242695,
title = {Artificial Intelligence and Machine Learning for Inborn Errors of Immunity: Current State and Future Promise},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
volume = {12},
number = {10},
pages = {2695-2704},
year = {2024},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2024.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S2213219824008286},
author = {Alexandra K. Martinson and Aaron T. Chin and Manish J. Butte and Nicholas L. Rider},
keywords = {Inborn errors of immunity, Artificial intelligence, Machine learning, Phenotyping, Electronic health records, Clinical decision support systems},
abstract = {Artificial intelligence (AI) and machine learning (ML) research within medicine has exponentially increased over the last decade, with studies showcasing the potential of AI/ML algorithms to improve clinical practice and outcomes. Ongoing research and efforts to develop AI-based models have expanded to aid in the identification of inborn errors of immunity (IEI). The use of larger electronic health record data sets, coupled with advances in phenotyping precision and enhancements in ML techniques, has the potential to significantly improve the early recognition of IEI, thereby increasing access to equitable care. In this review, we provide a comprehensive examination of AI/ML for IEI, covering the spectrum from data preprocessing for AI/ML analysis to current applications within immunology, and address the challenges associated with implementing clinical decision support systems to refine the diagnosis and management of IEI.}
}
@article{GE2023104458,
title = {Few-shot learning for medical text: A review of advances, trends, and opportunities},
journal = {Journal of Biomedical Informatics},
volume = {144},
pages = {104458},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104458},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300179X},
author = {Yao Ge and Yuting Guo and Sudeshna Das and Mohammed Ali Al-Garadi and Abeed Sarker},
keywords = {Few-shot learning, Natural language processing, Machine learning, Biomedical informatics},
abstract = {Background:
Few-shot learning (FSL) is a class of machine learning methods that require small numbers of labeled instances for training. With many medical topics having limited annotated text-based data in practical settings, FSL-based natural language processing (NLP) holds substantial promise. We aimed to conduct a review to explore the current state of FSL methods for medical NLP.
Methods:
We searched for articles published between January 2016 and October 2022 using PubMed/Medline, Embase, ACL Anthology, and IEEE Xplore Digital Library. We also searched the preprint servers (e.g., arXiv, medRxiv, and bioRxiv) via Google Scholar to identify the latest relevant methods. We included all articles that involved FSL and any form of medical text. We abstracted articles based on the data source, target task, training set size, primary method(s)/approach(es), and evaluation metric(s).
Results:
Fifty-one articles met our inclusion criteria—all published after 2018, and most since 2020 (42/51; 82%). Concept extraction/named entity recognition was the most frequently addressed task (21/51; 41%), followed by text classification (16/51; 31%). Thirty-two (61%) articles reconstructed existing datasets to fit few-shot scenarios, and MIMIC-III was the most frequently used dataset (10/51; 20%). 77% of the articles attempted to incorporate prior knowledge to augment the small datasets available for training. Common methods included FSL with attention mechanisms (20/51; 39%), prototypical networks (11/51; 22%), meta-learning (7/51; 14%), and prompt-based learning methods, the latter being particularly popular since 2021. Benchmarking experiments demonstrated relative underperformance of FSL methods on biomedical NLP tasks.
Conclusion:
Despite the potential for FSL in biomedical NLP, progress has been limited. This may be attributed to the rarity of specialized data, lack of standardized evaluation criteria, and the underperformance of FSL methods on biomedical topics. The creation of publicly-available specialized datasets for biomedical FSL may aid method development by facilitating comparative analyses.}
}
@article{SALIHOGLU20241376,
title = {Cat-E: A comprehensive web tool for exploring cancer targeting strategies},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {1376-1386},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024000771},
author = {Rana Salihoglu and Johannes Balkenhol and Gudrun Dandekar and Chunguang Liang and Thomas Dandekar and Elena Bencurova},
keywords = {Network analysis, Protein interaction, Cancer pathways, Cancer, Oncolytic virus, Immune modulation},
abstract = {Identifying potential cancer-associated genes and drug targets from omics data is challenging due to its diverse sources and analyses, requiring advanced skills and large amounts of time. To facilitate such analysis, we developed Cat-E (Cancer Target Explorer), a novel R/Shiny web tool designed for comprehensive analysis with evaluation according to cancer-related omics data. Cat-E is accessible at https://cat-e.bioinfo-wuerz.eu/. Cat-E compiles information on oncolytic viruses, cell lines, gene markers, and clinical studies by integrating molecular datasets from key databases such as OvirusTB, TCGA, DrugBANK, and PubChem. Users can use all datasets and upload their data to perform multiple analyses, such as differential gene expression analysis, metabolic pathway exploration, metabolic flux analysis, GO and KEGG enrichment analysis, survival analysis, immune signature analysis, single nucleotide variation analysis, dynamic analysis of gene expression changes and gene regulatory network changes, and protein structure prediction. Cancer target evaluation by Cat-E is demonstrated here on lung adenocarcinoma (LUAD) datasets. By offering a user-friendly interface and detailed user manual, Cat-E eliminates the need for advanced computational expertise, making it accessible to experimental biologists, undergraduate and graduate students, and oncology clinicians. It serves as a valuable tool for investigating genetic variations across diverse cancer types, facilitating the identification of novel diagnostic markers and potential therapeutic targets.}
}
@article{JIANG2024102049,
title = {Artificial intelligence and automation to power the future of chemistry},
journal = {Cell Reports Physical Science},
volume = {5},
number = {7},
pages = {102049},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.102049},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424003187},
author = {Xuefeng Jiang and Sanzhong Luo and Kuangbiao Liao and Shan Jiang and Jing Ma and Jun Jiang and Zhigang Shuai},
abstract = {In our traditional impression of chemical laboratories, researchers wear white coats and safety goggles to conduct experiments. However, many recent developments in the field make use of autonomous synthesis robots with integrated artificial intelligence (AI)-driven machine-learning units. These benchtop devices might outperform human chemists in terms of speed and accuracy, which could accelerate the discovery of molecules and materials for various applications. In this Voices piece, we ask a panel of experts from institutes in China: How are AI and automation shaping the future of chemistry?}
}
@article{RAVI2024200456,
title = {Ideological orientation and extremism detection in online social networking sites: A systematic review},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200456},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200456},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001303},
author = {Kamalakkannan Ravi and Jiann-Shiun Yuan},
keywords = {Extremism detection, Machine learning, Natural language processing, Predictive models, Social media, User-generated content},
abstract = {The rise of social networking sites has reshaped digital interactions, becoming fertile grounds for extremist ideologies, notably in the United States. Despite previous research, understanding and tackling online ideological extremism remains challenging. In this context, we conduct a systematic literature review to comprehensively analyze existing research and offer insights for both researchers and policymakers. Spanning from 2005 to 2023, our review includes 110 primary research articles across platforms like Twitter (X), Facebook, Reddit, TikTok, Telegram, and Parler. We observe a diverse array of methodologies, including natural language processing (NLP), machine learning (ML), deep learning (DL), graph-based methods, dictionary-based methods, and statistical approaches. Through synthesis, we aim to advance understanding and provide actionable recommendations for combating ideological extremism effectively on online social networking sites.}
}
@article{WANG2025109835,
title = {WCG-VMamba: A multi-modal classification model for corn disease},
journal = {Computers and Electronics in Agriculture},
volume = {230},
pages = {109835},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109835},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924012262},
author = {Haoyang Wang and Mingfang He and Minge Zhu and Genhua Liu},
keywords = {Corn disease, Image-text pairs, Classification, Multimodal, VMamba},
abstract = {Corn is one of the important food crops and industrial raw materials. However, maize diseases have seriously affected its yield and quality. In order to effectively identify maize diseases, digital image processing technology has been widely used in the agricultural field. The classification of diseases based on digital images enables early detection of corn diseases, reducing farmers’ losses. Although existing corn disease identification methods have made significant progress using deep learning technology for digital image processing, most of these technologies rely on single-modal data for identification and lack the connection between images and texts. To solve this problem, this paper proposes a cross-modal feature alignment fusion model called WCG-VMamba. Firstly, we propose a wavelet visual Mamba (WAVM) network, which integrates the advantages of different visual coding strategies and can reduce the influence of intrinsic noise and other factors on the validity of image features during extraction. Then, we introduce the Cross Modal Alignment Transformer (CMAT), which interacts image features with text features to capture their semantic correlation and determine the weight distribution of image and text features in the fusion process. We then use Transformer coding blocks to fuse features. Finally, the Gaussian Random Walk Duck Swarm Algorithm (GRW-DSA) is proposed to reduce errors in the duck swarm exploration process through Gaussian Random Walk, aiming to find the optimal learning rate. Experiments on self-built datasets and two common datasets show that WCG-VMamba can be effectively used in the task of corn disease classification. Compared with other excellent models such as MobileViT, MobilenetV3, SwinT, and DINOV2, better results have been achieved, our model achieves a recognition accuracy as high as 96.97%, proving its important practical application in promoting agricultural cross-modal models and corn disease control.}
}
@article{ZHOU2024127424,
title = {Two stages prompting for few-shot multi-intent detection},
journal = {Neurocomputing},
volume = {579},
pages = {127424},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127424},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224001954},
author = {Xingfa Zhou and Lan Yang and Xin Wang and Huayi Zhan and Rui Sun},
keywords = {Multi-intent detection, Few-shot learning, Prompt, Self-attention},
abstract = {This paper focuses on multi-intent detection in the few-shot scenario. Most prior works for multi-intent detection pick intent labels when estimated label-instance relevance scores are above a given threshold. However, these methods often perform poorly since it is nontrivial to precisely estimate the label-instance relevance scores and set an appropriate threshold in the few-shot scenario. In addition, these methods overlook the correlation among intents, which is vital to multi-intent detection. In light of these, we propose a prompt-based fine-tuning (PFT) method to tackle the issue of multi-intent detection in the few-shot scenario. Specifically, we first construct a prompt template to predict the number of intents. According to the number of intents, we then construct an intent prompt template to identify intents. To capture the correlation among intents, we also introduce a multi-view multi-head self-attention scheme based on PFT. Experimental results on two datasets demonstrate that our proposed method significantly outperforms the baselines.}
}
@article{DEWI20244195,
title = {Adjusted Reasoning Module for Deep Visual Question Answering Using Vision Transformer},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {4195-4216},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.057453},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008403},
author = {Christine Dewi and Hanna Prillysca Chernovita and Stephen Abednego Philemon and Christian {Adi Ananta} and Abbott Po Shun Chen},
keywords = {VQA, vision transformer, multimodal data, deep learning},
abstract = {Visual Question Answering (VQA) is an interdisciplinary artificial intelligence (AI) activity that integrates computer vision and natural language processing. Its purpose is to empower machines to respond to questions by utilizing visual information. A VQA system typically takes an image and a natural language query as input and produces a textual answer as output. One major obstacle in VQA is identifying a successful method to extract and merge textual and visual data. We examine “Fusion” Models that use information from both the text encoder and picture encoder to efficiently perform the visual question-answering challenge. For the transformer model, we utilize BERT and RoBERTa, which analyze textual data. The image encoder designed for processing image data utilizes ViT (Vision Transformer), Deit (Data-efficient Image Transformer), and BeIT (Image Transformers). The reasoning module of VQA was updated and layer normalization was incorporated to enhance the performance outcome of our effort. In comparison to the results of previous research, our proposed method suggests a substantial enhancement in efficacy. Our experiment obtained a 60.4% accuracy with the PathVQA dataset and a 69.2% accuracy with the VizWiz dataset.}
}
@article{MAI2025104125,
title = {RAF-AG: Report analysis framework for attack path generation},
journal = {Computers & Security},
volume = {148},
pages = {104125},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104125},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004309},
author = {Khang Mai and Jongmin Lee and Razvan Beuran and Ryosuke Hotchi and Sian En Ooi and Takayuki Kuroda and Yasuo Tan},
keywords = {Automated report analysis, Attack path generation, Graph alignment, Weak supervision, MITRE ATT&CK, Cybersecurity},
abstract = {Information sharing is a key practice in cybersecurity for coping with the ever-changing cyberattacks that are targeting computer systems. Thus, when cyber incidents happen, cyber threat intelligence (CTI) reports are prepared and shared among cybersecurity practitioners to help them get up-to-date information about those incidents. However, reading and analyzing the report text to comprehend the included information is a cumbersome process. Although techniques based on deep learning were proposed to speed up report analysis in order to obtain the enclosed essential information, such as attack path, training data insufficiency makes these methods inefficient in practical circumstances. This paper presents RAF-AG, a report analysis framework for attack path generation. To analyze CTI reports, RAF-AG utilizes the sentence dependency tree for entity and relation extraction, and a weak supervision approach for entity labeling. This is followed by graph building and graph alignment for generating the attack paths. Our approach resolves the data insufficiency problem in the cybersecurity domain by lowering the need for expert involvement. We evaluated RAF-AG by comparing the generated attack paths with those produced by AttacKG, a state-of-the-art automatic report analysis framework. RAF-AG was able to identify cyberattack steps by matching their appearance order inside the report, and link them with techniques from the MITRE ATT&CK knowledge base with an improved F1 score compared to AttacKG (0.708 versus 0.393).}
}
@article{SHIDIK2024100358,
title = {Indonesian disaster named entity recognition from multi source information using bidirectional LSTM (BiLSTM)},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {3},
pages = {100358},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100358},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124001525},
author = {Guruh Fajar Shidik and Filmada Ocky Saputra and Galuh Wilujeng Saraswati and Nurul Anisa Sri Winarsih and Muhammad Syaifur Rohman and Ricardus Anggi Pramunendar and Edi Jaya Kusuma and Danny Oka Ratmana and Valentijn Venus and Pulung Nurtantio Andono and Zainal Arifin Hasibuan},
keywords = {Disaster emergency information, Named entity recognition, Deep learning, BiLSTM networks, Random oversampling},
abstract = {Precise logistic support is essential after a disaster occurs. It must be timely, accurate, targeted, and based on existing needs. However, obtaining sufficient and accurate information related to logistic distribution locations remains a key problem. Therefore, implementing Named Entity Recognition (NER) can address this issue. In recent years, news coverage through Indonesian digital news media and social media accounts has emerged as a promising source for building a disaster data corpus. This study implemented NER to extract and identify named entities from text-based information, particularly from Indonesian digital news media. In addition to using regular entities from the NER standard, this study introduced new entities specialized for disaster-related information, including DISASTER, SCALE, SUPPLIES, CASUALTIES, and OUTSIDE. The new disaster corpus in the Indonesian language for the NER model was obtained with an imbalanced dataset composition. To overcome this problem, random oversampling was applied. This study also utilized the BiLSTM model to recognize each entity in new textual information, evaluating its performance when the proposed Indonesian disaster corpus was used as a training reference in the deep learning model. Several optimization algorithms applied in BiLSTM were evaluated. The results showed improved BiLSTM performance using Adam optimization and a balanced corpus. Performance indicators achieved were 93.4 %, 82.4 %, and 87.5 % for precision, recall, and F1-score, respectively. The BiLSTM network captured long-range dependencies in sequential data provided by NER. Oversampling ensured that the proposed NER model could precisely recognize all entities and reduce biased results. Thus, the BiLSTM method can better identify entities in the textual corpus of Indonesian disaster-related online news.}
}
@article{XIE2025108561,
title = {Do discussions in human-computer communities trigger group polarization? Insights from the media evocation paradigm},
journal = {Computers in Human Behavior},
volume = {165},
pages = {108561},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108561},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225000081},
author = {Zehang Xie and Shuoshuo Li and Wu Li},
keywords = {Group polarization, AIGC, Human-computer communities, Media evocation paradigm, Spiral of silence},
abstract = {This study investigates group polarization and the spiral of silence within human-computer communities, particularly as intelligent chatbots become increasingly integrated into online interactions. Grounded in the media evocation paradigm, two experiments were conducted: Experiment 1, using a 4 × 4 design, explored how varying stances held by humans and chatbots influence group polarization, revealing distinct polarization mechanisms based on the differing stances within online communities. Experiment 2 employed a 3 × 4 design to examine the impact of human identifiability on the spiral of silence, finding that higher identifiability led to increased conformity to majority opinions, while full anonymity intensified the spiral of silence, especially when chatbots held strong stances. These results contribute to the understanding of group polarization, the media evocation paradigm, and the spiral of silence within human-computer communities.}
}
@article{LIN2023101830,
title = {Automated scholarly paper review: Concepts, technologies, and challenges},
journal = {Information Fusion},
volume = {98},
pages = {101830},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101830},
url = {https://www.sciencedirect.com/science/article/pii/S156625352300146X},
author = {Jialiang Lin and Jiaxin Song and Zhangping Zhou and Yidong Chen and Xiaodong Shi},
keywords = {Automated scholarly paper review, Peer review, Academic publishing, Natural language processing, Artificial intelligence},
abstract = {Peer review is a widely accepted mechanism for research evaluation, playing a pivotal role in academic publishing. However, criticisms have long been leveled at this mechanism, mostly because of its poor efficiency and low reproducibility. Recent years have seen the application of artificial intelligence (AI) in assisting the peer review process. Nonetheless, with the involvement of humans, such limitations remain inevitable. In this paper, we propose the concept and pipeline of automated scholarly paper review (ASPR) and review the relevant literature and technologies of achieving a full-scale computerized review process. On the basis of the review and discussion, we conclude that there is already corresponding research and preliminary implementation at each stage of ASPR. We further look into the challenges in ASPR with the existing technologies. The major difficulties lie in inadequate data, imperfect document parsing and representation, defective human–computer interaction, and flawed deep logical reasoning. Moreover, we point out the future directions and discuss the possible moral and ethical issues of ASPR. In the foreseeable future, ASPR and peer review will coexist in a reinforcing manner before ASPR is able to fully undertake the reviewing workload from humans.}
}
@article{ALFARHOOD202453,
title = {CAML: A Context-Aware Metric Learning approach for improved recommender systems},
journal = {Alexandria Engineering Journal},
volume = {100},
pages = {53-60},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.05.025},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824004952},
author = {Sultan Alfarhood and Meshal Alfarhood},
keywords = {Recommender systems, Collaborative filtering, Metric learning, Attention mechanism},
abstract = {The primary goal of recommender systems is to identify and propose items that users might find appealing. A large number of these systems are heavily dependent on explicit interactions between the user and the item, which can often be infrequent. In this work, we introduce a unique model known as Context-Aware Metric Learning (CAML), designed to enhance the effectiveness of recommendations. The CAML model utilizes an attentive autoencoder to extract latent features from contextual context and incorporates these features into a metric learning framework. In particular, these extracted features act as a Gaussian prior for the embeddings of the items, thereby enhancing the precision of their positioning in the latent space. This integration not only boosts the precision of the recommendations but also increases computational efficiency, rendering CAML appropriate for both offline and online application scenarios. Our model’s evaluation on two real-world datasets reveals that it outperforms several existing baseline models, including those that do not incorporate contextual information such as CML and CPE, as well as other contextual recommendation models like CDL, CATA, and CML+F.}
}
@article{PRATT2023103217,
title = {Bringing advanced technology to strategic decision-making: The Decision Intelligence/Data Science (DI/DS) Integration framework},
journal = {Futures},
volume = {152},
pages = {103217},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103217},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723001222},
author = {Lorien Pratt and Christophe Bisson and Thierry Warin},
keywords = {Strategic decision making, Decision intelligence, Corporate foresight, Artificial intelligence, Complexity, Kahneman’s systems thinking},
abstract = {There is a widespread stated desire amongst both public and private organizations worldwide to engage in more significant “evidence-based reasoning” and to be more “data-driven.” We argue that these two goals are proxies for the often-unstated goal of improving the exploration of possible futures as foresights that could lead to better strategic decisions and improved business outcomes. From this perspective, data and analytics hold great promise and are necessary—but not sufficient—for improving strategic decision-making. Something more is needed to realize this potential. We specify how to fill this gap using an integration framework between technology and decision-makers, which is especially appropriate in complex and/or volatile environments. Our solution—which comprises a methodology as well as a software architecture—therefore unifies not only human decision makers to technology but each other and also integrates several disciplines that have been hitherto unnecessarily separated. Thereby, it could help organizations to address increasing challenges better as well as improve the exploration of possible futures.}
}
@article{XING2024102534,
title = {Voices in the digital storm: Unraveling online polarization with ChatGPT},
journal = {Technology in Society},
volume = {77},
pages = {102534},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102534},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24000824},
author = {Yunfei Xing and Justin Zuopeng Zhang and Guangqing Teng and Xiaotang Zhou},
keywords = {ChatGPT, Opinion polarization, social media, BERTopic, BERTSentiment},
abstract = {ChatGPT, an esteemed natural language processing model, has demonstrated remarkable capabilities in intelligent text generation, interactive conversation, and myriad additional tasks. The utilization of ChatGPT has generated a wide debate among users with different attitudes on social media platforms, culminating in the phenomenon of polarization. Based on confirmation bias theory, this paper presented a theoretical framework that elucidates the process of online polarization. Subsequently, we develop the sentiment classification (BERTSentiment) and topic identification (BERTopic) model leveraging the pre-trained BERT (Bidirectional Encoder Representations from Transformers) model. To empirically investigate the public sentiment regarding ChatGPT, an in-depth study was conducted on the X platform. The results indicate that although a small portion of users (approximately 10%) express negative sentiments regarding ChatGPT's ethical considerations, functionality, and accuracy, the majority of users exhibit either positive or neutral views. Among the public concerns, AI and bot functions, response quality, instant messaging, enterprise applications, and technological aspects emerge as the most prominent topics. This study sheds light on public perceptions regarding the progress and integration of emerging technologies. Moreover, it introduces a fresh data mining perspective that enhances our understanding of polarization in the context of social media research.}
}
@article{HE2025103901,
title = {Few-shot cross domain event discovery in narrative text},
journal = {Information Processing & Management},
volume = {62},
number = {1},
pages = {103901},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103901},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324002607},
author = {Ruifang He and Fei Huang and Jinsong Ma and Jinpeng Zhang and Yongkai Zhu and Shiqi Zhang and Jie Bai},
keywords = {Event discovery, Few-shot domain adaptation, Positive–negative balanced sampling, Parameter adapter},
abstract = {Cross-domain event detection presents notable challenges in the form of data scarcity, and existing few-shot algorithms only consider events whose types are predefined, resulting in low coverage or excessive trivial identification results. To address this issue, this paper proposes the task Few-shot Cross Domain Event Discovery, which includes two subtasks: Domain Event Discovery and Few-shot Domain Adaptation. The former aims to identify the type-agnostic event triggers, and the latter completes domain adaptation with only a few annotated domain samples. Additionally, we introduce a positive–negative balanced sampling mechanism and a novel domain parameter adapter for these two subtasks, respectively. Extensive experiments on the DuEE dataset and the ACE2005 dataset show that our proposed method outperforms the current state-of-the-art method by 6.3% in Mix-F1 score on average. Moreover, we achieve SOTA performance in all domains of the DuEE dataset.}
}
@article{FRANCIA2025100310,
title = {Automating materiality assessment with a data-driven document-based approach},
journal = {International Journal of Information Management Data Insights},
volume = {5},
number = {1},
pages = {100310},
year = {2025},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100310},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000995},
author = {Matteo Francia and Enrico Gallinucci and Matteo Golfarelli},
keywords = {Materiality assessment, Decision support system, Information retrieval, Sustainability reporting},
abstract = {Materiality assessment is a critical process for companies to understand the interest perceived by its stakeholders towards topics related to environmental, social, and governance issues. Materiality assessment helps companies define their growth and communicative strategies; recently, it has become crucial within sustainability reporting, i.e., the practice of annually declaring the activities conducted to pursue economic growth in a sustainable way for society. In this paper, we propose a data-driven and automated approach to carry out materiality assessment. Stakeholders’ perception of important topics is obtained by analyzing relevant textual documents (e.g., company reports, press releases, social media posts), identifying mentions of potentially interesting topics, and converting them to scores that produce materiality rankings or matrices. An iterative methodology is proposed to incrementally carry out materiality assessment by progressively building the domain knowledge required to automate the process. Efficiency and effectiveness evaluations are carried out in a real-world scenario.}
}
@article{JIANG2025100566,
title = {AI drug discovery tools and analysis technology: New methods aid in studying the compatibility of Traditional Chinese Medicine},
journal = {Pharmacological Research - Modern Chinese Medicine},
volume = {14},
pages = {100566},
year = {2025},
issn = {2667-1425},
doi = {https://doi.org/10.1016/j.prmcm.2024.100566},
url = {https://www.sciencedirect.com/science/article/pii/S2667142524002082},
author = {Qiwu Jiang and Suhan Yang and Shan He and Fei Li},
keywords = {Traditional Chinese Medicine compatibility, AI drug discovery tool, Combination prediction, Compatibility mechanisms, Compatibility ratio optimization},
abstract = {Introduction
The compatibility of Traditional Chinese Medicine (TCM) holds the potential for reducing toxicity and enhancing efficacy, serving as a crucial guide for the clinical application of TCM. In recent years, the development of artificial intelligence (AI) drug discovery tools has introduced novel approaches for analyzing the multichemical components of TCM, thereby saving time and efforts in experiments.
Methods
The keywords "Traditional Chinese Medicine" and "Artificial Intelligence", "Traditional Chinese Medicine" and "drug compatibility" were searched across various literature databases, including Web of Science, Google Scholar, PubMed, and Elsevier. Over 100 articles were reviewed, and after narrowing the selection to those focused on compatibility, the chosen studies were carefully analyzed to summarize the latest developments for this review.
Results
The review introduce AI drug discovery tools, including virtual screening, target prediction, ADMET prediction, and data mining, along with their roles in studying TCM compatibility. The results further provide insights of AI's application in TCM combination prediction, TCM compatibility mechanisms, and optimization of TCM compatibility ratio within the TCM compatibility research field.
Discussion
Traditional Chinese Medicine uses holistic formulas involving multiple components, targets, and pathways for disease treatment, but scientific explanations of these formulas are limited. AI aids TCM research by predicting combinations, mechanisms, and optimizing ratios, which improves efficiency and reducing costs. However, AI predictions may not be definitely accurate, and traditional expertise is still essential for validation. Future applications of AI in TCM require improved tools and collaboration between AI and TCM researchers.}
}
@article{LV2024115460,
title = {iSUMO-RsFPN: A predictor for identifying lysine SUMOylation sites based on multi-features and feature pyramid networks},
journal = {Analytical Biochemistry},
volume = {687},
pages = {115460},
year = {2024},
issn = {0003-2697},
doi = {https://doi.org/10.1016/j.ab.2024.115460},
url = {https://www.sciencedirect.com/science/article/pii/S0003269724000046},
author = {Zhe Lv and Xin Wei and Siqin Hu and Gang Lin and Wangren Qiu},
keywords = {SUMOylation, Feature extraction, Deep learning, Ensemble learning, Pre-training model},
abstract = {SUMOylation is a protein post-translational modification that plays an essential role in cellular functions. For predicting SUMO sites, numerous researchers have proposed advanced methods based on ordinary machine learning algorithms. These reported methods have shown excellent predictive performance, but there is room for improvement. In this study, we constructed a novel deep neural network Residual Pyramid Network (RsFPN), and developed an ensemble deep learning predictor called iSUMO-RsFPN. Initially, three feature extraction methods were employed to extract features from samples. Following this, weak classifiers were trained based on RsFPN for each feature type. Ultimately, the weak classifiers were integrated to construct the final classifier. Moreover, the predictor underwent systematically testing on an independent test dataset, where the results demonstrated a significant improvement over the existing state-of-the-art predictors. The code of iSUMO-RsFPN is free and available at https://github.com/454170054/iSUMO-RsFPN.}
}
@article{XIAO2024107442,
title = {MSGVUL: Multi-semantic integration vulnerability detection based on relational graph convolutional neural networks},
journal = {Information and Software Technology},
volume = {170},
pages = {107442},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107442},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000478},
author = {Wei Xiao and Zhengzhang Hou and Tao Wang and Chengxian Zhou and Chao Pan},
keywords = {Vulnerability detection, Code representation, Program slicing, Graph convolutional neural networks},
abstract = {Software security has drawn extensive attention as software projects have grown increasingly large and complex. Since the traditional manual or equipment vulnerability detection technology cannot meet today's software development needs, there is a recognized need to create more effective techniques to address security issues. Although various vulnerability detection systems have been proposed, most are based only on serialization or graph representation, to inadequate effect. We propose a system, MSGVUL, that provides superior vulnerability detection using a new multi-semantic approach. MSGVUL uses versatile and efficient code slicing employing a search algorithm based on sensitive data and functions and innovatively constructs an SSVEC model to fully integrate the semantic and structural information into the code. We also developed a novel BAG model, made up of BAP and PAG frameworks, that enables the hierarchical extraction of code vulnerability representations from the graph and sequence levels. The MSGVUL model is evaluated on slice-level and function-level vulnerability datasets, and the results demonstrate that the MSGVUL method outperforms other state-of-the-art methods.}
}
@incollection{2024243,
title = {Index},
editor = {Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth},
booktitle = {Trolley Crash},
publisher = {Academic Press},
pages = {243-248},
year = {2024},
isbn = {978-0-443-15991-6},
doi = {https://doi.org/10.1016/B978-0-44-315991-6.00021-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159916000212}
}
@article{LI2024104621,
title = {Artificial intelligence-powered pharmacovigilance: A review of machine and deep learning in clinical text-based adverse drug event detection for benchmark datasets},
journal = {Journal of Biomedical Informatics},
volume = {152},
pages = {104621},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104621},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400039X},
author = {Yiming Li and Wei Tao and Zehan Li and Zenan Sun and Fang Li and Susan Fenton and Hua Xu and Cui Tao},
keywords = {Pharmacovigilance, Machine learning/Deep learning, Adverse drug event (ADE) extraction, named-entity recognition (NER), Relation extraction (RE), Natural language processing (NLP)},
abstract = {Objective
The primary objective of this review is to investigate the effectiveness of machine learning and deep learning methodologies in the context of extracting adverse drug events (ADEs) from clinical benchmark datasets. We conduct an in-depth analysis, aiming to compare the merits and drawbacks of both machine learning and deep learning techniques, particularly within the framework of named-entity recognition (NER) and relation classification (RC) tasks related to ADE extraction. Additionally, our focus extends to the examination of specific features and their impact on the overall performance of these methodologies. In a broader perspective, our research extends to ADE extraction from various sources, including biomedical literature, social media data, and drug labels, removing the limitation to exclusively machine learning or deep learning methods.
Methods
We conducted an extensive literature review on PubMed using the query “(((machine learning [Medical Subject Headings (MeSH) Terms]) OR (deep learning [MeSH Terms])) AND (adverse drug event [MeSH Terms])) AND (extraction)”, and supplemented this with a snowballing approach to review 275 references sourced from retrieved articles.
Results
In our analysis, we included twelve articles for review. For the NER task, deep learning models outperformed machine learning models. In the RC task, gradient Boosting, multilayer perceptron and random forest models excelled. The Bidirectional Encoder Representations from Transformers (BERT) model consistently achieved the best performance in the end-to-end task. Future efforts in the end-to-end task should prioritize improving NER accuracy, especially for 'ADE' and 'Reason'.
Conclusion
These findings hold significant implications for advancing the field of ADE extraction and pharmacovigilance, ultimately contributing to improved drug safety monitoring and healthcare outcomes.}
}
@article{GACHKAR2025144448,
title = {Text-based algorithms for automating life cycle inventory analysis in building sector life cycle assessment studies},
journal = {Journal of Cleaner Production},
volume = {486},
pages = {144448},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.144448},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624038976},
author = {Sadaf Gachkar and Darya Gachkar and Erfan Ghofrani and Antonio {García Martínez} and Cecilio {Angulo Bahón}},
keywords = {Life cycle assessment, Inventory data collection, Text-based algorithms, Sustainability, Artificial intelligence, Natural language processing},
abstract = {Life Cycle Assessment (LCA) is essential for evaluating the environmental impact of sustainable activities in industry. Despite its importance, there exist challenges negatively impacting its deployment, particularly the time-consuming process of gathering inventory data. This research introduces a novel framework that leverages advanced text-based algorithms from Natural Language Processing (NLP), significantly enhancing the efficiency of data collection in LCA studies. Focusing on the inventory phase, the novelty of this research lies in its ability to reduce data collection time by an estimated 80%–90% compared to conventional methods and improve accuracy by directly extracting materials from bills of quantities (BoQs), which usually list all the construction materials. While our methodology shows promise, it faces challenges due to project complexity, particularly the need for consistent terminology between BoQ and reference databases, though future advancements in matching algorithms may enhance our approach’s efficiency. Real-world case studies demonstrate the framework’s effectiveness, offering flexibility across industries and system complexities.}
}
@article{SULIS2024105949,
title = {Introduction for computer law and security review: special issue “knowledge management for law”},
journal = {Computer Law & Security Review},
volume = {52},
pages = {105949},
year = {2024},
issn = {0267-3649},
doi = {https://doi.org/10.1016/j.clsr.2024.105949},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924000165},
author = {Emilio Sulis and Luigi Di Caro and Rohan Nanda}
}
@article{CHAMAKURI2025100343,
title = {A systematic review on recent methods on deep learning for automatic detection of Alzheimer's disease},
journal = {Medicine in Novel Technology and Devices},
volume = {25},
pages = {100343},
year = {2025},
issn = {2590-0935},
doi = {https://doi.org/10.1016/j.medntd.2024.100343},
url = {https://www.sciencedirect.com/science/article/pii/S2590093524000596},
author = {Radhakrishna Chamakuri and Hyma Janapana},
keywords = {Alzheimer disease diagnosis, Artificial intelligence, Deep learning, Ensemble learning, Machine learning},
abstract = {Alzheimer's disease (AD) is the most frequent cause of dementia, however, and it is caused by a number of different disorders. With regard to the elderly population all over the world, Alzheimer's disease is the seventh largest cause of mortality, disability, and reliance. Depression, social isolation, inactivity, alcohol, smoking, obesity, diabetes, high blood pressure, and age are all variables that can increase the likelihood of getting dementia. Other risk factors include social isolation, depression, and smoking. A diagnosis of Alzheimer's disease at an earlier stage may improve the odds of receiving care and therapy. Medical professionals often diagnose AD based on a limited number of symptoms. On the other hand, it is now possible to identify and categorize Alzheimer's disease (AD) because of technological advancements such as artificial intelligence (AI). However, to identify the current AI-enabled approaches, we must conduct an investigation into the state of the art. This breakthrough in diagnosis methodologies will enable the development of the Clinical Decision Support System (CDSS), capable of automatically diagnosing Alzheimer's disease (AD) without human expertise. In this publication, we conduct a systematic review of sixty research articles previously reviewed by other researchers. The systematic review sheds light on the synthesis of new knowledge and ideas. This study discusses the current approaches for machine learning, deep learning methods, ensemble models, transfer learning, and methods used for early Alzheimer's disease diagnosis. This paper provides answers to a large number of research issues and synthesizes fresh information that is helpful to the reader on many elements of AI-enabled approaches for Alzheimer's disease diagnosis. In addition, it has the potential to stimulate additional research into more effective methods of computer-based intelligent identification of Alzheimer's disease.}
}
@incollection{BRUTZMAN2024189,
title = {Chapter Eleven - Designing meaningful metrics to demonstrate ethical supervision of autonomous systems: How do you measure that?},
editor = {Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth},
booktitle = {Trolley Crash},
publisher = {Academic Press},
pages = {189-208},
year = {2024},
isbn = {978-0-443-15991-6},
doi = {https://doi.org/10.1016/B978-0-44-315991-6.00017-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159916000170},
author = {Don Brutzman and Curtis Blais},
keywords = {Ethics, ethical AI, autonomy, metrics, negligence, human–machine teams, Autonomous Vehicle Command Language (AVCL), Mission Execution Ontology (MEO), Dimensions of Autonomous Decision Making (DADM), TestDevOps, virtual environments, trust},
abstract = {Design and testing of meaningful metrics for artificial intelligence (AI) guiding ethical robots holds fundamental importance for useful progress and trustable operations. Moral responsibility and authority for ethical behaviors by remote autonomous systems ultimately lies with the humans responsible for unleashing individual robots. Lines of success or failure are sharply defined when delegating tasks to robots which have the capacity for life-saving or lethal force. Goals, constraints, and metrics that are commonly defined and shared by humans and robots can be mutually understood, formally verifiable as consistent, and further testable in repeatable ways. Metrics for AI are essential, as illustrated by the diverse topics explored throughout this book. It is interesting that commonplace gaps in applied AI often derive from “Here are the measurements we know how to take” which are too easily over-extrapolated or over-simplified into conclusions matching prior preconceptions. In other words, legacy metrics are appealing but might not broadly apply to general situations. We assert that necessary subsequent questions are “How do we define meaningful objectives and outcomes for a current autonomous system?” and “How do we measure those characteristics that indicate expected success/failure?” Since testing drives system evolution, such questions then become “Once we can measure meaningful results, how do we assemble exemplars into test suites that confirm successful completion across ongoing system lifecycles?” This chapter explores potential design principles for metrics that test ethical AI systems, in both real and virtual system frameworks. Such comprehensive test frameworks are essential for achieving meaningful human authority over autonomous robots. Final success is measurable when trust is achieved.}
}
@article{ASLAM2023110494,
title = {Attention-based multimodal sentiment analysis and emotion recognition using deep neural networks},
journal = {Applied Soft Computing},
volume = {144},
pages = {110494},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110494},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623005124},
author = {Ajwa Aslam and Allah Bux Sargano and Zulfiqar Habib},
keywords = {Sentiment analysis, Emotion recognition, Multimodal attention, Deep neural networks},
abstract = {There has been a growing interest in multimodal sentiment analysis and emotion recognition in recent years due to its wide range of practical applications. Multiple modalities allow for the integration of complementary information, improving the accuracy and precision of sentiment and emotion recognition tasks. However, working with multiple modalities presents several challenges, including handling data source heterogeneity, fusing information, aligning and synchronizing modalities, and designing effective feature extraction techniques that capture discriminative information from each modality. This paper introduces a novel framework called “Attention-based Multimodal Sentiment Analysis and Emotion Recognition (AMSAER)” to address these challenges. This framework leverages intra-modality discriminative features and inter-modality correlations in visual, audio, and textual modalities. It incorporates an attention mechanism to facilitate sentiment and emotion classification based on visual, textual, and acoustic inputs by emphasizing relevant aspects of the task. The proposed approach employs separate models for each modality to automatically extract discriminative semantic words, image regions, and audio features. A deep hierarchical model is then developed, incorporating intermediate fusion to learn hierarchical correlations between the modalities at bimodal and trimodal levels. Finally, the framework combines four distinct models through decision-level fusion to enable multimodal sentiment analysis and emotion recognition. The effectiveness of the proposed framework is demonstrated through extensive experiments conducted on the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset. The results confirm a notable performance improvement compared to state-of-the-art methods, attaining 85% and 93% accuracy for sentiment analysis and emotion classification, respectively. Additionally, when considering class-wise accuracy, the results indicate that the “angry” emotion and “positive” sentiment are classified more effectively than the other emotions and sentiments, achieving 96.80% and 93.14% accuracy, respectively.}
}
@article{LU2024102380,
title = {Privacy-preserving data integration and sharing in multi-party IoT environments: An entity embedding perspective},
journal = {Information Fusion},
volume = {108},
pages = {102380},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102380},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524001581},
author = {Junyu Lu and Henry Leung and Nan Xie},
keywords = {Data integration & sharing, Privacy preservation, Entity embedding},
abstract = {The increasing prevalence of IoT applications highlights the urgency for insightful data fusion and information acquisition, boosting data integration and sharing needs. However, challenges arise in multi-party data sharing due to inherent data heterogeneity and privacy concerns. To address these issues, this paper discusses the feasibility of using embedding vectors as the semantic representation, aiming to enhance interoperability across diverse data sources and lay the foundation for natural language-based data querying. At the specific method level, this paper proposes an improved entity tree embedding algorithm to reduce information loss and ameliorate the representation of entity semantics. Additionally, a privacy preservation mechanism based on the entity embedding approach is introduced to provide privacy protection for text-based data. Experimental results on address data demonstrate the mechanism’s efficacy in achieving privacy protection comparable to the widely adopted 2D Laplace plane noise method. Furthermore, incorporating the entity tree embedding into the privacy mechanism could yield more robust and reasonable results regarding location privacy and service quality, signifying the validity of the entity embedding results.}
}
@article{GARG2025112969,
title = {ATSumm: Auxiliary information enhanced approach for abstractive disaster tweet summarization with sparse training data},
journal = {Knowledge-Based Systems},
volume = {311},
pages = {112969},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.112969},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125000176},
author = {Piyush Kumar Garg and Roshni Chakraborty and Sourav Kumar Dandapat},
keywords = {Disasters, Abstractive summarization, Social media, Crisis scenario, Pointer generator network model, Deep learning, Data sparsity},
abstract = {The abundance of situational information on Twitter poses a challenge for users to manually discern vital and relevant information during disasters. A concise and human-interpretable overview of this information can aid decision makers implement efficient and quick disaster responses. Existing abstractive summarization approaches can be categorized as sentence- or key-phrase-based. This study focused on a sentence-based approach, which is typically implemented as a dual-phase procedure. The initial phase, known as the extractive phase, involves the identification of the most relevant tweets. The subsequent phase, which is referred to as the abstractive phase, generates a more human-interpretable summary. In this study, we adopted a methodology from prior research for the extractive phase. Most existing approaches employ deep learning-based frameworks for the abstractive phase of summarization. Such frameworks can either be pre-trained or require training from scratch. However, to achieve an appropriate level of performance, it is imperative to have substantial training data for both methods, which are not readily available. This study proposed an abstractive tweet summarizer (ATSumm) that effectively addresses the issue of data sparsity using auxiliary information. We introduced the auxiliary pointer generator network (AuxPGN) model, which utilizes a unique attention mechanism called key-phrase attention. This attention mechanism incorporates auxiliary information in the form of key-phrases and their corresponding importance scores from the input tweets. We evaluated the proposed approach through comparisons with 10 state-of-the-art approaches across 13 disaster datasets. The evaluation results indicated that ATSumm achieved superior performance compared with state-of-the-art approaches, with an improvement of 4−80% in the ROUGE-N F1-score.}
}
@article{MALBURG2023106727,
title = {Converting semantic web services into formal planning domain descriptions to enable manufacturing process planning and scheduling in industry 4.0},
journal = {Engineering Applications of Artificial Intelligence},
volume = {126},
pages = {106727},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106727},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623009119},
author = {Lukas Malburg and Patrick Klein and Ralph Bergmann},
keywords = {Semantic web services, Industry 4.0, Automated planning, Planning domain definition language, Cyber-physical workflows},
abstract = {To build intelligent manufacturing systems that react flexibly in case of failures or unexpected circumstances, manufacturing capabilities of production systems must be utilized as much as possible. Artificial Intelligence (AI) and, in particular, automated planning can contribute to this by enabling flexible production processes. To efficiently leverage automated planning, an almost complete planning domain description of the real-world is necessary. However, creating such planning descriptions is a demanding and error-prone task that requires high manual efforts even for domain experts. In addition, maintaining the encoded knowledge is laborious and, thus, can lead to outdated domain descriptions. To reduce the high efforts, already existing knowledge can be reused and transformed automatically into planning descriptions to benefit from organization-wide knowledge engineering activities. This paper presents a novel approach that reduces the described efforts by reusing existing knowledge for planning and scheduling in Industry 4.0 (I4.0). For this purpose, requirements for developing a converter that transforms existing knowledge are derived from literature. Based on these requirements, the SWS2PDDL converter is developed that transforms the knowledge into formal Planning Domain Definition Language (PDDL) descriptions. The approach’s usefulness is verified by a practical evaluation with a near real-world application scenario by generating failures in a physical smart factory and evaluating the generated re-planned production processes. When comparing the resulting plan quality to those achieved by using a manually modeled planning domain by a domain expert, the automatic transformation by SWS2PDDL leads to comparable or even better results without requiring the otherwise high manual modeling efforts.}
}
@article{TSIRMPAS2024108231,
title = {Neural natural language processing for long texts: A survey on classification and summarization},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108231},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108231},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003890},
author = {Dimitrios Tsirmpas and Ioannis Gkionis and Georgios Th. Papadopoulos and Ioannis Mademlis},
keywords = {Natural language processing, Long document, Document classification, Document summarization, Sentiment analysis, Deep neural networks},
abstract = {The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded online renders automated understanding of lengthy texts a critical issue. Relevant applications include automated Web mining, legal document review, medical records analysis, financial reports analysis, contract management, environmental impact assessment, news aggregation, etc. Despite the relatively recent development of efficient algorithms for analyzing long documents, practical tools in this field are currently flourishing. This article serves as an entry point into this dynamic domain and aims to achieve two objectives. First of all, it provides an introductory overview of the relevant neural building blocks, serving as a concise tutorial for the field. Secondly, it offers a brief examination of the current state-of-the-art in two key long document analysis tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Consequently, this article presents an introductory exploration of document-level analysis, addressing the primary challenges, concerns, and existing solutions. Finally, it offers a concise definition of “long text/document”, presents an original overarching taxonomy of common deep neural methods for long document analysis and lists publicly available annotated datasets that can facilitate further research in this area.}
}
@article{BAO2024110119,
title = {Further expansion from smart manufacturing system (SMS) to social smart manufacturing system (SSMS) based on industrial internet},
journal = {Computers & Industrial Engineering},
volume = {191},
pages = {110119},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110119},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224002407},
author = {Yuguang Bao and Xianyu Zhang and Chengjun Wang and Xinguo Ming},
keywords = {Smart Manufacturing System, Social Smart Manufacturing System, Social Manufacturing, Industrial Internet, System Architecture, Testbed Validation},
abstract = {The Industrial Internet (II) is driving the evolution of the manufacturing industry characterized by socially-engagement, servitization, universal interaction, and connection. With a single technical perspective, the requirements of “Sociality” and “Smartness” for future factories are often confused. The motivation of this study is to fill the existing gaps and provide practical knowledge for exploring how to design, synthesize, implement, and deploy a social smart manufacturing system (SSMS). Through an in-depth industrial practice investigation based on cooperative inquiry, an innovative factory model, namely II-supported SSMS, was proposed and synthesized for establishing a socialized aerospace part machining service ecosystem. The II-supported SSMS has been implemented and demonstrated via testbed validation. Furthermore, useful and portable practical knowledge was elicited from the industrial practice. A detailed six-step system architecture and a structural and hierarchical implementation strategy were presented, revealing the reference subsystem, integration logic, main technology tenants, and critical implementation processes of II-supported SSMS. Finally, the practical value and useful insights of II-supported SSMS were analyzed from the aspects of functionality, efficiency, flexibility, smartness, anthropocentric production, interconnectedness, collaborative intelligence, and scalability and portability. The potential risks, challenges, and barriers also were discussed.}
}
@incollection{OLENIK2024,
title = {Machine Learning and Omic Data for Prediction of Health and Chronic Diseases},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00284-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002840},
author = {Mark Olenik and Handan Melike Dönertaş},
keywords = {Chronic diseases, Health prediction, Machine learning, Omic data},
abstract = {Machine learning (ML) combined with diverse omic datasets offers transformative potential for predicting health outcomes and chronic diseases. By leveraging diverse omic data, ML models can identify biomarkers, enhance diagnostic accuracy, and enable personalized treatments. This chapter introduces the fundamental concepts of ML, key omic data sources, and the challenges associated with ML and omic-based disease prediction. Advances in technology, large-scale datasets, interpretable ML algorithms, and the digitization of healthcare are poised to revolutionize medical science, paving the way for precision medicine and early disease detection.}
}
@article{FORTH2023100263,
title = {BIM4EarlyLCA: An interactive visualization approach for early design support based on uncertain LCA results using open BIM},
journal = {Developments in the Built Environment},
volume = {16},
pages = {100263},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100263},
url = {https://www.sciencedirect.com/science/article/pii/S266616592300145X},
author = {Kasimir Forth and Alexander Hollberg and André Borrmann},
keywords = {LCA, BIM, Design decision support, Early design stages},
abstract = {To meet the European climate goals in the building sector, a holistic optimization of embodied greenhouse gas (GHG) emissions using the method of life cycle assessments (LCA) are necessary. The early design stages have high impact on the final performance of the buildings and are characterized by high uncertainty due to the lack of information and not yet taken decisions. Furthermore, most current LCA approaches based on Building Information Models (BIM) require high expertise and experience in both BIM and LCA and do not follow an intuitive visualization approach for other stakeholders and non-experts. This paper presents a novel design-decision-making approach for reducing embodied GHG emissions by interactive, model-based visualizations of uncertain LCA results. The proposed workflow is based on open BIM data formats, such as Industry Foundation Classes (IFC) and BIM Collaboration Format (BCF), and is developed for decision support for non-LCA experts in the early design stages. With the help of a user study, the prototypical implementation is tested by 103 participants with different levels of experience in BIM and LCA based on a case study. We evaluate the proposed approach regarding the support of open BIM data formats, different LCA visualization strategies, and the intuitiveness of different approaches to visualizing uncertain LCA results. The user study results show a broad acceptance and need for open BIM data formats and model-based LCA visualization but less for visualizing uncertainties, which needs further research. In conclusion, this interactive, model-based visualization approach using color coding supports non-LCA experts in the design decision-making process in early design stages.}
}
@article{LUO2025128768,
title = {A bi-consolidating model for joint relational triple extraction},
journal = {Neurocomputing},
volume = {614},
pages = {128768},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128768},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401539X},
author = {Xiaocheng Luo and Yanping Chen and Ruixue Tang and Caiwei Yang and Ruizhang Huang and Yongbin Qin},
keywords = {Pixel difference convolutions, Attention mechanism, Relational triple extraction, Joint entity and relation extraction},
abstract = {Current methods to extract relational triples directly make a prediction based on a possible entity pair in a raw sentence without depending on entity recognition. The task suffers from a serious semantic overlapping problem, in which several relation triples may share one or two entities in a sentence. In this paper, based on a two-dimensional sentence representation, a bi-consolidating model is proposed to address this problem by simultaneously reinforcing the local and global semantic features relevant to a relation triple. This model consists of a local consolidation component and a global consolidation component. The first component uses a pixel difference convolution to enhance semantic information of a possible triple representation from adjacent regions and mitigate noise in neighboring neighbors. The second component strengthens the triple representation based a channel attention and a spatial attention, which has the advantage to learn remote semantic dependencies in a sentence. They are helpful to improve the performance of both entity identification and relation type classification in relation triple extraction. After evaluated on several publish datasets, the bi-consolidating model achieves competitive performance. Analytical experiments demonstrate the effectiveness of our model for relational triple extraction and give motivation for other natural language processing tasks.}
}
@incollection{MIEVILLE2024,
title = {Modern Automation in Organic Synthesis Laboratories},
booktitle = {Reference Module in Chemistry, Molecular Sciences and Chemical Engineering},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-409547-2},
doi = {https://doi.org/10.1016/B978-0-323-96025-0.00047-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323960250000478},
author = {Pascal Miéville and Florian de Nanteuil},
keywords = {Automation, Autonomous laboratory, Data science, Flow chemistry, High throughput experimentation, Integration, Orchestration, Robotics, Sample transfer, Self-driving laboratory},
abstract = {The exponential growth of automation within organic chemistry has unlocked many opportunities for scientists worldwide. The chapter aims to provide a comprehensive understanding of the transformative potential and the complex landscape created by the evolution of automation in organic chemistry. It presents an overview dedicated to exploring the global evolution of automation in experimental chemistry, focusing on its pivotal concepts, cutting-edge technologies, and diverse applications that drive this new branch of experimental sciences. The integration of robotics, automation, global data management, and machine learning algorithms within the domain of organic synthesis signifies a transformative leap in laboratory practices. New tools harmonize to execute an extensive range of synthetic tasks, promising reproducibility, precision, and scalability in handling compounds and performing reactions. This approach optimizes molecule production, resulting in accelerated discovery processes, heightened efficiency, and a drastic reduction in errors. The chapter dives into various components underpinning this era of organic synthesis automation, encompassing the historical progression, current state-of-the-art in instrumentation, experimental coding, data processing automation, and the role of artificial intelligence in predictive modeling of reaction outcomes. Moreover, it explores the persistent challenges posed by these new technologies, addressing issues such as solid sampling, global laboratory integration, and examines implications touching on ethical considerations, safety concerns, and sustainability aspects.}
}
@incollection{ELLSWORTH2024239,
title = {Acronyms},
editor = {Peggy Wu and Michael Salpukas and Hsin-Fu Wu and Shannon Ellsworth},
booktitle = {Trolley Crash},
publisher = {Academic Press},
pages = {239-241},
year = {2024},
isbn = {978-0-443-15991-6},
doi = {https://doi.org/10.1016/B978-0-44-315991-6.00020-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159916000200},
author = {Shannon Ellsworth and Hsin-Fu ‘Sinker’ Wu}
}
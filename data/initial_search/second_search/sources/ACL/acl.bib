
@inproceedings{marquez-etal-2025-nlp,
    title = "{NLP}@{IIMAS}-{CLTL} at Multilingual Counterspeech Generation: Combating Hate Speech Using Contextualized Knowledge Graph Representations and {LLM}s",
	author = "M{\'a}rquez, David Salvador  and
      G{\'o}mez Adorno, Helena Montserrat  and
      Markov, Ilia  and
      B{\'a}ez Santamar{\'i}a, Selene",
	editor = "Bonaldi, Helena  and
      Vallecillo-Rodr{\'i}guez, Mar{\'i}a Estrella  and
      Zubiaga, Irune  and
      Montejo-R{\'a}ez, Arturo  and
      Soroa, Aitor  and
      Mart{\'i}n-Valdivia, Mar{\'i}a Teresa  and
      Guerini, Marco  and
      Agerri, Rodrigo",
	booktitle = "Proceedings of the First Workshop on Multilingual Counterspeech Generation",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.mcg-1.4/",
	pages = "29--36",
	abstract = "We present our approach for the shared task on Multilingual Counterspeech Generation (MCG) to counteract hate speech (HS) in Spanish, English, Basque, and Italian. To accomplish this, we followed two different strategies: 1) a graph-based generative model that encodes graph representations of knowledge related to hate speech, and 2) leveraging prompts for a large language model (LLM), specifically GPT-4o. We find that our graph-based approach tends to perform better in terms of traditional evaluation metrics (i.e., RougeL, BLEU, BERTScore), while the JudgeLM evaluation employed in the shared task favors the counter-narratives generated by the LLM-based approach, which was ranked second for English and third for Spanish on the leaderboard."
}

@proceedings{genaik-ws-2025-1,
    title = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.0/"
}

@inproceedings{martynova-etal-2025-learn,
    title = "Learn Together: Joint Multitask Finetuning of Pretrained {KG}-enhanced {LLM} for Downstream Tasks",
	author = "Martynova, Anastasia  and
      Tishin, Vladislav  and
      Semenova, Natalia",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.2/",
	pages = "13--19",
	abstract = "Recent studies have shown that a knowledge graph (KG) can enhance text data by providing structured background knowledge, which can significantly improve the language understanding skills of the LLM. Besides, finetuning of such models shows solid results on commonsense reasoning benchmarks. In this work, we introduce expandable Joint Multitask Finetuning on Pretrained KG-enchanced LLM approach for Question Answering (QA), Machine Reading Comprehension (MRC) and Knowledge Graph Question Answering (KGQA) tasks. Extensive experiments show competitive performance of joint finetuning QA+MRC+KGQA over single task approach with a maximum gain of 30{\%} accuracy."
}

@inproceedings{mahalingam-etal-2025-sketch,
    title = "{SKETCH}: Structured Knowledge Enhanced Text Comprehension for Holistic Retrieval",
	author = "Mahalingam, Aakash  and
      Gande, Vinesh Kumar  and
      Chadha, Aman  and
      Jain, Vinija  and
      Chaudhary, Divya",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.4/",
	pages = "27--42",
	abstract = "This paper discusses about the SKETCH approach which enhances text retrieval and context relevancy on large corpuses compared to the traditional baseline methods. The abstract attached below discusses this further. Abstract: Retrieval-Augmented Generation (RAG) systems have become pivotal in leveraging vast corpora to generate informed and contextually relevant responses, notably reducing hallucinations in Large Language Models. Despite significant advancements, these systems struggle to efficiently process and retrieve information from large datasets while maintaining a comprehensive understanding of the context. This paper introduces SKETCH, a novel methodology that enhances the RAG retrieval process by integrating semantic text retrieval with knowledge graphs, thereby merging structured and unstructured data for a more holistic comprehension. SKETCH, demonstrates substantial improvements in retrieval performance and maintains superior context integrity compared to traditional methods. Evaluated across four diverse datasets: QuALITY, QASPER, NarrativeQA, and Italian Cuisine{---}SKETCH consistently outperforms baseline approaches on key RAGAS metrics such as answer relevancy, faithfulness, context precision and context recall. Notably, on the Italian Cuisine dataset, SKETCH achieved an answer relevancy of 0.94 and a context precision of 0.99, representing the highest performance across all evaluated metrics. These results highlight SKETCH`s capability in delivering more accurate and contextually relevant responses, setting new benchmarks for future retrieval systems."
}

@inproceedings{barry-etal-2025-graphrag,
    title = "{G}raph{RAG}: Leveraging Graph-Based Efficiency to Minimize Hallucinations in {LLM}-Driven {RAG} for Finance Data",
	author = "Barry, Mariam  and
      Caillaut, Gaetan  and
      Halftermeyer, Pierre  and
      Qader, Raheel  and
      Mouayad, Mehdi  and
      Le Deit, Fabrice  and
      Cariolaro, Dimitri  and
      Gesnouin, Joseph",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.6/",
	pages = "54--65",
	abstract = "This study explores the integration of graph-based methods into Retrieval-Augmented Generation (RAG) systems to enhance efficiency, reduce hallucinations, and improve explainability, with a particular focus on financial and regulatory document retrieval. We propose two strategies{---}FactRAG and HybridRAG{---}which leverage knowledge graphs to improve RAG performance. Experiments conducted using Finance Bench, a benchmark for AI in finance, demonstrate that these approaches achieve a 6{\%} reduction in hallucinations and an 80{\%} decrease in token usage compared to conventional RAG methods. Furthermore, we evaluate HybridRAG by comparing the Digital Operational Resilience Act (DORA) from the European Union with the Federal Financial Institutions Examination Council (FFIEC) guidelines from the United States. The results reveal a significant improvement in computational efficiency, reducing contradiction detection complexity from $O(n^2)$ to $O(k \cdot n)${---}where $n$ is the number of chunks{---}and a remarkable 734-fold decrease in token consumption. Graph-based retrieval methods can improve the efficiency and cost-effectiveness of large language model (LLM) applications, though their performance and token usage depend on the dataset, knowledge graph design, and retrieval task."
}

@inproceedings{eldessouky-etal-2025-structured,
    title = "Structured Knowledge meets {G}en{AI}: A Framework for Logic-Driven Language Models",
	author = "Eldessouky, Farida Helmy  and
      Ehab, Nourhan  and
      Schindler, Carolin  and
      Abuelkheir, Mervat  and
      Minker, Wolfgang",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.7/",
	pages = "66--68",
	abstract = "Large Language Models (LLMs) excel at generating fluent text but struggle with context sensitivity, logical reasoning, and personalization without extensive fine-tuning. This paper presents a logical modulator: an adaptable communication layer between Knowledge Graphs (KGs) and LLMs as a way to address these limitations. Unlike direct KG-LLM integrations, our modulator is domain-agnostic and incorporates logical dependencies and commonsense reasoning in order to achieve contextual personalization. By enhancing KG interaction, this method will produce linguistically coherent and logically sound outputs, increasing interpretability and reliability in generative AI."
}

@inproceedings{mecharnia-daquin-2025-performance,
    title = "Performance and Limitations of Fine-Tuned {LLM}s in {SPARQL} Query Generation",
	author = "Mecharnia, Thamer  and
      d{'}Aquin, Mathieu",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.8/",
	pages = "69--77",
	abstract = "Generative AI has simplified information access by enabling natural language-driven interactions between users and automated systems. In particular, Question Answering (QA) has emerged as a key application of AI, facilitating efficient access to complex information through dialogue systems and virtual assistants. The Large Language Models (LLMs) combined with Knowledge Graphs (KGs) have further enhanced QA systems, allowing them to not only correctly interpret natural language but also retrieve precise answers from structured data sources such as Wikidata and DBpedia. However, enabling LLMs to generate machine-readable SPARQL queries from natural language questions (NLQs) remains challenging, particularly for complex questions. In this study, we present experiments in fine-tuning LLMs for the task of NLQ-to-SPARQL transformation. We rely on benchmark datasets for training and testing the fine-tuned models, generating queries directly from questions written in English (without further processing of the input or output). By conducting an analytical study, we examine the effectiveness of each model, as well as the limitations associated with using fine-tuned LLMs to generate SPARQL."
}

@inproceedings{dong-etal-2025-refining,
    title = "Refining Noisy Knowledge Graph with Large Language Models",
	author = "Dong, Na  and
      Kertkeidkachorn, Natthawut  and
      Liu, Xin  and
      Shirai, Kiyoaki",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.9/",
	pages = "78--86",
	abstract = "Knowledge graphs (KGs) represent structured real-world information composed by triplets of head entity, relation, and tail entity. These graphs can be constructed automatically from text or manually curated. However, regardless of the construction method, KGs often suffer from misinformation, incompleteness, and noise, which hinder their reliability and utility. This study addresses the challenge of noisy KGs, where incorrect or misaligned entities and relations degrade graph quality. Leveraging recent advancements in large language models (LLMs) with strong capabilities across diverse tasks, we explore their potential to detect and refine noise in KGs. Specifically, we propose a novel method, LLM{\_}sim, to enhance the detection and refinement of noisy triples. Our results confirm the effectiveness of this approach in elevating KG quality in noisy environments. Additionally, we apply our proposed method to Knowledge Graph Completion (KGC), a downstream KG task that aims to predict missing links and improve graph completeness. Traditional KGC methods assume that KGs are noise-free, which is unrealistic in practical scenarios. Our experiments analyze the impact of varying noise levels on KGC performance, revealing that LLMs can mitigate noise by identifying and refining incorrect entries, thus enhancing KG quality."
}

@inproceedings{regino-dos-reis-2025-llms,
    title = "Can {LLM}s be Knowledge Graph Curators for Validating Triple Insertions?",
	author = "Regino, Andr{\'e} Gomes  and
      dos Reis, Julio Cesar",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.10/",
	pages = "87--99",
	abstract = "As Knowledge Graphs (KGs) become central to modern applications, automated methods for validating RDF triples before insertion into these graphs are essential. The complexity and scalability challenges in manual validation processes have led researchers to explore Large Language Models (LLMs) as potential automated validators. This study investigates the feasibility of using LLMs to validate RDF triples by focusing on four distinct and complementary validation tasks: class and property alignment, URI standardization, semantic consistency, and syntactic correctness. We propose a systematic validation method that uses prompts to guide LLMs through each stage of the triple evaluation of the RDF. In our experiments, four models are evaluated across these tasks. Our results reveal that more advanced models like Llama-3-70B-Instruct offer superior accuracy and consistency. Our findings emphasize the practical open challenges of deploying LLMs in real-world RDF validation scenarios, including domain generalization, semantic drift, and the need for human-in-the-loop interventions. This investigation advances the research on the refinement and integration of LLM-based RDF validation techniques into KG management workflows."
}

@inproceedings{ozsoy-etal-2025-text2cypher,
    title = "{T}ext2{C}ypher: Bridging Natural Language and Graph Databases",
	author = "Ozsoy, Makbule Gulcin  and
      Messallem, Leila  and
      Besga, Jon  and
      Minneci, Gianandrea",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.11/",
	pages = "100--108",
	abstract = "Knowledge graphs use nodes, relationships, and properties to represent arbitrarily complex data. When stored in a graph database, the Cypher query language enables efficient modeling and querying of knowledge graphs. However, using Cypher requires specialized knowledge, which can present a challenge for non-expert users. Our work Text2Cypher aims to bridge this gap by translating natural language queries into Cypher query language and extending the utility of knowledge graphs to non-technical expert users. While large language models (LLMs) can be used for this purpose, they often struggle to capture complex nuances, resulting in incomplete or incorrect outputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more promising approach, but the limited availability of high-quality, publicly available Text2Cypher datasets makes this challenging. In this work, we show how we combined, cleaned and organized several publicly available datasets into a total of 44,387 instances, enabling effective fine-tuning and evaluation. Models fine-tuned on this dataset showed significant performance gains, with improvements in Google-BLEU and Exact Match scores over baseline models, highlighting the importance of high-quality datasets and fine-tuning in improving Text2Cypher performance."
}

@inproceedings{kumar-etal-2025-kgfakenet,
    title = "{KGF}ake{N}et: A Knowledge Graph-Enhanced Model for Fake News Detection",
	author = "Kumar, Anuj  and
      Kumar, Pardeep  and
      Yadav, Abhishek  and
      Ahlawat, Satyadev  and
      Prasad, Yamuna",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.12/",
	pages = "109--122",
	abstract = "The proliferation of fake news on social media has intensified the spread of misinformation, promoting societal biases, hate, and violence. While recent advancements in Generative AI (GenAI), particularly large language models (LLMs), have shown promise, these models often need more structured representation for accurate verification, as they rely on pre-trained data patterns without access to real-time or validated information. This study presents a framework that utilizes Open Information Extractor 6 (OpenIE6) to extract triplet relationships (subject-predicate-object) from statements and justifications to compute the cosine similarity between the Knowledge Graphs (KGs) of the statements and their supporting justification to precisely measure the relevance and alignment between them. This similarity feature is integrated with an attention mechanism over GenAI-generated embeddings to enhance the model`s ability to capture semantic features accurately. In addition, a Multi-Layer Perceptron (MLP) classifier is employed to integrate all features, resulting in a 4{\%} improvement in accuracy and a 5{\%} increase in F1-score over state-of-the-art LLM-based approaches."
}

@inproceedings{toshevska-etal-2025-style,
    title = "Style Knowledge Graph: Augmenting Text Style Transfer with Knowledge Graphs",
	author = "Toshevska, Martina  and
      Kalajdziski, Slobodan  and
      Gievska, Sonja",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.13/",
	pages = "123--135",
	abstract = "Text style transfer is the task of modifying the stylistic attributes of a given text while preserving its original meaning. This task has also gained interest with the advent of large language models. Although knowledge graph augmentation has been explored in various tasks, its potential for enhancing text style transfer has received limited attention. This paper proposes a method to create a Style Knowledge Graph (SKG) to facilitate and improve text style transfer. The SKG captures words, their attributes, and relations in a particular style, that serves as a knowledge resource to augment text style transfer. We conduct baseline experiments to evaluate the effectiveness of the SKG for augmenting text style transfer by incorporating relevant parts from the SKG in the prompt. The preliminary results demonstrate its potential for enhancing content preservation and style transfer strength in text style transfer tasks, while the results on fluency indicate promising outcomes with some room for improvement. We hope that the proposed SKG and the initial experiments will inspire further research in the field."
}

@inproceedings{kamaladdini-ezzabady-benamara-2025-entity,
    title = "Entity Quality Enhancement in Knowledge Graphs through {LLM}-based Question Answering",
	author = "Kamaladdini Ezzabady, Morteza  and
      Benamara, Farah",
	editor = "Gesese, Genet Asefa  and
      Sack, Harald  and
      Paulheim, Heiko  and
      Merono-Penuela, Albert  and
      Chen, Lihu",
	booktitle = "Proceedings of the Workshop on Generative AI and Knowledge Graphs (GenAIK)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2025.genaik-1.14/",
	pages = "136--145",
	abstract = "Most models for triple extraction from texts primarily focus on named entities. However, real-world applications often comprise non-named entities that pose serious challenges for entity linking and disambiguation. We focus on these entities and propose the first LLM-based entity revision framework to improve the quality of extracted triples via a multi-choice question-answering mechanism. When evaluated on two benchmark datasets, our results show a significant improvement, thereby generating more reliable triples for knowledge graphs."
}

@inproceedings{shukla-etal-2025-graphrag,
    title = "{G}raph{RAG} Analysis for Financial Narrative Summarization and A Framework for Optimizing Domain Adaptation",
	author = "Shukla, Neelesh Kumar  and
      Prabhakar, Prabhat  and
      Thangaraj, Sakthivel  and
      Singh, Sandeep  and
      Sun, Weiyi  and
      Venkatesan, C Prasanna  and
      Krishnamurthy, Viji",
	editor = "Chen, Chung-Chi  and
      Moreno-Sandoval, Antonio  and
      Huang, Jimin  and
      Xie, Qianqian  and
      Ananiadou, Sophia  and
      Chen, Hsin-Hsi",
	booktitle = "Proceedings of the Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal)",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.finnlp-1.2/",
	pages = "23--34",
	abstract = "Large Language Models (LLMs) have shown promise in summarizing complex documents, but their limitations in handling lengthy documents and capturing global information hinder their performance in tasks like Query-Focused Summarization (QFS). We explore GraphRAG, a retrieval-augmented generation approach that utilizes a globally summarized knowledge graph derived from an LLM. We apply GraphRAG to the Financial Narrative Summarization (FNS) dataset, which consists of lengthy financial reports. Our results show that a naive RAG approach outperforms GraphRAG in terms of comprehensiveness, directness, conciseness and completeness. However, we demonstrate that optimizing entity and relation extraction using an LLM as an optimizer can enhance GraphRAG`s performance. Our study highlights the need for domain-specific optimization to improve GraphRAG`s capabilities for summarization tasks in facts-heavy domains like finance. We propose an optimization framework that extends GraphRAG`s original domain adaptation strategy by incorporating entity and relations optimization, leading to improved performance in capturing relevant entities and relationships. Our findings contribute to the development of more effective summarization models for complex documents in finance and other domains."
}

@inproceedings{chen-etal-2025-noise,
    title = "Noise-powered Multi-modal Knowledge Graph Representation Framework",
	author = "Chen, Zhuo  and
      Fang, Yin  and
      Zhang, Yichi  and
      Guo, Lingbing  and
      Chen, Jiaoyan  and
      Pan, Jeff Z.  and
      Chen, Huajun  and
      Zhang, Wen",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.11/",
	pages = "141--155",
	abstract = "The rise of Multi-modal Pre-training highlights the necessity for a unified Multi-Modal Knowledge Graph (MMKG) representation learning framework. Such a framework is essential for embedding structured knowledge into multi-modal Large Language Models effectively, alleviating issues like knowledge misconceptions and multi-modal hallucinations. In this work, we explore the efficacy of models in accurately embedding entities within MMKGs through two pivotal tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking to robustly integrate multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets, demonstrating its versatility. Moreover, SNAG can not only function as a standalone model but also enhance other existing methods, providing stable performance improvements. Code and data are available at https://github.com/zjukg/SNAG."
}

@inproceedings{wang-etal-2025-knowledge,
    title = "Knowledge Graph Entity Typing with Curriculum Contrastive Learning",
	author = "Wang, Hao  and
      Nuo, Minghua  and
      Jiang, Shan",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.38/",
	pages = "574--583",
	abstract = "The Knowledge Graph Entity Typing (KGET) task aims to predict missing type annotations for entities in knowledge graphs. Most recent studies only focus on the structural information from an entity`s neighborhood or semantic information from textual representations of entities or relations. In this paper, inspired by curriculum learning and contrastive learning, we propose the CCLET model using the Curriculum Contrastive Learning strategy for KGET, which uses the Pre-trained Language Model (PLM) and the graph model to fuse the entity related semantic and the structural information of the Knowledge Graph (KG) respectively. Our CCLET model consists of two main parts. In the Knowledge Fusion part, we design an Enhanced-MLP architecture to fuse the text of the entity`s description, related triplet, and tuples; In the Curriculum Contrastive Learning part, we define the difficulty of the course by controlling the level of added noise, we aim to accurately learn with curriculum contrastive learning strategy from easy to difficult. Our extensive experiments demonstrate that the CCLET model outperforms recent state-of-the-art models, verifying its effectiveness in the KGET task."
}

@inproceedings{zhong-etal-2025-synthet2c,
    title = "{S}ynthe{T}2{C}: Generating Synthetic Data for Fine-Tuning Large Language Models on the {T}ext2{C}ypher Task",
	author = "Zhong, Zijie  and
      Zhong, Linqing  and
      Sun, Zhaoze  and
      Jin, Qingyun  and
      Qin, Zengchang  and
      Zhang, Xiaofan",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.46/",
	pages = "672--692",
	abstract = "Integrating Large Language Models (LLMs) with existing Knowledge Graph (KG) databases presents a promising avenue for enhancing LLMs' efficacy and mitigating their {\textquotedblleft}hallucinations{\textquotedblright}. Given that most KGs reside in graph databases accessible solely through specialized query languages (e.g., Cypher), it is critical to connect LLMs with KG databases by automating the translation of natural language into Cypher queries (termed as {\textquotedblleft}Text2Cypher{\textquotedblright} task). Prior efforts tried to bolster LLMs' proficiency in Cypher generation through Supervised Fine-Tuning (SFT). However, these explorations are hindered by the lack of annotated datasets of Query-Cypher pairs, resulting from the labor-intensive and domain-specific nature of such annotation. In this study, we propose SyntheT2C, a methodology for constructing a synthetic Query-Cypher pair dataset, comprising two distinct pipelines: (1) LLM-based prompting and (2) template-filling. SyntheT2C is applied to two medical KG databases, culminating in the creation of a synthetic dataset, MedT2C. Comprehensive experiments demonstrate that the MedT2C dataset effectively enhances the performance of backbone LLMs on Text2Cypher task via SFT. Both the SyntheT2C codebase and the MedT2C dataset will be released."
}

@inproceedings{liu-etal-2025-superficial,
    title = "From Superficial to Deep: Integrating External Knowledge for Follow-up Question Generation Using Knowledge Graph and {LLM}",
	author = "Liu, Jianyu  and
      Huang, Yi  and
      Bi, Sheng  and
      Feng, Junlan  and
      Qi, Guilin",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.55/",
	pages = "828--840",
	abstract = "In a conversational system, dynamically generating follow-up questions based on context can help users explore information and provide a better user experience. Humans are usually able to ask questions that involve some general life knowledge and demonstrate higher order cognitive skills. However, the questions generated by existing methods are often limited to shallow contextual questions that are uninspiring and have a large gap to the human level. In this paper, we propose a three-stage external knowledge-enhanced follow-up question generation method, which generates questions by identifying contextual topics, constructing a knowledge graph (KG) online, and finally combining these with a large language model to generate the final question. The model generates information-rich and exploratory follow-up questions by introducing external common sense knowledge and performing a knowledge fusion operation. Experiments show that compared to baseline models, our method generates questions that are more informative and closer to human questioning levels while maintaining contextual relevance."
}

@inproceedings{feng-he-2025-rgr,
    title = "{RGR}-{KBQA}: Generating Logical Forms for Question Answering Using Knowledge-Graph-Enhanced Large Language Model",
	author = "Feng, Tengfei  and
      He, Liang",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.205/",
	pages = "3057--3070",
	abstract = "In the field of natural language processing, Knowledge Base Question Answering (KBQA) is a challenging task that involves accurately retrieving answers from structured knowledge. Existing methods often face issues when generating query statements using LLMs, as the knowledge introduced may be imprecise and the models themselves may exhibit hallucination problems, leading to low accuracy, particularly when dealing with complex questions. To address these challenges, we introduce a novel semantic parsing approach called RGR-KBQA, which adopts a Retrieve-Generate-Retrieve framework. The first retrieval step introduces factual knowledge from a knowledge graph to enhance the semantic understanding capabilities of LLMs, thereby improving generation accuracy of logical form. The second step uses a fine-tuned model to generate the logical form, and the final step involves unsupervised relation and entity retrieval to further enhance generation accuracy. These two retrieval steps help alleviate the hallucination problems inherent in LLMs. Experimental results show that RGR-KBQA demonstrate promising performance on CWQ and WebQSP datasets."
}

@inproceedings{shen-etal-2025-reasoning,
    title = "Reasoning with Trees: Faithful Question Answering over Knowledge Graph",
	author = "Shen, Tiesunlong  and
      Wang, Jin  and
      Zhang, Xuejie  and
      Cambria, Erik",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.211/",
	pages = "3138--3157",
	abstract = "Recent advancements in large language models (LLMs) have shown remarkable progress in reasoning capabilities, yet they still face challenges in complex, multi-step reasoning tasks. This study introduces Reasoning with Trees (RwT), a novel framework that synergistically integrates LLMs with knowledge graphs (KGs) to enhance reasoning performance and interpretability. RwT reformulates knowledge graph question answering (KGQA) as a discrete decision-making problem, leveraging Monte Carlo Tree Search (MCTS) to iteratively refine reasoning paths. This approach mirrors human-like reasoning by dynamically integrating the LLM`s internal knowledge with external KG information. We propose a real-data guided iteration technique to train an evaluation model that assesses action values, improving the efficiency of the MCTS process. Experimental results on two benchmark KGQA datasets demonstrate that RwT significantly outperforms existing state-of-the-art methods, with an average performance improvement of 9.81{\%}. Notably, RwT achieves these improvements without requiring complete retraining of the LLM, offering a more efficient and adaptable approach to enhancing LLM reasoning capabilities."
}

@inproceedings{agarwal-etal-2025-aligning,
    title = "Aligning Complex Knowledge Graph Question Answering as Knowledge-Aware Constrained Code Generation",
	author = "Agarwal, Prerna  and
      Kumar, Nishant  and
      Jagannath, Srikanta Bedathur",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.267/",
	pages = "3952--3978",
	abstract = "Generating executable logical forms (LF) using Large Language Models (LLMs) in a few-shot setting for Knowledge Graph Question Answering (KGQA) is becoming popular. However, their performance is still limited due to very little exposure to the LF during pre-training of LLMs, resulting in many syntactically incorrect LF generation. If the LF generation task can be transformed to a more familiar task for the LLM, it can potentially reduce the syntax errors and elevate the generation quality. On the other hand, there exist specialized LLMs trained/fine-tuned on code in many programming languages. They can be leveraged to generate the LF as step-wise constrained code expression generation using modular functions in the LF. Based on this insight, we propose CodeAlignKGQA: a framework that aligns the LF generation as code generation that incorporates LF-specific constraints. We extract the question-specific subgraph information to enable Knowledge-Aware code generation. We additionally introduce a dynamic self-code-correction mechanism, to be applied as required. Our extensive experiments on Complex KGQA benchmarks such as KQA Pro demonstrate the effectiveness of our approach. CodeAlignKGQA surpasses all few-shot baselines on KQA Pro by 21{\%}, achieving a new state-of-the-art."
}

@inproceedings{pei-etal-2025-selfprompt,
    title = "{S}elf{P}rompt: Autonomously Evaluating {LLM} Robustness via Domain-Constrained Knowledge Guidelines and Refined Adversarial Prompts",
	author = "Pei, Aihua  and
      Yang, Zehua  and
      Zhu, Shunan  and
      Cheng, Ruoxi  and
      Jia, Ju",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.457/",
	pages = "6840--6854",
	abstract = "Traditional methods for evaluating the robustness of large language models (LLMs) often rely on standardized benchmarks, which can escalate costs and limit evaluations across varied domains. This paper introduces a novel framework designed to autonomously evaluate the robustness of LLMs by incorporating refined adversarial prompts and domain-constrained knowledge guidelines in the form of knowledge graphs. Our method systematically generates descriptive sentences from domain-constrained knowledge graph triplets to formulate adversarial prompts, enhancing the relevance and challenge of the evaluation. These prompts, generated by the LLM itself and tailored to evaluate its own robustness, undergo a rigorous filtering and refinement process, ensuring that only those with high textual fluency and semantic fidelity are used. This self-evaluation mechanism allows the LLM to evaluate its robustness without the need for external benchmarks. We assess the effectiveness of our framework through extensive testing on both proprietary models like ChatGPT and open-source models such as Llama-3.1, Phi-3, and Mistral. Results confirm that our approach not only reduces dependency on conventional data but also provides a targeted and efficient means of evaluating LLM robustness in constrained domains."
}

@inproceedings{dong-etal-2025-effiqa,
    title = "{E}ffi{QA}: Efficient Question-Answering with Strategic Multi-Model Collaboration on Knowledge Graphs",
	author = "Dong, Zixuan  and
      Peng, Baoyun  and
      Wang, Yufei  and
      Fu, Jia  and
      Wang, Xiaodong  and
      Zhou, Xin  and
      Shan, Yongxue  and
      Zhu, Kangchen  and
      Chen, Weiguo",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.479/",
	pages = "7180--7194",
	abstract = "While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm. EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA`s effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering."
}

@inproceedings{vollmers-etal-2025-contextual,
    title = "Contextual Augmentation for Entity Linking using Large Language Models",
	author = "Vollmers, Daniel  and
      Zahera, Hamada  and
      Moussallem, Diego  and
      Ngonga Ngomo, Axel-Cyrille",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.570/",
	pages = "8535--8545",
	abstract = "Entity Linking involves detecting and linking entity mentions in natural language texts to a knowledge graph. Traditional methods use a two-step process with separate models for entity recognition and disambiguation, which can be computationally intensive and less effective. We propose a fine-tuned model that jointly integrates entity recognition and disambiguation in a unified framework. Furthermore, our approach leverages large language models to enrich the context of entity mentions, yielding better disambiguation. We evaluated our approach on benchmark datasets and compared with several baselines. The evaluation results show that our approach achieves state-of-the-art performance on out-of-domain datasets."
}

@inproceedings{jia-etal-2025-medikal,
    title = "med{IKAL}: Integrating Knowledge Graphs as Assistants of {LLM}s for Enhanced Clinical Diagnosis on {EMR}s",
	author = "Jia, Mingyi  and
      Duan, Junwen  and
      Song, Yan  and
      Wang, Jianxin",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.624/",
	pages = "9278--9298",
	abstract = "Electronic Medical Records (EMRs), while integral to modern healthcare, present challenges for clinical reasoning and diagnosis due to their complexity and information redundancy. To address this, we proposed medIKAL (\textbf{I}ntegrating \textbf{K}nowledge Graphs as \textbf{A}ssistants of \textbf{L}LMs), a framework that combines Large Language Models (LLMs) with knowledge graphs (KGs) to enhance diagnostic capabilities. medIKAL assigns weighted importance to entities in medical records based on their type, enabling precise localization of candidate diseases within KGs. It innovatively employs a residual network-like approach, allowing initial diagnosis by the LLM to be merged into KG search results. Through a path-based reranking algorithm and a fill-in-the-blank style prompt template, it further refined the diagnostic process. We validated medIKAL`s effectiveness through extensive experiments on a newly introduced open-sourced Chinese EMR dataset, demonstrating its potential to improve clinical diagnosis in real-world settings."
}

@inproceedings{zhu-etal-2025-kg,
    title = "{KG}-{FPQ}: Evaluating Factuality Hallucination in {LLM}s with Knowledge Graph-based False Premise Questions",
	author = "Zhu, Yanxu  and
      Xiao, Jinlin  and
      Wang, Yuhang  and
      Sang, Jitao",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.698/",
	pages = "10472--10490",
	abstract = "Recent studies have demonstrated that large language models (LLMs) are susceptible to being misled by false premise questions (FPQs), leading to errors in factual knowledge, known as factuality hallucination. Existing benchmarks that assess this vulnerability primarily rely on manual construction, resulting in limited size and lack of expandability. In this work, we introduce an automated, scalable pipeline to create FPQs based on knowledge graphs (KGs). The first step is to modify true triplets extracted from KGs to create false premises. Subsequently, utilizing the state-of-the-art capabilities of GPTs, we generate semantically rich FPQs. Based on the proposed method, we present a comprehensive benchmark, the Knowledge Graph-based False Premise Questions (KG-FPQ), which contains approximately 178k FPQs across three knowledge domains, at six levels of confusability, and in two task formats. Using KG-FPQ, we conduct extensive evaluations on several representative LLMs and provide valuable insights. The KG-FPQ dataset and code are available at https://github.com/yanxuzhu/KG-FPQ."
}

@inproceedings{zhang-zhao-2025-collaborative,
    title = "A Collaborative Reasoning Framework Powered by Reinforcement Learning and Large Language Models for Complex Questions Answering over Knowledge Graph",
	author = "Zhang, Zhiqiang  and
      Zhao, Wen",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.712/",
	pages = "10672--10684",
	abstract = "Knowledge Graph Question Answering (KGQA) aims to automatically answer natural language questions by reasoning across multiple triples in knowledge graphs (KGs). Reinforcement learning (RL)-based methods are introduced to enhance model interpretability. Nevertheless, when addressing complex questions requiring long-term reasoning, the RL agent is usually misled by aimless exploration, as it lacks common learning practices with prior knowledge. Recently, large language models (LLMs) have been proven to encode vast amounts of knowledge about the world and possess remarkable reasoning capabilities. However, they often encounter challenges with hallucination issues, failing to address complex questions that demand deep and deliberate reasoning. In this paper, we propose a collaborative reasoning framework (CRF) powered by RL and LLMs to answer complex questions based on the knowledge graph. Our approach leverages the common sense priors contained in LLMs while utilizing RL to provide learning from the environment, resulting in a hierarchical agent that uses LLMs to solve the complex KGQA task. By combining LLMs and the RL policy, the high-level agent accurately identifies constraints encountered during reasoning, while the low-level agent conducts efficient path reasoning by selecting the most promising relations in KG. Extensive experiments conducted on four benchmark datasets clearly demonstrate the effectiveness of the proposed model, which surpasses state-of-the-art approaches."
}

@inproceedings{liu-etal-2025-filter,
    title = "Filter-then-Generate: Large Language Models with Structure-Text Adapter for Knowledge Graph Completion",
	author = "Liu, Ben  and
      Zhang, Jihai  and
      Lin, Fangquan  and
      Yang, Cheng  and
      Peng, Min",
	editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	month = jan,
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.740/",
	pages = "11181--11195",
	abstract = "Large Language Models (LLMs) present massive inherent knowledge and superior semantic comprehension capability, which have revolutionized various tasks in natural language processing. Despite their success, a critical gap remains in enabling LLMs to perform knowledge graph completion (KGC). Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning. Fundamentally, applying LLMs on KGC introduces several critical challenges, including a vast set of entity candidates, hallucination issue of LLMs, and under-exploitation of the graph structure. To address these challenges, we propose a novel instruction-tuning-based method, namely FtG. Specifically, we present a \textit{filter-then-generate} paradigm and formulate the KGC task into a multiple-choice question format. In this way, we can harness the capability of LLMs while mitigating the issue casused by hallucinations. Moreover, we devise a flexible ego-graph serialization prompt and employ a structure-text adapter to couple structure and text information in a contextualized manner. Experimental results demonstrate that FtG achieves substantial performance gain compared to existing state-of-the-art methods. The instruction dataset and code are available at \url{https://github.com/LB0828/FtG}."
}

@inproceedings{mori-2024-cognitive,
    title = "Cognitive Model of Listener Response Generation and Its Application to Dialogue Systems",
	author = "Mori, Taiga",
	editor = "Inoue, Koji  and
      Fu, Yahui  and
      Axelsson, Agnes  and
      Ohashi, Atsumoto  and
      Madureira, Brielen  and
      Zenimoto, Yuki  and
      Mohapatra, Biswesh  and
      Stricker, Armand  and
      Khosla, Sopan",
	booktitle = "Proceedings of the 20th Workshop of Young Researchers' Roundtable on Spoken Dialogue Systems",
	month = sep,
	year = "2024",
	address = "Kyoto, Japan",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.yrrsds-1.15/",
	pages = "40--42",
	abstract = "In this position paper, we introduce our efforts in modeling listener response generation and its application to dialogue systems. We propose that the cognitive process of generating listener responses involves four levels: attention level, word level, propositional information level, and activity level, with different types of responses used depending on the level. Attention level responses indicate that the listener is listening to and paying attention to the speaker`s speech. Word-level responses demonstrate the listener`s knowledge or understanding of a single representation. Propositional information level responses indicate the listener`s understanding, empathy, and emotions towards a single propositional information. Activity level responses are oriented towards activities. Additionally, we briefly report on our current initiative in generating propositional information level responses using a knowledge graph and LLMs."
}

@inproceedings{heddaya-etal-2024-causal,
    title = "Causal Micro-Narratives",
	author = "Heddaya, Mourad  and
      Zeng, Qingcheng  and
      Zentefis, Alexander  and
      Voigt, Rob  and
      Tan, Chenhao",
	editor = "Lal, Yash Kumar  and
      Clark, Elizabeth  and
      Iyyer, Mohit  and
      Chaturvedi, Snigdha  and
      Brei, Anneliese  and
      Brahman, Faeze  and
      Chandu, Khyathi Raghavi",
	booktitle = "Proceedings of the The 6th Workshop on Narrative Understanding",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.wnu-1.12/",
	doi = "10.18653/v1/2024.wnu-1.12",
	pages = "67--84",
	abstract = "We present a novel approach to classify causal micro-narratives from text. These narratives are sentence-level explanations of the cause(s) and/or effect(s) of a target subject. The approach requires only a subject-specific ontology of causes and effects, and we demonstrate it with an application to inflation narratives. Using a human-annotated dataset spanning historical and contemporary US news articles for training, we evaluate several large language models (LLMs) on this multi-label classification task. The best-performing model{---}a fine-tuned Llama 3.1 8B{---}achieves F1 scores of 0.87 on narrative detection and 0.71 on narrative classification. Comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements. This research establishes a framework for extracting causal micro-narratives from real-world data, with wide-ranging applications to social science research."
}

@inproceedings{rashad-etal-2024-factalign,
    title = "{F}act{A}lign: Fact-Level Hallucination Detection and Classification Through Knowledge Graph Alignment",
	author = "Rashad, Mohamed  and
      Zahran, Ahmed  and
      Amin, Abanoub  and
      Abdelaal, Amr  and
      Altantawy, Mohamed",
	editor = "Ovalle, Anaelia  and
      Chang, Kai-Wei  and
      Cao, Yang Trista  and
      Mehrabi, Ninareh  and
      Zhao, Jieyu  and
      Galstyan, Aram  and
      Dhamala, Jwala  and
      Kumar, Anoop  and
      Gupta, Rahul",
	booktitle = "Proceedings of the 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.trustnlp-1.8/",
	doi = "10.18653/v1/2024.trustnlp-1.8",
	pages = "79--84",
	abstract = "This paper proposes a novel black-box approach for fact-level hallucination detection and classification by transforming the problem into a knowledge graph alignment task. This approach allows us to classify detected hallucinations as either intrinsic or extrinsic. The paper starts by discussing the field of hallucination detection and introducing several approaches to related work. Then, we introduce the proposed FactAlign approach for hallucination detection and discuss how we can use it to classify hallucinations as either intrinsic or extrinsic. Experiments are carried out to evaluate the proposed method against state-of-the-art methods on the hallucination detection task using the WikiBio GPT-3 hallucination dataset, and on the hallucination type classification task using the XSum hallucination annotations dataset. The experimental results show that our method achieves a 0.889 F1 score for the hallucination detection and 0.825 F1 for the hallucination type classification, without any further training, fine-tuning, or producing multiple samples of the LLM response."
}

@inproceedings{saetia-etal-2024-financial,
    title = "Financial Product Ontology Population with Large Language Models",
	author = "Saetia, Chanatip  and
      Phruetthiset, Jiratha  and
      Chalothorn, Tawunrat  and
      Lertsutthiwong, Monchai  and
      Taerungruang, Supawat  and
      Buabthong, Pakpoom",
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Tutubalina, Elena  and
      Nikishina, Irina  and
      Ramesh, Arti  and
      Sakhovskiy, Andrey  and
      Usbeck, Ricardo  and
      Penn, Gerald  and
      Valentino, Marco",
	booktitle = "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.textgraphs-1.4/",
	pages = "53--60",
	abstract = "Ontology population, which aims to extract structured data to enrich domain-specific ontologies from unstructured text, typically faces challenges in terms of data scarcity and linguistic complexity, particularly in specialized fields such as retail banking. In this study, we investigate the application of large language models (LLMs) to populate domain-specific ontologies of retail banking products from Thai corporate documents. We compare traditional span-based approaches to LLMs-based generative methods, with different prompting techniques. Our findings reveal that while span-based methods struggle with data scarcity and the complex linguistic structure, LLMs-based generative approaches substantially outperform, achieving a 61.05{\%} F1 score, with the most improvement coming from providing examples in the prompts. This improvement highlights the potential of LLMs for ontology population tasks, offering a scalable and efficient solution for structured information extraction in especially in low-resource language settings."
}

@inproceedings{chepurova-etal-2024-prompt,
    title = "Prompt Me One More Time: A Two-Step Knowledge Extraction Pipeline with Ontology-Based Verification",
	author = "Chepurova, Alla  and
      Kuratov, Yuri  and
      Bulatov, Aydar  and
      Burtsev, Mikhail",
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Tutubalina, Elena  and
      Nikishina, Irina  and
      Ramesh, Arti  and
      Sakhovskiy, Andrey  and
      Usbeck, Ricardo  and
      Penn, Gerald  and
      Valentino, Marco",
	booktitle = "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.textgraphs-1.5/",
	pages = "61--77",
	abstract = "This study explores a method for extending real-world knowledge graphs (specifically, Wikidata) by extracting triplets from texts with the aid of Large Language Models (LLMs). We propose a two-step pipeline that includes the initial extraction of entity candidates, followed by their refinement and linkage to the canonical entities and relations of the knowledge graph. Finally, we utilize Wikidata relation constraints to select only verified triplets. We compare our approach to a model that was fine-tuned on a machine-generated dataset and demonstrate that it performs better on natural data. Our results suggest that LLM-based triplet extraction from texts, with subsequent verification, is a viable method for real-world applications."
}

@inproceedings{sakhovskiy-etal-2024-textgraphs,
    title = "{T}ext{G}raphs 2024 Shared Task on Text-Graph Representations for Knowledge Graph Question Answering",
	author = {Sakhovskiy, Andrey  and
      Salnikov, Mikhail  and
      Nikishina, Irina  and
      Usmanova, Aida  and
      Kraft, Angelie  and
      M{\"o}ller, Cedric  and
      Banerjee, Debayan  and
      Huang, Junbo  and
      Jiang, Longquan  and
      Abdullah, Rana  and
      Yan, Xi  and
      Ustalov, Dmitry  and
      Tutubalina, Elena  and
      Usbeck, Ricardo  and
      Panchenko, Alexander},
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Tutubalina, Elena  and
      Nikishina, Irina  and
      Ramesh, Arti  and
      Sakhovskiy, Andrey  and
      Usbeck, Ricardo  and
      Penn, Gerald  and
      Valentino, Marco",
	booktitle = "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.textgraphs-1.9/",
	pages = "116--125",
	abstract = "This paper describes the results of the Knowledge Graph Question Answering (KGQA) shared task that was co-located with the TextGraphs 2024 workshop. In this task, given a textual question and a list of entities with the corresponding KG subgraphs, the participating system should choose the entity that correctly answers the question. Our competition attracted thirty teams, four of which outperformed our strong ChatGPT-based zero-shot baseline. In this paper, we overview the participating systems and analyze their performance according to a large-scale automatic evaluation. To the best of our knowledge, this is the first competition aimed at the KGQA problem using the interaction between large language models (LLMs) and knowledge graphs."
}

@inproceedings{kurdiukov-etal-2024-nlp,
    title = "nlp{\_}enjoyers at {T}ext{G}raphs-17 Shared Task: Text-Graph Representations for Knowledge Graph Question Answering using all-{MPN}et",
	author = "Kurdiukov, Nikita  and
      Zinkovich, Viktoriia  and
      Karpukhin, Sergey  and
      Tikhomirov, Pavel",
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Tutubalina, Elena  and
      Nikishina, Irina  and
      Ramesh, Arti  and
      Sakhovskiy, Andrey  and
      Usbeck, Ricardo  and
      Penn, Gerald  and
      Valentino, Marco",
	booktitle = "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.textgraphs-1.10/",
	pages = "126--130",
	abstract = "This paper presents a model for solving the Multiple Choice Question Answering (MCQA) problem, focusing on the impact of subgraph extraction from a Knowledge Graph on model performance. The proposed method combines textual and graph information by adding linearized subgraphs directly into the main question prompt with separate tokens, enhancing the performance of models working with each modality separately. The study also includes an examination of Large Language Model (LLM) backbones and the benefits of linearized subgraphs and sequence length, with efficient training achieved through fine-tuning with LoRA. The top benchmark, using subgraphs and MPNet, achieved an F1 score of 0.3887. The main limitation of the experiments is the reliance on pre-generated subgraphs/triplets from the graph, and the lack of exploration of in-context learning and prompting strategies with decoder-based architectures."
}

@inproceedings{tang-etal-2024-hw,
    title = "{HW}-{TSC} at {T}ext{G}raphs-17 Shared Task: Enhancing Inference Capabilities of {LLM}s with Knowledge Graphs",
	author = "Tang, Wei  and
      Qiao, Xiaosong  and
      Zhao, Xiaofeng  and
      Zhang, Min  and
      Su, Chang  and
      Li, Yuang  and
      Li, Yinglu  and
      Liu, Yilun  and
      Yao, Feiyu  and
      Tao, Shimin  and
      Yang, Hao  and
      Xianghui, He",
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Tutubalina, Elena  and
      Nikishina, Irina  and
      Ramesh, Arti  and
      Sakhovskiy, Andrey  and
      Usbeck, Ricardo  and
      Penn, Gerald  and
      Valentino, Marco",
	booktitle = "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.textgraphs-1.11/",
	pages = "131--136",
	abstract = "In this paper, we present an effective method for TextGraphs-17 Shared Task. This task requires selecting an entity from the candidate entities that is relevant to the given question and answer. The selection process is aided by utilizing the shortest path graph in the knowledge graph, connecting entities in the query to the candidate entity. This task aims to explore how to enhance LLMs output with KGs, although current LLMs have certain logical reasoning capabilities, they may not be certain about their own outputs, and the answers they produce may be correct by chance through incorrect paths. In this case, we have introduced a LLM prompt design strategy based on self-ranking and emotion. Specifically, we let the large model score its own answer choices to reflect its confidence in the answer. Additionally, we add emotional incentives to the prompts to encourage the model to carefully examine the questions. Our submissions was conducted under zero-resource setting, and we achieved the second place in the task with an F1-score of 0.8321."
}

@inproceedings{belikova-etal-2024-jellybell,
    title = "{J}elly{B}ell at {T}ext{G}raphs-17 Shared Task: Fusing Large Language Models with External Knowledge for Enhanced Question Answering",
	author = "Belikova, Julia  and
      Beliakin, Evegeniy  and
      Konovalov, Vasily",
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Tutubalina, Elena  and
      Nikishina, Irina  and
      Ramesh, Arti  and
      Sakhovskiy, Andrey  and
      Usbeck, Ricardo  and
      Penn, Gerald  and
      Valentino, Marco",
	booktitle = "Proceedings of TextGraphs-17: Graph-based Methods for Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.textgraphs-1.15/",
	pages = "154--160",
	abstract = "This work describes an approach to develop Knowledge Graph Question Answering (KGQA) system for TextGraphs-17 shared task. The task focuses on the fusion of Large Language Models (LLMs) with Knowledge Graphs (KGs). The goal is to select a KG entity (out of several candidates) which corresponds to an answer given a textual question. Our approach applies LLM to identify the correct answer among the list of possible candidates. We confirm that integrating external information is particularly beneficial when the subject entities are not well-known, and using RAG can negatively impact the performance of LLM on questions related to popular entities, as the retrieved context might be misleading. With our result, we achieved 2nd place in the post-evaluation phase."
}

@inproceedings{camboim-de-sa-etal-2024-socio,
    title = "Socio-cultural adapted chatbots: Harnessing Knowledge Graphs and Large Language Models for enhanced context awarenes",
	author = "Camboim de S{\'a}, Jader  and
      Anastasiou, Dimitra  and
      Da Silveira, Marcos  and
      Pruski, C{\'e}dric",
	editor = {Hosseini-Kivanani, Nina  and
      H{\"o}hn, Sviatlana  and
      Anastasiou, Dimitra  and
      Migge, Bettina  and
      Soltan, Angela  and
      Dippold, Doris  and
      Kamlovskaya, Ekaterina  and
      Philippy, Fred},
	booktitle = "Proceedings of the 1st Worskhop on Towards Ethical and Inclusive Conversational AI: Language Attitudes, Linguistic Diversity, and Language Rights (TEICAI 2024)",
	month = mar,
	year = "2024",
	address = "St Julians, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.teicai-1.4/",
	pages = "21--27",
	abstract = "Understanding the socio-cultural context is crucial in machine translation (MT). Although conversational AI systems and chatbots, in particular, are not designed for translation, they can be used for MT purposes. Yet, chatbots often struggle to identify any socio-cultural context during user interactions. In this paper, we highlight this challenge with real-world examples from popular chatbots. We advocate for the use of knowledge graphs as an external source of information that can potentially encapsulate socio-cultural contexts, aiding chatbots in enhancing translation. We further present a method to exploit external knowledge and extract contextual information that can significantly improve text translation, as evidenced by our interactions with these chatbots."
}

@inproceedings{fernandez-etal-2024-incremental,
    title = "Incremental Learning for Knowledge-Grounded Dialogue Systems in Industrial Scenarios",
	author = "Fernandez, Izaskun  and
      Aceta, Cristina  and
      Fernandez, Cristina  and
      Torres, Maria Ines  and
      Etxalar, Aitor  and
      Mendez, Ariane  and
      Agirre, Maia  and
      Torralbo, Manuel  and
      Del Pozo, Arantza  and
      Agirre, Joseba  and
      Artetxe, Egoitz  and
      Altuna, Iker",
	editor = "Kawahara, Tatsuya  and
      Demberg, Vera  and
      Ultes, Stefan  and
      Inoue, Koji  and
      Mehri, Shikib  and
      Howcroft, David  and
      Komatani, Kazunori",
	booktitle = "Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2024",
	address = "Kyoto, Japan",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.sigdial-1.8/",
	doi = "10.18653/v1/2024.sigdial-1.8",
	pages = "92--102",
	abstract = "In today`s industrial landscape, seamless collaboration between humans and machines is essential and requires a shared knowledge of the operational domain. In this framework, the technical knowledge for operator assistance has traditionally been derived from static sources such as technical documents. However, experienced operators hold invaluable know-how that can significantly contribute to support other operators. This work focuses on enhancing the operator assistance tasks in the manufacturing industry by leveraging spoken natural language interaction. More specifically, a Human-in-the-Loop (HIL) incremental learning approach is proposed to integrate this expertise into a domain knowledge graph (KG) dynamically, along with the use of in-context learning for Large Language Models (LLMs) to benefit other capabilities of the system. Preliminary results of the experimentation carried out in an industrial scenario, where the graph size was increased in a 25{\%}, demonstrate that the incremental enhancing of the KG benefits the dialogue system`s performance."
}

@inproceedings{wilcock-2024-anticipating,
    title = "Anticipating Follow-Up Questions in Exploratory Information Search",
	author = "Wilcock, Graham",
	editor = "Kawahara, Tatsuya  and
      Demberg, Vera  and
      Ultes, Stefan  and
      Inoue, Koji  and
      Mehri, Shikib  and
      Howcroft, David  and
      Komatani, Kazunori",
	booktitle = "Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2024",
	address = "Kyoto, Japan",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.sigdial-1.9/",
	doi = "10.18653/v1/2024.sigdial-1.9",
	pages = "103--109",
	abstract = "The paper describes methods for anticipating follow-up questions in exploratory information search. There are two main cases: information stored in knowledge graphs, and information in unstructured texts such as Wikipedia. In the first case, follow-up questions are anticipated by extracting subgraphs relevant to user queries, passing the subgraphs to an LLM to generate responses. In the second case, entities and their relationships are extracted from the texts and added to short-term knowledge graphs relevant to initial queries. Follow-up questions are then anticipated by extracting subgraphs relevant to subsequent queries and passing the subgraphs to the LLM, as in the first case. The short-term graphs in dialogue memory are often sufficient to answer follow-up questions. If they are not, the described steps are repeated as required."
}

@inproceedings{schneider-etal-2024-bridging,
    title = "Bridging Information Gaps in Dialogues with Grounded Exchanges Using Knowledge Graphs",
	author = "Schneider, Phillip  and
      Machner, Nektarios  and
      Jokinen, Kristiina  and
      Matthes, Florian",
	editor = "Kawahara, Tatsuya  and
      Demberg, Vera  and
      Ultes, Stefan  and
      Inoue, Koji  and
      Mehri, Shikib  and
      Howcroft, David  and
      Komatani, Kazunori",
	booktitle = "Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2024",
	address = "Kyoto, Japan",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.sigdial-1.10/",
	doi = "10.18653/v1/2024.sigdial-1.10",
	pages = "110--120",
	abstract = "Knowledge models are fundamental to dialogue systems for enabling conversational interactions, which require handling domain-specific knowledge. Ensuring effective communication in information-providing conversations entails aligning user understanding with the knowledge available to the system. However, dialogue systems often face challenges arising from semantic inconsistencies in how information is expressed in natural language compared to how it is represented within the system`s internal knowledge. To address this problem, we study the potential of large language models for conversational grounding, a mechanism to bridge information gaps by establishing shared knowledge between dialogue participants. Our approach involves annotating human conversations across five knowledge domains to create a new dialogue corpus called BridgeKG. Through a series of experiments on this dataset, we empirically evaluate the capabilities of large language models in classifying grounding acts and identifying grounded information items within a knowledge graph structure. Our findings offer insights into how these models use in-context learning for conversational grounding tasks and common prediction errors, which we illustrate with examples from challenging dialogues. We discuss how the models handle knowledge graphs as a semantic layer between unstructured dialogue utterances and structured information items."
}

@inproceedings{vukovic-etal-2024-dialogue,
    title = "Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought Decoding",
	author = "Vukovic, Renato  and
      Arps, David  and
      van Niekerk, Carel  and
      Ruppik, Benjamin Matthias  and
      Lin, Hsien-chin  and
      Heck, Michael  and
      Gasic, Milica",
	editor = "Kawahara, Tatsuya  and
      Demberg, Vera  and
      Ultes, Stefan  and
      Inoue, Koji  and
      Mehri, Shikib  and
      Howcroft, David  and
      Komatani, Kazunori",
	booktitle = "Proceedings of the 25th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2024",
	address = "Kyoto, Japan",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.sigdial-1.33/",
	doi = "10.18653/v1/2024.sigdial-1.33",
	pages = "370--384",
	abstract = "State-of-the-art task-oriented dialogue systems typically rely on task-specific ontologies for fulfilling user queries. The majority of task-oriented dialogue data, such as customer service recordings, comes without ontology and annotation. Such ontologies are normally built manually, limiting the application of specialised systems. Dialogue ontology construction is an approach for automating that process and typically consists of two steps: term extraction and relation extraction. In this work, we focus on relation extraction in a transfer learning set-up. To improve the generalisation, we propose an extension to the decoding mechanism of large language models. We adapt Chain-of-Thought (CoT) decoding, recently developed for reasoning problems, to generative relation extraction. Here, we generate multiple branches in the decoding space and select the relations based on a confidence threshold. By constraining the decoding to ontology terms and relations, we aim to decrease the risk of hallucination. We conduct extensive experimentation on two widely used datasets and find improvements in performance on target ontology for source fine-tuned and one-shot prompted large language models."
}

@inproceedings{wu-etal-2024-ehdchat,
    title = "{EHDC}hat: A Knowledge-Grounded, Empathy-Enhanced Language Model for Healthcare Interactions",
	author = "Wu, Shenghan  and
      Hsu, Wynne  and
      Lee, Mong Li",
	editor = "Hale, James  and
      Chawla, Kushal  and
      Garg, Muskan",
	booktitle = "Proceedings of the Second Workshop on Social Influence in Conversations (SICon 2024)",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.sicon-1.10/",
	doi = "10.18653/v1/2024.sicon-1.10",
	pages = "141--151",
	abstract = "Large Language Models (LLMs) excel at a range of tasks but often struggle with issues like hallucination and inadequate empathy support. To address hallucinations, we ground our dialogues in medical knowledge sourced from external repositories such as Disease Ontology and DrugBank. To improve empathy support, we develop the Empathetic Healthcare Dialogues dataset, which utilizes multiple dialogue strategies in each response. This dataset is then used to fine-tune an LLM, and we introduce a lightweight, adaptable method called Strategy Combination Guidance to enhance the emotional support capabilities of the fine-tuned model, named EHDChat. Our evaluations show that EHDChat significantly outperforms existing models in providing emotional support and medical accuracy, demonstrating the effectiveness of our approach in enhancing empathetic and informed AI interactions in healthcare."
}

@inproceedings{nagao-katsurai-2024-researcher,
    title = "Researcher Representations Based on Aggregating Embeddings of Publication Titles: A Case Study in a {J}apanese Academic Database",
	author = "Nagao, Hiroyoshi  and
      Katsurai, Marie",
	editor = "Ghosal, Tirthankar  and
      Singh, Amanpreet  and
      Waard, Anita  and
      Mayr, Philipp  and
      Naik, Aakanksha  and
      Weller, Orion  and
      Lee, Yoonjoo  and
      Shen, Shannon  and
      Qin, Yanxia",
	booktitle = "Proceedings of the Fourth Workshop on Scholarly Document Processing (SDP 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.sdp-1.26/",
	pages = "277--282",
	abstract = "Constructing researcher representations is crucial for search and recommendation in academic databases. While recent studies presented methods based on knowledge graph embeddings, obtaining a complete graph of academic entities might be sometimes challenging due to the lack of linked data.By contrast, the textual list of publications of each researcher, which represents their research interests and expertise, is usually easy to obtain.Therefore, this study focuses on creating researcher representations based on textual embeddings of their publication titles and assesses their practicality. We aggregate embeddings of each researcher`s multiple publications into a single vector and apply it to research field classification and similar researcher search tasks. We experimented with multiple language models and embedding aggregation methods to compare their performance.From the model perspective, we confirmed the effectiveness of using sentence embedding models and a simple averaging approach."
}

@inproceedings{laube-eliasmith-2024-qavsa,
    title = "{QAVSA}: Question Answering using Vector Symbolic Algebras",
	author = "Laube, Ryan  and
      Eliasmith, Chris",
	editor = "Zhao, Chen  and
      Mosbach, Marius  and
      Atanasova, Pepa  and
      Goldfarb-Tarrent, Seraphina  and
      Hase, Peter  and
      Hosseini, Arian  and
      Elbayad, Maha  and
      Pezzelle, Sandro  and
      Mozes, Maximilian",
	booktitle = "Proceedings of the 9th Workshop on Representation Learning for NLP (RepL4NLP-2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.repl4nlp-1.14/",
	pages = "191--202",
	abstract = "With the advancement of large pretrained language models (PLMs), many question answering (QA) benchmarks have been developed in order to evaluate the reasoning capabilities of these models. Augmenting PLMs with external knowledge in the form of Knowledge Graphs (KGs) has been a popular method to improve their reasoning capabilities, and a common method to reason over KGs is to use Graph Neural Networks (GNNs). As an alternative to GNNs to augment PLMs, we propose a novel graph reasoning module using Vector Symbolic Algebra (VSA) graph representations and a k-layer MLP. We demonstrate that our VSA-based model performs as well as QA-GNN, a model combining a PLM and a GNN-module, on 3 multiple-choice question answering (MCQA) datasets. Our model has a simpler architecture than QA-GNN and also converges 39{\%} faster during training."
}

@inproceedings{vasilakis-etal-2024-evaluation,
    title = "Evaluation of Pretrained Language Models on Music Understanding",
	author = "Vasilakis, Yannis  and
      Bittner, Rachel  and
      Pauwels, Johan",
	editor = "Kruspe, Anna  and
      Oramas, Sergio  and
      Epure, Elena V.  and
      Sordo, Mohamed  and
      Weck, Benno  and
      Doh, SeungHeon  and
      Won, Minz  and
      Manco, Ilaria  and
      Meseguer-Brocal, Gabriel",
	booktitle = "Proceedings of the 3rd Workshop on NLP for Music and Audio (NLP4MusA)",
	month = nov,
	year = "2024",
	address = "Oakland, USA",
	publisher = "Association for Computational Lingustics",
	url = "https://aclanthology.org/2024.nlp4musa-1.16/",
	pages = "98--106",
	abstract = "Music-text multimodal systems have enabled new approaches to Music Information Research (MIR) applications. Despite the reported success, there has been little effort in evaluating the musical knowledge of Large Language Models (LLM). We demonstrate that LLMs suffer from prompt sensitivity, inability to model negation and sensitivity towards specific words. We quantified these properties as a triplet-based accuracy, evaluating the ability to model the relative similarity of labels in a hierarchical ontology. We leveraged Audioset ontology to generate triplets consisting of anchor, positive and negative label for genre/instruments sub-tree and use six general-purpose Transformer-based models. Triplets required filtering, as some were difficult to judge and therefore relatively uninformative for evaluation purposes. Despite the relatively high accuracy reported, inconsistencies are evident in all six models, suggesting that off-the-shelf LLMs need adaptation to music before use."
}

@inproceedings{c-r-etal-2024-legen,
    title = "{L}e{G}en: Complex Information Extraction from Legal sentences using Generative Models",
	author = "C R, Chaitra  and
      Kulkarni, Sankalp  and
      Sagi, Sai Rama Akash Varma  and
      Pandey, Shashank  and
      Yalavarthy, Rohit  and
      Chakraborty, Dipanjan  and
      Upadhyay, Prajna Devi",
	editor = "Aletras, Nikolaos  and
      Chalkidis, Ilias  and
      Barrett, Leslie  and
      Goanț{\u{a}}, C{\u{a}}t{\u{a}}lina  and
      Preoțiuc-Pietro, Daniel  and
      Spanakis, Gerasimos",
	booktitle = "Proceedings of the Natural Legal Language Processing Workshop 2024",
	month = nov,
	year = "2024",
	address = "Miami, FL, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.nllp-1.1/",
	doi = "10.18653/v1/2024.nllp-1.1",
	pages = "1--17",
	abstract = "Constructing legal knowledge graphs from unstructured legal texts is a complex challenge due to the intricate nature of legal language. While open information extraction (OIE) techniques can convert text into triples of the form subject, relation, object, they often fall short of capturing the nuanced relationships within lengthy legal sentences, necessitating more sophisticated approaches known as complex information extraction. This paper proposes $LeGen$ {--} an end-to-end approach leveraging pre-trained large language models (GPT-4o, T5, BART) to perform complex information extraction from legal sentences. $LeGen$ learns and represents the discourse structure of legal sentences, capturing both their complexity and semantics. It minimizes error propagation typical in multi-step pipelines and achieves up to a 32.2{\%} gain on the Indian Legal benchmark. Additionally, it demonstrates competitive performance on open information extraction benchmarks. A promising application of the resulting legal knowledge graphs is in developing question-answering systems for government schemes, tailored to the Next Billion Users who struggle with the complexity of legal language. Our code and data are available at https://github.com/prajnaupadhyay/LegalIE"
}

@inproceedings{chowdhury-etal-2024-cross,
    title = "Cross Examine: An Ensemble-based approach to leverage Large Language Models for Legal Text Analytics",
	author = "Chowdhury, Saurav  and
      Joshi, Suyog  and
      Dey, Lipika",
	editor = "Aletras, Nikolaos  and
      Chalkidis, Ilias  and
      Barrett, Leslie  and
      Goanț{\u{a}}, C{\u{a}}t{\u{a}}lina  and
      Preoțiuc-Pietro, Daniel  and
      Spanakis, Gerasimos",
	booktitle = "Proceedings of the Natural Legal Language Processing Workshop 2024",
	month = nov,
	year = "2024",
	address = "Miami, FL, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.nllp-1.16/",
	doi = "10.18653/v1/2024.nllp-1.16",
	pages = "194--204",
	abstract = "Legal documents are complex in nature, describing a course of argumentative reasoning that is followed to settle a case. Churning through large volumes of legal documents is a daily requirement for a large number of professionals who need access to the information embedded in them. Natural language processing methods that help in document summarization with key information components, insight extraction and question answering play a crucial role in legal text processing. Most of the existing document analysis systems use supervised machine learning, which require large volumes of annotated training data for every different application and are expensive to build. In this paper we propose a legal text analytics pipeline using Large Language Models (LLM), which can work with little or no training data. For document summarization, we propose an iterative pipeline using retrieval augmented generation to ensure that the generated text remains contextually relevant. For question answering, we propose a novel ontology-driven ensemble approach similar to cross-examination that exploits questioning and verification principles. A knowledge graph, created with the extracted information, stores the key entities and relationships reflecting the repository content structure. A new dataset is created with Indian court documents related to bail applications for cases filed under Protection of Children from Sexual Offences (POCSO) Act, 2012 an Indian law to protect children from sexual abuse and offences. Analysis of insights extracted from the answers reveal patterns of crime and social conditions leading to those crimes, which are important inputs for social scientists as well as legal system."
}

@inproceedings{shao-nakashole-2024-linearizing,
    title = "On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-{SQL}",
	author = "Shao, Yutong  and
      Nakashole, Ndapa",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.8/",
	doi = "10.18653/v1/2024.naacl-long.8",
	pages = "131--156",
	abstract = "Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear.This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model`s ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model`s internal mechanisms, including the ego-centric nature of structure node encodings and the potential for model compression due to modality fusion redundancy. Overall, this work sheds light on the inner workings of linearization-based methods and could potentially provide guidance for future research."
}

@inproceedings{sun-etal-2024-head,
    title = "Head-to-Tail: How Knowledgeable are Large Language Models ({LLM}s)? {A}.{K}.{A}. Will {LLM}s Replace Knowledge Graphs?",
	author = "Sun, Kai  and
      Xu, Yifan  and
      Zha, Hanwen  and
      Liu, Yue  and
      Dong, Xin Luna",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.18/",
	doi = "10.18653/v1/2024.naacl-long.18",
	pages = "311--325",
	abstract = "Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs?To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 16 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities."
}

@inproceedings{li-etal-2024-cosign,
    title = "{COSIGN}: Contextual Facts Guided Generation for Knowledge Graph Completion",
	author = "Li, Jinpeng  and
      Yu, Hang  and
      Luo, Xiangfeng  and
      Liu, Qian",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.93/",
	doi = "10.18653/v1/2024.naacl-long.93",
	pages = "1669--1682",
	abstract = "Knowledge graph completion (KGC) aims to infer missing facts based on existing facts within a KG. Recently, research on generative models (GMs) has addressed the limitations of embedding methods in terms of generality and scalability. However, GM-based methods are sensitive to contextual facts on KG, so the contextual facts of poor quality can cause GMs to generate erroneous results. To improve the performance of GM-based methods for various KGC tasks, we propose a COntextual FactS GuIded GeneratioN (COSIGN) model. First, to enhance the inference ability of the generative model, we designed a contextual facts collector to achieve human-like retrieval behavior. Second, a contextual facts organizer is proposed to learn the organized capabilities of LLMs through knowledge distillation. Finally, the organized contextual facts as the input of the inference generator to generate missing facts. Experimental results demonstrate that COSIGN outperforms state-of-the-art baseline techniques in terms of performance."
}

@inproceedings{ding-etal-2024-zrllm,
    title = "zr{LLM}: Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models",
	author = "Ding, Zifeng  and
      Cai, Heling  and
      Wu, Jingpei  and
      Ma, Yunpu  and
      Liao, Ruotong  and
      Xiong, Bo  and
      Tresp, Volker",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.104/",
	doi = "10.18653/v1/2024.naacl-long.104",
	pages = "1877--1895",
	abstract = "Modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they face a strong challenge in modeling the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic meanings stay close in the embedding space, enabling TKGF models to recognize zero-shot relations even without any observed graph context. Experimental results show that our approach helps TKGF models to achieve much better performance in forecasting the facts with previously unseen relations, while still maintaining their ability in link forecasting regarding seen relations."
}

@inproceedings{agrawal-etal-2024-knowledge,
    title = "Can Knowledge Graphs Reduce Hallucinations in {LLM}s? : A Survey",
	author = "Agrawal, Garima  and
      Kumarage, Tharindu  and
      Alghamdi, Zeyad  and
      Liu, Huan",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.219/",
	doi = "10.18653/v1/2024.naacl-long.219",
	pages = "3947--3960",
	abstract = "The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field."
}

@inproceedings{luo-etal-2024-knowla,
    title = "{K}now{LA}: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation",
	author = "Luo, Xindi  and
      Sun, Zequn  and
      Zhao, Jing  and
      Zhao, Zhe  and
      Hu, Wei",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.396/",
	doi = "10.18653/v1/2024.naacl-long.396",
	pages = "7153--7166",
	abstract = "Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that KnowLA can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts."
}

@inproceedings{tian-etal-2024-theory,
    title = "A Theory Guided Scaffolding Instruction Framework for {LLM}-Enabled Metaphor Reasoning",
	author = "Tian, Yuan  and
      Xu, Nan  and
      Mao, Wenji",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.428/",
	doi = "10.18653/v1/2024.naacl-long.428",
	pages = "7738--7755",
	abstract = "Metaphor detection is a challenging task in figurative language processing, which aims to distinguish between metaphorical and literal expressions in text. Existing methods tackle metaphor detection via training or fine-tuning discriminative models on labeled data. However, these approaches struggle to explain the underlying reasoning process behind the metaphorical/literal judgment. Recently, large language models (LLMs) have shown promise in language reasoning tasks. Although promising, LLM-based methods for metaphor detection and reasoning are still faced with the challenging issue of bringing the explainable concepts for metaphor reasoning and their linguistic manifestation. To fill this gap, we propose a novel Theory guided Scaffolding Instruction (TSI) framework that instructs an LLM to infer the underlying reasoning process of metaphor detection guided by metaphor theories for the first time. Our work is inspired by a pedagogical strategy called scaffolding instruction, which encourages educators to provide questioning and support as scaffolding so as to assist learners in constructing the understanding of pedagogical goals step by step. We first construct a metaphor knowledge graph grounded in metaphor theory which serves as the instructional structure to obtain a series of scaffolding questions, directing the LLM to incrementally generate the reasoning process for metaphor understanding through dialogue interactions. During this theory guided instruction process, we explore the LLM`s mastery boundary and provide the relevant knowledge as scaffolding support when the question is beyond the LLM`s capability. Experimental results verify that our method significantly outperforms both the LLM-based reasoning methods and the SOTA methods in metaphor detection, indicating the facilitation of metaphor and instruction theories in guiding LLM-based reasoning process."
}

@inproceedings{sakai-etal-2024-pre,
    title = "Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?",
	author = "Sakai, Yusuke  and
      Kamigaito, Hidetaka  and
      Hayashi, Katsuhiko  and
      Watanabe, Taro",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.447/",
	doi = "10.18653/v1/2024.naacl-long.447",
	pages = "8091--8106",
	abstract = "Knowledge graphs (KGs) consist of links that describe relationships between entities. Due to the difficulty of manually enumerating all relationships between entities, automatically completing them is essential for KGs. Knowledge Graph Completion (KGC) is a task that infers unseen relationships between entities in a KG. Traditional embedding-based KGC methods (e.g. RESCAL, TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc.) infer missing links using only the knowledge from training data. In contrast, the recent Pre-trained Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training, which means it can estimate missing links between entities by reusing memorized knowledge from pre-training without inference. This part is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications. To address this issue, we analyze whether PLM-based KGC methods make inferences or merely access memorized knowledge. For this purpose, we propose a method for constructing synthetic datasets specified in this analysis and conclude that PLMs acquire the inference abilities required for KGC through pre-training, even though the performance improvements mostly come from textual information of entities and relations."
}

@inproceedings{su-etal-2024-semi,
    title = "Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning",
	author = "Su, Xin  and
      Le, Tiep  and
      Bethard, Steven  and
      Howard, Phillip",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-long.475/",
	doi = "10.18653/v1/2024.naacl-long.475",
	pages = "8597--8613",
	abstract = "An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model`s parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model`s parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning."
}

@inproceedings{du-etal-2024-zhujiu,
    title = "{Z}hu{J}iu-Knowledge: A Fairer Platform for Evaluating Multiple Knowledge Types in Large Language Models",
	author = "Du, Pengfan  and
      Liang, Sirui  and
      Zhang, Baoli  and
      Cao, Pengfei  and
      Chen, Yubo  and
      Liu, Kang  and
      Zhao, Jun",
	editor = "Chang, Kai-Wei  and
      Lee, Annie  and
      Rajani, Nazneen",
	booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.naacl-demo.20/",
	doi = "10.18653/v1/2024.naacl-demo.20",
	pages = "194--206",
	abstract = "The swift advancement in large language models (LLMs) has heightened the importance of model evaluations. LLMs have acquired a substantial amount of knowledge, and evaluating the knowledge of these LLMs is crucial. To address this, we introduce the ZhuJiu-Knowledge benchmark which carefully considers the following factors: (1) For knowledge scope, we concentrate on three domains: commonsense knowledge, world knowledge, language knowledge, which comes from ATOMIC, Conceptnet, Wikidata, and Wordnet. (2) For data construction, to prevent data contamination, we utilize knowledge derived from corpora and knowledge graphs to formulate novel questions which are ensured not to appear in the training corpus. A multitude of prompts is purposefully devised to mitigate the impact of prompt design on evaluation and to further analyze the LLMs' sensitivity to various prompts. (3) For evaluation criteria, we propose a novel voting methodology for assessing generative text, aligning the model`s evaluation with human preferences to reduce biases inherent in individual model assessments. We evaluate 14 current mainstream LLMs and conduct a comprehensive discussion and analysis of their results. The ZhuJiu-Knowledge benchmark and open-participation leaderboard are publicly released at http://zhujiu-knowledge.top and we also provide a demo video at https://youtu.be/QJp4qlEHVH8."
}

@inproceedings{priya-etal-2024-knowledge,
    title = "Knowledge-enhanced Response Generation in Dialogue Systems: Current Advancements and Emerging Horizons",
	author = "Priya, Priyanshu  and
      Varshney, Deeksha  and
      Firdaus, Mauajama  and
      Ekbal, Asif",
	editor = "Klinger, Roman  and
      Okazaki, Naozaki  and
      Calzolari, Nicoletta  and
      Kan, Min-Yen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Summaries",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-tutorials.13/",
	pages = "80--87",
	abstract = "This tutorial provides an in-depth exploration of Knowledge-enhanced Dialogue Systems (KEDS), diving into their foundational aspects, methodologies, advantages, and practical applications. Topics include the distinction between internal and external knowledge integration, diverse methodologies employed in grounding dialogues, and innovative approaches to leveraging knowledge graphs for enhanced conversation quality. Furthermore, the tutorial touches upon the rise of biomedical text mining, the advent of domain-specific language models, and the challenges and strategies specific to medical dialogue generation. The primary objective is to give attendees a comprehensive understanding of KEDS. By delineating the nuances of these systems, the tutorial aims to elucidate their significance, highlight advancements made using deep learning, and pinpoint the current challenges. Special emphasis is placed on showcasing how KEDS can be fine-tuned for domain-specific requirements, with a spotlight on the healthcare sector. The tutorial is crafted for both beginners and intermediate researchers in the dialogue systems domain, with a focus on those keen on advancing research in KEDS. It will also be valuable for practitioners in sectors like healthcare, seeking to integrate advanced dialogue systems."
}

@inproceedings{gajbhiye-etal-2024-amended,
    title = "{AM}en{D}e{D}: Modelling Concepts by Aligning Mentions, Definitions and Decontextualised Embeddings",
	author = "Gajbhiye, Amit  and
      Bouraoui, Zied  and
      Espinosa Anke, Luis  and
      Schockaert, Steven",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.72/",
	pages = "801--811",
	abstract = "Contextualised Language Models (LM) improve on traditional word embeddings by encoding the meaning of words in context. However, such models have also made it possible to learn high-quality decontextualised concept embeddings. Three main strategies for learning such embeddings have thus far been considered: (i) fine-tuning the LM to directly predict concept embeddings from the name of the concept itself, (ii) averaging contextualised representations of mentions of the concept in a corpus, and (iii) encoding definitions of the concept. As these strategies have complementary strengths and weaknesses, we propose to learn a unified embedding space in which all three types of representations can be integrated. We show that this allows us to outperform existing approaches in tasks such as ontology completion, which heavily depends on access to high-quality concept embeddings. We furthermore find that mentions and definitions are well-aligned in the resulting space, enabling tasks such as target sense verification, even without the need for any fine-tuning."
}

@inproceedings{wasi-etal-2024-banglaautokg,
    title = "{B}angla{A}uto{KG}: Automatic {B}angla Knowledge Graph Construction with Semantic Neural Graph Filtering",
	author = "Wasi, Azmine Toushik  and
      Rafi, Taki Hasan  and
      Islam, Raima  and
      Chae, Dong-Kyu",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.189/",
	pages = "2100--2106",
	abstract = "Knowledge Graphs (KGs) have proven essential in information processing and reasoning applications because they link related entities and give context-rich information, supporting efficient information retrieval and knowledge discovery; presenting information flow in a very effective manner. Despite being widely used globally, Bangla is relatively underrepresented in KGs due to a lack of comprehensive datasets, encoders, NER (named entity recognition) models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient information processing and reasoning applications in the language. Addressing the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework that is able to automatically construct Bengali KGs from any Bangla text. We utilize multilingual LLMs to understand various languages and correlate entities and relations universally. By employing a translation dictionary to identify English equivalents and extracting word features from pre-trained BERT models, we construct the foundational KG. To reduce noise and align word embeddings with our goal, we employ graph-based polynomial filters. Lastly, we implement a GNN-based semantic filter, which elevates contextual understanding and trims unnecessary edges, culminating in the formation of the definitive KG. Empirical findings and case studies demonstrate the universal effectiveness of our model, capable of autonomously constructing semantically enriched KGs from any text. Data and code are available here: https://github.com/azminewasi/BanglaAutoKG"
}

@inproceedings{wei-etal-2024-collabkg,
    title = "{C}ollab{KG}: A Learnable Human-Machine-Cooperative Information Extraction Toolkit for (Event) Knowledge Graph Construction",
	author = "Wei, Xiang  and
      Chen, Yufeng  and
      Cheng, Ning  and
      Cui, Xingyu  and
      Xu, Jinan  and
      Han, Wenjuan",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.310/",
	pages = "3490--3506",
	abstract = "In order to construct or extend entity-centric and event-centric knowledge graphs (KG and EKG), the information extraction (IE) annotation toolkit is essential. However, existing IE toolkits have several non-trivial problems, such as not supporting multi-tasks, and not supporting automatic updates. In this work, we present CollabKG, a learnable human-machine-cooperative IE toolkit for KG and EKG construction. Specifically, for the multi-task issue, CollabKG unifies different IE subtasks, including named entity recognition (NER), entity-relation triple extraction (RE), and event extraction (EE), and supports both KG and EKG. Then, combining advanced prompting-based IE technology, the human-machine-cooperation mechanism with Large Language Models (LLMs) as the assistant machine is presented which can provide a lower cost as well as a higher performance. Lastly, owing to the two-way interaction between the human and machine, CollabKG with learning ability allows self-renewal. Besides, CollabKG has several appealing features (e.g., customization, training-free, and label propagation) that make the system powerful and high-productivity. We holistically compare our toolkit with other existing tools on these features. Human evaluation quantitatively illustrates that CollabKG significantly improves annotation quality, efficiency, and stability simultaneously."
}

@inproceedings{mousavi-etal-2024-construction,
    title = "Construction of Paired Knowledge Graph - Text Datasets Informed by Cyclic Evaluation",
	author = "Mousavi, Ali  and
      Zhan, Xin  and
      Bai, He  and
      Shi, Peng  and
      Rekatsinas, Theodoros  and
      Han, Benjamin  and
      Li, Yunyao  and
      Pound, Jeffrey  and
      Susskind, Joshua M.  and
      Schluter, Natalie  and
      Ilyas, Ihab F.  and
      Jaitly, Navdeep",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.335/",
	pages = "3782--3803",
	abstract = "Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Informed by these observations, we construct a new, improved dataset called \textbf{LAGRANGE} using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology."
}

@inproceedings{winter-etal-2024-ddxgym,
    title = "{DD}x{G}ym: Online Transformer Policies in a Knowledge Graph Based Natural Language Environment",
	author = "Winter, Benjamin  and
      Figueroa Rosero, Alexei Gustavo  and
      Loeser, Alexander  and
      Gers, Felix Alexander  and
      Figueroa Rosero, Nancy Katerina  and
      Krestel, Ralf",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.396/",
	pages = "4438--4448",
	abstract = "Differential diagnosis (DDx) is vital for physicians and challenging due to the existence of numerous diseases and their complex symptoms. Model training for this task is generally hindered by limited data access due to privacy concerns. To address this, we present DDxGym, a specialized OpenAI Gym environment for clinical differential diagnosis. DDxGym formulates DDx as a natural-language-based reinforcement learning (RL) problem, where agents emulate medical professionals, selecting examinations and treatments for patients with randomly sampled diseases. This RL environment utilizes data labeled from online resources, evaluated by medical professionals for accuracy. Transformers, while effective for encoding text in DDxGym, are unstable in online RL. For that reason we propose a novel training method using an auxiliary masked language modeling objective for policy optimization, resulting in model stabilization and significant performance improvement over strong baselines. Following this approach, our agent effectively navigates large action spaces and identifies universally applicable actions. All data, environment details, and implementation, including experiment reproduction code, are made publicly available."
}

@inproceedings{huang-etal-2024-distill,
    title = "Distill, Fuse, Pre-train: Towards Effective Event Causality Identification with Commonsense-Aware Pre-trained Model",
	author = "Huang, Peixin  and
      Zhao, Xiang  and
      Hu, Minghao  and
      Tan, Zhen  and
      Xiao, Weidong",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.450/",
	pages = "5029--5040",
	abstract = "Event Causality Identification (ECI) aims to detect causal relations between events in unstructured texts. This task is challenged by the lack of data and explicit causal clues. Some methods incorporate explicit knowledge from external knowledge graphs (KGs) into Pre-trained Language Models (PLMs) to tackle these issues, achieving certain accomplishments. However, they ignore that existing KGs usually contain trivial knowledge which may prejudice the performance. Moreover, they simply integrate the concept triplets, underutilizing the deep interaction between the text and external graph. In this paper, we propose an effective pipeline DFP, i.e., Distill, Fuse and Pre-train, to build a commonsense-aware pre-trained model which integrates reliable task-specific knowledge from commonsense graphs. This pipeline works as follows: (1) To leverage the reliable knowledge, commonsense graph distillation is proposed to distill commonsense graphs and obtain the meta-graph which contain credible task-oriented knowledge. (2) To model the deep interaction between the text and external graph, heterogeneous information fusion is proposed to fuse them through a commonsense-aware memory network. (3) Continual pre-training designs three continual pre-training tasks to further align and fuse the text and the commonsense meta-graph. Through extensive experiments on two benchmarks, we demonstrate the validity of our pipeline."
}

@inproceedings{hu-etal-2024-eee,
    title = "{EEE}-{QA}: Exploring Effective and Efficient Question-Answer Representations",
	author = "Hu, Zhanghao  and
      Yang, Yijun  and
      Xu, Junjie  and
      Qiu, Yifu  and
      Chen, Pinzhen",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.490/",
	pages = "5520--5525",
	abstract = "Current approaches to question answering rely on pre-trained language models (PLMs) like RoBERTa. This work challenges the existing question-answer encoding convention and explores finer representations. We begin with testing various pooling methods compared to using the begin-of-sentence token as a question representation for better quality. Next, we explore opportunities to simultaneously embed all answer candidates with the question. This enables cross-reference between answer choices and improves inference throughput via reduced memory usage. Despite their simplicity and effectiveness, these methods have yet to be widely studied in current frameworks. We experiment with different PLMs, and with and without the integration of knowledge graphs. Results prove that the memory efficacy of the proposed techniques with little sacrifice in performance. Practically, our work enhances 38-100{\%} throughput with 26-65{\%} speedups on consumer-grade GPUs by allowing for considerably larger batch sizes. Our work sends a message to the community with promising directions in both representation quality and efficiency for the question-answering task in natural language processing."
}

@inproceedings{sawczyn-etal-2024-empowering,
    title = "Empowering Small-Scale Knowledge Graphs: A Strategy of Leveraging General-Purpose Knowledge Graphs for Enriched Embeddings",
	author = "Sawczyn, Albert  and
      Binkowski, Jakub  and
      Bielak, Piotr  and
      Kajdanowicz, Tomasz",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.512/",
	pages = "5768--5782",
	abstract = "Knowledge-intensive tasks pose a significant challenge for Machine Learning (ML) techniques. Commonly adopted methods, such as Large Language Models (LLMs), often exhibit limitations when applied to such tasks. Nevertheless, there have been notable endeavours to mitigate these challenges, with a significant emphasis on augmenting LLMs through Knowledge Graphs (KGs). While KGs provide many advantages for representing knowledge, their development costs can deter extensive research and applications. Addressing this limitation, we introduce a framework for enriching embeddings of small-scale domain-specific Knowledge Graphs with well-established general-purpose KGs. Adopting our method, a modest domain-specific KG can benefit from a performance boost in downstream tasks when linked to a substantial general-purpose KG. Experimental evaluations demonstrate a notable enhancement, with up to a 44{\%} increase observed in the Hits@10 metric. This relatively unexplored research direction can catalyze more frequent incorporation of KGs in knowledge-intensive tasks, resulting in more robust, reliable ML implementations, which hallucinates less than prevalent LLM solutions."
}

@inproceedings{agarwal-etal-2024-ethical,
    title = "Ethical Reasoning and Moral Value Alignment of {LLM}s Depend on the Language We Prompt Them in",
	author = "Agarwal, Utkarsh  and
      Tanmay, Kumar  and
      Khandelwal, Aditi  and
      Choudhury, Monojit",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.560/",
	pages = "6330--6340",
	abstract = "Ethical reasoning is a crucial skill for Large Language Models (LLMs). However, moral values are not universal, but rather influenced by language and culture. This paper explores how three prominent LLMs {--} GPT-4, ChatGPT, and Llama2Chat-70B {--} perform ethical reasoning in different languages and if their moral judgement depend on the language in which they are prompted. We extend the study of ethical reasoning of LLMs by (CITATION) to a multilingual setup following their framework of probing LLMs with ethical dilemmas and policies from three branches of normative ethics: deontology, virtue, and consequentialism. We experiment with six languages: English, Spanish, Russian, Chinese, Hindi, and Swahili. We find that GPT-4 is the most consistent and unbiased ethical reasoner across languages, while ChatGPT and Llama2Chat-70B show significant moral value bias when we move to languages other than English. Interestingly, the nature of this bias significantly vary across languages for all LLMs, including GPT-4."
}

@inproceedings{jiayang-etal-2024-eventground,
    title = "{E}vent{G}round: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs",
	author = "Jiayang, Cheng  and
      Qiu, Lin  and
      Chan, Chunkit  and
      Liu, Xin  and
      Song, Yangqiu  and
      Zhang, Zheng",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.587/",
	pages = "6622--6642",
	abstract = "Narrative reasoning relies on the understanding of eventualities in story contexts, which requires a wealth of background world knowledge. To help machines leverage such knowledge, existing solutions can be categorized into two groups. Some focus on implicitly modeling eventuality knowledge by pretraining language models (LMs) with eventuality-aware objectives. However, this approach breaks down knowledge structures and lacks interpretability. Others explicitly collect world knowledge of eventualities into structured eventuality-centric knowledge graphs (KGs). However, existing research on leveraging these knowledge sources for free-texts is limited. In this work, we propose an initial comprehensive framework called EventGround, which aims to tackle the problem of grounding free-texts to eventuality-centric KGs for contextualized narrative reasoning. We identify two critical problems in this direction: the event representation and sparsity problems. We provide simple yet effective parsing and partial information extraction methods to tackle these problems. Experimental results demonstrate that our approach consistently outperforms baseline models when combined with graph neural network (GNN) or large language model (LLM) based graph reasoning models. Our framework, incorporating grounded knowledge, achieves state-of-the-art performance while providing interpretable evidence."
}

@inproceedings{qian-etal-2024-harnessing,
    title = "Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing",
	author = "Qian, Zhenyu  and
      Qian, Yiming  and
      Song, Yuting  and
      Gao, Fei  and
      Jin, Hai  and
      Yu, Chen  and
      Xie, Xia",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.707/",
	pages = "8035--8049",
	abstract = "Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across ten diverse benchmark datasets. Moreover, to address the challenge of explainability, we propose an uncertainty estimation based on perturbation, along with a calibration scheme to quantify the confidence scores of the generated answers. Our confidence measure achieves an AUC of 0.8 or higher on seven out of the ten datasets in predicting the correctness of the answer generated by LLM."
}

@inproceedings{wang-etal-2024-ideate,
    title = "{IDEATE}: Detecting {AI}-Generated Text Using Internal and External Factual Structures",
	author = "Wang, Quan  and
      Zhang, Licheng  and
      Guo, Zikang  and
      Mao, Zhendong",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.751/",
	pages = "8556--8568",
	abstract = "The effective detection of AI-generated text is a vital principle to ensure responsible use of large language models (LLMs). Previous studies mainly focused on discovering and utilizing internal evidences contained in the text itself to perform the detection, while ignoring external evidences implicated in an established knowledge graph (KG) which may also be key discriminative factors between AI-generated and human-written text. To address this deficiency, we propose IDEATE, a novel hierarchical graph network that utilizes both internal and external factual structures to detect AI-generated text. IDEATE consists of a mention-level subgraph at the bottom to describe internal factual structures of mentioned entities reflected in the input text, and an entity-level subgraph at the top to describe external factual structures of mentioned entities reflected in an external KG. Hierarchical graph convolution is then applied successively on the two subgraphs, through which the two types of factual structures will be embedded into the output and used for the final detection. Extensive experiments on four benchmarking datasets show that IDEATE consistently outperforms current state-of-the-art methods in detecting text generated by various LLMs, ranging from GPT-2 to the more powerful ChatGPT, verifying the necessity and superiority of introducing external evidences for AI-generated text detection."
}

@inproceedings{wang-etal-2024-kc,
    title = "{KC}-{G}en{R}e: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion",
	author = "Wang, Yilin  and
      Hu, Minghao  and
      Huang, Zhen  and
      Li, Dongsheng  and
      Yang, Dong  and
      Lu, Xicheng",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.845/",
	pages = "9668--9680",
	abstract = "The goal of knowledge graph completion (KGC) is to predict missing facts among entities. Previous methods for KGC re-ranking are mostly built on non-generative language models to obtain the probability of each candidate. Recently, generative large language models (LLMs) have shown outstanding performance on several tasks such as information extraction and dialog systems. Leveraging them for KGC re-ranking is beneficial for leveraging the extensive pre-trained knowledge and powerful generative capabilities. However, it may encounter new problems when accomplishing the task, namely mismatch, misordering and omission. To this end, we introduce KC-GenRe, a knowledge-constrained generative re-ranking method based on LLMs for KGC. To overcome the mismatch issue, we formulate the KGC re-ranking task as a candidate identifier sorting generation problem implemented by generative LLMs. To tackle the misordering issue, we develop a knowledge-guided interactive training method that enhances the identification and ranking of candidates. To address the omission issue, we design a knowledge-augmented constrained inference method that enables contextual prompting and controlled generation, so as to obtain valid rankings. Experimental results show that KG-GenRe achieves state-of-the-art performance on four datasets, with gains of up to 6.7{\%} and 7.7{\%} in the MRR and Hits@1 metric compared to previous methods, and 9.0{\%} and 11.1{\%} compared to that without re-ranking. Extensive analysis demonstrates the effectiveness of components in KG-GenRe."
}

@inproceedings{li-etal-2024-kehrl,
    title = "{KEHRL}: Learning Knowledge-Enhanced Language Representations with Hierarchical Reinforcement Learning",
	author = "Li, Dongyang  and
      Zhang, Taolin  and
      Huang, Longtao  and
      Wang, Chengyu  and
      He, Xiaofeng  and
      Xue, Hui",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.847/",
	pages = "9693--9704",
	abstract = "Knowledge-enhanced pre-trained language models (KEPLMs) leverage relation triples from knowledge graphs (KGs) and integrate these external data sources into language models via self-supervised learning. Previous works treat knowledge enhancement as two independent operations, i.e., knowledge injection and knowledge integration. In this paper, we propose to learn Knowledge-Enhanced language representations with Hierarchical Reinforcement Learning (KEHRL), which jointly addresses the problems of detecting positions for knowledge injection and integrating external knowledge into the model in order to avoid injecting inaccurate or irrelevant knowledge. Specifically, a high-level reinforcement learning (RL) agent utilizes both internal and prior knowledge to iteratively detect essential positions in texts for knowledge injection, which filters out less meaningful entities to avoid diverting the knowledge learning direction. Once the entity positions are selected, a relevant triple filtration module is triggered to perform low-level RL to dynamically refine the triples associated with polysemic entities through binary-valued actions. Experiments validate KEHRL`s effectiveness in probing factual knowledge and enhancing the model`s performance on various natural language understanding tasks."
}

@inproceedings{nie-etal-2024-know,
    title = "Know-Adapter: Towards Knowledge-Aware Parameter-Efficient Transfer Learning for Few-shot Named Entity Recognition",
	author = "Nie, Binling  and
      Shao, Yiming  and
      Wang, Yigang",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.854/",
	pages = "9777--9786",
	abstract = "Parameter-Efficient Fine-Tuning (PEFT) is a promising approach to mitigate the challenges about the model adaptation of pretrained language models (PLMs) for the named entity recognition (NER) task. Recent studies have highlighted the improvements that can be made to the quality of information retrieved from PLMs by adding explicit knowledge from external source like KGs to otherwise naive PEFTs. In this paper, we propose a novel knowledgeable adapter, Know-adapter, to incorporate structure and semantic knowledge of knowledge graphs into PLMs for few-shot NER. First, we construct a related KG entity type sequence for each sentence using a knowledge retriever. However, the type system of a domain-specific NER task is typically independent of that of current KGs and thus exhibits heterogeneity issue inevitably, which makes matching between the original NER and KG types (e.g. Person in NER potentially matches President in KBs) less likely, or introduces unintended noises. Thus, then we design a unified taxonomy based on KG ontology for KG entity types and NER labels. This taxonomy is used to build a learnable shared representation module, which provides shared representations for both KG entity type sequences and NER labels. Based on these shared representations, our Know-adapter introduces high semantic relevance knowledge and structure knowledge from KGs as inductive bias to guide the updating process of the adapter. Additionally, the shared representations guide the learnable representation module to reduce noise in the unsupervised expansion of label words. Extensive experiments on multiple NER datasets show the superiority of Know-Adapter over other state-of-the-art methods in both full-resource and low-resource settings."
}

@inproceedings{lin-etal-2024-kpatch,
    title = "{KP}atch: Knowledge Patch to Pre-trained Language Model for Zero-Shot Stance Detection on Social Media",
	author = "Lin, Shuohao  and
      Chen, Wei  and
      Gao, Yunpeng  and
      Jiang, Zhishu  and
      Liao, Mengqi  and
      Zhang, Zhiyu  and
      Zhao, Shuyuan  and
      Wan, Huaiyu",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.871/",
	pages = "9961--9973",
	abstract = "Zero-shot stance detection on social media (ZSSD-SM) aims to distinguish the attitude in tweets towards an unseen target. Previous work capture latent variables between source and target domains to perform this task, but the lack of context knowledge hinders the detection performance. Recent studies have been devoted to obtaining the accurate representation of tweets by bringing additional facts from Knowledge Graph (KG), showing promising performance. However, these knowledge injection methods still suffer from two challenges: (i) The pipeline of knowledge injection causes error accumulation and (ii) irrelevant knowledge makes them fail to understand the semantics. In this paper, we propose a novel knowledge injection method for ZSSD-SM, which adopts two training stages, namely knowledge compression and task guidance, to flexibly inject knowledge into the pre-trained language model (PLM) and adaptively expand tweets context. Specifically, in the knowledge compression stage, the latent representation of KG is reconstructed by the triplet denoising task and compressed into external matrices; while in the task guidance stage, the frozen matrices are employed to guide the PLM to adaptively extract its own context-related knowledge, and then complete the fine-tuning of the ZSSD-SM task. Extensive experiments on multiple datasets show the effectiveness of our proposed method. The code is available at: https://github.com/ShuohaoLin/KPatch."
}

@inproceedings{wang-etal-2024-leros,
    title = "Leros: Learning Explicit Reasoning on Synthesized Data for Commonsense Question Answering",
	author = "Wang, Chenhao  and
      Cao, Pengfei  and
      Li, Jiachun  and
      Chen, Yubo  and
      Liu, Kang  and
      Jiang, Xiaojian  and
      Xu, Jiexin  and
      Qiuxia, Li  and
      Zhao, Jun",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.900/",
	pages = "10303--10315",
	abstract = "Recent work shows large language models can be prompted to generate useful rationales for commonsense question answering (CQA), which can improve the performance of both themselves and other models. However, the cost of deployment and further tuning is relatively expensive for the large models. Some work explores to distill the the rationale-generation ability to convenient small-sized models, yet it typically requires human-authored QA instances during the distillation. In this paper, we propose a novel framework that leverages both knowledge graphs and large language models to synthesize rationale-augmented CQA data. Based on it, we train Leros, a model that can generate helpful rationales to assist generic QA models to accomplish unseen CQA tasks. Empirical results demonstrate Leros can substantially enhance the performance of QA models on five unseen CQA benchmarks, providing better gains than both same-sized counterpart models trained with downstream data and 10x larger language models. Our work reveals a novel way to integrate knowledge from both knowledge graphs and large language models into smaller models. The codes and synthesized resources are publicly available at https://github.com/wchrepo/leros."
}

@inproceedings{park-pado-2024-multi,
    title = "Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for {K}orean",
	author = "Park, Dojun  and
      Pad{\'o}, Sebastian",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1024/",
	pages = "11723--11744",
	abstract = "Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperforms its counterpart in the style dimension while reference-based models retain an edge regarding accuracy. Overall, RemBERT emerges as the most promising model. Through our evaluation, we offer an insight into the translation quality in a more fine-grained, interpretable manner."
}

@inproceedings{xu-etal-2024-multi,
    title = "Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models",
	author = "Xu, Derong  and
      Zhang, Ziheng  and
      Lin, Zhenxi  and
      Wu, Xian  and
      Zhu, Zhihong  and
      Xu, Tong  and
      Zhao, Xiangyu  and
      Zheng, Yefeng  and
      Chen, Enhong",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1044/",
	pages = "11956--11968",
	abstract = "Knowledge graph completion (KGC) is a widely used method to tackle incompleteness in knowledge graphs (KGs) by making predictions for missing links. Description-based KGC leverages pre-trained language models to learn entity and relation representations with their names or descriptions, which shows promising results. However, the performance of description-based KGC is still limited by the quality of text and the incomplete structure, as it lacks sufficient entity descriptions and relies solely on relation names, leading to sub-optimal results. To address this issue, we propose MPIKGC, a general framework to compensate for the deficiency of contextualized knowledge and improve KGC by querying large language models (LLMs) from various perspectives, which involves leveraging the reasoning, explanation, and summarization capabilities of LLMs to expand entity descriptions, understand relations, and extract structures, respectively. We conducted extensive evaluation of the effectiveness and improvement of our framework based on four description-based KGC models, for both link prediction and triplet classification tasks. All codes and generated data will be publicly available after review."
}

@inproceedings{liu-etal-2024-primo,
    title = "{PRIMO}: Progressive Induction for Multi-hop Open Rule Generation",
	author = "Liu, Jianyu  and
      Bi, Sheng  and
      Qi, Guilin",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1137/",
	pages = "12988--12998",
	abstract = "Open rules refer to the implication from premise atoms to hypothesis atoms, which captures various relationships between instances in the real world. Injecting open rule knowledge into the machine helps to improve the performance of downstream tasks such as dialogue and relation extraction. Existing approaches focus on single-hop open rule generation, ignoring scenarios involving multiple hops, leading to logical inconsistencies between premise and hypothesis atoms, as well as semantic duplication of generated rule atoms. To address these issues, we propose a progressive multi-stage open rule generation method called PRIMO. We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure consisting of generation, extraction, and rank modules to fully leverage the latent knowledge within the language model across multiple dimensions. Furthermore, we employ reinforcement learning from human feedback to further optimize model, enhancing the model`s understanding of commonsense knowledge. Experimental results demonstrate that compared to baseline models, PRIMO significantly enhances rule quality and diversity while reducing the repetition rate of rule atoms."
}

@inproceedings{zhang-etal-2024-prompt,
    title = "Prompt-based Generation of Natural Language Explanations of Synthetic Lethality for Cancer Drug Discovery",
	author = "Zhang, Ke  and
      Feng, Yimiao  and
      Zheng, Jie",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1150/",
	pages = "13131--13142",
	abstract = "Synthetic lethality (SL) offers a promising approach for targeted anti-cancer therapy. Deeply understanding SL gene pair mechanisms is vital for anti-cancer drug discovery. However, current wet-lab and machine learning-based SL prediction methods lack user-friendly and quantitatively evaluable explanations. To address these problems, we propose a prompt-based pipeline for generating natural language explanations. We first construct a natural language dataset named NexLeth. This dataset is derived from New Bing through prompt-based queries and expert annotations and contains 707 instances. NexLeth enhances the understanding of SL mechanisms and it is a benchmark for evaluating SL explanation methods. For the task of natural language generation for SL explanations, we combine subgraph explanations from an SL knowledge graph (KG) with instructions to construct novel personalized prompts, so as to inject the domain knowledge into the generation process. We then leverage the prompts to fine-tune pre-trained biomedical language models on our dataset. Experimental results show that the fine-tuned model equipped with designed prompts performs better than existing biomedical language models in terms of text quality and explainability, suggesting the potential of our dataset and the fine-tuned model for generating understandable and reliable explanations of SL mechanisms."
}

@inproceedings{kruit-etal-2024-retrieval,
    title = "Retrieval-based Question Answering with Passage Expansion Using a Knowledge Graph",
	author = "Kruit, Benno  and
      Xu, Yiming  and
      Kalo, Jan-Christoph",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1225/",
	pages = "14063--14072",
	abstract = "Recent advancements in dense neural retrievers and language models have led to large improvements in state-of-the-art approaches to open-domain Question Answering (QA) based on retriever-reader architectures. However, issues stemming from data quality and imbalances in the use of dense embeddings have hindered performance, particularly for less common entities and facts. To tackle these problems, this study explores a multi-modal passage retrieval model`s potential to bolster QA system performance. This study poses three key questions: (1) Can a distantly supervised question-relation extraction model enhance retrieval using a knowledge graph (KG), compensating for dense neural retrievers' shortcomings with rare entities? (2) How does this multi-modal approach compare to existing QA systems based on textual features? (3) Can this QA system alleviate poor performance on less common entities on common benchmarks? We devise a multi-modal retriever combining entity features and textual data, leading to improved retrieval precision in some situations, particularly for less common entities. Experiments across different datasets confirm enhanced performance for entity-centric questions, but challenges remain in handling complex generalized questions."
}

@inproceedings{chen-etal-2024-self,
    title = "Self-Improvement Programming for Temporal Knowledge Graph Question Answering",
	author = "Chen, Zhuo  and
      Zhang, Zhao  and
      Li, Zixuan  and
      Wang, Fei  and
      Zeng, Yutao  and
      Jin, Xiaolong  and
      Xu, Yongjun",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1270/",
	pages = "14579--14594",
	abstract = "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for TKGQA (Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of Large Language Models (LLMs) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given. Then, it aligns these drafts to TKGs with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1 metric."
}

@inproceedings{amorim-etal-2024-text2story,
    title = "text2story: A Python Toolkit to Extract and Visualize Story Components of Narrative Text",
	author = "Amorim, Evelin  and
      Campos, Ricardo  and
      Jorge, Alipio  and
      Mota, Pedro  and
      Almeida, R{\'u}ben",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1369/",
	pages = "15761--15772",
	abstract = "Story components, namely, events, time, participants, and their relations are present in narrative texts from different domains such as journalism, medicine, finance, and law. The automatic extraction of narrative elements encompasses several NLP tasks such as Named Entity Recognition, Semantic Role Labeling, Event Extraction, Coreference resolution, and Temporal Inference. The text2story python, an easy-to-use modular library, supports the narrative extraction and visualization pipeline. The package contains an array of narrative extraction tools that can be used separately or in sequence. With this toolkit, end users can process free text in English or Portuguese and obtain formal representations, like standard annotation files or a formal logical representation. The toolkit also enables narrative visualization as Message Sequence Charts (MSC), Knowledge Graphs, and Bubble Diagrams, making it useful to visualize and transform human-annotated narratives. The package combines the use of off-the-shelf and custom tools and is easily patched (replacing existing components) and extended (e.g. with new visualizations). It includes an experimental module for narrative element effectiveness assessment and being is therefore also a valuable asset for researchers developing solutions for narrative extraction. To evaluate the baseline components, we present some results of the main annotators embedded in our packages for datasets in English and Portuguese. We also compare the results with the extraction of narrative elements by GPT-3, a robust LLM model."
}

@inproceedings{murata-kawahara-2024-time,
    title = "Time-aware {COMET}: A Commonsense Knowledge Model with Temporal Knowledge",
	author = "Murata, Eiki  and
      Kawahara, Daisuke",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1405/",
	pages = "16162--16174",
	abstract = "To better handle commonsense knowledge, which is difficult to acquire in ordinary training of language models, commonsense knowledge graphs and commonsense knowledge models have been constructed. The former manually and symbolically represents commonsense, and the latter stores these graphs' knowledge in the models' parameters. However, the existing commonsense knowledge models that deal with events do not consider granularity or time axes. In this paper, we propose a time-aware commonsense knowledge model, TaCOMET. The construction of TaCOMET consists of two steps. First, we create TimeATOMIC using ChatGPT, which is a commonsense knowledge graph with time. Second, TaCOMET is built by continually finetuning an existing commonsense knowledge model on TimeATOMIC. TimeATOMIC and continual finetuning let the model make more time-aware generations with rich commonsense than the existing commonsense models. We also verify the applicability of TaCOMET on a robotic decision-making task. TaCOMET outperformed the existing commonsense knowledge model when proper times are input. Our dataset and models will be made publicly available."
}

@inproceedings{yan-etal-2024-trelm,
    title = "{TRELM}: Towards Robust and Efficient Pre-training for Knowledge-Enhanced Language Models",
	author = "Yan, Junbing  and
      Wang, Chengyu  and
      Zhang, Taolin  and
      He, Xiaofeng  and
      Huang, Jun  and
      Zhang, Wei  and
      Huang, Longtao  and
      Xue, Hui",
	editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.1461/",
	pages = "16790--16801",
	abstract = "KEPLMs are pre-trained models that utilize external knowledge to enhance language understanding. Previous language models facilitated knowledge acquisition by incorporating knowledge-related pre-training tasks learned from relation triples in knowledge graphs. However, these models do not prioritize learning embeddings for entity-related tokens. Updating all parameters in KEPLM is computationally demanding. This paper introduces TRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced Language Models. We observe that text corpora contain entities that follow a long-tail distribution, where some are suboptimally optimized and hinder the pre-training process. To tackle this, we employ a robust approach to inject knowledge triples and employ a knowledge-augmented memory bank to capture valuable information. Moreover, updating a small subset of neurons in the feed-forward networks (FFNs) that store factual knowledge is both sufficient and efficient. Specifically, we utilize dynamic knowledge routing to identify knowledge paths in FFNs and selectively update parameters during pre-training. Experimental results show that TRELM achieves at least a 50{\%} reduction in pre-training time and outperforms other KEPLMs in knowledge probing tasks and multiple knowledge-aware language understanding tasks."
}

@inproceedings{pertsas-etal-2024-annotated,
    title = "An Annotated Dataset for Transformer-based Scholarly Information Extraction and Linguistic Linked Data Generation",
	author = "Pertsas, Vayianos  and
      Kasapaki, Marialena  and
      Constantopoulos, Panos",
	editor = "Chiarcos, Christian  and
      Gkirtzou, Katerina  and
      Ionov, Maxim  and
      Khan, Fahad  and
      McCrae, John P.  and
      Ponsoda, Elena Montiel  and
      Chozas, Patricia Mart{\'i}n",
	booktitle = "Proceedings of the 9th Workshop on Linked Data in Linguistics @ LREC-COLING 2024",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.ldl-1.11/",
	pages = "84--93",
	abstract = "We present a manually curated and annotated, multidisciplinary dataset of 15,262 sentences from research articles (abstract and main text) that can be used for transformer-based extraction from scholarly publications of three types of entities: 1) research methods, named entities of variable length, 2) research goals, entities that appear as textual spans of variable length with mostly fixed lexico-syntactic-structure, and 3) research activities, entities that appear as textual spans of variable length with complex lexico-syntactic structure. We explore the capabilities of our dataset by using it for training/fine-tuning various ML and transformer-based models. We compare our finetuned models as well as LLM responses (chatGPT 3.5) based on 10-shot learning, by measuring F1 scores in token-based, entity-based strict and entity-based partial evaluations across interdisciplinary and discipline-specific datasets in order to capture any possible differences in discipline-oriented writing styles. Results show that fine tuning of transformer-based models significantly outperforms the performance of few- shot learning of LLMs such as chatGPT, highlighting the significance of annotation datasets in such tasks. Our dataset can also be used as a source for linguistic linked data by itself. We demonstrate this by presenting indicative queries in SPARQL, executed over such an RDF knowledge graph."
}

@inproceedings{m-bran-etal-2024-knowledge,
    title = "Knowledge Graph Extraction from Total Synthesis Documents",
	author = "M Bran, Andres  and
      Jon{\v{c}}ev, Zlatko  and
      Schwaller, Philippe",
	editor = "Edwards, Carl  and
      Wang, Qingyun  and
      Li, Manling  and
      Zhao, Lawrence  and
      Hope, Tom  and
      Ji, Heng",
	booktitle = "Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.langmol-1.9/",
	doi = "10.18653/v1/2024.langmol-1.9",
	pages = "74--84",
	abstract = "Knowledge graphs (KGs) have emerged as a powerful tool for organizing and integrating complex information, making it a suitable format for scientific knowledge. However, translating scientific knowledge into KGs is challenging as a wide variety of styles and elements to present data and ideas is used. Although efforts for KG extraction (KGE) from scientific documents exist, evaluation remains challenging and field-dependent; and existing benchmarks do not focuse on scientific information. Furthermore, establishing a general benchmark for this task is challenging as not all scientific knowledge has a ground-truth KG representation, making any benchmark prone to ambiguity. Here we propose Graph of Organic Synthesis Benchmark (GOSyBench), a benchmark for KG extraction from scientific documents in chemistry, that leverages the native KG-like structure of synthetic routes in organic chemistry. We develop KG-extraction algorithms based on LLMs (GPT-4, Claude, Mistral) and VLMs (GPT-4o), the best of which reaches 73{\%} recovery accuracy and 59{\%} precision, leaving a lot of room for improvement. We expect GOSyBench can serve as a valuable resource for evaluating and advancing KGE methods in the scientific domain, ultimately facilitating better organization, integration, and discovery of scientific knowledge."
}

@inproceedings{chen-etal-2024-retrieval,
    title = "Retrieval-Augmented Knowledge Integration into Language Models: A Survey",
	author = {Chen, Yuxuan  and
      R{\"o}der, Daniel  and
      Erker, Justus-Jonas  and
      Hennig, Leonhard  and
      Thomas, Philippe  and
      M{\"o}ller, Sebastian  and
      Roller, Roland},
	editor = "Li, Sha  and
      Li, Manling  and
      Zhang, Michael JQ  and
      Choi, Eunsol  and
      Geva, Mor  and
      Hase, Peter  and
      Ji, Heng",
	booktitle = "Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.knowllm-1.5/",
	doi = "10.18653/v1/2024.knowllm-1.5",
	pages = "45--63",
	abstract = "This survey analyses how external knowledge can be integrated into language models in the context of retrieval-augmentation.The main goal of this work is to give an overview of: (1) Which external knowledge can be augmented? (2) Given a knowledge source, how to retrieve from it and then integrate the retrieved knowledge? To achieve this, we define and give a mathematical formulation of retrieval-augmented knowledge integration (RAKI). We discuss retrieval and integration techniques separately in detail, for each of the following knowledge formats: knowledge graph, tabular and natural language."
}

@inproceedings{chu-etal-2024-patent,
    title = "Patent Response System Optimised for Faithfulness: Procedural Knowledge Embodiment with Knowledge Graph and Retrieval Augmented Generation",
	author = "Chu, Jung-Mei  and
      Lo, Hao-Cheng  and
      Hsiang, Jieh  and
      Cho, Chun-Chieh",
	editor = "Li, Sha  and
      Li, Manling  and
      Zhang, Michael JQ  and
      Choi, Eunsol  and
      Geva, Mor  and
      Hase, Peter  and
      Ji, Heng",
	booktitle = "Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.knowllm-1.12/",
	doi = "10.18653/v1/2024.knowllm-1.12",
	pages = "146--155",
	abstract = "A successful response to Office Action is crucial for an invention to obtain a patent. While previous attempts have applied generalised LLMs, such as GPT-4, in the response process, there remains significant room for improvement in generating faithful, unbiased, and practically valuable responses. To address this issue, we propose the Patent Response System Optimised for Faithfulness (PRO). PRO explicitly incorporates procedural knowledge used by patent agents during drafting arguments in response. This framework comprises several key components: (1) Our proposed PRLLM is a LLM tailored for patent responses, designed to have comprehensive patent domain-specific knowledge. (2) Our proposed PPNet encodes legal interpretations and relationships between technical components from judicial sources through a knowledge graph. (3) The augmented generation processes retrieve relevant information from both the patent text and PPNet to augment the PRLLM`s input and generate faithful responses. Results show that PRO significantly reduces unfaithfulness across six error types compared to several settings. For instance, PRO outperforms GPT-4 by an average of 39{\%} in terms of faithfulness. This demonstrates the effectiveness of our domain-specific approach in improving the quality of automated patent responses."
}

@proceedings{kallm-2024-knowledge,
    title = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.0/"
}

@inproceedings{son-etal-2024-multi,
    title = "Multi-hop Database Reasoning with Virtual Knowledge Graph",
	author = "Son, Juhee  and
      Seonwoo, Yeon  and
      Yoon, Seunghyun  and
      Thorne, James  and
      Oh, Alice",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.1/",
	doi = "10.18653/v1/2024.kallm-1.1",
	pages = "1--11",
	abstract = "Application of LLM to database queries on natural language sentences has demonstrated impressive results in both single and multi-hop scenarios.In the existing methodologies, the requirement to re-encode query vectors at each stage for processing multi-hop queries presents a significant bottleneck to the inference speed.This paper proposes VKGFR (Virtual Knowledge Graph based Fact Retriever) that leverages large language models to extract representations corresponding to a sentence`s knowledge graph, significantly enhancing inference speed for multi-hop reasoning without performance loss.Given that both the queries and natural language database sentences can be structured as a knowledge graph, we suggest extracting a Virtual Knowledge Graph (VKG) representation from sentences with LLM.Over the pre-constructed VKG, our VKGFR conducts retrieval with a tiny model structure, showing performance improvements with higher computational efficiency. We evaluate VKGFR on the WikiNLDB and MetaQA dataset, designed for multi-hop database reasoning over text. The results indicate 13x faster inference speed on the WikiNLDB dataset without performance loss."
}

@inproceedings{papaluca-etal-2024-zero,
    title = "Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large Language Models",
	author = "Papaluca, Andrea  and
      Krefl, Daniel  and
      Rodr{\'i}guez M{\'e}ndez, Sergio  and
      Lensky, Artem  and
      Suominen, Hanna",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.2/",
	doi = "10.18653/v1/2024.kallm-1.2",
	pages = "12--23",
	abstract = "In this work, we tested the Triplet Extraction (TE) capabilities of a variety of Large Language Models (LLMs) of different sizes in the Zero- and Few-Shots settings. In detail, we proposed a pipeline that dynamically gathers contextual information from a Knowledge Base (KB), both in the form of context triplets and of (sentence, triplets) pairs as examples, and provides it to the LLM through a prompt. The additional context allowed the LLMs to be competitive with all the older fully trained baselines based on the Bidirectional Long Short-Term Memory (BiLSTM) Network architecture. We further conducted a detailed analysis of the quality of the gathered KB context, finding it to be strongly correlated with the final TE performance of the model. In contrast, the size of the model appeared to only logarithmically improve the TE capabilities of the LLMs. We release the code on GitHub for reproducibility."
}

@inproceedings{mendes-etal-2024-application,
    title = "Application of Generative {AI} as an Enterprise Wikibase Knowledge Graph {Q}{\&}{A} System",
	author = "Mendes, Ren{\^e}  and
      Oliveira, Dimas  and
      Garcia, Victor",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.4/",
	doi = "10.18653/v1/2024.kallm-1.4",
	pages = "35--42",
	abstract = "Generative AI and Large Language Models are increasingly used in business contexts. One application involves natural language conversations contextualized by company data, which can be accomplished by Enterprise Knowledge Graphs, standardized representations of data. This paper outlines an architecture for implementation of an Enterprise Knowledge Graph using open-source Wikibase software. Additionally, it is presented a Knowledge Graph Q{\&}A System powered by Generative AI."
}

@inproceedings{vuth-etal-2024-kgast,
    title = "{KGAST}: From Knowledge Graphs to Annotated Synthetic Texts",
	author = "Vuth, Nakanyseth  and
      S{\'e}rasset, Gilles  and
      Schwab, Didier",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.5/",
	doi = "10.18653/v1/2024.kallm-1.5",
	pages = "43--55",
	abstract = "In recent years, the use of synthetic data, either as a complement or a substitute for original data, has emerged as a solution to challenges such as data scarcity and security risks. This paper is an initial attempt to automatically generate such data for Information Extraction tasks. We accomplished this by developing a novel synthetic data generation framework called KGAST, which leverages Knowledge Graphs and Large Language Models. In our preliminary study, we conducted simple experiments to generate synthetic versions of two datasets{---}a French security defense dataset and an English general domain dataset, after which we evaluated them both intrinsically and extrinsically. The results indicated that synthetic data can effectively complement original data, improving the performance of models on classes with limited training samples. This highlights KGAST`s potential as a tool for generating synthetic data for Information Extraction tasks."
}

@inproceedings{wasi-2024-hrgraph,
    title = "{HRG}raph: Leveraging {LLM}s for {HR} Data Knowledge Graphs with Information Propagation-based Job Recommendation",
	author = "Wasi, Azmine Toushik",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.6/",
	doi = "10.18653/v1/2024.kallm-1.6",
	pages = "56--62",
	abstract = "Knowledge Graphs (KGs) serving as semantic networks, prove highly effective in managing complex interconnected data in different domains, by offering a unified, contextualized, and structured representation with flexibility that allows for easy adaptation to evolving knowledge. Processing complex Human Resources (HR) data, KGs can help in different HR functions like recruitment, job matching, identifying learning gaps, and enhancing employee retention. Despite their potential, limited efforts have been made to implement practical HR knowledge graphs. This study addresses this gap by presenting a framework for effectively developing HR knowledge graphs from documents using Large Language Models. The resulting KG can be used for a variety of downstream tasks, including job matching, identifying employee skill gaps, and many more. In this work, we showcase instances where HR KGs prove instrumental in precise job matching, yielding advantages for both employers and employees. Empirical evidence from experiments with information propagation in KGs and Graph Neural Nets, along with case studies underscores the effectiveness of KGs in tasks such as job and employee recommendations and job area classification. Code and data are available at : https://github.com/azminewasi/HRGraph"
}

@inproceedings{gurgurov-etal-2024-adapting,
    title = "Adapting Multilingual {LLM}s to Low-Resource Languages with Knowledge Graphs via Adapters",
	author = "Gurgurov, Daniil  and
      Hartmann, Mareike  and
      Ostermann, Simon",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.7/",
	doi = "10.18653/v1/2024.kallm-1.7",
	pages = "63--74",
	abstract = "This paper explores the integration of graph knowledge from linguistic ontologies into multilingual Large Language Models (LLMs) using adapters to improve performance for low-resource languages (LRLs) in sentiment analysis (SA) and named entity recognition (NER). Building upon successful parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we propose a similar approach for incorporating knowledge from multilingual graphs, connecting concepts in various languages with each other through linguistic relationships, into multilingual LLMs for LRLs. Specifically, we focus on eight LRLs {---} Maltese, Bulgarian, Indonesian, Nepali, Javanese, Uyghur, Tibetan, and Sinhala {---} and employ language-specific adapters fine-tuned on data extracted from the language-specific section of ConceptNet, aiming to enable knowledge transfer across the languages covered by the knowledge graph. We compare various fine-tuning objectives, including standard Masked Language Modeling (MLM), MLM with full-word masking, and MLM with targeted masking, to analyze their effectiveness in learning and integrating the extracted graph data. Through empirical evaluation on language-specific tasks, we assess how structured graph knowledge affects the performance of multilingual LLMs for LRLs in SA and NER, providing insights into the potential benefits of adapting language models for low-resource scenarios."
}

@inproceedings{cauter-yakovets-2024-ontology,
    title = "Ontology-guided Knowledge Graph Construction from Maintenance Short Texts",
	author = "van Cauter, Zeno  and
      Yakovets, Nikolay",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.8/",
	doi = "10.18653/v1/2024.kallm-1.8",
	pages = "75--84",
	abstract = "Large-scale knowledge graph construction remains infeasible since it requires significant human-expert involvement. Further complications arise when building graphs from domain-specific data due to their unique vocabularies and associated contexts. In this work, we demonstrate the ability of open-source large language models (LLMs), such as Llama-2 and Llama-3, to extract facts from domain-specific Maintenance Short Texts (MSTs). We employ an approach which combines ontology-guided triplet extraction and in-context learning. By using only 20 semantically similar examples with the Llama-3-70B-Instruct model, we achieve performance comparable to previous methods that relied on fine-tuning techniques like SpERT and REBEL. This indicates that domain-specific fact extraction can be accomplished through inference alone, requiring minimal labeled data. This opens up possibilities for effective and efficient semi-automated knowledge graph construction for domain-specific data."
}

@inproceedings{canal-esteve-gutierrez-2024-educational,
    title = "Educational Material to Knowledge Graph Conversion: A Methodology to Enhance Digital Education",
	author = "Canal-Esteve, Miquel  and
      Gutierrez, Yoan",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.9/",
	doi = "10.18653/v1/2024.kallm-1.9",
	pages = "85--91",
	abstract = "This article argues that digital educational content should be structured as knowledge graphs (KGs). Unlike traditional repositories such as Moodle, a KG offers a more flexible representation of the relationships between concepts, facilitating intuitive navigation and discovery of connections. In addition, it integrates effectively with Large Language Models, enhancing personalized explanations, answers, and recommendations. This article studies different proposals based on semantics and knowledge modelling to determine the most appropriate ways to strengthen intelligent educational technologies."
}

@inproceedings{yuan-vlachos-2024-zero,
    title = "Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs",
	author = "Yuan, Moy  and
      Vlachos, Andreas",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.11/",
	doi = "10.18653/v1/2024.kallm-1.11",
	pages = "105--115",
	abstract = "Despite progress in automated fact-checking, most systems require a significant amount of labeled training data, which is expensive. In this paper, we propose a novel zero-shot method, which instead of operating directly on the claim and evidence sentences, decomposes them into semantic triples augmented using external knowledge graphs, and uses large language models trained for natural language inference. This allows it to generalize to adversarial datasets and domains that supervised models require specific training data for. Our empirical results show that our approach outperforms previous zero-shot approaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being comparable or better than supervised models on the adversarial and the out-of-domain datasets."
}

@inproceedings{zhang-etal-2024-fine-tuning,
    title = "Fine-tuning Language Models for Triple Extraction with Data Augmentation",
	author = "Zhang, Yujia  and
      Sadler, Tyler  and
      Taesiri, Mohammad Reza  and
      Xu, Wenjie  and
      Reformat, Marek",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.12/",
	doi = "10.18653/v1/2024.kallm-1.12",
	pages = "116--124",
	abstract = "Advanced language models with impressive capabilities to process textual information can more effectively extract high-quality triples, which are the building blocks of knowledge graphs. Our work examines language models' abilities to extract entities and the relationships between them. We use a diverse data augmentation process to fine-tune large language models to extract triples from the text. Fine-tuning is performed using a mix of trainers from HuggingFace and five public datasets, such as different variations of the WebNLG, SKE, DocRed, FewRel, and KELM. Evaluation involves comparing model outputs with test-set triples based on several criteria, such as type, partial, exact, and strict accuracy.The obtained results outperform ChatGPT and even match or exceed the performance of GPT-4."
}

@inproceedings{shah-etal-2024-improving,
    title = "Improving {LLM}-based {KGQA} for multi-hop Question Answering with implicit reasoning in few-shot examples",
	author = "Shah, Mili  and
      Cahoon, Joyce  and
      Milletari, Mirco  and
      Tian, Jing  and
      Psallidas, Fotis  and
      Mueller, Andreas  and
      Litombe, Nick",
	editor = "Biswas, Russa  and
      Kaffee, Lucie-Aim{\'e}e  and
      Agarwal, Oshin  and
      Minervini, Pasquale  and
      Singh, Sameer  and
      de Melo, Gerard",
	booktitle = "Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.kallm-1.13/",
	doi = "10.18653/v1/2024.kallm-1.13",
	pages = "125--135",
	abstract = "Large language models (LLMs) have shown remarkable capabilities in generating natural language texts for various tasks. However, using LLMs for question answering on knowledge graphs still remains a challenge, especially for questions requiring multi-hop reasoning. In this paper, we present a novel planned query guidance approach that improves large language model (LLM) performance in multi-hop question answering on knowledge graphs (KGQA). We do this by designing few-shot examples that implicitly demonstrate a systematic reasoning methodology to answer multi-hop questions. We evaluate our approach for two graph query languages, Cypher and SPARQL, and show that the queries generated using our strategy outperform the queries generated using a baseline LLM and typical few-shot examples by up to 24.66{\%} and 7.7{\%} in execution match accuracy for the MetaQA and the Spider benchmarks respectively. We also conduct an ablation study to analyze the incremental effects of the different techniques of designing few-shot examples. Our results suggest that our approach enables the LLM to effectively leverage the few-shot examples to generate queries for multi-hop KGQA."
}

@inproceedings{salman-etal-2024-tiny,
    title = "Tiny But Mighty: A Crowdsourced Benchmark Dataset for Triple Extraction from Unstructured Text",
	author = "Salman, Muhammad  and
      Haller, Armin  and
      Rodriguez Mendez, Sergio J.  and
      Naseem, Usman",
	editor = "Bunt, Harry  and
      Ide, Nancy  and
      Lee, Kiyong  and
      Petukhova, Volha  and
      Pustejovsky, James  and
      Romary, Laurent",
	booktitle = "Proceedings of the 20th Joint ACL - ISO Workshop on Interoperable Semantic Annotation @ LREC-COLING 2024",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.isa-1.10/",
	pages = "71--81",
	abstract = "In the context of Natural Language Processing (NLP) and Semantic Web applications, constructing Knowledge Graphs (KGs) from unstructured text plays a vital role. Several techniques have been developed for KG construction from text, but the lack of standardized datasets hinders the evaluation of triple extraction methods. The evaluation of existing KG construction approaches is based on structured data or manual investigations. To overcome this limitation, this work introduces a novel dataset specifically designed to evaluate KG construction techniques from unstructured text. Our dataset consists of a diverse collection of compound and complex sentences meticulously annotated by human annotators with potential triples (subject, verb, object). The annotations underwent further scrutiny by expert ontologists to ensure accuracy and consistency. For evaluation purposes, the proposed F-measure criterion offers a robust approach to quantify the relatedness and assess the alignment between extracted triples and the ground-truth triples, providing a valuable tool for evaluating the performance of triple extraction systems. By providing a diverse collection of high-quality triples, our proposed benchmark dataset offers a comprehensive training and evaluation set for refining the performance of state-of-the-art language models on a triple extraction task. Furthermore, this dataset encompasses various KG-related tasks, such as named entity recognition, relation extraction, and entity linking."
}

@inproceedings{tang-etal-2024-cadge-context,
    title = "{CADGE}: Context-Aware Dialogue Generation Enhanced with Graph-Structured Knowledge Aggregation",
	author = "Tang, Chen  and
      Zhang, Hongbo  and
      Loakman, Tyler  and
      Yang, Bohao  and
      Goetze, Stefan  and
      Lin, Chenghua",
	editor = "Mahamood, Saad  and
      Minh, Nguyen Le  and
      Ippolito, Daphne",
	booktitle = "Proceedings of the 17th International Natural Language Generation Conference",
	month = sep,
	year = "2024",
	address = "Tokyo, Japan",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.inlg-main.31/",
	pages = "371--383",
	abstract = "Commonsense knowledge is crucial to many natural language processing tasks. Existing works usually incorporate graph knowledge with conventional graph neural networks (GNNs), resulting in a sequential pipeline that compartmentalizes the encoding processes for textual and graph-based knowledge. This compartmentalization does, however, not fully exploit the contextual interplay between these two types of input knowledge. In this paper, a novel context-aware graph-attention model (Context-aware GAT) is proposed, designed to effectively assimilate global features from relevant knowledge graphs through a context-enhanced knowledge aggregation mechanism. Specifically, the proposed framework employs an innovative approach to representation learning that harmonizes heterogeneous features by amalgamating flattened graph knowledge with text data. The hierarchical application of graph knowledge aggregation within connected subgraphs, complemented by contextual information, to bolster the generation of commonsense-driven dialogues is analyzed. Empirical results demonstrate that our framework outperforms conventional GNN-based language models in terms of performance. Both, automated and human evaluations affirm the significant performance enhancements achieved by our proposed model over the concept flow baseline."
}

@inproceedings{jhalani-etal-2024-precision,
    title = "Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models",
	author = "Jhalani, Manas  and
      K M, Annervaz  and
      Bhattacharyya, Pushpak",
	editor = "Lalitha Devi, Sobha  and
      Arora, Karunesh",
	booktitle = "Proceedings of the 21st International Conference on Natural Language Processing (ICON)",
	month = dec,
	year = "2024",
	address = "AU-KBC Research Centre, Chennai, India",
	publisher = "NLP Association of India (NLPAI)",
	url = "https://aclanthology.org/2024.icon-1.3/",
	pages = "21--36",
	abstract = "In the realm of multimodal tasks, Visual Question Answering (VQA) plays a crucial role by addressing natural language questions grounded in visual content. Knowledge-Based Visual Question Answering (KBVQA) advances this concept by adding external knowledge along with images to respond to questions. We introduce an approach for KBVQA, augmenting the existing vision-language transformer encoder-decoder (OFA) model . Our main contribution involves enhancing questions by incorporating relevant external knowledge extracted from knowledge graphs, using a dynamic triple extraction"
}

@inproceedings{saraswat-etal-2024-story,
    title = "Story-Yarn : An Interactive Story Building Application",
	author = "Saraswat, Hryadyansh  and
      D. Shete, Snehal  and
      Dangi, Vikas  and
      Agrawal, Kushagra  and
      Aggarwal, Anuj  and
      Nigam, Aditya",
	editor = "Lalitha Devi, Sobha  and
      Arora, Karunesh",
	booktitle = "Proceedings of the 21st International Conference on Natural Language Processing (ICON)",
	month = dec,
	year = "2024",
	address = "AU-KBC Research Centre, Chennai, India",
	publisher = "NLP Association of India (NLPAI)",
	url = "https://aclanthology.org/2024.icon-1.28/",
	pages = "248--255",
	abstract = "Story building is an important part of language and overall development of a child. Developing an interactive and artificial intelligence (AI) based solution to create stories for children is an open and challenging problem. Methods combining large language models (LLMs) and knowledge graphs (KGs) have further enabled high quality and coherent story generation. In this work, we present a platform, Story Yarn, developed for interactive story creation for children. We customise a KG, using children stories, which captures relationships between components of stories. This customised KG is then used along with LLM to collaboratively create a story. We have also built a simple app to facilitate user interaction. This platform can aid the creative development of children, and can be used at home or in schools."
}

@inproceedings{jana-etal-2024-pollcardiokg,
    title = "{P}oll{C}ardio{KG}: A Dynamic Knowledge Graph of Interaction Between Pollution and Cardiovascular Diseases",
	author = "Jana, Sudeshna  and
      Roy, Anunak  and
      Sinha, Manjira  and
      Dasgupta, Tirthankar",
	editor = "Lalitha Devi, Sobha  and
      Arora, Karunesh",
	booktitle = "Proceedings of the 21st International Conference on Natural Language Processing (ICON)",
	month = dec,
	year = "2024",
	address = "AU-KBC Research Centre, Chennai, India",
	publisher = "NLP Association of India (NLPAI)",
	url = "https://aclanthology.org/2024.icon-1.61/",
	pages = "522--530",
	abstract = "In recent decades, environmental pollution has become a pressing global health concern. According to the World Health Organization (WHO), a significant portion of the population is exposed to air pollutant levels exceeding safety guidelines. Cardiovascular diseases (CVDs) {---} including coronary artery disease, heart attacks, and strokes {---} are particularly significant health effects of this exposure. In this paper, we investigate the effects of air pollution on cardiovascular health by constructing a dynamic knowledge graph based on extensive biomedical literature. This paper provides a comprehensive exploration of entity identification and relation extraction, leveraging advanced language models. Additionally, we demonstrate how in-context learning with large language models can enhance the accuracy and efficiency of the extraction process. The constructed knowledge graph enables us to analyze the relationships between pollutants and cardiovascular diseases over the years, providing deeper insights into the long-term impact of cumulative exposure, underlying causal mechanisms, vulnerable populations, and the role of emerging contaminants in worsening various cardiac outcomes."
}

@inproceedings{saleh-etal-2024-sg,
    title = "{SG}-{RAG}: Multi-Hop Question Answering With Large Language Models Through Knowledge Graphs",
	author = "Saleh, Ahmmad O. M.  and
      Tur, Gokhan  and
      Saygin, Yucel",
	editor = "Abbas, Mourad  and
      Freihat, Abed Alhakim",
	booktitle = "Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024)",
	month = oct,
	year = "2024",
	address = "Trento",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.icnlsp-1.45/",
	pages = "439--448"
}

@inproceedings{zhao-etal-2024-divknowqa,
    title = "{DIVKNOWQA}: Assessing the Reasoning Ability of {LLM}s via Open-Domain Question Answering over Knowledge Base and Text",
	author = "Zhao, Wenting  and
      Liu, Ye  and
      Niu, Tong  and
      Wan, Yao  and
      Yu, Philip  and
      Joty, Shafiq  and
      Zhou, Yingbo  and
      Yavuz, Semih",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-naacl.5/",
	doi = "10.18653/v1/2024.findings-naacl.5",
	pages = "51--68",
	abstract = "Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrievalaugmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) Generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges."
}

@inproceedings{kong-etal-2024-bilateral,
    title = "Bilateral Masking with prompt for Knowledge Graph Completion",
	author = "Kong, Yonghui  and
      Fan, Cunhang  and
      Chen, Yujie  and
      Zhang, Shuai  and
      Lv, Zhao  and
      Tao, Jianhua",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-naacl.17/",
	doi = "10.18653/v1/2024.findings-naacl.17",
	pages = "240--249",
	abstract = "The pre-trained language model (PLM) has achieved significant success in the field of knowledge graph completion (KGC) by effectively modeling entity and relation descriptions. In recent studies, the research in this field has been categorized into methods based on word matching and sentence matching, with the former significantly lags behind. However, there is a critical issue in word matching methods, which is that these methods fail to obtain satisfactory single embedding representations for entities.To address this issue and enhance entity representation, we propose the Bilateral Masking with prompt for Knowledge Graph Completion (BMKGC) approach.Our methodology employs prompts to narrow the distance between the predicted entity and the known entity. Additionally, the BMKGC model incorporates a bi-encoder architecture, enabling simultaneous predictions at both the head and tail. Furthermore, we propose a straightforward technique to augment positive samples, mitigating the problem of degree bias present in knowledge graphs and thereby improving the model`s robustness. Experimental results conclusively demonstrate that BMKGC achieves state-of-the-art performance on the WN18RR dataset."
}

@inproceedings{agarwal-etal-2024-bring,
    title = "Bring Your Own {KG}: Self-Supervised Program Synthesis for Zero-Shot {KGQA}",
	author = "Agarwal, Dhruv  and
      Das, Rajarshi  and
      Khosla, Sopan  and
      Gangadharaiah, Rashmi",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-naacl.57/",
	doi = "10.18653/v1/2024.findings-naacl.57",
	pages = "896--919",
	abstract = "We present BYOKG, a universal question-answering (QA) system that can operate on any knowledge graph (KG), requires no human-annotated training data, and can be ready to use within a day{---}attributes that are out-of-scope for current KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to comprehend information present in an unseen KG through exploration{---}starting at random nodes, inspecting the labels of adjacent nodes and edges, and combining them with their prior world knowledge. Exploration in BYOKG leverages an LLM-backed symbolic agent that generates a diverse set of query-program exemplars, which are then used to ground a retrieval-augmented reasoning procedure to synthesize programs for arbitrary questions. BYOKG is effective over both small- and large-scale graphs, showing dramatic gains in zero-shot QA accuracy of 27.89 and 59.88 F1 on GrailQA and MetaQA, respectively. We further find that performance of BYOKG reliably improves with continued exploration as well as improvements in the base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1 on a sub-sampled zero-shot split of GrailQA. Lastly, we verify our universality claim by evaluating BYOKG on a domain-specific materials science KG and show that it improves zero-shot performance by 46.33 F1."
}

@inproceedings{peng-etal-2024-deja,
    title = "Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning",
	author = "Peng, Miao  and
      Liu, Ben  and
      Xu, Wenjie  and
      Jiang, Zihao  and
      Zhu, Jiahui  and
      Peng, Min",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-naacl.75/",
	doi = "10.18653/v1/2024.findings-naacl.75",
	pages = "1178--1191",
	abstract = "Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via contrastive estimation between queries and candidates. By introducing virtual time prefix tokens, it applies a prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks under different settings. We evaluate ChapTER on four transductive and three few-shot inductive TKGR benchmarks, and experimental results demonstrate that ChapTER achieves superior performance compared to competitive baselines with only 0.17{\%} tuned parameters. We conduct thorough analysis to verify the effectiveness, flexibility and efficiency of ChapTER."
}

@inproceedings{liao-etal-2024-gentkg,
    title = "{G}en{TKG}: Generative Forecasting on Temporal Knowledge Graph with Large Language Models",
	author = "Liao, Ruotong  and
      Jia, Xu  and
      Li, Yangzhe  and
      Ma, Yunpu  and
      Tresp, Volker",
	editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-naacl.268/",
	doi = "10.18653/v1/2024.findings-naacl.268",
	pages = "4303--4317",
	abstract = "The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional embedding-based and rule-based methods dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval-augmented generation framework named GenTKG combining a temporal logical rule-based retrieval strategy and few-shot parameter-efficient instruction tuning to solve the above challenges, respectively. Extensive experiments have shown that GenTKG outperforms conventional methods of temporal relational forecasting with low computation resources using extremely limited training data as few as 16 samples. GenTKG also highlights remarkable cross-domain generalizability with outperforming performance on unseen datasets without re-training, and in-domain generalizability regardless of time split in the same dataset. Our work reveals the huge potential of LLMs in the tKG domain and opens a new frontier for generative forecasting on tKGs. The code and data are released here: \url{https://github.com/mayhugotong/GenTKG}."
}

@inproceedings{li-etal-2024-dalk,
    title = "{DALK}: Dynamic Co-Augmentation of {LLM}s and {KG} to answer {A}lzheimer`s Disease Questions with Scientific Literature",
	author = "Li, Dawei  and
      Yang, Shu  and
      Tan, Zhen  and
      Baik, Jae Young  and
      Yun, Sukwon  and
      Lee, Joseph  and
      Chacko, Aaron  and
      Hou, Bojian  and
      Duong-Tran, Duy  and
      Ding, Ying  and
      Liu, Huan  and
      Shen, Li  and
      Chen, Tianlong",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.119/",
	doi = "10.18653/v1/2024.findings-emnlp.119",
	pages = "2187--2205",
	abstract = "Recent advancements in large language models (LLMs) have achieved promising performances across various applications. Nonetheless, the ongoing challenge of integrating long-tail knowledge continues to impede the seamless adoption of LLMs in specialized domains. In this work, we introduce DALK, a.k.a. Dynamic Co-Augmentation of LLMs and KG, to address this limitation and demonstrate its ability on studying Alzheimer`s Disease (AD), a specialized sub-field in biomedicine and a global health priority. With a synergized framework of LLM and KG mutually enhancing each other, we first leverage LLM to construct an evolving AD-specific knowledge graph (KG) sourced from AD-related scientific literature, and then we utilize a coarse-to-fine sampling method with a novel self-aware knowledge retrieval approach to select appropriate knowledge from the KG to augment LLM inference capabilities. The experimental results, conducted on our constructed AD question answering (ADQA) benchmark, underscore the efficacy of DALK. Additionally, we perform a series of detailed analyses that can offer valuable insights and guidelines for the emerging topic of mutually enhancing KG and LLM."
}

@inproceedings{jiang-etal-2024-mm,
    title = "{MM}-{C}hat{A}lign: A Novel Multimodal Reasoning Framework based on Large Language Models for Entity Alignment",
	author = "Jiang, Xuhui  and
      Shen, Yinghan  and
      Shi, Zhichao  and
      Xu, Chengjin  and
      Li, Wei  and
      Zihe, Huang  and
      Guo, Jian  and
      Wang, Yuanzhuo",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.148/",
	doi = "10.18653/v1/2024.findings-emnlp.148",
	pages = "2637--2654",
	abstract = "Multimodal entity alignment (MMEA) integrates multi-source and cross-modal knowledge graphs, a crucial yet challenging task for data-centric applications.Traditional MMEA methods derive the visual embeddings of entities and combine them with other modal data for alignment by embedding similarity comparison.However, these methods are hampered by the limited comprehension of visual attributes and deficiencies in realizing and bridging the semantics of multimodal data. To address these challenges, we propose MM-ChatAlign, a novel framework that utilizes the visual reasoning abilities of MLLMs for MMEA.The framework features an embedding-based candidate collection module that adapts to various knowledge representation strategies, effectively filtering out irrelevant reasoning candidates. Additionally, a reasoning and rethinking module, powered by MLLMs, enhances alignment by efficiently utilizing multimodal information.Extensive experiments on four MMEA datasets demonstrate MM-ChatAlign`s superiority and underscore the significant potential of MLLMs in MMEA tasks.The source code is available at https://github.com/jxh4945777/MMEA/."
}

@inproceedings{wang-etal-2024-infuserki,
    title = "{I}nfuser{KI}: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration",
	author = "Wang, Fali  and
      Bao, Runxue  and
      Wang, Suhang  and
      Yu, Wenchao  and
      Liu, Yanchi  and
      Cheng, Wei  and
      Chen, Haifeng",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.209/",
	doi = "10.18653/v1/2024.findings-emnlp.209",
	pages = "3675--3688",
	abstract = "Large Language Models (LLMs) have achieved exceptional capabilities in open generation across various domains, yet they encounter difficulties with tasks that require intensive knowledge. To address these challenges, methods for integrating knowledge have been developed, which augment LLMs with domain-specific knowledge graphs through external modules. These approaches, however, face data inefficiency issues as they necessitate the processing of both known and unknown knowledge for fine-tuning. Thus, our research focuses on a novel problem: efficiently integrating unknown knowledge into LLMs without unnecessary overlap of known knowledge. A risk of introducing new knowledge is the potential forgetting of existing knowledge. To mitigate this risk, we propose the innovative InfuserKI framework. This framework employs transformer internal states to determine when to enrich LLM outputs with additional information, effectively preventing knowledge forgetting. Performance evaluations using the UMLS-2.5k and MetaQA domain knowledge graphs reveal that InfuserKI not only successfully integrates new knowledge but also outperforms state-of-the-art baselines, reducing knowledge forgetting by 9{\%} and 6{\%}, respectively."
}

@inproceedings{noriega-atala-etal-2024-happen,
    title = "When and Where Did it Happen? An Encoder-Decoder Model to Identify Scenario Context",
	author = "Noriega-Atala, Enrique  and
      Vacareanu, Robert  and
      Ashton, Salena Torres  and
      Pyarelal, Adarsh  and
      Morrison, Clayton T  and
      Surdeanu, Mihai",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.219/",
	doi = "10.18653/v1/2024.findings-emnlp.219",
	pages = "3821--3829",
	abstract = "We introduce a neural architecture finetuned for the task of scenario context generation: The relevant location and time of an event or entity mentioned in text. Contextualizing information extraction helps to scope the validity of automated finings when aggregating them as knowledge graphs. Our approach uses a high-quality curated dataset of time and location annotations in a corpus of epidemiology papers to train an encoder-decoder architecture. We also explored the use of data augmentation techniques during training. Our findings suggest that a relatively small fine-tuned encoder-decoder model performs better than out-of-the-box LLMs and semantic role labeling parsers to accurate predict the relevant scenario information of a particular entity or event."
}

@inproceedings{ji-etal-2024-retrieval,
    title = "Retrieval and Reasoning on {KG}s: Integrate Knowledge Graphs into Large Language Models for Complex Question Answering",
	author = "Ji, Yixin  and
      Wu, Kaixin  and
      Li, Juntao  and
      Chen, Wei  and
      Zhong, Mingjie  and
      Jia, Xu  and
      Zhang, Min",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.446/",
	doi = "10.18653/v1/2024.findings-emnlp.446",
	pages = "7598--7610",
	abstract = "Despite Large Language Models (LLMs) have performed impressively in various Natural Language Processing (NLP) tasks, their inherent hallucination phenomena severely challenge their credibility in complex reasoning. Combining explainable Knowledge Graphs (KGs) with LLMs is a promising path to address this issue. However, structured KGs are difficult to utilize, and how to make LLMs understand and incorporate them is a challenging topic. We thereby reorganize a more efficient structure of KGs, while designing the KG-related instruction tuning and continual pre-training strategies to enable LLMs to learn and internalize this form of representation effectively. Moreover, we construct subgraphs to further enhance the retrieval capabilities of KGs via CoT reasoning. Extensive experiments on two KGQA datasets demonstrate that our model achieves convincing performance compared to strong baselines."
}

@inproceedings{wang-etal-2024-learning-plan,
    title = "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs",
	author = "Wang, Junjie  and
      Chen, Mingyang  and
      Hu, Binbin  and
      Yang, Dan  and
      Liu, Ziqi  and
      Shen, Yue  and
      Wei, Peng  and
      Zhang, Zhiqiang  and
      Gu, Jinjie  and
      Zhou, Jun  and
      Pan, Jeff Z.  and
      Zhang, Wen  and
      Chen, Huajun",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.459/",
	doi = "10.18653/v1/2024.findings-emnlp.459",
	pages = "7813--7835",
	abstract = "Improving the performance of large language models (LLMs) in complex question-answering (QA) scenarios has always been a research focal point. Recent studies have attempted to enhance LLMs' performance by combining step-wise planning with external retrieval. While effective for advanced models like GPT-3.5, smaller LLMs face challenges in decomposing complex questions, necessitating supervised fine-tuning. Previous work has relied on manual annotation and knowledge distillation from teacher LLMs, which are time-consuming and not accurate enough. In this paper, we introduce a novel framework for enhancing LLMs' planning capabilities by using planning data derived from knowledge graphs (KGs). LLMs fine-tuned with this data have improved planning capabilities, better equipping them to handle complex QA tasks that involve retrieval. Evaluations on multiple datasets, including our newly proposed benchmark, highlight the effectiveness of our framework and the benefits of KG-derived planning data."
}

@inproceedings{mousavi-etal-2024-dyknow,
    title = "{D}y{K}now: Dynamically Verifying Time-Sensitive Factual Knowledge in {LLM}s",
	author = "Mousavi, Seyed Mahed  and
      Alghisi, Simone  and
      Riccardi, Giuseppe",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.471/",
	doi = "10.18653/v1/2024.findings-emnlp.471",
	pages = "8014--8029",
	abstract = "LLMs acquire knowledge from massive data snapshots collected at different timestamps. Their knowledge is then commonly evaluated using static benchmarks. However, factual knowledge is generally subject to time-sensitive changes, and static benchmarks cannot address those cases. We present an approach to dynamically evaluate the knowledge in LLMs and their time-sensitiveness against Wikidata, a publicly available up-to-date knowledge graph. We evaluate the time-sensitive knowledge in twenty-four private and open-source LLMs, as well as the effectiveness of four editing methods in updating the outdated facts. Our results show that 1) outdatedness is a critical problem across state-of-the-art LLMs; 2) LLMs output inconsistent answers when prompted with slight variations of the question prompt; and 3) the performance of the state-of-the-art knowledge editing algorithms is very limited, as they can not reduce the cases of outdatedness and output inconsistency."
}

@inproceedings{yang-etal-2024-advancing,
    title = "Advancing Cross-Lingual Entity Alignment with Large Language Models: Tailored Sample Segmentation and Zero-Shot Prompts",
	author = "Yang, Linyan  and
      Cheng, Jingwei  and
      Zhang, Fu",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.475/",
	doi = "10.18653/v1/2024.findings-emnlp.475",
	pages = "8122--8138",
	abstract = "In recent years, the advent of large language models (LLMs) like GPT and Llama has significantly influenced numerous domains, particularly in advancing natural language processing (NLP) capabilities. LLMs have shown remarkable performance in NLP tasks such as relation extraction (RE) and knowledge graph completion (KGC), enhancing activities related to knowledge graphs. As a result, there is a growing interest in integrating LLMs into cross-lingual entity alignment (EA) task, which aims to identify equivalent entities across various knowledge graphs, thereby improving the performance of current baselines. However, employing LLMs for entity alignment poses challenges in efficiently handling large-scale data, generating suitable data samples, and adapting prompts for the EA task. To tackle these challenges, we propose Seg-Align, an innovative framework that integrating distance feature extraction, sample **Seg**mentation, and zero-shot prompts. Through extensive experiments on two widely used cross-lingual benchmark datasets, we have not only demonstrated the effectiveness of our proposed sample segmentation algorithm but also highlighted the state-of-the-art performance of Seg-Align. Code is available at https://github.com/yangxiaoxiaoly/Seg-Align."
}

@inproceedings{li-etal-2024-contor,
    title = "{CONTOR}: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules",
	author = "Li, Na  and
      Bailleux, Thomas  and
      Bouraoui, Zied  and
      Schockaert, Steven",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.488/",
	doi = "10.18653/v1/2024.findings-emnlp.488",
	pages = "8316--8334",
	abstract = "We consider the problem of finding plausible rules that are missing from a given ontology. A number of strategies for this problem have already been considered in the literature. Little is known about the relative performance of these strategies, however, as they have thus far been evaluated on different ontologies. Moreover, existing evaluations have focused on distinguishing held-out ontology rules from randomly corrupted ones, which often makes the task unrealistically easy and leads to the presence of incorrectly labelled negative examples. To address these concerns, we introduce a benchmark with manually annotated hard negatives and use this benchmark to evaluate ontology completion models. In addition to previously proposed models, we test the effectiveness of several approaches that have not yet been considered for this task, including LLMs and simple but effective hybrid strategies."
}

@inproceedings{zhang-etal-2024-salmon,
    title = "{SALMON}: A Structure-Aware Language Model with logicality and densification strategy for Temporal Knowledge Graph Reasoning",
	author = "Zhang, Fu  and
      Lin, Jinghao  and
      Cheng, Jingwei",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.511/",
	doi = "10.18653/v1/2024.findings-emnlp.511",
	pages = "8761--8774",
	abstract = "Temporal knowledge graph reasoning (TKGR) is a crucial task that involves reasoning at known timestamps to complete the future facts and has attracted more and more attention in recent years. The current TKGR models are mainly based on graph neural networks or tensor decomposition techniques. Few works in TKGR focus on pre-trained language models (PLMs) which have powerful sequence modeling capabilities to capture the temporal associations between facts. In this paper, we propose a model SALMON: a Structure-Aware Language Model with logicality and densification strategy. Specifically, we design a PLM-based framework with a structure-aware layer inside to jointly capture the temporal evolving pattern and structural information in TKGs. To further enhance the model`s ability to infer causal associations of facts, we propose a logical judging module, which can guide the model to prioritize learning the most relevant evolving information of logical causal associations in TKGs during the training process. Moreover, we propose a densification strategy based on large language models, through a carefully crafted Chain of Thought prompt, to dig out some knowledge necessary for reasoning about fact associations, thereby making the model perform better. Extensive experimental results demonstrate the superiority of our model over the state-of-the-art baselines."
}

@inproceedings{li-etal-2024-linked,
    title = "{LINKED}: Eliciting, Filtering and Integrating Knowledge in Large Language Model for Commonsense Reasoning",
	author = "Li, Jiachun  and
      Cao, Pengfei  and
      Wang, Chenhao  and
      Jin, Zhuoran  and
      Chen, Yubo  and
      Liu, Kang  and
      Jiang, Xiaojian  and
      Xu, Jiexin  and
      Zhao, Jun",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.519/",
	doi = "10.18653/v1/2024.findings-emnlp.519",
	pages = "8886--8905",
	abstract = "Large language models (LLMs) sometimes demonstrate poor performance on knowledge-intensive tasks, commonsense reasoning is one of them. Researchers typically address these issues by retrieving related knowledge from knowledge graphs or employing self-enhancement methods to elicit knowledge in LLMs. However, noisy knowledge and invalid reasoning issues hamper their ability to answer questions accurately. To this end, we propose a novel method named eliciting, filtering and integrating knowledge in large language model (LINKED). In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning. With our comprehensive experiments on two complex commonsense reasoning benchmarks, our method outperforms SOTA baselines (up to 9.0{\%} improvement of accuracy). Besides, to measure the positive and negative impact of the injected knowledge, we propose a new metric called effectiveness-preservation score for the knowledge enhancement works. Finally, through extensive experiments, we conduct an in-depth analysis and find many meaningful conclusions about LLMs in commonsense reasoning tasks."
}

@inproceedings{zhang-etal-2024-question,
    title = "Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering",
	author = "Zhang, Yu  and
      Chen, Kehai  and
      Bai, Xuefeng  and
      Kang, Zhao  and
      Guo, Quanjiang  and
      Zhang, Min",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.524/",
	doi = "10.18653/v1/2024.findings-emnlp.524",
	pages = "8972--8985",
	abstract = "Knowledge graph question answering (KGQA) involves answering natural language questions by leveraging structured information stored in a knowledge graph. Typically, KGQA initially retrieve a targeted subgraph from a large-scale knowledge graph, which serves as the basis for reasoning models to address queries. However, the retrieved subgraph inevitably brings distraction information for knowledge utilization, impeding the model`s ability to perform accurate reasoning. To address this issue, we propose a Question-guided Knowledge Graph Re-scoring method (Q-KGR) to eliminate noisy pathways for the input question, thereby focusing specifically on pertinent factual knowledge.Moreover, we introduce Knowformer, a parameter-efficient method for injecting the re-scored knowledge graph into large language models to enhance their ability to perform factual reasoning.Extensive experiments on multiple KGQA benchmarks demonstrate the superiority of our method over existing systems."
}

@inproceedings{rajan-etal-2024-knowledge,
    title = "Knowledge-based Consistency Testing of Large Language Models",
	author = "Rajan, Sai Sathiesh  and
      Soremekun, Ezekiel  and
      Chattopadhyay, Sudipta",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.596/",
	doi = "10.18653/v1/2024.findings-emnlp.596",
	pages = "10185--10196",
	abstract = "In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs). Specifically, we propose an automated testing framework (called KONTEST) which leverages a knowledge graph to construct test cases. KONTEST probes and measures the inconsistencies in the LLM`s knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle). KONTEST further mitigates knowledge gaps via a weighted LLM model ensemble. Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KONTEST generates 19.2{\%} error inducing inputs (1917 errors from 9979 test inputs). It also reveals a 16.5{\%} knowledge gap across all tested LLMs. A mitigation method informed by KONTEST`s test suite reduces LLM knowledge gap by 32.48{\%}. Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60{\%}-68{\%} effective in knowledge construction."
}

@inproceedings{li-etal-2024-framework,
    title = "A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval",
	author = "Li, Yading  and
      Song, Dandan  and
      Zhou, Changzhi  and
      Tian, Yuhang  and
      Wang, Hao  and
      Yang, Ziyi  and
      Zhang, Shuhao",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.670/",
	doi = "10.18653/v1/2024.findings-emnlp.670",
	pages = "11472--11485",
	abstract = "Knowledge graphs (KGs) can provide explainable reasoning for large language models (LLMs), alleviating their hallucination problem. Knowledge graph question answering (KGQA) is a typical benchmark to evaluate the methods enhancing LLMs with KG. Previous methods on KG-enhanced LLM for KGQA either enhance LLMs with KG retrieval in a single round or perform multi-hop KG reasoning in multiple rounds with LLMs. Both of them conduct retrieving and reasoning based solely on the whole original question, without any processing to the question. To tackle this limitation, we propose a framework of KG-enhanced LLM based on question decomposition and atomic retrieval, called KELDaR. We introduce question decomposition tree as the framework for LLM reasoning. This approach extracts the implicit information of reasoning steps within complex questions, serving as a guide to facilitate atomic retrieval on KG targeting the atomic-level simple questions at leaves of the tree. Additionally, we design strategies for atomic retrieval, which extract and retrieve question-relevant KG subgraphs to assist the few-shot LLM in answering atomic-level questions. Experiments on KGQA datasets demonstrate that our framework outperforms existing reasoning-based baselines. And in a low-cost setting without additional training or fine-tuning, our framework achieves competitive or superior results compared to most existing training-based baselines."
}

@inproceedings{yu-etal-2024-llms,
    title = "{LLM}s as Collaborator: Demands-Guided Collaborative Retrieval-Augmented Generation for Commonsense Knowledge-Grounded Open-Domain Dialogue Systems",
	author = "Yu, Jiong  and
      Wu, Sixing  and
      Chen, Jiahao  and
      Zhou, Wei",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.794/",
	doi = "10.18653/v1/2024.findings-emnlp.794",
	pages = "13586--13612",
	abstract = "Capturing the unique knowledge demands for each dialogue context plays a crucial role in commonsense knowledge-grounded response generation. However, current CoT-based and RAG-based methods are still unsatisfactory in the era of LLMs because 1) CoT often overestimates the capabilities of LLMs and treats them as isolated knowledge Producers; thus, CoT only uses the inherent knowledge of LLM itself and then suffers from the hallucination and outdated knowledge, and 2) RAG underestimates LLMs because LLMs are the passive Receivers that can only use the knowledge retrieved by external retrievers. In contrast, this work regards LLMs as interactive Collaborators and proposes a novel DCRAG (Demands-Guided Collaborative RAG) to leverage the knowledge from both LLMs and the external knowledge graph. Specifically, DCRAG designs three Thought-then-Generate stages to collaboratively investigate knowledge demands, followed by a Demands-Guided Knowledge Retrieval to retrieve external knowledge by interacting with LLMs. Extensive experiments and in-depth analyses on English DailyDialog and Chinese Diamante datasets proved DCRAG can effectively capture knowledge demands and bring higher-quality responses."
}

@inproceedings{chen-etal-2024-llm,
    title = "{LLM}-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments",
	author = "Chen, Ruirui  and
      Jiang, Weifeng  and
      Qin, Chengwei  and
      Rawal, Ishaan Singh  and
      Tan, Cheston  and
      Choi, Dongkyu  and
      Xiong, Bo  and
      Ai, Bo",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.844/",
	doi = "10.18653/v1/2024.findings-emnlp.844",
	pages = "14438--14451",
	abstract = "The important challenge of keeping knowledge in Large Language Models (LLMs) up-to-date has led to the development of various methods for incorporating new facts. However, existing methods for such knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates. To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straightforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo significantly surpasses current state-of-the-art (SOTA) knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits."
}

@inproceedings{huang-etal-2024-less,
    title = "Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop {KGQA}",
	author = "Huang, Wenyu  and
      Zhou, Guancheng  and
      Wang, Hongru  and
      Vougiouklis, Pavlos  and
      Lapata, Mirella  and
      Pan, Jeff Z.",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-emnlp.927/",
	doi = "10.18653/v1/2024.findings-emnlp.927",
	pages = "15787--15803",
	abstract = "Retrieval-Augmented Generation (RAG) is widely used to inject external non-parametric knowledge into large language models (LLMs). Recent works suggest that Knowledge Graphs (KGs) contain valuable external knowledge for LLMs. Retrieving information from KGs differs from extracting it from document sets. Most existing approaches seek to directly retrieve relevant subgraphs, thereby eliminating the need for extensive SPARQL annotations, traditionally required by semantic parsing methods. In this paper, we model the subgraph retrieval task as a conditional generation task handled by small language models. Specifically, we define a subgraph identifier as a sequence of relations, each represented as a special token stored in the language models. Our base generative subgraph retrieval model, consisting of only 220M parameters, achieves competitive retrieval performance compared to state-of-the-art models relying on 7B parameters, demonstrating that small language models are capable of performing the subgraph retrieval task. Furthermore, our largest 3B model, when plugged with an LLM reader, sets new SOTA end-to-end performance on both the WebQSP and CWQ benchmarks. Our model and data will be made available online: https://github.com/hwy9855/GSR."
}

@inproceedings{li-etal-2024-contextualization,
    title = "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
	author = "Li, Dawei  and
      Tan, Zhen  and
      Chen, Tianlong  and
      Liu, Huan",
	editor = "Graham, Yvette  and
      Purver, Matthew",
	booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
	month = mar,
	year = "2024",
	address = "St. Julian{'}s, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-eacl.32/",
	pages = "458--477",
	abstract = "While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the \textit{Contextualization Distillation} strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks{---}reconstruction and contextualization{---}allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into how to generate high-quality corpora for KGC, as well as the selection of suitable distillation tasks."
}

@inproceedings{agarwal-etal-2024-indifoodvqa,
    title = "{I}ndi{F}ood{VQA}: Advancing Visual Question Answering and Reasoning with a Knowledge-Infused Synthetic Data Generation Pipeline",
	author = "Agarwal, Pulkit  and
      Sravanthi, Settaluri  and
      Bhattacharyya, Pushpak",
	editor = "Graham, Yvette  and
      Purver, Matthew",
	booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
	month = mar,
	year = "2024",
	address = "St. Julian{'}s, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-eacl.78/",
	pages = "1158--1176",
	abstract = "Large Vision Language Models (VLMs) like GPT-4, LLaVA, and InstructBLIP exhibit extraordinary capabilities for both knowledge understanding and reasoning. However, the reasoning capabilities of such models on sophisticated problems that require external knowledge of a specific domain have not been assessed well, due to the unavailability of necessary datasets. In this work, we release a first-of-its-kind dataset called IndiFoodVQA with around 16.7k data samples, consisting of explicit knowledge-infused questions, answers, and reasons. We also release IndiFoodKG, a related Knowledge Graph (KG) with 79k triples. The data has been created with minimal human intervention via an automated pipeline based on InstructBlip and GPT-3.5. We also present a methodology to extract knowledge from the KG and use it to both answer and reason upon the questions. We employ different models to report baseline zero-shot and fine-tuned results. Fine-tuned VLMs on our data showed an improvement of {\textasciitilde}25{\%} over the corresponding base model, highlighting the fact that current VLMs need domain-specific fine-tuning to excel in specialized settings. Our findings reveal that (1) explicit knowledge infusion during question generation helps in making questions that have more grounded knowledge, and (2) proper knowledge retrieval can often lead to better-answering potential in such cases. The data and code is available at https://github.com/SLSravanthi/IndifoodVQA."
}

@inproceedings{jiao-etal-2024-text2db,
    title = "{T}ext2{DB}: Integration-Aware Information Extraction with Large Language Model Agents",
	author = "Jiao, Yizhu  and
      Li, Sha  and
      Zhou, Sizhe  and
      Ji, Heng  and
      Han, Jiawei",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.12/",
	doi = "10.18653/v1/2024.findings-acl.12",
	pages = "185--205",
	abstract = "The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE, Text2DB, that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for \textit{what to extract} and adapting to the given DB/KB schema for \textit{how to extract} on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-Plan-Analyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation."
}

@inproceedings{li-etal-2024-towards-verifiable,
    title = "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution",
	author = "Li, Xinze  and
      Cao, Yixin  and
      Pan, Liangming  and
      Ma, Yubo  and
      Sun, Aixin",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.28/",
	doi = "10.18653/v1/2024.findings-acl.28",
	pages = "493--516",
	abstract = "Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. Although language attribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new {\textquotedblleft}Conscious Incompetence{\textquotedblright} setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs' citation generation, emphasizing the importance of incorporating the {\textquotedblleft}Conscious Incompetence{\textquotedblright} setting, and the critical role of retrieval accuracy."
}

@inproceedings{liu-etal-2024-conversational,
    title = "Conversational Question Answering with Language Models Generated Reformulations over Knowledge Graph",
	author = "Liu, Lihui  and
      Hill, Blaine  and
      Du, Boxin  and
      Wang, Fei  and
      Tong, Hanghang",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.48/",
	doi = "10.18653/v1/2024.findings-acl.48",
	pages = "839--850",
	abstract = "Conversational question answering (ConvQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit question-answer pairs. These inputs are easy for human beings to understand given a conversation history, but hard for a machine to interpret, which can degrade ConvQA performance. To address this problem, we propose a reinforcement learning (RL) based model, CoRnNet, which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CoRnNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model`s output via reformulations generated by LLMs. The learned question representation is then used by a RL model to locate the correct answer in a KG. Extensive experimental results show that CoRnNet outperforms state-of-the-art ConvQA models."
}

@inproceedings{luo-etal-2024-chatkbqa,
    title = "{C}hat{KBQA}: A Generate-then-Retrieve Framework for Knowledge Base Question Answering with Fine-tuned Large Language Models",
	author = "Luo, Haoran  and
      E, Haihong  and
      Tang, Zichen  and
      Peng, Shiyao  and
      Guo, Yikai  and
      Zhang, Wentai  and
      Ma, Chenghao  and
      Dong, Guanting  and
      Song, Meina  and
      Lin, Wei  and
      Zhu, Yifan  and
      Luu, Anh Tuan",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.122/",
	doi = "10.18653/v1/2024.findings-acl.122",
	pages = "2039--2056",
	abstract = "Knowledge Base Question Answering (KBQA) aims to answer natural language questions over large-scale knowledge bases (KBs), which can be summarized into two crucial steps: knowledge retrieval and semantic parsing. However, three core challenges remain: inefficient knowledge retrieval, mistakes of retrieval adversely impacting semantic parsing, and the complexity of previous KBQA methods. To tackle these challenges, we introduce ChatKBQA, a novel and simple generate-then-retrieve KBQA framework, which proposes first generating the logical form with fine-tuned LLMs, then retrieving and replacing entities and relations with an unsupervised retrieval method, to improve both generation and retrieval more directly. Experimental results show that ChatKBQA achieves new state-of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This work can also be regarded as a new paradigm for combining LLMs with knowledge graphs (KGs) for interpretable and knowledge-required question answering."
}

@inproceedings{nguyen-etal-2024-direct,
    title = "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs",
	author = "Nguyen, Minh-Vuong  and
      Luo, Linhao  and
      Shiri, Fatemeh  and
      Phung, Dinh  and
      Li, Yuan-Fang  and
      Vu, Thuy-Trang  and
      Haffari, Gholamreza",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.168/",
	doi = "10.18653/v1/2024.findings-acl.168",
	pages = "2862--2883",
	abstract = "Large language models (LLMs) have demonstrated strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT generated by LLMs, indicating that they often arrive at correct answers through incorrect reasoning."
}

@inproceedings{jiang-etal-2024-efficient,
    title = "Efficient Knowledge Infusion via {KG}-{LLM} Alignment",
	author = "Jiang, Zhouyu  and
      Zhong, Ling  and
      Sun, Mengshu  and
      Xu, Jun  and
      Sun, Rui  and
      Cai, Hui  and
      Luo, Shuhan  and
      Zhang, Zhiqiang",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.176/",
	doi = "10.18653/v1/2024.findings-acl.176",
	pages = "2986--2999",
	abstract = "To tackle the problem of domain-specific knowledge scarcity within large language models (LLMs), knowledge graph-retrievalaugmented method has been proven to be an effective and efficient technique for knowledge infusion. However, existing approaches face two primary challenges: knowledge mismatch between public available knowledge graphs and the specific domain of the task at hand, and poor information compliance of LLMs with knowledge graphs. In this paper, we leverage a small set of labeled samples and a large-scale corpus to efficiently construct domain-specific knowledge graphs by an LLM, addressing the issue of knowledge mismatch. Additionally, we propose a three-stage KG-LLM alignment strategy to enhance the LLM`s capability to utilize information from knowledge graphs. We conduct experiments with a limited-sample setting on two biomedical question-answering datasets, and the results demonstrate that our approach outperforms existing baselines."
}

@inproceedings{fang-etal-2024-dara,
    title = "$\texttt{DARA}$: Decomposition-Alignment-Reasoning Autonomous Language Agent for Question Answering over Knowledge Graphs",
	author = "Fang, Haishuo  and
      Zhu, Xiaodan  and
      Gurevych, Iryna",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.203/",
	doi = "10.18653/v1/2024.findings-acl.203",
	pages = "3406--3432",
	abstract = "Answering Questions over Knowledge Graphs (KGQA) is key to well-functioning autonomous language agents in various real-life applications. To improve the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA, we propose the Decomposition-Alignment-Reasoning Agent (DARA) framework. DARA effectively parses questions into formal queries through a dual mechanism: high-level iterative task decomposition and low-level task grounding. Importantly, DARA can be efficiently trained with a small number of high-quality reasoning trajectories. Our experimental results demonstrate that DARA fine-tuned on LLMs (e.g. Llama-2-7B, Mistral) outperforms both in-context learning-based agents with GPT-4 and alternative fine-tuned agents, across different benchmarks, making such models more accessible for real-life applications. We also show that DARA attains performance comparable to state-of-the-art enumerating-and-ranking-based methods for KGQA."
}

@inproceedings{wold-etal-2024-compositional,
    title = "Compositional Generalization with Grounded Language Models",
	author = "Wold, Sondre  and
      Simon, {\'E}tienne  and
      Charpentier, Lucas  and
      Kostylev, Egor  and
      Velldal, Erik  and
      {\O}vrelid, Lilja",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.205/",
	doi = "10.18653/v1/2024.findings-acl.205",
	pages = "3447--3460",
	abstract = "Grounded language models use external sources of information, such as knowledge graphs, to meet some of the general challenges associated with pre-training. By extending previous work on compositional generalization in semantic parsing, we allow for a controlled evaluation of the degree to which these models learn and generalize from patterns in knowledge graphs. We develop a procedure for generating natural language questions paired with knowledge graphs that targets different aspects of compositionality and further avoids grounding the language models in information already encoded implicitly in their weights. We evaluate existing methods for combining language models with knowledge graphs and find them to struggle with generalization to sequences of unseen lengths and to novel combinations of seen base components. While our experimental results provide some insight into the expressive power of these models, we hope our work and released datasets motivate future research on how to better combine language models with structured knowledge representations."
}

@inproceedings{wang-etal-2024-llm,
    title = "{LLM} as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs",
	author = "Wang, Kai  and
      Xu, Yuwei  and
      Wu, Zhiyong  and
      Luo, Siqiang",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.224/",
	doi = "10.18653/v1/2024.findings-acl.224",
	pages = "3742--3759",
	abstract = "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-resource KG datasets and find that ProLINK outperforms previous methods in three-shot, one-shot, and zero-shot reasoning tasks, exhibiting average performance improvements by 20{\%}, 45{\%}, and 147{\%}, respectively. Furthermore, ProLINK demonstrates strong robustness for various LLM promptings as well as full-shot scenarios."
}

@inproceedings{tian-etal-2024-kg,
    title = "{KG}-Adapter: Enabling Knowledge Graph Integration in Large Language Models through Parameter-Efficient Fine-Tuning",
	author = "Tian, Shiyu  and
      Luo, Yangyang  and
      Xu, Tianze  and
      Yuan, Caixia  and
      Jiang, Huixing  and
      Wei, Chen  and
      Wang, Xiaojie",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.229/",
	doi = "10.18653/v1/2024.findings-acl.229",
	pages = "3813--3828",
	abstract = "Although large language models (LLMs) show remarkable capabilities and generalizability across various tasks, they are criticized for lack of expertise. One promising solution is to combine knowledge graphs (KGs) with LLMs, and recent studies focus on integrating KGs into LLMs through prompt-based methods. However, these approaches fail to use the structural information of the KGs, suffer from the problem of knowledge conflict, and over-reliance on super LLMs. To address these challenges, we propose KG-Adapter, a parameter-level KG integration method based on parameter-efficient fine-tuning (PEFT). Specifically, we introduce a novel adapter structure designed for decoder-only LLMs, which can encode KGs from both node-centered and relation-centered perspectives, and then perform joint reasoning with LLMs to generate responses end-to-end. Experiments with diverse models on four datasets for two different tasks all demonstrate significant improvements. With only 28M parameters trained, we make the 7B-parameter LLM outperform the previous full-parameter fine-tuned state-of-the-art method and comparable to the prompt-based ChatGPT methods."
}

@inproceedings{cheng-etal-2024-call,
    title = "Call Me When Necessary: {LLM}s can Efficiently and Faithfully Reason over Structured Environments",
	author = "Cheng, Sitao  and
      Zhuang, Ziyuan  and
      Xu, Yong  and
      Yang, Fangkai  and
      Zhang, Chaoyun  and
      Qin, Xiaoting  and
      Huang, Xiang  and
      Chen, Ling  and
      Lin, Qingwei  and
      Zhang, Dongmei  and
      Rajmohan, Saravan  and
      Zhang, Qi",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.254/",
	doi = "10.18653/v1/2024.findings-acl.254",
	pages = "4275--4295",
	abstract = "Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graphs and tables. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous works adopt LLMs to incrementally build a reasoning path, where LLMs either invoke tools or pick up items by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA and two TableQA datasets show the effectiveness of Readi, significantly surpassing previous LLM-based methods (by 9.1{\%} Hit@1 on WebQSP, 12.4{\%} on MQA-3H and 9.5{\%} on WTQ), comparable with state-of-the-art fine-tuned methods (67{\%} on CWQ and 74.7{\%} on WebQSP) and substantially boosting the vanilla LLMs (by 14.9{\%} on CWQ). Our code will be available on \url{https://aka.ms/readi}."
}

@inproceedings{yan-etal-2024-multi,
    title = "Multi-modal Concept Alignment Pre-training for Generative Medical Visual Question Answering",
	author = "Yan, Quan  and
      Duan, Junwen  and
      Wang, Jianxin",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.319/",
	doi = "10.18653/v1/2024.findings-acl.319",
	pages = "5378--5389",
	abstract = "Medical Visual Question Answering (Med-VQA) seeks to accurately respond to queries regarding medical images, a task particularly challenging for open-ended questions. This study unveils the Multi-modal Concept Alignment Pre-training (MMCAP) approach for generative Med-VQA, leveraging a knowledge graph sourced from medical image-caption datasets and the Unified Medical Language System. MMCAP advances the fusion of visual and textual medical knowledge via a graph attention network and a transformer decoder. Additionally, it incorporates a Type Conditional Prompt in the fine-tuning phase, markedly boosting the accuracy and relevance of answers to open-ended questions. Our tests on benchmark datasets illustrate MMCAP`s superiority over existing methods, demonstrating its high efficiency in data-limited settings and effective knowledge-image alignment capability."
}

@inproceedings{liu-etal-2024-knowledge-graph,
    title = "Knowledge Graph-Enhanced Large Language Models via Path Selection",
	author = "Liu, Haochen  and
      Wang, Song  and
      Zhu, Yaochen  and
      Dong, Yushun  and
      Li, Jundong",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.376/",
	doi = "10.18653/v1/2024.findings-acl.376",
	pages = "6311--6321",
	abstract = "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP."
}

@inproceedings{yu-etal-2024-onsep,
    title = "{ONSEP}: A Novel Online Neural-Symbolic Framework for Event Prediction Based on Large Language Model",
	author = "Yu, Xuanqing  and
      Sun, Wangtao  and
      Li, Jingwei  and
      Liu, Kang  and
      Liu, Chengbao  and
      Tan, Jie",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.378/",
	doi = "10.18653/v1/2024.findings-acl.378",
	pages = "6335--6350",
	abstract = "In the realm of event prediction, temporal knowledge graph forecasting (TKGF) stands as a pivotal technique. Previous approaches face the challenges of not utilizing experience during testing and relying on a single short-term history, which limits adaptation to evolving data. In this paper, we introduce the Online Neural-Symbolic Event Prediction (ONSEP) framework, which innovates by integrating dynamic causal rule mining (DCRM) and dual history augmented generation (DHAG). DCRM dynamically constructs causal rules from real-time data, allowing for swift adaptation to new causal relationships. In parallel, DHAG merges short-term and long-term historical contexts, leveraging a bi-branch approach to enrich event prediction. Our framework demonstrates notable performance enhancements across diverse datasets, with significant Hit@k (k=1,3,10) improvements, showcasing its ability to augment large language models (LLMs) for event prediction without necessitating extensive retraining. The ONSEP framework not only advances the field of TKGF but also underscores the potential of neural-symbolic approaches in adapting to dynamic data environments."
}

@inproceedings{gao-etal-2024-two,
    title = "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models",
	author = "Gao, Yifu  and
      Qiao, Linbo  and
      Kan, Zhigang  and
      Wen, Zhihua  and
      He, Yongquan  and
      Li, Dongsheng",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.401/",
	doi = "10.18653/v1/2024.findings-acl.401",
	pages = "6719--6734",
	abstract = "Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM`s intrinsic knowledge to mine temporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text representations of the LLM in a non-shallow way, which helps the open-source LLM deeply understand the temporal order and structural dependencies among the retrieved facts through instruction tuning. Experimental results on two widely used datasets demonstrate the superiority of our model."
}

@inproceedings{ren-etal-2024-identifying,
    title = "Identifying Semantic Induction Heads to Understand In-Context Learning",
	author = "Ren, Jie  and
      Guo, Qipeng  and
      Yan, Hang  and
      Liu, Dongrui  and
      Zhang, Quanshi  and
      Qiu, Xipeng  and
      Lin, Dahua",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.412/",
	doi = "10.18653/v1/2024.findings-acl.412",
	pages = "6916--6932",
	abstract = "Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to subject tokens, they recall object tokens and increase the output logits of those object tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs."
}

@inproceedings{sun-etal-2024-oda,
    title = "{ODA}: Observation-Driven Agent for integrating {LLM}s and Knowledge Graphs",
	author = "Sun, Lei  and
      Tao, Zhengwei  and
      Li, Youdi  and
      Arakawa, Hiroshi",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.442/",
	doi = "10.18653/v1/2024.findings-acl.442",
	pages = "7417--7431",
	abstract = "The integration of Large Language Models (LLMs) and knowledge graphs (KGs) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate LLMs and KGs often navigate the task-solving process solely based on the LLM`s analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in KGs. To address this, we introduce Observation-Driven Agent (ODA), a novel AI agent framework tailored for tasks involving KGs. ODA incorporates KG reasoning abilities via global observation, which enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, ODA demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87{\%} and 8.9{\%}."
}

@inproceedings{wang-etal-2024-ecok,
    title = "{EC}o{K}: Emotional Commonsense Knowledge Graph for Mining Emotional Gold",
	author = "Wang, Zhunheng  and
      Liu, Xiaoyi  and
      Hu, Mengting  and
      Ying, Rui  and
      Jiang, Ming  and
      Wu, Jianfeng  and
      Xie, Yalan  and
      Gao, Hang  and
      Cheng, Renhong",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.480/",
	doi = "10.18653/v1/2024.findings-acl.480",
	pages = "8055--8074",
	abstract = "The demand for understanding and expressing emotions in the field of natural language processing is growing rapidly. Knowledge graphs, as an important form of knowledge representation, have been widely utilized in various emotion-related tasks. However, existing knowledge graphs mainly focus on the representation and reasoning of general factual knowledge, while there are still significant deficiencies in the understanding and reasoning of emotional knowledge. In this work, we construct a comprehensive and accurate emotional commonsense knowledge graph, ECoK. We integrate cutting-edge theories from multiple disciplines such as psychology, cognitive science, and linguistics, and combine techniques such as large language models and natural language processing. By mining a large amount of text, dialogue, and sentiment analysis data, we construct rich emotional knowledge and establish the knowledge generation model COMET-ECoK. Experimental results show that ECoK contains high-quality emotional reasoning knowledge, and the performance of our knowledge generation model surpasses GPT-4-Turbo, which can help downstream tasks better understand and reason about emotions. Our data and code is available from https://github.com/ZornWang/ECoK."
}

@inproceedings{yang-etal-2024-knowledge,
    title = "Knowledge Context Modeling with Pre-trained Language Models for Contrastive Knowledge Graph Completion",
	author = "Yang, Guangqian  and
      Liu, Yi  and
      Zhang, Lei  and
      Zhang, Licheng  and
      Xie, Hongtao  and
      Mao, Zhendong",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.509/",
	doi = "10.18653/v1/2024.findings-acl.509",
	pages = "8619--8630",
	abstract = "Text-based knowledge graph completion (KGC) methods utilize pre-trained language models for triple encoding and further fine-tune the model to achieve completion. Despite their excellent performance, they neglect the knowledge context in inferring process. Intuitively, knowledge contexts, which refer to the neighboring triples around the target triples, are important information for triple inferring, since they provide additional detailed information about the entities. To this end, we propose a novel framework named KnowC, which models the knowledge context as additional prompts with pre-trained language models for knowledge graph completion. Given the substantial number of neighbors typically associated with entities, along with the constrained input token capacity of language models, we further devise several strategies to sample the neighbors. We conduct extensive experiments on common datasets FB15k-237, WN18RR and Wikidata5M, experiments show that KnowC achieves state-of-the-art performance."
}

@inproceedings{yang-etal-2024-pek,
    title = "{PEK}: A Parameter-Efficient Framework for Knowledge-Grounded Dialogue Generation",
	author = "Yang, Pan  and
      Song, Dandan  and
      Wu, Zhijing  and
      Zhou, Yanru",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.550/",
	doi = "10.18653/v1/2024.findings-acl.550",
	pages = "9261--9273",
	abstract = "Pre-trained language models (PLMs) have shown great dialogue generation capability in different scenarios. However, the huge VRAM consumption when fine-tuning them is one of their drawbacks. PEFT approaches can significantly reduce the number of trainable parameters, which enables us to fine-tune larger dialogue generation models. However, the reduction in parameter quantity can diminish a PLM`s expressive capacity and affect the PLM`s learning from certain specific examples like knowledge-related conversations. Previous works have demonstrated that injecting external knowledge into dialogue generation models can improve the model`s performance in knowledge-related conversations. Nonetheless, these methods are designed for the scenario where most parameters of the entire framework are trainable. In this paper, we propose PEK, a parameter-efficient framework for knowledge-enhanced dialogue generation. It enables PLMs to leverage external knowledge documents and knowledge graphs to enhance its generation capabilities with an acceptable number of trainable parameters. Evaluation results on the Wizard of Wikipedia and CMU{\_}DoG datasets show that our approach outperforms baseline methods on multiple evaluation metrics, which validates the effectiveness of our approach."
}

@inproceedings{wang-etal-2024-imapscore,
    title = "imap{S}core: Medical Fact Evaluation Made Easy",
	author = "Wang, Huimin  and
      Zhao, Yutian  and
      Wu, Xian  and
      Zheng, Yefeng",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.610/",
	doi = "10.18653/v1/2024.findings-acl.610",
	pages = "10242--10257",
	abstract = "Automatic evaluation of natural language generation (NLG) tasks has gained extensive research interests, since it can rapidly assess the performance of large language models (LLMs). However, automatic NLG evaluation struggles with medical QA because it fails to focus on the crucial correctness of medical facts throughout the generated text. To address this, this paper introduces a new data structure, \textit{imap}, designed to capture key information in questions and answers, enabling evaluators to focus on essential details. The \textit{imap} comprises three components: Query, Constraint, and Inform, each of which is in the form of term-value pairs to represent medical facts in a structural manner. We then introduce \textit{imap}Score, which compares the corresponding medical term-value pairs in the \textit{imap} to score generated texts. We utilize GPT-4 to extract \textit{imap} from questions, human-annotated answers, and generated responses. To mitigate the diversity in medical terminology for fair term-value pairs comparison, we use a medical knowledge graph to assist GPT-4 in determining matches. To compare \textit{imap}Score with existing NLG metrics, we establish a new benchmark dataset. The experimental results show that \textit{imap}Score consistently outperforms state-of-the-art metrics, demonstrating an average improvement of 79.8{\%} in correlation with human scores. Furthermore, incorporating \textit{imap} into n-gram, embedding, and LLM metrics boosts the base versions, increasing correlation with human scores by averages of 89.9{\%}, 81.7{\%}, and 32.6{\%}, respectively."
}

@inproceedings{sawczyn-etal-2024-developing,
    title = "Developing {PUGG} for {P}olish: A Modern Approach to {KBQA}, {MRC}, and {IR} Dataset Construction",
	author = "Sawczyn, Albert  and
      Viarenich, Katsiaryna  and
      Wojtasik, Konrad  and
      Domoga{\l}a, Aleksandra  and
      Oleksy, Marcin  and
      Piasecki, Maciej  and
      Kajdanowicz, Tomasz",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.652/",
	doi = "10.18653/v1/2024.findings-acl.652",
	pages = "10978--10996",
	abstract = "Advancements in AI and natural language processing have revolutionized machine-human language interactions, with question answering (QA) systems playing a pivotal role. The knowledge base question answering (KBQA) task, utilizing structured knowledge graphs (KG), allows for handling extensive knowledge-intensive questions. However, a significant gap exists in KBQA datasets, especially for low-resource languages. Many existing construction pipelines for these datasets are outdated and inefficient in human labor, and modern assisting tools like Large Language Models (LLM) are not utilized to reduce the workload. To address this, we have designed and implemented a modern, semi-automated approach for creating datasets, encompassing tasks such as KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR), tailored explicitly for low-resource environments. We executed this pipeline and introduced the PUGG dataset, the first Polish KBQA dataset, and novel datasets for MRC and IR. Additionally, we provide a comprehensive implementation, insightful findings, detailed statistics, and evaluation of baseline models."
}

@inproceedings{yu-etal-2024-enhancing,
    title = "Enhancing Distractor Generation for Multiple-Choice Questions with Retrieval Augmented Pretraining and Knowledge Graph Integration",
	author = "Yu, Han Cheng  and
      Shih, Yu An  and
      Law, Kin Man  and
      Hsieh, KaiYu  and
      Cheng, Yu Chen  and
      Ho, Hsin Chih  and
      Lin, Zih An  and
      Hsu, Wen-Chuan  and
      Fan, Yao-Chung",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.655/",
	doi = "10.18653/v1/2024.findings-acl.655",
	pages = "11019--11029",
	abstract = "In this paper, we tackle the task of distractor generation (DG) for multiple-choice questions. Our study introduces two key designs. First, we propose the concept of retrieval augmented pretraining, which involves refining the language model pretraining to align it more closely with the downstream task of DG. Second, we explore the integration of knowledge graphs and language models to further enhance the performance of DG. Our study unveils promising directions for further development in DG by showcasing the efficacy of knowledge augmentation and task-specific pretraining. These findings demonstrate the potential for leveraging both strategies to enhance the quality and performance of DG systems."
}

@inproceedings{meem-etal-2024-pat,
    title = "{PAT}-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering",
	author = "Meem, Jannat  and
      Rashid, Muhammad  and
      Dong, Yue  and
      Hristidis, Vagelis",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.777/",
	doi = "10.18653/v1/2024.findings-acl.777",
	pages = "13129--13148",
	abstract = "Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. {\textquoteleft}Who was the US president in 1970?'). Little work has studied questions whose temporal context is relative to the present time (e.g. {\textquoteleft}Who was the previous US president?'). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. {\textquoteleft}before', {\textquoteleft}previous') are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model (TEMPREASON-T5) on PAT-Questions through direct prompting and retrieval-augmented generation (RAG). The results highlight the limitations of existing solutions in PATQA and motivate the need for new methods to improve PATQA reasoning capabilities."
}

@inproceedings{zhang-etal-2024-light,
    title = "Light Up the Shadows: Enhance Long-Tailed Entity Grounding with Concept-Guided Vision-Language Models",
	author = "Zhang, Yikai  and
      He, Qianyu  and
      Wang, Xintao  and
      Yuan, Siyu  and
      Liang, Jiaqing  and
      Xiao, Yanghua",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.793/",
	doi = "10.18653/v1/2024.findings-acl.793",
	pages = "13379--13389",
	abstract = "Multi-Modal Knowledge Graphs (MMKGs) have proven valuable for various downstream tasks. However, scaling them up is challenging because building large-scale MMKGs often introduces mismatched images (i.e., noise). Most entities in KGs belong to the long tail, meaning there are few images of them available online. This scarcity makes it difficult to determine whether a found image matches the entity. To address this, we draw on the Triangle of Reference Theory and suggest enhancing vision-language models with concept guidance. Specifically, we introduce COG, a two-stage framework with COncept-Guided vision-language models. The framework comprises a Concept Integration module, which effectively identifies image-text pairs of long-tailed entities, and an Evidence Fusion module, which offers explainability and enables human verification. To demonstrate the effectiveness of COG, we create a dataset of 25k image-text pairs of long-tailed entities. Our comprehensive experiments show that COG not only improves the accuracy of recognizing long-tailed image-text pairs compared to baselines but also offers flexibility and explainability."
}

@inproceedings{yadav-etal-2024-tox,
    title = "Tox-{BART}: Leveraging Toxicity Attributes for Explanation Generation of Implicit Hate Speech",
	author = "Yadav, Neemesh  and
      Masud, Sarah  and
      Goyal, Vikram  and
      Akhtar, Md Shad  and
      Chakraborty, Tanmoy",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.831/",
	doi = "10.18653/v1/2024.findings-acl.831",
	pages = "13967--13983",
	abstract = "Employing language models to generate explanations for an incoming implicit hate post is an active area of research. The explanation is intended to make explicit the underlying stereotype and aid content moderators. The training often combines top-k relevant knowledge graph (KG) tuples to provide world knowledge and improve performance on standard metrics. Interestingly, our study presents conflicting evidence for the role of the quality of KG tuples in generating implicit explanations. Consequently, simpler models incorporating external toxicity signals outperform KG-infused models. Compared to the KG-based setup, we observe a comparable performance for SBIC (LatentHatred) datasets with a performance variation of +0.44 (+0.49), +1.83 (-1.56), and -4.59 (+0.77) in BLEU, ROUGE-L, and BERTScore. Further human evaluation and error analysis reveal that our proposed setup produces more precise explanations than zero-shot GPT-3.5, highlighting the intricate nature of the task."
}

@inproceedings{xu-etal-2024-knowledge,
    title = "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
	author = "Xu, Ran  and
      Cui, Hejie  and
      Yu, Yue  and
      Kan, Xuan  and
      Shi, Wenqi  and
      Zhuang, Yuchen  and
      Wang, May Dongmei  and
      Jin, Wei  and
      Ho, Joyce  and
      Yang, Carl",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.916/",
	doi = "10.18653/v1/2024.findings-acl.916",
	pages = "15496--15523",
	abstract = "Clinical natural language processing faces challenges like complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that ClinGen consistently enhances performance across various tasks by 7.7{\%}-8.7{\%} on average, effectively aligning the distribution of real datasets and enriching the diversity of generated training instances."
}

@inproceedings{xia-etal-2024-chain,
    title = "Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting",
	author = "Xia, Yuwei  and
      Wang, Ding  and
      Liu, Qiang  and
      Wang, Liang  and
      Wu, Shu  and
      Zhang, Xiao-Yu",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.findings-acl.955/",
	doi = "10.18653/v1/2024.findings-acl.955",
	pages = "16144--16159",
	abstract = "Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based on given histories. Most recent graph-based models excel at capturing structural information within TKGs but lack semantic comprehension abilities. Nowadays, with the surge of LLMs, the LLM-based TKG prediction model has emerged. However, the existing LLM-based model exhibits three shortcomings: (1) It only focuses on the first-order history for prediction while ignoring high-order historical information, resulting in the provided information for LLMs being extremely limited. (2) LLMs struggle with optimal reasoning performance under heavy historical information loads. (3) For TKG prediction, the temporal reasoning capability of LLM alone is limited. To address the first two challenges, we propose Chain-of-History (CoH) reasoning which explores high-order histories step-by-step, achieving effective utilization of high-order historical information for LLMs on TKG prediction. To address the third issue, we design CoH as a plug-and-play module to enhance the performance of graph-based models for TKG prediction. Extensive experiments on three datasets and backbones demonstrate the effectiveness of CoH."
}

@inproceedings{tan-etal-2024-enhancing-fact,
    title = "Enhancing Fact Verification with Causal Knowledge Graphs and Transformer-Based Retrieval for Deductive Reasoning",
	author = "Tan, Fiona Anting  and
      Desai, Jay  and
      Sengamedu, Srinivasan H.",
	editor = "Schlichtkrull, Michael  and
      Chen, Yulong  and
      Whitehouse, Chenxi  and
      Deng, Zhenyun  and
      Akhtar, Mubashara  and
      Aly, Rami  and
      Guo, Zhijiang  and
      Christodoulopoulos, Christos  and
      Cocarascu, Oana  and
      Mittal, Arpit  and
      Thorne, James  and
      Vlachos, Andreas",
	booktitle = "Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER)",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.fever-1.20/",
	doi = "10.18653/v1/2024.fever-1.20",
	pages = "151--169",
	abstract = "The ability to extract and verify factual information from free-form text is critical in an era where vast amounts of unstructured data are available, yet unreliable sources abound. This paper focuses on enhancing causal deductive reasoning, a key component of factual verification, through the lens of accident investigation, where determining the probable causes of events is paramount. Deductive reasoning refers to the task of drawing conclusions based on a premise. While some deductive reasoning benchmarks exist, none focus on causal deductive reasoning and are from real-world applications. Recently, large language models (LLMs) used with prompt engineering techniques like retrieval-augmented generation (RAG) have demonstrated remarkable performance across various natural language processing benchmarks. However, adapting these techniques to handle scenarios with no knowledge bases and to different data structures, such as graphs, remains an ongoing challenge. In our study, we introduce a novel framework leveraging LLMs' decent ability to detect and infer causal relations to construct a causal Knowledge Graph (KG) which represents knowledge that the LLM recognizes. Additionally, we propose a RoBERTa-based Transformer Graph Neural Network (RoTG) specifically designed to select relevant nodes within this KG. Integrating RoTG-retrieved causal chains into prompts effectively enhances LLM performance, demonstrating usefulness of our approach in advancing LLMs' causal deductive reasoning capabilities."
}

@inproceedings{gautam-pop-2024-factgenius,
    title = "{F}act{G}enius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs",
	author = "Gautam, Sushant  and
      Pop, Roxana",
	editor = "Schlichtkrull, Michael  and
      Chen, Yulong  and
      Whitehouse, Chenxi  and
      Deng, Zhenyun  and
      Akhtar, Mubashara  and
      Aly, Rami  and
      Guo, Zhijiang  and
      Christodoulopoulos, Christos  and
      Cocarascu, Oana  and
      Mittal, Arpit  and
      Thorne, James  and
      Vlachos, Andreas",
	booktitle = "Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER)",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.fever-1.30/",
	doi = "10.18653/v1/2024.fever-1.30",
	pages = "297--306",
	abstract = "Fact-checking is a crucial natural language processing (NLP) task that verifies the truthfulness of claims by considering reliable evidence. Traditional methods are labour- intensive, and most automatic approaches focus on using documents as evidence. In this paper, we focus on the relatively understudied fact-checking with Knowledge Graph data as evidence and experiment on the recently introduced FactKG benchmark. We present FactGenius, a novel method that enhances fact- checking by combining zero-shot prompting of large language models (LLMs) with fuzzy text matching on knowledge graphs (KGs). Our method employs LLMs for filtering relevant connections from the graph and validates these connections via distance-based matching. The evaluation of FactGenius on an existing benchmark demonstrates its effectiveness, as we show it significantly outperforms state-of- the-art methods. The code and materials are available at https://github.com/SushantGautam/FactGenius."
}

@inproceedings{opsahl-2024-fact,
    title = "Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals",
	author = "Opsahl, Tobias Aanderaa",
	editor = "Schlichtkrull, Michael  and
      Chen, Yulong  and
      Whitehouse, Chenxi  and
      Deng, Zhenyun  and
      Akhtar, Mubashara  and
      Aly, Rami  and
      Guo, Zhijiang  and
      Christodoulopoulos, Christos  and
      Cocarascu, Oana  and
      Mittal, Arpit  and
      Thorne, James  and
      Vlachos, Andreas",
	booktitle = "Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER)",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.fever-1.32/",
	doi = "10.18653/v1/2024.fever-1.32",
	pages = "307--316",
	abstract = "Despite recent success in natural language processing (NLP), fact verification still remains a difficult task. Due to misinformation spreading increasingly fast, attention has been directed towards automatically verifying the correctness of claims. In the domain of NLP, this is usually done by training supervised machine learning models to verify claims by utilizing evidence from trustworthy corpora. We present efficient methods for verifying claims on a dataset where the evidence is in the form of structured knowledge graphs. We use the FactKG dataset, which is constructed from the DBpedia knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval process, from fine-tuned language models to simple logical retrievals, we are able to construct models that both require less computational resources and achieve better test-set accuracy."
}

@inproceedings{chen-etal-2024-new,
    title = "A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning",
	author = "Chen, Zhongwu  and
      Bai, Long  and
      Li, Zixuan  and
      Huang, Zhen  and
      Jin, Xiaolong  and
      Dou, Yong",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.81/",
	doi = "10.18653/v1/2024.emnlp-main.81",
	pages = "1366--1381",
	abstract = "Conventional Knowledge Graph Reasoning (KGR) models learn the embeddings of KG components over the structure of KGs, but their performances are limited when the KGs are severely incomplete. Recent LLM-enhanced KGR models input KG structural information into LLMs. However, they require fine-tuning on open-source LLMs and are not applicable to closed-source LLMs. Therefore, in this paper, to leverage the knowledge in LLMs without fine-tuning to assist and enhance conventional KGR models, we propose a new three-stage pipeline, including knowledge alignment, KG reasoning and entity reranking. Specifically, in the alignment stage, we propose three strategies to align the knowledge in LLMs to the KG schema by explicitly associating unconnected nodes with semantic relations. Based on the enriched KGs, we train structure-aware KGR models to integrate aligned knowledge to original knowledge existing in KGs. In the reranking stage, after obtaining the results of KGR models, we rerank the top-scored entities with LLMs to recall correct answers further. Experiments show our pipeline can enhance the KGR performance in both incomplete and general situations. Code and datasets are available."
}

@inproceedings{zhang-etal-2024-cross-domain,
    title = "Cross-domain {NER} with Generated Task-Oriented Knowledge: An Empirical Study from Information Density Perspective",
	author = "Zhang, Zhihao  and
      Lee, Sophia Yat Mei  and
      Wu, Junshuang  and
      Zhang, Dong  and
      Li, Shoushan  and
      Cambria, Erik  and
      Zhou, Guodong",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.95/",
	doi = "10.18653/v1/2024.emnlp-main.95",
	pages = "1595--1609",
	abstract = "Cross-domain Named Entity Recognition (CDNER) is crucial for Knowledge Graph (KG) construction and natural language processing (NLP), enabling learning from source to target domains with limited data. Previous studies often rely on manually collected entity-relevant sentences from the web or attempt to bridge the gap between tokens and entity labels across domains. These approaches are time-consuming and inefficient, as these data are often weakly correlated with the target task and require extensive pre-training.To address these issues, we propose automatically generating task-oriented knowledge (GTOK) using large language models (LLMs), focusing on the reasoning process of entity extraction. Then, we employ task-oriented pre-training (TOPT) to facilitate domain adaptation. Additionally, current cross-domain NER methods often lack explicit explanations for their effectiveness. Therefore, we introduce the concept of information density to better evaluate the model`s effectiveness before performing entity recognition.We conduct systematic experiments and analyses to demonstrate the effectiveness of our proposed approach and the validity of using information density for model evaluation."
}

@inproceedings{wu-etal-2024-cotkr,
    title = "{C}o{TKR}: Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering",
	author = "Wu, Yike  and
      Huang, Yi  and
      Hu, Nan  and
      Hua, Yuncheng  and
      Qi, Guilin  and
      Chen, Jiaoyan  and
      Pan, Jeff Z.",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.205/",
	doi = "10.18653/v1/2024.emnlp-main.205",
	pages = "3501--3520",
	abstract = "Recent studies have explored the use of Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) for Knowledge Graph Question Answering (KGQA). They typically require rewriting retrieved subgraphs into natural language formats comprehensible to LLMs. However, when tackling complex questions, the knowledge rewritten by existing methods may include irrelevant information, omit crucial details, or fail to align with the question`s semantics. To address them, we propose a novel rewriting method CoTKR, Chain- of-Thought Enhanced Knowledge Rewriting, for generating reasoning traces and corresponding knowledge in an interleaved manner, thereby mitigating the limitations of single-step knowledge rewriting. Additionally, to bridge the preference gap between the knowledge rewriter and the question answering (QA) model, we propose a training strategy PAQAF, Preference Alignment from Question Answering Feedback, for leveraging feedback from the QA model to further optimize the knowledge rewriter. We conduct experiments using various LLMs across several KGQA benchmarks. Experimental results demonstrate that, compared with previous knowledge rewriting methods, CoTKR generates the most beneficial knowledge representation for QA models, which significantly improves the performance of LLMs in KGQA."
}

@inproceedings{toroghi-etal-2024-right,
    title = "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
	author = "Toroghi, Armin  and
      Guo, Willis  and
      Abdollah Pour, Mohammad Mahdi  and
      Sanner, Scott",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.378/",
	doi = "10.18653/v1/2024.emnlp-main.378",
	pages = "6601--6633",
	abstract = "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *{\textquotedblleft}In which city was Silvio Berlusconi`s first wife born?{\textquotedblright}*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *{\textquotedblleft}Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?{\textquotedblright}* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons ($R^3$), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks{---}question answering, claim verification, and preference matching{---}our findings showcase $R^3$ as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors."
}

@inproceedings{qian-etal-2024-timer4,
    title = "{T}ime{R}$^4$ : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering",
	author = "Qian, Xinying  and
      Zhang, Ying  and
      Zhao, Yu  and
      Zhou, Baohang  and
      Sui, Xuhui  and
      Zhang, Li  and
      Song, Kehui",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.394/",
	doi = "10.18653/v1/2024.emnlp-main.394",
	pages = "6942--6952",
	abstract = "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMs' temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR$^4$.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8{\%} and 22.5{\%} on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4."
}

@inproceedings{chen-etal-2024-xplainllm,
    title = "{X}plain{LLM}: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in {LLM}s",
	author = "Chen, Zichen  and
      Chen, Jianda  and
      Singh, Ambuj  and
      Sra, Misha",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.432/",
	doi = "10.18653/v1/2024.emnlp-main.432",
	pages = "7578--7596",
	abstract = "Large Language Models (LLMs) have achieved remarkable success in natural language tasks, yet understanding their reasoning processes remains a significant challenge. We address this by introducing XplainLLM, a dataset accompanying an explanation framework designed to enhance LLM transparency and reliability. Our dataset comprises 24,204 instances where each instance interprets the LLM`s reasoning behavior using knowledge graphs (KGs) and graph attention networks (GAT), and includes explanations of LLMs such as the decoder-only Llama-3 and the encoder-only RoBERTa. XplainLLM also features a framework for generating grounded explanations and the \textit{debugger-scores} for multidimensional quality analysis. Our explanations include \textit{why-choose} and \textit{why-not-choose} components, \textit{reason-elements}, and \textit{debugger-scores} that collectively illuminate the LLM`s reasoning behavior. Our evaluations demonstrate XplainLLM`s potential to reduce hallucinations and improve grounded explanation generation in LLMs. XplainLLM is a resource for researchers and practitioners to build trust and verify the reliability of LLM outputs. Our code and dataset are publicly available."
}

@inproceedings{zhang-soh-2024-extract,
    title = "Extract, Define, Canonicalize: An {LLM}-based Framework for Knowledge Graph Construction",
	author = "Zhang, Bowen  and
      Soh, Harold",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.548/",
	doi = "10.18653/v1/2024.emnlp-main.548",
	pages = "9820--9836",
	abstract = "In this work, we are interested in automated methods for knowledge graph creation (KGC) from input text. Progress on large language models (LLMs) has prompted a series of recent works applying them to KGC, e.g., via zero/few-shot prompting. Despite successes on small domain-specific datasets, these models face difficulties scaling up to text common in many real-world applications. A principal issue is that, in prior methods, the KG schema has to be included in the LLM prompt to generate valid triplets; larger and more complex schemas easily exceed the LLMs' context window length. Furthermore, there are scenarios where a fixed pre-defined schema is not available and we would like the method to construct a high-quality KG with a succinct self-generated schema. To address these problems, we propose a three-phase framework named Extract-Define-Canonicalize (EDC): open information extraction followed by schema definition and post-hoc canonicalization. EDC is flexible in that it can be applied to settings where a pre-defined target schema is available and when it is not; in the latter case, it constructs a schema automatically and applies self-canonicalization. To further improve performance, we introduce a trained component that retrieves schema elements relevant to the input text; this improves the LLMs' extraction performance in a retrieval-augmented generation-like manner. We demonstrate on three KGC benchmarks that EDC is able to extract high-quality triplets without any parameter tuning and with significantly larger schemas compared to prior works. Code for EDC is available at https://github.com/clear-nus/edc."
}

@inproceedings{ko-etal-2024-evidence,
    title = "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering",
	author = "Ko, Sungho  and
      Cho, Hyunjin  and
      Chae, Hyungjoo  and
      Yeo, Jinyoung  and
      Lee, Dongha",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.594/",
	doi = "10.18653/v1/2024.emnlp-main.594",
	pages = "10636--10651",
	abstract = "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challenging. Existing methods, like concatenation or free-form textual conversion of triples, have limitations, including duplicated entities or relations, reduced evidence density, and failure to highlight crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an LLM as a fact summarizer through distillation and preference alignment. Our extensive expeirments show that EFSum improves LLM`s zero-shot QA performance with its helpful and faithful summaries, especially when noisy facts are retrieved."
}

@inproceedings{wang-etal-2024-enhancing-high,
    title = "Enhancing High-order Interaction Awareness in {LLM}-based Recommender Model",
	author = "Wang, Xinfeng  and
      Cui, Jin  and
      Fukumoto, Fumiyo  and
      Suzuki, Yoshimi",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.653/",
	doi = "10.18653/v1/2024.emnlp-main.653",
	pages = "11696--11711",
	abstract = "Large language models (LLMs) have demonstrated prominent reasoning capabilities in recommendation tasks by transforming them into text-generation tasks. However, existing approaches either disregard or ineffectively model the user-item high-order interactions. To this end, this paper presents an enhanced LLM-based recommender (ELMRec). We enhance whole-word embeddings to substantially enhance LLMs' interpretation of graph-constructed interactions for recommendations, without requiring graph pre-training. This finding may inspire endeavors to incorporate rich knowledge graphs into LLM-based recommenders via whole-word embedding. We also found that LLMs often recommend items based on users' earlier interactions rather than recent ones, and present a reranking solution. Our ELMRec outperforms state-of-the-art (SOTA) methods, especially achieving a 124.3{\%} to 293.7{\%} improvement over SOTA LLM-based methods in direct recommendations. Our code is available online."
}

@inproceedings{ge-etal-2024-dkec,
    title = "{DKEC}: Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction",
	author = "Ge, Xueren  and
      Satpathy, Abhishek  and
      Williams, Ronald Dean  and
      Stankovic, John  and
      Alemzadeh, Homa",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.712/",
	doi = "10.18653/v1/2024.emnlp-main.712",
	pages = "12798--12813",
	abstract = "Multi-label text classification (MLTC) tasks in the medical domain often face the long-tail label distribution problem. Prior works have explored hierarchical label structures to find relevant information for few-shot classes, but mostly neglected to incorporate external knowledge from medical guidelines. This paper presents DKEC, Domain Knowledge Enhanced Classification for diagnosis prediction with two innovations: (1) automated construction of heterogeneous knowledge graphs from external sources to capture semantic relations among diverse medical entities, (2) incorporating the heterogeneous knowledge graphs in few-shot classification using a label-wise attention mechanism. We construct DKEC using three online medical knowledge sources and evaluate it on a real-world Emergency Medical Services (EMS) dataset and a public electronic health record (EHR) dataset. Results show that DKEC outperforms the state-of-the-art label-wise attention networks and transformer models of different sizes, particularly for the few-shot classes. More importantly, it helps the smaller language models achieve comparable performance to large language models."
}

@inproceedings{lin-etal-2024-improving,
    title = "Improving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning",
	author = "Lin, Jiashi  and
      Wang, Lifang  and
      Lu, Xinyu  and
      Hu, Zhongtian  and
      Zhang, Wei  and
      Lu, Wenxuan",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.772/",
	doi = "10.18653/v1/2024.emnlp-main.772",
	pages = "13948--13959",
	abstract = "Knowledge Graphs (KGs) often suffer from incomplete knowledge, which which restricts their utility. Recently, Contrastive Learning (CL) has been introduced to Knowledge Graph Completion (KGC), significantly improving the discriminative capabilities of KGC models and setting new benchmarks in performance. However, existing contrastive methods primarily focus on individual triples, overlooking the broader structural connectivities and topologies of KGs. This narrow focus limits a comprehensive understanding of the graph`s structural knowledge. To address this gap, we propose StructKGC, a novel contrastive learning framework designed to flexibly accommodate the diverse topologies inherent in KGs. Additionally, we introduce four contrastive tasks specifically tailored to KG data: Vertex-level CL, Neighbor-level CL, Path-level CL, and Relation composition level CL. These tasks are trained synergistically during the fine-tuning of pre-trained language models (PLMs), allowing for a more nuanced capture of subgraph semantics. To validate the effectiveness of our method, we perform a comprehensive set of experiments on several real-world datasets. The experimental results demonstrate that our approach achieves SOTA performance under standard supervised and low-resource settings. Furthermore, the different levels of structure-aware tasks introduced can mutually reinforce each other, leading to consistent performance improvements."
}

@inproceedings{li-etal-2024-mocokgc,
    title = "{M}o{C}o{KGC}: Momentum Contrast Entity Encoding for Knowledge Graph Completion",
	author = "Li, Qingyang  and
      Zhong, Yanru  and
      Qin, Yuchu",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.832/",
	doi = "10.18653/v1/2024.emnlp-main.832",
	pages = "14940--14952",
	abstract = "In recent years, numerous studies have sought to enhance the capabilities of pretrained language models (PLMs) for Knowledge Graph Completion (KGC) tasks by integrating structural information from knowledge graphs. However, existing approaches have not effectively combined the structural attributes of knowledge graphs with the textual descriptions of entities to generate robust entity encodings.To address this issue, this paper proposes MoCoKGC (Momentum Contrast Entity Encoding for Knowledge Graph Completion), which incorporates three primary encoders: the entity-relation encoder, the entity encoder, and the momentum entity encoder. Momentum contrastive learning not only provides more negative samples but also allows for the gradual updating of entity encodings. Consequently, we reintroduce the generated entity encodings into the encoder to incorporate the graph`s structural information.Additionally, MoCoKGC enhances the inferential capabilities of the entity-relation encoder through deep prompts of relations. On the standard evaluation metric, Mean Reciprocal Rank (MRR), the MoCoKGC model demonstrates superior performance, achieving a 7.1{\%} improvement on the WN18RR dataset and an 11{\%} improvement on the Wikidata5M dataset, while also surpassing the current best model on the FB15k-237 dataset. Through a series of experiments, this paper thoroughly examines the role and contribution of each component and parameter of the model."
}

@inproceedings{qiu-etal-2024-joint,
    title = "Joint Pre-Encoding Representation and Structure Embedding for Efficient and Low-Resource Knowledge Graph Completion",
	author = "Qiu, Chenyu  and
      Qian, Pengjiang  and
      Wang, Chuang  and
      Yao, Jian  and
      Liu, Li  and
      Wei, Fang  and
      Eddie, Eddie Y.k.",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.851/",
	doi = "10.18653/v1/2024.emnlp-main.851",
	pages = "15257--15269",
	abstract = "Knowledge graph completion (KGC) aims to infer missing or incomplete parts in knowledge graph. The existing models are generally divided into structure-based and description-based models, among description-based models often require longer training and inference times as well as increased memory usage. In this paper, we propose Pre-Encoded Masked Language Model (PEMLM) to efficiently solve KGC problem. By encoding textual descriptions into semantic representations before training, the necessary resources are significantly reduced. Furthermore, we introduce a straightforward but effective fusion framework to integrate structural embedding with pre-encoded semantic description, which enhances the model`s prediction performance on 1-N relations. The experimental results demonstrate that our proposed strategy attains state-of-the-art performance on the WN18RR (MRR+5.4{\%} and Hits@1+6.4{\%}) and UMLS datasets. Compared to existing models, we have increased inference speed by 30x and reduced training memory by approximately 60{\%}."
}

@inproceedings{jiang-etal-2024-tkgt,
    title = "{TKGT}: Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented {LLM}s",
	author = "Jiang, Peiwen  and
      Lin, Xinbo  and
      Zhao, Zibo  and
      Ma, Ruhui  and
      Chen, Yvonne Jie  and
      Cheng, Jinhua",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.901/",
	doi = "10.18653/v1/2024.emnlp-main.901",
	pages = "16112--16126",
	abstract = "The task of text-to-table receives widespread attention, yet its importance and difficulty are underestimated. Existing works use simple datasets similar to table-to-text tasks and employ methods that ignore domain structures. As a bridge between raw text and statistical analysis, the text-to-table task often deals with complex semi-structured texts that refer to specific domain topics in the real world with entities and events, especially from those of social sciences. In this paper, we analyze the limitations of benchmark datasets and methods used in the text-to-table literature and redefine the text-to-table task to improve its compatibility with long text-processing tasks. Based on this redefinition, we propose a new dataset called CPL (Chinese Private Lending), which consists of judgments from China and is derived from a real-world legal academic project. We further propose TKGT (Text-KG-Table), a two stages domain-aware pipeline, which firstly generates domain knowledge graphs (KGs) classes semi-automatically from raw text with the mixed information extraction (Mixed-IE) method, then adopts the hybrid retrieval augmented generation (Hybird-RAG) method to transform it to tables for downstream needs under the guidance of KGs classes. Experiment results show that TKGT achieves state-of-the-art (SOTA) performance on both traditional datasets and the CPL. Our data and main code are available at https://github.com/jiangpw41/TKGT."
}

@inproceedings{conia-etal-2024-towards,
    title = "Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs",
	author = "Conia, Simone  and
      Lee, Daniel  and
      Li, Min  and
      Minhas, Umar Farooq  and
      Potdar, Saloni  and
      Li, Yunyao",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.914/",
	doi = "10.18653/v1/2024.emnlp-main.914",
	pages = "16343--16360",
	abstract = "Translating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by transcreation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129{\%} and 62{\%} relative improvement compared to NLLB-200 and GPT-4, respectively."
}

@inproceedings{zhang-etal-2024-atap,
    title = "{ATAP}: Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models",
	author = "Zhang, Fu  and
      Ding, Yifan  and
      Cheng, Jingwei",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.919/",
	doi = "10.18653/v1/2024.emnlp-main.919",
	pages = "16456--16472",
	abstract = "The mission of commonsense knowledge graph completion (CKGC) is to infer missing facts from known commonsense knowledge. CKGC methods can be roughly divided into two categories: triple-based methods and text-based methods. Due to the imbalanced distribution of entities and limited structural information, triple-based methods struggle with long-tail entities. Text-based methods alleviate this issue, but require extensive training and fine-tuning of language models, which reduces efficiency. To alleviate these problems, we propose ATAP, the first CKGC framework that utilizes automatically generated continuous prompt templates combined with pre-trained language models (PLMs). Moreover, ATAP uses a carefully designed new prompt template training strategy, guiding PLMs to generate optimal prompt templates for CKGC tasks. Combining the rich knowledge of PLMs with the template automatic augmentation strategy, ATAP effectively mitigates the long-tail problem and enhances CKGC performance. Results on benchmark datasets show that ATAP achieves state-of-the-art performance overall."
}

@inproceedings{lin-etal-2024-varying,
    title = "Varying Sentence Representations via Condition-Specified Routers",
	author = "Lin, Ziyong  and
      Wang, Quansen  and
      Jia, Zixia  and
      Zheng, Zilong",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.963/",
	doi = "10.18653/v1/2024.emnlp-main.963",
	pages = "17390--17401",
	abstract = "Semantic similarity between two sentences is inherently subjective and can vary significantly based on the specific aspects emphasized. Consequently, traditional sentence encoders must be capable of generating conditioned sentence representations that account for diverse conditions or aspects. In this paper, we propose a novel yet efficient framework based on transformer-style language models that facilitates advanced conditioned sentence representation while maintaining model parameters and computational efficiency. Empirical evaluations on the Conditional Semantic Textual Similarity and Knowledge Graph Completion tasks demonstrate the superiority of our proposed framework."
}

@inproceedings{hwang-etal-2024-mp2d,
    title = "{MP}2{D}: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs",
	author = "Hwang, Yerin  and
      Kim, Yongil  and
      Jang, Yunah  and
      Bang, Jeesoo  and
      Bae, Hyunkyung  and
      Jung, Kyomin",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.979/",
	doi = "10.18653/v1/2024.emnlp-main.979",
	pages = "17682--17702",
	abstract = "Despite advancements in on-topic dialogue systems, effectively managing topic shifts within dialogues remains a persistent challenge, largely attributed to the limited availability of training datasets. To address this issue, we propose Multi-Passage to Dialogue (MP2D), a data generation framework that automatically creates conversational question-answering datasets with natural topic transitions. By leveraging the relationships between entities in a knowledge graph, MP2D maps the flow of topics within a dialogue, effectively mirroring the dynamics of human conversation. It retrieves relevant passages corresponding to the topics and transforms them into dialogues through the passage-to-dialogue method. Through quantitative and qualitative experiments, we demonstrate MP2D`s efficacy in generating dialogue with natural topic shifts. Furthermore, this study introduces a novel benchmark for topic shift dialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large Language Models (LLMs) struggle to handle topic shifts in dialogue effectively, and we showcase the performance improvements of models trained on datasets generated by MP2D across diverse topic shift dialogue tasks."
}

@inproceedings{xu-etal-2024-generate,
    title = "Generate-on-Graph: Treat {LLM} as both Agent and {KG} for Incomplete Knowledge Graph Question Answering",
	author = "Xu, Yao  and
      He, Shizhu  and
      Chen, Jiabei  and
      Wang, Zihao  and
      Song, Yangqiu  and
      Tong, Hanghang  and
      Liu, Guang  and
      Zhao, Jun  and
      Liu, Kang",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.1023/",
	doi = "10.18653/v1/2024.emnlp-main.1023",
	pages = "18410--18430",
	abstract = "To address the issues of insufficient knowledge and hallucination in Large Language Models (LLMs), numerous studies have explored integrating LLMs with Knowledge Graphs (KGs). However, these methods are typically evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where all factual triples required for each question are entirely covered by the given KG. In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs. In fact, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate these real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the factual triples for each question, and construct corresponding datasets. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG), which can generate new factual triples while exploring KGs. Specifically, GoG performs reasoning through a Thinking-Searching-Generating framework, which treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets demonstrate that our GoG outperforms all previous methods."
}

@inproceedings{zhai-etal-2024-towards,
    title = "Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering",
	author = "Zhai, Weihe  and
      Zubiaga, Arkaitz  and
      Liu, Bingquan  and
      Sun, Chengjie  and
      Zhao, Yalong",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.1052/",
	doi = "10.18653/v1/2024.emnlp-main.1052",
	pages = "18920--18930",
	abstract = "The fusion of language models (LMs) and knowledge graphs (KGs) is widely used in commonsense question answering, but generating faithful explanations remains challenging. Current methods often overlook path decoding faithfulness, leading to divergence between graph encoder outputs and model predictions. We identify confounding effects and LM-KG misalignment as key factors causing spurious explanations. To address this, we introduce the LM-KG Fidelity metric to assess KG representation reliability and propose the LM-KG Distribution-aware Alignment (LKDA) algorithm to improve explanation faithfulness. Without ground truth, we evaluate KG explanations using the proposed Fidelity-Sparsity Trade-off Curve. Experiments on CommonsenseQA and OpenBookQA show that LKDA significantly enhances explanation fidelity and model performance, highlighting the need to address distributional misalignment for reliable commonsense reasoning."
}

@inproceedings{park-etal-2024-generative,
    title = "Generative Subgraph Retrieval for Knowledge Graph{--}Grounded Dialog Generation",
	author = "Park, Jinyoung  and
      Joo, Minseok  and
      Kim, Joo-Kyung  and
      Kim, Hyunwoo J.",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.1179/",
	doi = "10.18653/v1/2024.emnlp-main.1179",
	pages = "21167--21182",
	abstract = "Knowledge graph{--}grounded dialog generation requires retrieving a dialog-relevant subgraph from the given knowledge base graph and integrating it with the dialog history. Previous works typically represent the graph using an external encoder, such as graph neural networks, and retrieve relevant triplets based on the similarity between single-vector representations of triplets and the dialog history. However, these external encoders fail to leverage the rich knowledge of pretrained language models, and the retrieval process is also suboptimal due to the information bottleneck caused by the single-vector abstraction of the dialog history. In this work, we propose Dialog generation with Generative Subgraph Retrieval (DialogGSR), which retrieves relevant knowledge subgraphs by directly generating their token sequences on top of language models. For effective generative subgraph retrieval, we introduce two key methods: (i) structure-aware knowledge graph linearization with self-supervised graph-specific tokens and (ii) graph-constrained decoding utilizing graph structural proximity-based entity informativeness scores for valid and relevant generative retrieval. DialogGSR achieves state-of-the-art performance in knowledge graph{--}grounded dialog generation, as demonstrated on OpenDialKG and KOMODIS datasets."
}

@inproceedings{zhang-etal-2024-knowledge-graph,
    title = "Knowledge Graph Enhanced Large Language Model Editing",
	author = "Zhang, Mengqi  and
      Ye, Xiaotian  and
      Liu, Qiang  and
      Ren, Pengjie  and
      Wu, Shu  and
      Chen, Zhumin",
	editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.1261/",
	doi = "10.18653/v1/2024.emnlp-main.1261",
	pages = "22647--22662",
	abstract = "Large language models (LLMs) are pivotal in advancing natural language processing (NLP) tasks, yet their efficacy is hampered by inaccuracies and outdated knowledge. Model editing emerges as a promising solution to address these challenges. However, existing editing methods struggle to track and incorporate changes in knowledge associated with edits, which limits the generalization ability of post-edit LLMs in processing edited knowledge. To tackle these problems, we propose a novel model editing method that leverages knowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we first utilize a knowledge graph augmentation module to uncover associated knowledge that has changed due to editing, obtaining its internal representations within LLMs. This approach allows knowledge alterations within LLMs to be reflected through an external graph structure. Subsequently, we design a graph-based knowledge edit module to integrate structured knowledge into the model editing. This ensures that the updated parameters reflect not only the modifications of the edited knowledge but also the changes in other associated knowledge resulting from the editing process. Comprehensive experiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME significantly improves the generalization capabilities of post-edit LLMs in employing edited knowledge."
}

@inproceedings{banerjee-etal-2024-context,
    title = "Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context",
	author = "Banerjee, Somnath  and
      Sahoo, Amruit  and
      Layek, Sayan  and
      Dutta, Avik  and
      Hazra, Rima  and
      Mukherjee, Animesh",
	editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
	month = nov,
	year = "2024",
	address = "Miami, Florida, US",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-industry.23/",
	doi = "10.18653/v1/2024.emnlp-industry.23",
	pages = "290--302",
	abstract = "This paper introduces a novel framework that combines graph-driven context retrieval in conjunction to knowledge graphs based enhancement, honing the proficiency of LLMs, especially in domain specific community question answering platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on various LLMs with different parameter sizes to evaluate their ability to ground knowledge and determine factual accuracy in answers to open-ended questions. Our methodology GraphContextGen consistently outperforms dominant text-based retrieval systems, demonstrating its robustness and adaptability to a larger number of use cases. This advancement highlights the importance of pairing context rich data retrieval with LLMs, offering a renewed approach to knowledge sourcing and generation in AI systems. We also show that, due to rich contextual data retrieval, the crucial entities, along with the generated answer, remain factually coherent with the gold answer. We shall release the source code and datasets upon acceptance."
}

@inproceedings{chan-etal-2024-adapting,
    title = "Adapting {LLM}s for Structured Natural Language {API} Integration",
	author = "Chan, Robin  and
      Mirylenka, Katsiaryna  and
      Gschwind, Thomas  and
      Miksovic, Christoph  and
      Scotton, Paolo  and
      Toniato, Enrico  and
      Labbi, Abdel",
	editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
	month = nov,
	year = "2024",
	address = "Miami, Florida, US",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-industry.74/",
	doi = "10.18653/v1/2024.emnlp-industry.74",
	pages = "991--1000",
	abstract = "API integration is crucial for enterprise systems, as it enables seamless interaction between applications within workflows. However, the diversity and complexity of the API landscape present significant challenges in combining API calls based on user intent.Existing methods rely on named entity recognition (NER) and knowledge graphs, but struggle to generate more complex control flow structures, such as conditionals and loops.We propose a novel framework that leverages the success of large language models (LLMs) in code generation to integrate APIs based on natural language input. Our approach involves fine-tuning an LLM using automatically generated API flows derived from OpenAPI specifications.We further evaluate the effectiveness of enforcing the syntax and schema adherence through constrained decoding.To enable systematic comparison, we introduce targeted test suites to assess the generalization capabilities of these approaches and their ability to retain structured knowledge.Our findings show that LLMs fine-tuned on OpenAPI specifications can (a) learn structural API constraints implicitly during training, and (b) achieve significant improvements in both in-distribution and out-of-distribution performance over NER and retrieval-augmented generation (RAG)-based approaches."
}

@inproceedings{pradeep-etal-2024-convkgyarn,
    title = "{C}onv{KGY}arn: Spinning Configurable and Scalable Conversational Knowledge Graph {QA} Datasets with Large Language Models",
	author = "Pradeep, Ronak  and
      Lee, Daniel  and
      Mousavi, Ali  and
      Pound, Jeffrey  and
      Sang, Yisi  and
      Lin, Jimmy  and
      Ilyas, Ihab  and
      Potdar, Saloni  and
      Arefiyan, Mostafa  and
      Li, Yunyao",
	editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
	month = nov,
	year = "2024",
	address = "Miami, Florida, US",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-industry.89/",
	doi = "10.18653/v1/2024.emnlp-industry.89",
	pages = "1176--1206",
	abstract = "The rapid evolution of Large Language Models (LLMs) and conversational assistants necessitates dynamic, scalable, and configurable conversational datasets for training and evaluation.These datasets must accommodate diverse user interaction modes, including text and voice, each presenting unique modeling challenges. Knowledge Graphs (KGs), with their structured and evolving nature, offer an ideal foundation for current and precise knowledge.Although human-curated KG-based conversational datasets exist, they struggle to keep pace with the rapidly changing user information needs.We present ConvKGYarn, a scalable method for generating up-to-date and configurable conversational KGQA datasets. Qualitative psychometric analyses demonstrate ConvKGYarn`s effectiveness in producing high-quality data comparable to popular conversational KGQA datasets across various metrics.ConvKGYarn excels in adhering to human interaction configurations and operating at a significantly larger scale.We showcase ConvKGYarn`s utility by testing LLMs on diverse conversations {---} exploring model behavior on conversational KGQA sets with different configurations grounded in the same KG fact set.Our results highlight the ability of ConvKGYarn to improve KGQA foundations and evaluate parametric knowledge of LLMs, thus offering a robust solution to the constantly evolving landscape of conversational assistants."
}

@inproceedings{chen-etal-2024-knowledge,
    title = "Knowledge-augmented Financial Market Analysis and Report Generation",
	author = "Chen, Yuemin  and
      Wu, Feifan  and
      Wang, Jingwei  and
      Qian, Hao  and
      Liu, Ziqi  and
      Zhang, Zhiqiang  and
      Zhou, Jun  and
      Wang, Meng",
	editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
	month = nov,
	year = "2024",
	address = "Miami, Florida, US",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-industry.90/",
	doi = "10.18653/v1/2024.emnlp-industry.90",
	pages = "1207--1217",
	abstract = "Crafting a convincing financial market analysis report necessitates a wealth of market information and the expertise of financial analysts, posing a highly challenging task. While large language models (LLMs) have enabled the automated generation of financial market analysis text, they still face issues such as hallucinations, errors in financial knowledge, and insufficient capability to reason about complex financial problems, which limits the quality of the generation. To tackle these shortcomings, we propose a novel task and a retrieval-augmented framework grounded in a financial knowledge graph (FKG). The proposed framework is compatible with commonly used instruction-tuning methods. Experiments demonstrate that our framework, coupled with a small-scale language model fine-tuned with instructions, can significantly enhance the logical consistency and quality of the generated analysis texts, outperforming both large-scale language models and other retrieval-augmented baselines."
}

@inproceedings{wu-etal-2024-kmatrix,
    title = "{KM}atrix: A Flexible Heterogeneous Knowledge Enhancement Toolkit for Large Language Model",
	author = "Wu, Shun  and
      Wu, Di  and
      Luo, Kun  and
      Zhang, XueYou  and
      Zhao, Jun  and
      Liu, Kang",
	editor = "Hernandez Farias, Delia Irazu  and
      Hope, Tom  and
      Li, Manling",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-demo.29/",
	doi = "10.18653/v1/2024.emnlp-demo.29",
	pages = "280--290",
	abstract = "Knowledge-Enhanced Large Language Models (K-LLMs) system enhances Large Language Models (LLMs) abilities using external knowledge. Existing K-LLMs toolkits mainly focus on free-textual knowledge, lacking support for heterogeneous knowledge like tables and knowledge graphs, and fall short in comprehensive datasets, models, and user-friendly experience. To address this gap, we introduce KMatrix: a flexible heterogeneous knowledge enhancement toolkit for LLMs including verbalizing-retrieval and parsing-query methods. Our modularity and control-logic flow diagram design flexibly supports the entire lifecycle of various complex K-LLMs systems, including training, evaluation, and deployment. To assist K-LLMs system research, a series of related knowledge, datasets, and models are integrated into our toolkit, along with performance analyses of K-LLMs systems enhanced by different types of knowledge. Using our toolkit, developers can rapidly build, evaluate, and deploy their own K-LLMs systems."
}

@inproceedings{schneider-etal-2024-comparative,
    title = "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation",
	author = "Schneider, Phillip  and
      Klettner, Manuel  and
      Simperl, Elena  and
      Matthes, Florian",
	editor = "Graham, Yvette  and
      Purver, Matthew",
	booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers)",
	month = mar,
	year = "2024",
	address = "St. Julian{'}s, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.eacl-short.31/",
	pages = "358--367",
	abstract = "Generating natural language text from graph-structured data is essential for conversational information seeking. Semantic triples derived from knowledge graphs can serve as a valuable source for grounding responses from conversational agents by providing a factual basis for the information they communicate. This is especially relevant in the context of large language models, which offer great potential for conversational interaction but are prone to hallucinating, omitting, or producing conflicting information. In this study, we conduct an empirical analysis of conversational large language models in generating natural language text from semantic triples. We compare four large language models of varying sizes with different prompting techniques. Through a series of benchmark experiments on the WebNLG dataset, we analyze the models' performance and identify the most common issues in the generated predictions. Our findings show that the capabilities of large language models in triple verbalization can be significantly improved through few-shot prompting, post-processing, and efficient fine-tuning techniques, particularly for smaller models that exhibit lower zero-shot performance."
}

@inproceedings{ravaut-etal-2024-parameter,
    title = "Parameter-Efficient Conversational Recommender System as a Language Processing Task",
	author = "Ravaut, Mathieu  and
      Zhang, Hao  and
      Xu, Lu  and
      Sun, Aixin  and
      Liu, Yong",
	editor = "Graham, Yvette  and
      Purver, Matthew",
	booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = mar,
	year = "2024",
	address = "St. Julian{'}s, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.eacl-long.9/",
	pages = "152--165",
	abstract = "Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumber-some training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on two benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of PECRS on recommendation and conversation. Our code is available at: https://github.com/Ravoxsg/efficient{\_}unified{\_}crs."
}

@inproceedings{ushio-etal-2024-relentless,
    title = "A {R}el{E}nt{L}ess Benchmark for Modelling Graded Relations between Named Entities",
	author = "Ushio, Asahi  and
      Camacho-Collados, Jose  and
      Schockaert, Steven",
	editor = "Graham, Yvette  and
      Purver, Matthew",
	booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = mar,
	year = "2024",
	address = "St. Julian{'}s, Malta",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.eacl-long.152/",
	pages = "2473--2486",
	abstract = "Relations such as {\textquotedblleft}is influenced by{\textquotedblright}, {\textquotedblleft}is known for{\textquotedblright} or {\textquotedblleft}is a competitor of{\textquotedblright} are inherently graded: we can rank entity pairs based on how well they satisfy these relations, but it is hard to draw a line between those pairs that satisfy them and those that do not. Such graded relations play a central role in many applications, yet they are typically not covered by existing Knowledge Graphs. In this paper, we consider the possibility of using Large Language Models (LLMs) to fill this gap. To this end, we introduce a new benchmark, in which entity pairs have to be ranked according to how much they satisfy a given graded relation. The task is formulated as a few-shot ranking problem, where models only have access to a description of the relation and five prototypical instances. We use the proposed benchmark to evaluate state-of-the-art relation embedding strategies as well as several publicly available LLMs and closed conversational models such as GPT-4. We find that smaller language models struggle to outperform a naive baseline. Overall, the best results are obtained with the 11B parameter Flan-T5 model and the 13B parameter OPT model, where further increasing the model size does not seem to be beneficial. For all models, a clear gap with human performance remains."
}

@inproceedings{shichman-etal-2024-propbank,
    title = "{P}rop{B}ank-Powered Data Creation: Utilizing Sense-Role Labelling to Generate Disaster Scenario Data",
	author = "Shichman, Mollie Frances  and
      Bonial, Claire  and
      Hudson, Taylor A.  and
      Blodgett, Austin  and
      Ferraro, Francis  and
      Rudinger, Rachel",
	editor = "Bonial, Claire  and
      Bonn, Julia  and
      Hwang, Jena D.",
	booktitle = "Proceedings of the Fifth International Workshop on Designing Meaning Representations @ LREC-COLING 2024",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.dmr-1.1/",
	pages = "1--10",
	abstract = "For human-robot dialogue in a search-and-rescue scenario, a strong knowledge of the conditions and objects a robot will face is essential for effective interpretation of natural language instructions. In order to utilize the power of large language models without overwhelming the limited storage capacity of a robot, we propose PropBank-Powered Data Creation. PropBank-Powered Data Creation is an expert-in-the-loop data generation pipeline which creates training data for disaster-specific language models. We leverage semantic role labeling and Rich Event Ontology resources to efficiently develop seed sentences for fine-tuning a smaller, targeted model that could operate onboard a robot for disaster relief. We developed 32 sentence templates, which we used to make 2 seed datasets of 175 instructions for earthquake search and rescue and train derailment response. We further leverage our seed datasets as evaluation data to test our baseline fine-tuned models."
}

@inproceedings{giarelis-etal-2024-unified,
    title = "A Unified {LLM}-{KG} Framework to Assist Fact-Checking in Public Deliberation",
	author = "Giarelis, Nikolaos  and
      Mastrokostas, Charalampos  and
      Karacapilidis, Nikos",
	editor = "Hautli-Janisz, Annette  and
      Lapesa, Gabriella  and
      Anastasiou, Lucas  and
      Gold, Valentin  and
      Liddo, Anna De  and
      Reed, Chris",
	booktitle = "Proceedings of the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.delite-1.2/",
	pages = "13--19",
	abstract = "Fact-checking plays a crucial role in public deliberation by promoting transparency, accuracy, credibility, and accountability. Aiming to augment the efficiency and adoption of current public deliberation platforms, which mostly rely on the abilities of participants to meaningfully process and interpret the associated content, this paper explores the combination of deep learning and symbolic reasoning. Specifically, it proposes a framework that unifies the capabilities of Large Language Models (LLMs) and Knowledge Graphs (KGs), and reports on an experimental evaluation. This evaluation is conducted through a questionnaire asking users to assess a baseline LLM against the proposed framework, using a series of fact-checking metrics, namely readability, coverage, non-redundancy, and quality. The experimentation results are promising and confirm the potential of combining the capabilities of these two technologies in the context of public deliberation and digital democracy."
}

@inproceedings{kalra-etal-2024-hypa,
    title = "{H}y{PA}-{RAG}: A Hybrid Parameter Adaptive Retrieval-Augmented Generation System for {AI} Legal and Policy Applications",
	author = "Kalra, Rishi  and
      Wu, Zekun  and
      Gulley, Ayesha  and
      Hilliard, Airlie  and
      Guan, Xin  and
      Koshiyama, Adriano  and
      Treleaven, Philip Colin",
	editor = "Kumar, Sachin  and
      Balachandran, Vidhisha  and
      Park, Chan Young  and
      Shi, Weijia  and
      Hayati, Shirley Anugrah  and
      Tsvetkov, Yulia  and
      Smith, Noah  and
      Hajishirzi, Hannaneh  and
      Kang, Dongyeop  and
      Jurgens, David",
	booktitle = "Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for a Domain, Application, Group, or Individual (CustomNLP4U)",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.customnlp4u-1.18/",
	doi = "10.18653/v1/2024.customnlp4u-1.18",
	pages = "237--256",
	abstract = "While Large Language Models (LLMs) excel in text generation and question-answering, their effectiveness in AI legal and policy applications is limited by outdated knowledge, hallucinations, and inadequate reasoning in complex contexts. Retrieval-Augmented Generation (RAG) systems improve response accuracy by integrating external knowledge but struggle with retrieval errors, poor context integration, and high costs, particularly in interpreting AI legal texts. This paper introduces a Hybrid Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy, exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity classifier for adaptive parameter tuning, a hybrid retrieval strategy combining dense, sparse, and knowledge graph methods, and an evaluation framework with specific question types and metrics. By dynamically adjusting parameters, HyPA-RAG significantly improves retrieval accuracy and response fidelity. Testing on LL144 shows enhanced correctness, faithfulness, and contextual precision, addressing the need for adaptable NLP systems in complex, high-stakes AI legal and policy applications."
}

@inproceedings{de-deyne-etal-2024-gpt,
    title = "Can {GPT}-4 Recover Latent Semantic Relational Information from Word Associations? A Detailed Analysis of Agreement with Human-annotated Semantic Ontologies.",
	author = "De Deyne, Simon  and
      Liu, Chunhua  and
      Frermann, Lea",
	editor = "Zock, Michael  and
      Chersoni, Emmanuele  and
      Hsu, Yu-Yin  and
      de Deyne, Simon",
	booktitle = "Proceedings of the Workshop on Cognitive Aspects of the Lexicon @ LREC-COLING 2024",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.cogalex-1.8/",
	pages = "68--78",
	abstract = "Word associations, i.e., spontaneous responses to a cue word, provide not only a window into the human mental lexicon but have also been shown to be a repository of common-sense knowledge and can underpin efforts in lexicography and the construction of dictionaries. Especially the latter tasks require knowledge about the relations underlying the associations (e.g., Taxonomic vs. Situational); however, to date, there is neither an established ontology of relations nor an effective labelling paradigm. Here, we test GPT-4`s ability to infer semantic relations for human-produced word associations. We use four human-labelled data sets of word associations and semantic features, with differing relation inventories and various levels of annotator agreement. We directly prompt GPT-4 with detailed relation definitions without further fine-tuning or training. Our results show that while GPT-4 provided a good account of higher-level classifications (e.g. Taxonomic vs Situational), prompting instructions alone cannot obtain similar performance for detailed classifications (e.g. superordinate, subordinate or coordinate relations) despite high agreement among human annotators. This suggests that latent relations can at least be partially recovered from word associations and highlights ways in which LLMs could be improved and human annotation protocols could adapted to reduce coding ambiguity."
}

@inproceedings{valiev-tutubalina-2024-hse,
    title = "{HSE} {NLP} Team at {MEDIQA}-{CORR} 2024 Task: In-Prompt Ensemble with Entities and Knowledge Graph for Medical Error Correction",
	author = "Valiev, Airat  and
      Tutubalina, Elena",
	editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Bitterman, Danielle",
	booktitle = "Proceedings of the 6th Clinical Natural Language Processing Workshop",
	month = jun,
	year = "2024",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.clinicalnlp-1.47/",
	doi = "10.18653/v1/2024.clinicalnlp-1.47",
	pages = "470--482",
	abstract = "This paper presents our LLM-based system designed for the MEDIQA-CORR @ NAACL-ClinicalNLP 2024 Shared Task 3, focusing on medical error detection and correction in medical records. Our approach consists of three key components: entity extraction, prompt engineering, and ensemble. First, we automatically extract biomedical entities such as therapies, diagnoses, and biological species. Next, we explore few-shot learning techniques and incorporate graph information from the MeSH database for the identified entities. Finally, we investigate two methods for ensembling: (i) combining the predictions of three previous LLMs using an AND strategy within a prompt and (ii) integrating the previous predictions into the prompt as separate {\textquoteleft}expert' solutions, accompanied by trust scores representing their performance. The latter system ranked second with a BERTScore score of 0.8059 and third with an aggregated score of 0.7806 out of the 15 teams' solutions in the shared task."
}

@inproceedings{usmanova-usbeck-2024-structuring,
    title = "Structuring Sustainability Reports for Environmental Standards with {LLM}s guided by Ontology",
	author = "Usmanova, Aida  and
      Usbeck, Ricardo",
	editor = "Stammbach, Dominik  and
      Ni, Jingwei  and
      Schimanski, Tobias  and
      Dutia, Kalyan  and
      Singh, Alok  and
      Bingler, Julia  and
      Christiaen, Christophe  and
      Kushwaha, Neetu  and
      Muccione, Veruska  and
      A. Vaghefi, Saeid  and
      Leippold, Markus",
	booktitle = "Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.climatenlp-1.13/",
	doi = "10.18653/v1/2024.climatenlp-1.13",
	pages = "168--177",
	abstract = "Following the introduction of the European Sustainability Reporting Standard (ESRS), companies will have to adapt to a new policy and provide mandatory sustainability reports. However, implementing such reports entails a challenge, such as the comprehension of a large number of textual information from various sources. This task can be accelerated by employing Large Language Models (LLMs) and ontologies to effectively model the domain knowledge. In this study, we extended an existing ontology to model ESRS Topical Standard for disclosure. The developed ontology would enable automated reasoning over the data and assist in constructing Knowledge Graphs (KGs). Moreover, the proposed ontology extension would also help to identify gaps in companies' sustainability reports with regard to the ESRS requirements.Additionally, we extracted knowledge from corporate sustainability reports via LLMs guided with a proposed ontology and developed their KG representation."
}

@inproceedings{penkov-2024-mitigating,
    title = "Mitigating Hallucinations in Large Language Models via Semantic Enrichment of Prompts: Insights from {B}io{BERT} and Ontological Integration",
	author = "Penkov, Stanislav",
	booktitle = "Proceedings of the Sixth International Conference on Computational Linguistics in Bulgaria (CLIB 2024)",
	month = sep,
	year = "2024",
	address = "Sofia, Bulgaria",
	publisher = "Department of Computational Linguistics, Institute for Bulgarian Language, Bulgarian Academy of Sciences",
	url = "https://aclanthology.org/2024.clib-1.30/",
	pages = "272--276",
	abstract = "The advent of Large Language Models (LLMs) has been transformative for natural language processing, yet their tendency to produce {\textquotedblleft}hallucinations{\textquotedblright}{---}outputs that are factually incorrect or entirely fabricated{---} remains a significant hurdle. This paper introduces a proactive methodology for reducing hallucinations by strategically enriching LLM prompts. This involves identifying key entities and contextual cues from varied domains and integrating this information into the LLM prompts to guide the model towards more accurate and relevant responses. Leveraging examples from BioBERT for biomedical entity recognition and ChEBI for chemical ontology, we illustrate a broader approach that encompasses semantic prompt enrichment as a versatile tool for enhancing LLM output accuracy. By examining the potential of semantic and ontological enrichment in diverse contexts, we aim to present a scalable strategy for improving the reliability of AI-generated content, thereby contributing to the ongoing efforts to refine LLMs for a wide range of applications."
}

@inproceedings{knez-zitnik-2024-towards,
    title = "Towards Using Automatically Enhanced Knowledge Graphs to Aid Temporal Relation Extraction",
	author = "Knez, Timotej  and
      {\v{Z}}itnik, Slavko",
	editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Thompson, Paul  and
      Ondov, Brian",
	booktitle = "Proceedings of the First Workshop on Patient-Oriented Language Processing (CL4Health) @ LREC-COLING 2024",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.cl4health-1.16/",
	pages = "131--136",
	abstract = "Temporal relation extraction in medical document analysis is crucial for understanding patient histories and treatment outcomes. This paper introduces a novel approach leveraging a bimodal model integrating textual content and a knowledge graph, to enhance temporal relation extraction. The paper presents ongoing research in constructing an optimal knowledge graph by augmenting PrimeKG with dynamically expanded information using a language model-generated knowledge graph, and further personalize the information with patient-specific graphs tailored for relation prediction. The pipeline for constructing this enriched knowledge graph is detailed, aiming to improve the capabilities of temporal relation extraction models. The preliminary results show that adding a simple knowledge graph to the temporal relation extraction model can significantly increase the performance, achieving new state-of-the-art results. While the research in using enhanced knowledge graphs is still ongoing, this paper lays the groundwork for leveraging common knowledge to advance temporal relation extraction in medical contexts. This approach holds promise for enhancing the understanding of patient histories and treatment outcomes, potentially leading to improved healthcare decision-making and patient care."
}

@inproceedings{yang-etal-2024-kg,
    title = "{KG}-Rank: Enhancing Large Language Models for Medical {QA} with Knowledge Graphs and Ranking Techniques",
	author = "Yang, Rui  and
      Liu, Haoran  and
      Marrese-Taylor, Edison  and
      Zeng, Qingcheng  and
      Ke, Yuhe  and
      Li, Wanxin  and
      Cheng, Lechao  and
      Chen, Qingyu  and
      Caverlee, James  and
      Matsuo, Yutaka  and
      Li, Irene",
	editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Miwa, Makoto  and
      Roberts, Kirk  and
      Tsujii, Junichi",
	booktitle = "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.bionlp-1.13/",
	doi = "10.18653/v1/2024.bionlp-1.13",
	pages = "155--166",
	abstract = "Large Language Models (LLMs) have significantly advanced healthcare innovation on generation capabilities. However, their application in real clinical settings is challenging due to potential deviations from medical facts and inherent biases. In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain. Specifically, upon receiving a question, we initially retrieve triplets from a medical KG to gather factual information. Subsequently, we innovatively apply ranking methods to refine the ordering of these triplets, aiming to yield more precise answers. To the best of our knowledge, KG-Rank is the first application of ranking models combined with KG in medical QA specifically for generating long answers. Evaluation of four selected medical QA datasets shows that KG-Rank achieves an improvement of over 18{\%} in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it realizes a 14{\%} improvement in ROUGE-L, showing the effectiveness and potential of KG-Rank."
}

@inproceedings{arsenyan-etal-2024-large,
    title = "Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from {EMR} notes",
	author = "Arsenyan, Vahan  and
      Bughdaryan, Spartak  and
      Shaya, Fadi  and
      Small, Kent Wilson  and
      Shahnazaryan, Davit",
	editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Miwa, Makoto  and
      Roberts, Kirk  and
      Tsujii, Junichi",
	booktitle = "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.bionlp-1.23/",
	doi = "10.18653/v1/2024.bionlp-1.23",
	pages = "295--317",
	abstract = "The automatic construction of knowledge graphs (KGs) is an important research area in medicine, with far-reaching applications spanning drug discovery and clinical trial design. These applications hinge on the accurate identification of interactions among medical and biological entities. In this study, we propose an end-to-end machine learning solution based on large language models (LLMs) that utilize electronic medical record notes to construct KGs. The entities used in the KG construction process are diseases, factors, treatments, as well as manifestations that coexist with the patient while experiencing the disease. Given the critical need for high-quality performance in medical applications, we embark on a comprehensive assessment of 12 LLMs of various architectures, evaluating their performance and safety attributes. To gauge the quantitative efficacy of our approach by assessing both precision and recall, we manually annotate a dataset provided by the Macula and Retina Institute. We also assess the qualitative performance of LLMs, such as the ability to generate structured outputs or the tendency to hallucinate. The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation. Additionally, we provide guided prompt design to utilize such LLMs. The application of the proposed methodology is demonstrated on age-related macular degeneration."
}

@inproceedings{el-khettari-etal-2024-mention,
    title = "Mention-Agnostic Information Extraction for Ontological Annotation of Biomedical Articles",
	author = "El Khettari, Oumaima  and
      Nishida, Noriki  and
      Liu, Shanshan  and
      Munne, Rumana Ferdous  and
      Yamagata, Yuki  and
      Quiniou, Solen  and
      Chaffron, Samuel  and
      Matsumoto, Yuji",
	editor = "Demner-Fushman, Dina  and
      Ananiadou, Sophia  and
      Miwa, Makoto  and
      Roberts, Kirk  and
      Tsujii, Junichi",
	booktitle = "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.bionlp-1.37/",
	doi = "10.18653/v1/2024.bionlp-1.37",
	pages = "457--473",
	abstract = "Biomedical information extraction is crucial for advancing research, enhancing healthcare, and discovering treatments by efficiently analyzing extensive data. Given the extensive amount of biomedical data available, automated information extraction methods are necessary due to manual extraction`s labor-intensive, expertise-dependent, and costly nature. In this paper, we propose a novel two-stage system for information extraction where we annotate biomedical articles based on a specific ontology (HOIP). The major challenge is annotating relation between biomedical processes often not explicitly mentioned in text articles. Here, we first predict the candidate processes and then determine the relationships between these processes. The experimental results show promising outcomes in mention-agnostic process identification using Large Language Models (LLMs). In relation classification, BERT-based supervised models still outperform LLMs significantly. The end-to-end evaluation results suggest the difficulty of this task and room for improvement in both process identification and relation classification."
}

@inproceedings{qian-2024-automating,
    title = "Automating Idiom Translation with Cross-Lingual Natural Language Generation Grounded In Semantic Analyses Using Large Language Models",
	author = "Qian, Ming",
	editor = "Martindale, Marianna  and
      Campbell, Janice  and
      Savenkov, Konstantin  and
      Goel, Shivali",
	booktitle = "Proceedings of the 16th Conference of the Association for Machine Translation in the Americas (Volume 2: Presentations)",
	month = sep,
	year = "2024",
	address = "Chicago, USA",
	publisher = "Association for Machine Translation in the Americas",
	url = "https://aclanthology.org/2024.amta-presentations.7/",
	pages = "95--115",
	abstract = "Idioms exhibit varying degrees of semantic transparency, making their translation challenging. Cross-language differences in idiom usage and connotations add complexity. Using a large language modeling (LLM) approach, we automate Chinese-to-English idiom translation in three steps: (1) Semantic analysis of Chinese idioms using ontology or FrameNet to identify key concepts/relationships like action, purpose, outcome, and context. (2) Generation of multi-word English expressions reflecting these concepts. (3) Selection of the top English idiom candidate that closely matches the Chinese idiom`s meaning. Applied to examples like {\textquoteleft}破釜沉舟', {\textquoteleft}刀山火海', and {\textquoteleft}抛砖引玉', our method performs on par with human experts. The semantic reasoning approach enhances transparency in LLM decisions, simulating logical inferences over the semantic framework."
}

@inproceedings{du-etal-2024-bi,
    title = "Bi-Directional Multi-Granularity Generation Framework for Knowledge Graph-to-Text with Large Language Model",
	author = "Du, Haowei  and
      Li, Chen  and
      Zhang, Dinghao  and
      Zhao, Dongyan",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-short.14/",
	doi = "10.18653/v1/2024.acl-short.14",
	pages = "147--152",
	abstract = "The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. Existing methods generate the whole target text based on all KG triples at once and may incorporate incorrect KG triples for each sentence. To this end, we propose the bi-directional multi-granularity generation framework. Instead of generating the whole text at a time, we construct the sentence level generation based on the corresponding triples and generate the graph-level text as a result. Moreover, we design a backward relation extraction task to enhance the correctness of relational information. Our method achieves the new state-of-the-art in benchmark dataset WebNLG and further analysis shows the efficiency of different modules."
}

@inproceedings{wang-etal-2024-instructprotein,
    title = "{I}nstruct{P}rotein: Aligning Human and Protein Language via Knowledge Instruction",
	author = "Wang, Zeyuan  and
      Zhang, Qiang  and
      Ding, Keyan  and
      Qin, Ming  and
      Zhuang, Xiang  and
      Li, Xiaotong  and
      Chen, Huajun",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.62/",
	doi = "10.18653/v1/2024.acl-long.62",
	pages = "1114--1136",
	abstract = "Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing the annotation imbalance and the absence of instructional signals in the existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by a large margin."
}

@inproceedings{yuan-etal-2024-analogykb,
    title = "{ANALOGYKB}: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base",
	author = "Yuan, Siyu  and
      Chen, Jiangjie  and
      Sun, Changzhi  and
      Liang, Jiaqing  and
      Xiao, Yanghua  and
      Yang, Deqing",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.68/",
	doi = "10.18653/v1/2024.acl-long.68",
	pages = "1249--1265",
	abstract = "Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities. Resources of this paper can be found at https://github.com/siyuyuan/analogykb."
}

@inproceedings{dong-etal-2024-modality,
    title = "Modality-Aware Integration with Large Language Models for Knowledge-Based Visual Question Answering",
	author = "Dong, Junnan  and
      Zhang, Qinggang  and
      Zhou, Huachi  and
      Zha, Daochen  and
      Zheng, Pai  and
      Huang, Xiao",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.132/",
	doi = "10.18653/v1/2024.acl-long.132",
	pages = "2417--2429",
	abstract = "Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA ($\texttt{MAIL}$). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, $(i)$ we propose a two-stage prompting strategy with LLMs to densely embody the image into a *scene graph* with detailed visual features; $(ii)$ We construct a coupled *concept graph* by linking the mentioned entities with external facts. $(iii)$ A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments show the superiority of $\texttt{MAIL}$."
}

@inproceedings{cai-etal-2024-improving-event,
    title = "Improving Event Definition Following For Zero-Shot Event Detection",
	author = "Cai, Zefan  and
      Kung, Po-Nien  and
      Suvarna, Ashima  and
      Ma, Mingyu  and
      Bansal, Hritik  and
      Chang, Baobao  and
      Brantingham, P. Jeffrey  and
      Wang, Wei  and
      Peng, Nanyun",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.157/",
	doi = "10.18653/v1/2024.acl-long.157",
	pages = "2842--2863",
	abstract = "Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations.In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type.Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection."
}

@inproceedings{chen-etal-2024-timeline,
    title = "Timeline-based Sentence Decomposition with In Context Learning for Temporal Fact Extraction",
	author = "Chen, Jianhao  and
      Ouyang, Haoyuan  and
      Ren, Junyang  and
      Ding, Wentao  and
      Hu, Wei  and
      Qu, Yuzhong",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.187/",
	doi = "10.18653/v1/2024.acl-long.187",
	pages = "3415--3432",
	abstract = "Facts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets."
}

@inproceedings{chen-etal-2024-sac,
    title = "{SAC}-{KG}: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graph",
	author = "Chen, Hanzhu  and
      Shen, Xu  and
      Lv, Qitan  and
      Wang, Jie  and
      Ni, Xiaoqi  and
      Ye, Jieping",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.238/",
	doi = "10.18653/v1/2024.acl-long.238",
	pages = "4345--4360",
	abstract = "Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks across specialized domains, where the acquisition of precise and dependable knowledge is crucial. However, existing KG construction methods heavily rely on human intervention to attain qualified KGs, which severely hinders the practical applicability in real-world scenarios. To address this challenge, we propose a general KG construction framework, named **SAC-KG**, to exploit large language models (LLMs) as **S**killed **A**utomatic **C**onstructors for domain **K**nowledge **G**raph. SAC-KG effectively involves LLMs as domain experts to generate specialized and precise multi-level KGs. Specifically, SAC-KG consists of three components: Generator, Verifier, and Pruner. For a given entity, Generator produces its relations and tails from raw domain corpora, to construct a specialized single-level KG. Verifier and Pruner then work together to ensure precision by correcting generation errors and determining whether newly produced tails require further iteration for the next-level KG. Experiments demonstrate that SAC-KG automatically constructs a domain KG at the scale of over one million nodes and achieves a precision of 89.32{\%}, leading to a superior performance with over 20{\%} increase in precision rate compared to existing state-of-the-art methods for the KG construction task."
}

@inproceedings{plenz-frank-2024-graph,
    title = "Graph Language Models",
	author = "Plenz, Moritz  and
      Frank, Anette",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.245/",
	doi = "10.18653/v1/2024.acl-long.245",
	pages = "4477--4494",
	abstract = "While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs {--} which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure {--} but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM`s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility."
}

@inproceedings{jiang-etal-2024-unlocking,
    title = "Unlocking the Power of Large Language Models for Entity Alignment",
	author = "Jiang, Xuhui  and
      Shen, Yinghan  and
      Shi, Zhichao  and
      Xu, Chengjin  and
      Li, Wei  and
      Li, Zixuan  and
      Guo, Jian  and
      Shen, Huawei  and
      Wang, Yuanzhuo",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.408/",
	doi = "10.18653/v1/2024.acl-long.408",
	pages = "7566--7583",
	abstract = "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs' capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA`s superior performance, highlighting LLMs' potential in facilitating EA tasks.The source code is available at https://anonymous.4open.science/r/ChatEA/."
}

@inproceedings{agarwal-etal-2024-symkgqa,
    title = "{S}ym{KGQA}: Few-Shot Knowledge Graph Question Answering via Symbolic Program Generation and Execution",
	author = "Agarwal, Prerna  and
      Kumar, Nishant  and
      Bedathur, Srikanta",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.545/",
	doi = "10.18653/v1/2024.acl-long.545",
	pages = "10119--10140",
	abstract = "Semantic Parsing of natural language questions into their executable logical form (LF) has shown state-of-the-art (SOTA) performance for Knowledge Graph Question Answering (KGQA). However, these methods are not applicable for real-world applications, due to lack of KG-specific training data. Recent advances in the capabilities of Large Language Models (LLMs) has led towards generating low-level LFs such as SPARQL and S-Expression in a few-shot setting. Unfortunately, these methods: (1) are limited to the knowledge of underlying LLM about the LF, (2) performs inferior for the harder complex benchmarks such as KQA Pro, (3) suffers while grounding the generated LF to a specific Knowledge Graph. Recently, a new LF called KoPL has been introduced that explicitly models complex reasoning process step-by-step in a symbolic manner and has shown SOTA on KQA Pro in fully-supervised setting. Inspired by this, we propose SymKGQA framework that generates step-by-step Symbolic LF i.e., KoPL in a few-shot in-context learning setting using LLM. Our framework is not dependent on pre-trained information of LLM about KoPL. We further build a Retrieval-Augmented Generation based Question-Aware Contextual KoPL (QUACK) resolver to ground the generated LF. Our experiments with different LLMs and few-shot settings demonstrate that SymKGQA outperforms all other few-shot and even many of the fully-supervised KGQA approaches."
}

@inproceedings{wen-etal-2024-mindmap,
    title = "{M}ind{M}ap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
	author = "Wen, Yilin  and
      Wang, Zifeng  and
      Sun, Jimeng",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.558/",
	doi = "10.18653/v1/2024.acl-long.558",
	pages = "10370--10388",
	abstract = "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named MindMap, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question {\&} answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference."
}

@inproceedings{lee-etal-2024-multimodal,
    title = "Multimodal Reasoning with Multimodal Knowledge Graph",
	author = "Lee, Junlin  and
      Wang, Yequan  and
      Li, Jing  and
      Zhang, Min",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.579/",
	doi = "10.18653/v1/2024.acl-long.579",
	pages = "10767--10782",
	abstract = "Multimodal reasoning with large language models (LLMs) often suffers from hallucinations and the presence of deficient or outdated knowledge within LLMs. Some approaches have sought to mitigate these issues by employing textual knowledge graphs, but their singular modality of knowledge limits comprehensive cross-modal understanding. In this paper, we propose the Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) method, which leverages multimodal knowledge graphs (MMKGs) to learn rich and semantic knowledge across modalities, significantly enhancing the multimodal reasoning capabilities of LLMs. In particular, a relation graph attention network is utilized for encoding MMKGs and a cross-modal alignment module is designed for optimizing image-text alignment. A MMKG-grounded dataset is constructed to equip LLMs with initial expertise in multimodal reasoning through pretraining. Remarkably, MR-MKG achieves superior performance while training on only a small fraction of parameters, approximately 2.25{\%} of the LLM`s parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that our MR-MKG method outperforms previous state-of-the-art models."
}

@inproceedings{fang-etal-2024-complex,
    title = "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs",
	author = "Fang, Tianqing  and
      Chen, Zeming  and
      Song, Yangqiu  and
      Bosselut, Antoine",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.613/",
	doi = "10.18653/v1/2024.acl-long.613",
	pages = "11365--11384",
	abstract = "Event commonsense reasoning requires the ability to reason about the relationship between events, as well as infer implicit contextunderlying that relationship. However, data scarcity makes it challenging for language models to learn to generate commonsense infer-ences for contexts and questions involving interactions between complex events. To address this demand, we present COM2 (COMplexCOMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or theeffect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules andlarge language models into multiple-choice and text generation questions. Our experiments show that language models trained on COM2 exhibit significant improve ments in complex reasoning ability, resulting in enhanced zero-shot performance in both in-domain and out-of-domain tasks for question answering and generative commonsense reasoning, without expensive human annotations"
}

@inproceedings{markowitz-etal-2024-tree,
    title = "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
	author = "Markowitz, Elan  and
      Ramakrishna, Anil  and
      Dhamala, Jwala  and
      Mehrabi, Ninareh  and
      Peris, Charith  and
      Gupta, Rahul  and
      Chang, Kai-Wei  and
      Galstyan, Aram",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.665/",
	doi = "10.18653/v1/2024.acl-long.665",
	pages = "12302--12319",
	abstract = "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks. Code is available at https://github.com/amazon-science/tree-of-traversals"
}

@inproceedings{panda-etal-2024-holmes,
    title = "{HOLMES}: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using {LLM}s",
	author = "Panda, Pranoy  and
      Agarwal, Ankush  and
      Devaguptapu, Chaitanya  and
      Kaul, Manohar  and
      Ap, Prathosh",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.717/",
	doi = "10.18653/v1/2024.acl-long.717",
	pages = "13263--13282",
	abstract = "Given unstructured text, Large Language Models (LLMs) are adept at answering simple (single-hop) questions. However, as the complexity of the questions increase, the performance of LLMs degrade. We believe this is due to the overhead associated with understanding the complex question followed by filtering and aggregating unstructured information in the raw text. Recent methods try to reduce this burden by integrating structured knowledge triples into the raw text, aiming to provide a structured overview that simplifies information processing. However, this simplistic approach is query-agnostic and the extracted facts are ambiguous as they lack context. To address these drawbacks and to enable LLMs to answer complex (multi-hop) questions with ease, we propose to use a knowledge graph (KG) that is context-aware and is distilled to contain query-relevant information. The use of our compressed distilled KG as input to the LLM results in our method utilizing up to 67{\%} fewer tokens to represent the query relevant information present in the supporting documents, compared to the state-of-the-art (SoTA) method.Our experiments show consistent improvements over the SoTA across several metrics (EM, F1, BERTScore, and Human Eval) on two popular benchmark datasets (HotpotQA and MuSiQue)."
}

@inproceedings{dehghan-etal-2024-ewek,
    title = "{EWEK}-{QA} : Enhanced Web and Efficient Knowledge Graph Retrieval for Citation-based Question Answering Systems",
	author = "Dehghan, Mohammad  and
      Alomrani, Mohammad  and
      Bagga, Sunyam  and
      Alfonso-Hermelo, David  and
      Bibi, Khalil  and
      Ghaddar, Abbas  and
      Zhang, Yingxue  and
      Li, Xiaoguang  and
      Hao, Jianye  and
      Liu, Qun  and
      Lin, Jimmy  and
      Chen, Boxing  and
      Parthasarathi, Prasanna  and
      Biparva, Mahdi  and
      Rezagholizadeh, Mehdi",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.764/",
	doi = "10.18653/v1/2024.acl-long.764",
	pages = "14169--14187",
	abstract = "The emerging citation-based QA systems are gaining more attention especially in generative AI search applications. The importance of extracted knowledge provided to these systems is vital from both accuracy (completeness of information) and efficiency (extracting the information in a timely manner). In this regard, citation-based QA systems are suffering from two shortcomings. First, they usually rely only on web as a source of extracted knowledge and adding other external knowledge sources can hamper the efficiency of the system. Second, web-retrieved contents are usually obtained by some simple heuristics such as fixed length or breakpoints which might lead to splitting information into pieces. To mitigate these issues, we propose our enhanced web and efficient knowledge graph (KG) retrieval solution (EWEK-QA) to enrich the content of the extracted knowledge fed to the system. This has been done through designing an adaptive web retriever and incorporating KGs triples in an efficient manner. We demonstrate the effectiveness of over the open-source state-of-the-art (SoTA) web-based and KG baseline models using a comprehensive set of quantitative and human evaluation experiments. Our model is able to: first, improve the web-retriever baseline in terms of extracting more relevant passages ({\ensuremath{>}}20{\%}), the coverage of answer span ({\ensuremath{>}}25{\%}) and self containment ({\ensuremath{>}}35{\%}); second, obtain and integrate KG triples into its pipeline very efficiently (by avoiding any LLM calls) to outperform the web-only and KG-only SoTA baselines significantly in 7 quantitative QA tasks and our human evaluation."
}

@inproceedings{wu-etal-2024-coke,
    title = "{COKE}: A Cognitive Knowledge Graph for Machine Theory of Mind",
	author = "Wu, Jincenzi  and
      Chen, Zhuang  and
      Deng, Jiawen  and
      Sabour, Sahand  and
      Meng, Helen  and
      Huang, Minlie",
	editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-long.848/",
	doi = "10.18653/v1/2024.acl-long.848",
	pages = "15984--16007",
	abstract = "Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKE using LLMs and build a powerful generation model COLM tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications."
}

@inproceedings{zhou-etal-2024-cogmg,
    title = "{C}og{MG}: Collaborative Augmentation Between Large Language Model and Knowledge Graph",
	author = "Zhou, Tong  and
      Chen, Yubo  and
      Liu, Kang  and
      Zhao, Jun",
	editor = "Cao, Yixin  and
      Feng, Yang  and
      Xiong, Deyi",
	booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
	month = aug,
	year = "2024",
	address = "Bangkok, Thailand",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.acl-demos.35/",
	doi = "10.18653/v1/2024.acl-demos.35",
	pages = "365--373",
	abstract = "Large language models have become integral to question-answering applications despite their propensity for generating hallucinations and factually inaccurate content. Querying knowledge graphs to reduce hallucinations in LLM meets the challenge of incomplete knowledge coverage in knowledge graphs. On the other hand, updating knowledge graphs by information extraction and knowledge graph completion faces the knowledge update misalignment issue. In this work, we introduce a collaborative augmentation framework, CogMG, leveraging knowledge graphs to address the limitations of LLMs in QA scenarios, explicitly targeting the problems of incomplete knowledge coverage and knowledge update misalignment. The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands. We demonstrate the efficacy of this approach through a supervised fine-tuned LLM within an agent framework, showing significant improvements in reducing hallucinations and enhancing factual accuracy in QA responses. Our code and video are publicly available."
}

@article{chen-etal-2023-opal,
    title = "{OPAL}: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue",
	author = "Chen, Zhi  and
      Liu, Yuncong  and
      Chen, Lu  and
      Zhu, Su  and
      Wu, Mengyue  and
      Yu, Kai",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "11",
	year = "2023",
	address = "Cambridge, MA",
	publisher = "MIT Press",
	url = "https://aclanthology.org/2023.tacl-1.5/",
	doi = "10.1162/tacl_a_00534",
	pages = "68--84",
	abstract = "This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user`s constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks."
}

@inproceedings{youn-tagkopoulos-2023-kglm,
    title = "{KGLM}: Integrating Knowledge Graph Structure in Language Models for Link Prediction",
	author = "Youn, Jason  and
      Tagkopoulos, Ilias",
	editor = "Palmer, Alexis  and
      Camacho-collados, Jose",
	booktitle = "Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.starsem-1.20/",
	doi = "10.18653/v1/2023.starsem-1.20",
	pages = "217--224",
	abstract = "The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art performance for the link prediction task on the benchmark datasets."
}

@inproceedings{richardson-heck-2023-syndicom,
    title = "Syndicom: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback",
	author = "Richardson, Christopher  and
      Sundar, Anirudh  and
      Heck, Larry",
	editor = "Stoyanchev, Svetlana  and
      Joty, Shafiq  and
      Schlangen, David  and
      Dusek, Ondrej  and
      Kennington, Casey  and
      Alikhani, Malihe",
	booktitle = "Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2023",
	address = "Prague, Czechia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.sigdial-1.27/",
	doi = "10.18653/v1/2023.sigdial-1.27",
	pages = "297--308",
	abstract = "Commonsense reasoning is a critical aspect of human communication. Despite recent advances in conversational AI driven by large language models, commonsense reasoning remains a challenging task. In this work, we introduce Syndicom - a method for improving commonsense in dialogue response generation. Syndicom consists of two components. The first component is a dataset composed of commonsense dialogues created from a knowledge graph and synthesized into natural language. This dataset includes both valid and invalid responses to dialogue contexts, along with natural language feedback (NLF) for the invalid responses. The second contribution is a two-step procedure: training a model to predict natural language feedback (NLF) for invalid responses, and then training a response generation model conditioned on the predicted NLF, the invalid response, and the dialogue. Syndicom is scalable and does not require reinforcement learning. Empirical results on three tasks are evaluated using a broad range of metrics. Syndicom achieves a relative improvement of 53{\%} over ChatGPT on ROUGE-1, and human evaluators prefer Syndicom over ChatGPT 57{\%} of the time. We will publicly release the code and the full dataset."
}

@inproceedings{heinisch-etal-2023-accept,
    title = "{ACCEPT} at {S}em{E}val-2023 Task 3: An Ensemble-based Approach to Multilingual Framing Detection",
	author = "Heinisch, Philipp  and
      Plenz, Moritz  and
      Frank, Anette  and
      Cimiano, Philipp",
	editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Da San Martino, Giovanni  and
      Tayyar Madabushi, Harish  and
      Kumar, Ritesh  and
      Sartori, Elisa},
	booktitle = "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.semeval-1.187/",
	doi = "10.18653/v1/2023.semeval-1.187",
	pages = "1347--1358",
	abstract = "This paper describes the system and experimental results of an ensemble-based approach tomultilingual framing detection for the submission of the ACCEPT team to the SemEval-2023 Task 3 on Framing Detection (Subtask 2). The approach is based on an ensemble that combines three different methods: a classifier based on large language models, a classifier based on static word embeddings, and an approach that uses external commonsense knowledge graphs, in particular, ConceptNet. The results of the three classification heads are aggregated into an overall prediction for each frame class. Our best submission yielded a micro F1-score of 50.69{\%} (rank 10) and a macro F1-score of 50.20{\%} (rank 3) for English articles. Our experimental results show that static word embeddings and knowledge graphs are useful components for frame detection, while the ensemble of all three methods combines the strengths of our three proposed methods. Through system ablations, we show that the commonsenseguided knowledge graphs are the outperforming method for many languages."
}

@inproceedings{yanez-etal-2023-review,
    title = "A Review in Knowledge Extraction from Knowledge Bases",
	author = "Yanez, Fabio  and
      Montoyo, Andr{\'e}s  and
      Gutierrez, Yoan  and
      Mu{\~n}oz, Rafael  and
      Suarez, Armando",
	editor = "Mitkov, Ruslan  and
      Angelova, Galia",
	booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
	month = sep,
	year = "2023",
	address = "Varna, Bulgaria",
	publisher = "INCOMA Ltd., Shoumen, Bulgaria",
	url = "https://aclanthology.org/2023.ranlp-1.12/",
	pages = "109--116",
	abstract = "Generative language models achieve the state of the art in many tasks within natural language processing (NLP). Although these models correctly capture syntactic information, they fail to interpret knowledge (semantics). Moreover, the lack of interpretability of these models promotes the use of other technologies as a replacement or complement to generative language models. This is the case with research focused on incorporating knowledge by resorting to knowledge bases mainly in the form of graphs. The generation of large knowledge graphs is carried out with unsupervised or semi-supervised techniques, which promotes the validation of this knowledge with the same type of techniques due to the size of the generated databases. In this review, we will explain the different techniques used to test and infer knowledge from graph structures with machine learning algorithms. The motivation of validating and inferring knowledge is to use correct knowledge in subsequent tasks with improved embeddings."
}

@inproceedings{anuradha-etal-2023-evaluating,
    title = "Evaluating of Large Language Models in Relationship Extraction from Unstructured Data: Empirical Study from Holocaust Testimonies",
	author = "Anuradha, Isuri  and
      Ha, Le An  and
      Mitkov, Ruslan  and
      Nahar, Vinita",
	editor = "Mitkov, Ruslan  and
      Angelova, Galia",
	booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
	month = sep,
	year = "2023",
	address = "Varna, Bulgaria",
	publisher = "INCOMA Ltd., Shoumen, Bulgaria",
	url = "https://aclanthology.org/2023.ranlp-1.13/",
	pages = "117--123",
	abstract = "Relationship extraction from unstructured data remains one of the most challenging tasks in the field of Natural Language Processing (NLP). The complexity of relationship extraction arises from the need to comprehend the underlying semantics, syntactic structures, and contextual dependencies within the text. Unstructured data poses challenges with diverse linguistic patterns, implicit relationships, contextual nuances, complicating accurate relationship identification and extraction. The emergence of Large Language Models (LLMs), such as GPT (Generative Pre-trained Transformer), has indeed marked a significant advancement in the field of NLP. In this work, we assess and evaluate the effectiveness of LLMs in relationship extraction in the Holocaust testimonies within the context of the Historical realm. By delving into this domain-specific context, we aim to gain deeper insights into the performance and capabilities of LLMs in accurately capturing and extracting relationships within the Holocaust domain by developing a novel knowledge graph to visualise the relationships of the Holocaust. To the best of our knowledge, there is no existing study which discusses relationship extraction in Holocaust testimonies. The majority of current approaches for Information Extraction (IE) in historic documents are either manual or OCR based. Moreover, in this study, we found that the Subject-Object-Verb extraction using GPT3-based relations produced more meaningful results compared to the Semantic Role labeling-based triple extraction."
}

@inproceedings{cheng-etal-2023-complementary,
    title = "Complementary Roles of Inference and Language Models in {QA}",
	author = "Cheng, Liang  and
      Hosseini, Mohammad Javad  and
      Steedman, Mark",
	editor = "Surdeanu, Mihai  and
      Riloff, Ellen  and
      Chiticariu, Laura  and
      Frietag, Dayne  and
      Hahn-Powell, Gus  and
      Morrison, Clayton T.  and
      Noriega-Atala, Enrique  and
      Sharp, Rebecca  and
      Valenzuela-Escarcega, Marco",
	booktitle = "Proceedings of the 2nd Workshop on Pattern-based Approaches to NLP in the Age of Deep Learning",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.pandl-1.8/",
	doi = "10.18653/v1/2023.pandl-1.8",
	pages = "75--91",
	abstract = "Answering open-domain questions through unsupervised methods poses challenges for both machine-reading (MR) and language model (LM) -based approaches. The MR-based approach suffers from sparsity issues in extracted knowledge graphs (KGs), while the performance of the LM-based approach significantly depends on the quality of the retrieved context for questions. In this paper, we compare these approaches and propose a novel methodology that leverages directional predicate entailment (inference) to address these limitations. We use entailment graphs (EGs), with natural language predicates as nodes and entailment as edges, to enhance parsed KGs by inferring unseen assertions, effectively mitigating the sparsity problem in the MR-based approach. We also show EGs improve context retrieval for the LM-based approach. Additionally, we present a Boolean QA task, demonstrating that EGs exhibit comparable directional inference capabilities to large language models (LLMs). Our results highlight the importance of inference in open-domain QA and the improvements brought by leveraging EGs."
}

@inproceedings{salnikov-etal-2023-large,
    title = "Large Language Models Meet Knowledge Graphs to Answer Factoid Questions",
	author = "Salnikov, Mikhail  and
      Le, Hai  and
      Rajput, Prateek  and
      Nikishina, Irina  and
      Braslavski, Pavel  and
      Malykh, Valentin  and
      Panchenko, Alexander",
	editor = "Huang, Chu-Ren  and
      Harada, Yasunari  and
      Kim, Jong-Bok  and
      Chen, Si  and
      Hsu, Yu-Yin  and
      Chersoni, Emmanuele  and
      A, Pranav  and
      Zeng, Winnie Huiheng  and
      Peng, Bo  and
      Li, Yuxi  and
      Li, Junlin",
	booktitle = "Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation",
	month = dec,
	year = "2023",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.paclic-1.63/",
	pages = "635--644"
}

@inproceedings{sen-etal-2023-knowledge,
    title = "Knowledge Graph-augmented Language Models for Complex Question Answering",
	author = "Sen, Priyanka  and
      Mavadia, Sandeep  and
      Saffari, Amir",
	editor = "Dalvi Mishra, Bhavana  and
      Durrett, Greg  and
      Jansen, Peter  and
      Neves Ribeiro, Danilo  and
      Wei, Jason",
	booktitle = "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
	month = jun,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.nlrse-1.1/",
	doi = "10.18653/v1/2023.nlrse-1.1",
	pages = "1--8",
	abstract = "Large language models have shown impressive abilities to reason over input text, however, they are prone to hallucinations. On the other hand, end-to-end knowledge graph question answering (KGQA) models output responses grounded in facts, but they still struggle with complex reasoning, such as comparison or ordinal questions. In this paper, we propose a new method for complex question answering where we combine a knowledge graph retriever based on an end-to-end KGQA model with a language model that reasons over the retrieved facts to return an answer. We observe that augmenting language model prompts with retrieved KG facts improves performance over using a language model alone by an average of 83{\%}. In particular, we see improvements on complex questions requiring count, intersection, or multi-hop reasoning operations."
}

@inproceedings{baek-etal-2023-knowledge,
    title = "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
	author = "Baek, Jinheon  and
      Aji, Alham Fikri  and
      Saffari, Amir",
	editor = "Dalvi Mishra, Bhavana  and
      Durrett, Greg  and
      Jansen, Peter  and
      Neves Ribeiro, Danilo  and
      Wei, Jason",
	booktitle = "Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE)",
	month = jun,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.nlrse-1.7/",
	doi = "10.18653/v1/2023.nlrse-1.7",
	pages = "78--106",
	abstract = "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user`s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48{\%} in average, across multiple LLMs of various sizes."
}

@inproceedings{remy-etal-2023-detecting,
    title = "Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning",
	author = "Remy, Fran{\c{c}}ois  and
      Khabibullina, Alfiya  and
      Demeester, Thomas",
	editor = "Bhatia, Archna  and
      Evang, Kilian  and
      Garcia, Marcos  and
      Giouli, Voula  and
      Han, Lifeng  and
      Taslimipoor, Shiva",
	booktitle = "Proceedings of the 19th Workshop on Multiword Expressions (MWE 2023)",
	month = may,
	year = "2023",
	address = "Dubrovnik, Croatia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.mwe-1.11/",
	doi = "10.18653/v1/2023.mwe-1.11",
	pages = "73--80",
	abstract = "This paper shines a light on the potential of definition-based semantic models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs) in clinical terminology. Our study focuses on biomedical entities defined in the UMLS ontology and aims to help prioritize the translation efforts of these entities. In particular, we develop an effective tool for scoring the idiomaticity of biomedical MWEs based on the degree of similarity between the semantic representations of those MWEs and a weighted average of the representation of their constituents. We achieve this using a biomedical language model trained to produce similar representations for entity names and their definitions, called BioLORD. The importance of this definition-based approach is highlighted by comparing the BioLORD model to two other state-of-the-art biomedical language models based on Transformer: SapBERT and CODER. Our results show that the BioLORD model has a strong ability to identify idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity estimation helps ontology translators to focus on more challenging MWEs."
}

@inproceedings{axelsson-skantze-2023-using,
    title = "Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs",
	author = "Axelsson, Agnes  and
      Skantze, Gabriel",
	editor = "Gatt, Albert  and
      Gardent, Claire  and
      Cripwell, Liam  and
      Belz, Anya  and
      Borg, Claudia  and
      Erdem, Aykut  and
      Erdem, Erkut",
	booktitle = "Proceedings of the Workshop on Multimodal, Multilingual Natural Language Generation and Multilingual WebNLG Challenge (MM-NLG 2023)",
	month = sep,
	year = "2023",
	address = "Prague, Czech Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.mmnlg-1.5/",
	pages = "39--54",
	abstract = "In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task, even with relatively little training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model`s understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text."
}

@inproceedings{wold-etal-2023-text,
    title = "Text-To-{KG} Alignment: Comparing Current Methods on Classification Tasks",
	author = "Wold, Sondre  and
      {\O}vrelid, Lilja  and
      Velldal, Erik",
	editor = "Hruschka, Estevam  and
      Mitchell, Tom  and
      Rahman, Sajjadur  and
      Mladeni{\'c}, Dunja  and
      Grobelnik, Marko",
	booktitle = "Proceedings of the First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)",
	month = jul,
	year = "2023",
	address = "Toronto, ON, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.matching-1.1/",
	doi = "10.18653/v1/2023.matching-1.1",
	pages = "1--13",
	abstract = "In contrast to large text corpora, knowledge graphs (KG) provide dense and structured representations of factual information. This makes them attractive for systems that supplement or ground the knowledge found in pre-trained language models with an external knowledge source. This has especially been the case for classification tasks, where recent work has focused on creating pipeline models that retrieve information from KGs like ConceptNet as additional context. Many of these models consist of multiple components, and although they differ in the number and nature of these parts, they all have in common that for some given text query, they attempt to identify and retrieve a relevant subgraph from the KG. Due to the noise and idiosyncrasies often found in KGs, it is not known how current methods compare to a scenario where the aligned subgraph is completely relevant to the query. In this work, we try to bridge this knowledge gap by reviewing current approaches to text-to-KG alignment and evaluating them on two datasets where manually created graphs are available, providing insights into the effectiveness of current methods. We release our code for reproducibility."
}

@inproceedings{baek-etal-2023-knowledge-augmented,
    title = "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
	author = "Baek, Jinheon  and
      Aji, Alham Fikri  and
      Saffari, Amir",
	editor = "Hruschka, Estevam  and
      Mitchell, Tom  and
      Rahman, Sajjadur  and
      Mladeni{\'c}, Dunja  and
      Grobelnik, Marko",
	booktitle = "Proceedings of the First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)",
	month = jul,
	year = "2023",
	address = "Toronto, ON, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.matching-1.7/",
	doi = "10.18653/v1/2023.matching-1.7",
	pages = "70--98",
	abstract = "Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user`s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48{\%} in average, across multiple LLMs of various sizes."
}

@inproceedings{strakova-etal-2023-extending,
    title = "Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned {LLM}s Suggestions",
	author = "Strakov{\'a}, Jana  and
      Fu{\v{c}}{\'i}kov{\'a}, Eva  and
      Haji{\v{c}}, Jan  and
      Ure{\v{s}}ov{\'a}, Zde{\v{n}}ka",
	editor = "Prange, Jakob  and
      Friedrich, Annemarie",
	booktitle = "Proceedings of the 17th Linguistic Annotation Workshop (LAW-XVII)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.law-1.9/",
	doi = "10.18653/v1/2023.law-1.9",
	pages = "85--95",
	abstract = "In this project, we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task, namely adding descriptive words (verbs) to an existing (but incomplete, as of yet) ontology of event types. Several research questions have been focused on, from the investigation of a possible heuristics to provide at least hints to annotators which verbs to include and which are outside the current version of the ontology, to the possible use of the automatic scores to help the annotators to be more efficient in finding a threshold for identifying verbs that cannot be assigned to any existing class and therefore they are to be used as seeds for a new class. We have also carefully examined the correlation of the automatic scores with the human annotation. While the correlation turned out to be strong, its influence on the annotation proper is modest due to its near linearity, even though the mere fact of such pre-annotation leads to relatively short annotation times."
}

@inproceedings{salnikov-etal-2023-answer,
    title = "Answer Candidate Type Selection: Text-To-Text Language Model for Closed Book Question Answering Meets Knowledge Graphs",
	author = "Salnikov, Mikhail  and
      Lysyuk, Maria  and
      Braslavski, Pavel  and
      Razzhigaev, Anton  and
      Malykh, Valentin A.  and
      Panchenko, Alexander",
	editor = "Georges, Munir  and
      Herygers, Aaricia  and
      Friedrich, Annemarie  and
      Roth, Benjamin",
	booktitle = "Proceedings of the 19th Conference on Natural Language Processing (KONVENS 2023)",
	month = sep,
	year = "2023",
	address = "Ingolstadt, Germany",
	publisher = "Association for Computational Lingustics",
	url = "https://aclanthology.org/2023.konvens-main.16/",
	pages = "155--164"
}

@inproceedings{hashem-etal-2023-generating,
    title = "Generating Faithful Text From a Knowledge Graph with Noisy Reference Text",
	author = "Hashem, Tahsina  and
      Wang, Weiqing  and
      Wijaya, Derry Tanti  and
      Ali, Mohammed Eunus  and
      Li, Yuan-Fang",
	editor = "Keet, C. Maria  and
      Lee, Hung-Yi  and
      Zarrie{\ss}, Sina",
	booktitle = "Proceedings of the 16th International Natural Language Generation Conference",
	month = sep,
	year = "2023",
	address = "Prague, Czechia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.inlg-main.8/",
	doi = "10.18653/v1/2023.inlg-main.8",
	pages = "106--122",
	abstract = "Knowledge Graph (KG)-to-Text generation aims at generating fluent natural-language text that accurately represents the information of a given knowledge graph. While significant progress has been made in this task by exploiting the power of pre-trained language models (PLMs) with appropriate graph structure-aware modules, existing models still fall short of generating faithful text, especially when the ground-truth natural-language text contains additional information that is not present in the graph. In this paper, we develop a KG-to-text generation model that can generate faithful natural-language text from a given graph, in the presence of noisy reference text. Our framework incorporates two core ideas: Firstly, we utilize contrastive learning to enhance the model`s ability to differentiate between faithful and hallucinated information in the text, thereby encouraging the decoder to generate text that aligns with the input graph. Secondly, we empower the decoder to control the level of hallucination in the generated text by employing a controllable text generation technique. We evaluate our model`s performance through the standard quantitative metrics as well as a ChatGPT-based quantitative and qualitative analysis. Our evaluation demonstrates the superior performance of our model over state-of-the-art KG-to-text models on faithfulness."
}

@inproceedings{xie-etal-2023-lambdakg,
    title = "{L}ambda{KG}: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings",
	author = "Xie, Xin  and
      Li, Zhoubo  and
      Wang, Xiaohan  and
      Xi, ZeKun  and
      Zhang, Ningyu",
	editor = "Saha, Sriparna  and
      Sujaini, Herry",
	booktitle = "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics: System Demonstrations",
	month = nov,
	year = "2023",
	address = "Bali, Indonesia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.ijcnlp-demo.4/",
	doi = "10.18653/v1/2023.ijcnlp-demo.4",
	pages = "25--33"
}

@inproceedings{pratik-etal-2023-improving,
    title = "Improving the Evaluation of {NLP} Approaches for Scientific Text Annotation with Ontology Embedding-Based Semantic Similarity Metrics",
	author = "Devkota, Pratik  and
      D. Mohanty, Somya  and
      Manda, Prashanti",
	editor = "D. Pawar, Jyoti  and
      Lalitha Devi, Sobha",
	booktitle = "Proceedings of the 20th International Conference on Natural Language Processing (ICON)",
	month = dec,
	year = "2023",
	address = "Goa University, Goa, India",
	publisher = "NLP Association of India (NLPAI)",
	url = "https://aclanthology.org/2023.icon-1.47/",
	pages = "516--522",
	abstract = "Lexical Simplification is a challenging task that aims to improve the readability of text for nonnative people, people with dyslexia, and any linguistic impairments. It consists of 3 components: 1) Complex Word Identification 2) Substitute Generation 3) Substitute Ranking. Current methods use contextual information as a primary source in all three stages of the simplification pipeline. We argue that while context is an important measure, it alone is not sufficient in the process. In the complex word identification step, contextual information is inadequate, moreover, heavy feature engineering is required to use additional linguistic features. This paper presents a novel architecture for complex word identification that uses a pre-trained transformer model`s information flow through its hidden layers as a feature representation that implicitly encodes all the features required for identification. We portray how database methods and masked language modeling can be complementary to one another in substitute generation and ranking process that is built on the foundational pillars of Simplicity, Grammatical and Semantic correctness, and context preservation. We show that our proposed model generalizes well and outperforms the current state-of-the-art on wellknown datasets."
}

@inproceedings{kinshuk-etal-2023-infusing,
    title = "Infusing Knowledge into Large Language Models with Contextual Prompts",
	author = "Vasisht, Kinshuk  and
      Ganesan, Balaji  and
      Kumar, Vikas  and
      Bhatnagar, Vasudha",
	editor = "D. Pawar, Jyoti  and
      Lalitha Devi, Sobha",
	booktitle = "Proceedings of the 20th International Conference on Natural Language Processing (ICON)",
	month = dec,
	year = "2023",
	address = "Goa University, Goa, India",
	publisher = "NLP Association of India (NLPAI)",
	url = "https://aclanthology.org/2023.icon-1.65/",
	pages = "657--662",
	abstract = "Knowledge infusion is a promising method for enhancing Large Language Models for domainspecific NLP tasks rather than pre-training models over large data from scratch. These augmented LLMs typically depend on additional pre-training or knowledge prompts from an existing knowledge graph, which is impractical in many applications. In contrast, knowledge infusion directly from relevant documents is more generalisable and alleviates the need for structured knowledge graphs while also being useful for entities that are usually not found in any knowledge graph. With this motivation, we propose a simple yet generalisable approach for knowledge infusion by generating prompts from the context in the input text. Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs."
}

@inproceedings{jensen-hojmark-2023-formalizing,
    title = "Formalizing content creation and evaluation methods for {AI}-generated social media content",
	author = "Jensen, Christian  and
      H{\o}jmark, Axel",
	editor = "Gehrmann, Sebastian  and
      Wang, Alex  and
      Sedoc, Jo{\~a}o  and
      Clark, Elizabeth  and
      Dhole, Kaustubh  and
      Chandu, Khyathi Raghavi  and
      Santus, Enrico  and
      Sedghamiz, Hooman",
	booktitle = "Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.gem-1.3/",
	pages = "22--41",
	abstract = "This study explores the use of large language models (LLMs), such as ChatGPT and GPT-4, in creating high-quality text-based social media content for businesses on LinkedIn. We introduce a novel architecture incorporating external knowledge bases and a multi-step writing approach, which extracts facts from company websites to form a knowledge graph. Our method`s efficacy is assessed using the {\textquotedblleft}Long-LinkedIn{\textquotedblright} evaluation dataset designed for long-form post generation. Results indicate that our iterative refinement significantly improves content quality. However, knowledge-enhanced prompts occasionally reduced quality due to potential formulation issues. LLM-based evaluations, particularly using ChatGPT, showcased potential as a less resource-intensive alternative to human assessments, with a notable alignment between the two evaluation techniques."
}

@inproceedings{ren-etal-2023-towards,
    title = "Towards Informative Open-ended Text Generation with Dynamic Knowledge Triples",
	author = "Ren, Zixuan  and
      Zhao, Yang  and
      Zong, Chengqing",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.210/",
	doi = "10.18653/v1/2023.findings-emnlp.210",
	pages = "3189--3203",
	abstract = "Pretrained language models (PLMs), especially large language models (LLMs) demonstrate impressive capabilities in open-ended text generation. While our statistical results show that LLMs often suffer from over-concentrated information, where the generated texts overly focus on the given prompt and fail to provide sufficient background and detailed information as humans do. To address this issue, we propose a dynamic knowledge-guided informative open-ended text generation approach, that utilizes a knowledge graph to help the model generate more contextually related entities and detailed facts. Specifically, we first employ a local knowledge filter to extract relevant knowledge from the comprehensive knowledge graph for a given topic sentence. Then we introduce a dynamic knowledge selector to predict the entity to be mentioned in the subsequent sentence. Finally, we utilize a knowledge-enhanced text generator to produce a more informative output. To evaluate the effectiveness of our approach, we evaluate the proposed approach in two scenarios: fine-tuning for small PLMs and prompt tuning for LLMs. Experimental results show that our approach could generate more informative texts than baselines."
}

@inproceedings{ma-etal-2023-kapalm,
    title = "{KAPALM}: Knowledge gr{AP}h enh{A}nced Language Models for Fake News Detection",
	author = "Ma, Jing  and
      Chen, Chen  and
      Hou, Chunyan  and
      Yuan, Xiaojie",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.263/",
	doi = "10.18653/v1/2023.findings-emnlp.263",
	pages = "3999--4009",
	abstract = "Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media is usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge. However, majority of these methods focus on news entity information and ignore the structured knowledge among news entities. To address this issue, in this work, we propose a Knowledge grAPh enhAnced Language Model (KAPALM) which is a novel model that fuses coarse- and fine-grained representations of entity knowledge from Knowledge Graphs (KGs). Firstly, we identify entities in news content and link them to entities in KGs. Then, a subgraph of KGs is extracted to provide structured knowledge of entities in KGs and fed into a graph neural network to obtain the coarse-grained knowledge representation. This subgraph is pruned to provide fine-grained knowledge and fed into the attentive graph and graph pooling layer. Finally, we integrate the coarse- and fine-grained entity knowledge representations with the textual representation for fake news detection. The experimental results on two benchmark datasets show that our method is superior to state-of-the-art baselines. In addition, it is competitive in the few-shot scenario."
}

@inproceedings{chepurova-etal-2023-better,
    title = "Better Together: Enhancing Generative Knowledge Graph Completion with Language Models and Neighborhood Information",
	author = "Chepurova, Alla  and
      Bulatov, Aydar  and
      Kuratov, Yuri  and
      Burtsev, Mikhail",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.352/",
	doi = "10.18653/v1/2023.findings-emnlp.352",
	pages = "5306--5316",
	abstract = "Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which limits their potential performance. Knowledge Graph Completion (KGC) techniques aim to address this issue. However, traditional KGC methods are computationally intensive and impractical for large-scale KGs, necessitating the learning of dense node embeddings and computing pairwise distances. Generative transformer-based language models (e.g., T5 and recent KGT5) offer a promising solution as they can predict the tail nodes directly. In this study, we propose to include node neighborhoods as additional information to improve KGC methods based on language models. We examine the effects of this imputation and show that, on both inductive and transductive Wikidata subsets, our method outperforms KGT5 and conventional KGC approaches. We also provide an extensive analysis of the impact of neighborhood on model prediction and show its importance. Furthermore, we point the way to significantly improve KGC through more effective neighborhood selection."
}

@inproceedings{zhou-etal-2023-inductive,
    title = "Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information",
	author = "Zhou, Wentao  and
      Zhao, Jun  and
      Gui, Tao  and
      Zhang, Qi  and
      Huang, Xuanjing",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.431/",
	doi = "10.18653/v1/2023.findings-emnlp.431",
	pages = "6491--6502",
	abstract = "The inductive inference of the knowledge graph aims to complete the potential relations between the new unknown entities in the graph. Most existing methods are based on entity-independent features such as graph structure information and relationship information to inference. However, the neighborhood of these new entities is often too sparse to obtain enough information to build these features effectively. In this work, we propose a knowledge graph inductive inference method that fuses ontology information. Based on the enclosing subgraph, we bring in feature embeddings of concepts corresponding to entities to learn the semantic information implicit in the ontology. Considering that the ontology information of entities may be missing, we build a type constraint regular loss to explicitly model the semantic connections between entities and concepts, and thus capture the missing concepts of entities. Experimental results show that our approach significantly outperforms large language models like ChatGPT on two benchmark datasets, YAGO21K-610 and DB45K-165, and improves the MRR metrics by 15.4{\%} and 44.1{\%}, respectively, when compared with the state-of-the-art methods."
}

@inproceedings{zhong-etal-2023-romqa,
    title = "{R}o{MQA}: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",
	author = "Zhong, Victor  and
      Shi, Weijia  and
      Yih, Wen-tau  and
      Zettlemoyer, Luke",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.470/",
	doi = "10.18653/v1/2023.findings-emnlp.470",
	pages = "7055--7067",
	abstract = "We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer question answering (QA). RoMQA contains clusters of questions that are derived from related constraints mined from the Wikidata knowledge graph. RoMQA evaluates robustness of QA models to varying constraints by measuring worst-case performance within each question cluster. Compared to prior QA datasets, RoMQA has more human-written questions that require reasoning over more evidence text and have, on average, many more correct answers. In addition, human annotators rate RoMQA questions as more natural or likely to be asked by people. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zeroshot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints, but can be made more robust by tuning on clusters of related questions. Our results show that RoMQA is a challenging benchmark for large language models, and provides a quantifiable test to build more robust QA methods."
}

@inproceedings{wei-etal-2023-kicgpt,
    title = "{KICGPT}: Large Language Model with Knowledge in Context for Knowledge Graph Completion",
	author = "Wei, Yanbin  and
      Huang, Qiushi  and
      Zhang, Yu  and
      Kwok, James",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.580/",
	doi = "10.18653/v1/2023.findings-emnlp.580",
	pages = "8667--8683",
	abstract = "Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC and they can be categorized into two main classes, including triple-based and test-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced distributions of entities. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate the limitations in the two approaches, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever, to alleviate the long-tail problem without incurring additional training overhead. In the proposed KICGPT model, we propose an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide LLM. Empirical results on benchmark datasets demonstrate the effectiveness of the proposed KICGPT model with lighter training overhead and no finetuning."
}

@inproceedings{kim-etal-2023-kg,
    title = "{KG}-{GPT}: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
	author = "Kim, Jiho  and
      Kwon, Yeonsu  and
      Jo, Yohan  and
      Choi, Edward",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.631/",
	doi = "10.18653/v1/2023.findings-emnlp.631",
	pages = "9410--9421",
	abstract = "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs."
}

@inproceedings{chen-etal-2023-exploring-context,
    title = "Exploring In-Context Learning for Knowledge Grounded Dialog Generation",
	author = "Chen, Qinyu  and
      Wu, Wenhao  and
      Li, Sujian",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.675/",
	doi = "10.18653/v1/2023.findings-emnlp.675",
	pages = "10071--10081",
	abstract = "Large neural-based dialog generation models have been applied in many real-life scenarios, yet they are prone to hallucination and tend to produce factually inaccurate outputs which raise great concerns. To alleviate this problem, we propose a plug-and-play retrieval-based framework IKA, which leverages in-context learning and retrieval techniques to enhance LLMs on knowledge grounded dialog generation. We design thorough experiments on a large-scale knowledge graph with 1M+ facts to investigate the effectiveness and generalization of our framework. Experiments show that our method surpasses previous training-based SOTA by a large margin, specifically 46.67{\%} in BLEU4, 26.01{\%} in ROUGE-L, 122.90{\%} in BARTScore and 30.50{\%} in Entity Coverage F1. Further analysis show promising abilities of LLMs to perform knowledge-intensive tasks, which is previously considered weak and understudied."
}

@inproceedings{luo-etal-2023-systematic,
    title = "Systematic Assessment of Factual Knowledge in Large Language Models",
	author = "Luo, Linhao  and
      Vu, Trang  and
      Phung, Dinh  and
      Haf, Reza",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.885/",
	doi = "10.18653/v1/2023.findings-emnlp.885",
	pages = "13272--13286",
	abstract = "Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context."
}

@inproceedings{chen-etal-2023-mixture,
    title = "Mixture of Soft Prompts for Controllable Data Generation",
	author = "Chen, Derek  and
      Lee, Celine  and
      Lu, Yunan  and
      Rosati, Domenic  and
      Yu, Zhou",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.988/",
	doi = "10.18653/v1/2023.findings-emnlp.988",
	pages = "14815--14833",
	abstract = "Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating multi-attribute data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks."
}

@inproceedings{lu-etal-2023-pivoine,
    title = "{PIVOINE}: Instruction Tuning for Open-world Entity Profiling",
	author = "Lu, Keming  and
      Pan, Xiaoman  and
      Song, Kaiqiang  and
      Zhang, Hongming  and
      Yu, Dong  and
      Chen, Jianshu",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-emnlp.1009/",
	doi = "10.18653/v1/2023.findings-emnlp.1009",
	pages = "15108--15127",
	abstract = "This work considers the problem of Open-world Entity Profiling, a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE is considered a more general situation where entities and relations could be beyond a predefined ontology. We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction-tuning dataset for Open-world Entity Profiling enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world Entity Profiling with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional methods and ChatGPT-based baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge of entity profiling."
}

@inproceedings{schwertmann-etal-2023-model,
    title = "Model-Agnostic Bias Measurement in Link Prediction",
	author = "Schwertmann, Lena  and
      Kannan Ravi, Manoj Prabhakar  and
      de Melo, Gerard",
	editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
	booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
	month = may,
	year = "2023",
	address = "Dubrovnik, Croatia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-eacl.121/",
	doi = "10.18653/v1/2023.findings-eacl.121",
	pages = "1632--1648",
	abstract = "Link prediction models based on factual knowledge graphs are commonly used in applications such as search and question answering. However, work investigating social bias in these models has been limited. Previous work focused on knowledge graph embeddings, so more recent classes of models achieving superior results by fine-tuning Transformers have not yet been investigated. We therefore present a model-agnostic approach for bias measurement leveraging fairness metrics to compare bias in knowledge graph embedding-based predictions (KG only) with models that use pre-trained, Transformer-based language models (KG+LM). We further create a dataset to measure gender bias in occupation predictions and assess whether the KG+LM models are more or less biased than KG only models. We find that gender bias tends to be higher for the KG+LM models and analyze potential connections to the accuracy of the models and the data bias inherent in our dataset. Finally, we discuss the limitations and ethical considerations of our work. The repository containing the source code and the data set is publicly available at \url{https://github.com/lena-schwert/comparing-bias-in-KG-models}."
}

@inproceedings{ding-etal-2023-unified,
    title = "A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific {NLP} Tasks",
	author = "Ding, Ruiqing  and
      Han, Xiao  and
      Wang, Leye",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.24/",
	doi = "10.18653/v1/2023.findings-acl.24",
	pages = "353--369",
	abstract = "By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models (PLMs) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose KnowledgeDA, a unified domain language model development service to enhance the task-specific training procedure with domain knowledge graphs. Given domain-specific task texts input, KnowledgeDA can automatically generate a domain-specific language model following three steps: (i) localize domain knowledge entities in texts via an embedding-similarity approach; (ii) generate augmented samples by retrieving replaceable domain entity pairs from two views of both knowledge graph and training data; (iii) select high-quality augmented samples for fine-tuning via confidence-based assessment. We implement a prototype of KnowledgeDA to learn language models for two domains, healthcare and software development. Experiments on domain-specific text classification and QA tasks verify the effectiveness and generalizability of KnowledgeDA."
}

@inproceedings{gao-etal-2023-learning,
    title = "Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion",
	author = "Gao, Yifu  and
      He, Yongquan  and
      Kan, Zhigang  and
      Han, Yi  and
      Qiao, Linbo  and
      Li, Dongsheng",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.28/",
	doi = "10.18653/v1/2023.findings-acl.28",
	pages = "417--430",
	abstract = "Temporal knowledge graph completion that predicts missing links for incomplete temporal knowledge graphs (TKG) is gaining increasing attention. Most existing works have achieved good results by incorporating time information into static knowledge graph embedding methods. However, they ignore the contextual nature of the TKG structure, i.e., query-specific subgraph contains both structural and temporal neighboring facts. This paper presents the SToKE, a novel method that employs the pre-trained language model (PLM) to learn joint Structural and Temporal Contextualized Knowledge Embeddings.Specifically, we first construct an event evolution tree (EET) for each query to enable PLMs to handle the TKG, which can be seen as a structured event sequence recording query-relevant structural and temporal contexts. We then propose a novel temporal embedding and structural matrix to learn the time information and structural dependencies of facts in EET.Finally, we formulate TKG completion as a mask prediction problem by masking the missing entity of the query to fine-tune pre-trained language models. Experimental results on three widely used datasets show the superiority of our model."
}

@inproceedings{misra-etal-2023-triggering,
    title = "Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks",
	author = "Misra, Kanishka  and
      Nogueira dos Santos, Cicero  and
      Shakeri, Siamak",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.62/",
	doi = "10.18653/v1/2023.findings-acl.62",
	pages = "972--985",
	abstract = "Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random-walks over structured knowledge graphs. Specifically, we use soft-prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random-walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require multi-hop reasoning."
}

@inproceedings{yu-etal-2023-folkscope,
    title = "{F}olk{S}cope: Intention Knowledge Graph Construction for {E}-commerce Commonsense Discovery",
	author = "Yu, Changlong  and
      Wang, Weiqi  and
      Liu, Xin  and
      Bai, Jiaxin  and
      Song, Yangqiu  and
      Li, Zheng  and
      Gao, Yifan  and
      Cao, Tianyu  and
      Yin, Bing",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.76/",
	doi = "10.18653/v1/2023.findings-acl.76",
	pages = "1173--1191",
	abstract = "Understanding users' intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework, to reveal the structure of humans' minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models (LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph. LLMs first generate intention assertions via e-commerce specific prompts to explain shopping behaviors, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility and typicality labels of sampled intentions as training data in order to populate human judgments to all automatic generations. Last, to structurize the assertions, we propose pattern mining and conceptualization to form more condensed and abstract knowledge. Extensive evaluations and study demonstrate that our constructed knowledge graph can well model e-commerce knowledge and have many potential applications."
}

@inproceedings{li-etal-2023-graph,
    title = "Graph Reasoning for Question Answering with Triplet Retrieval",
	author = "Li, Shiyang  and
      Gao, Yifan  and
      Jiang, Haoming  and
      Yin, Qingyu  and
      Li, Zheng  and
      Yan, Xifeng  and
      Zhang, Chao  and
      Yin, Bing",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.208/",
	doi = "10.18653/v1/2023.findings-acl.208",
	pages = "3366--3375",
	abstract = "Answering complex questions often requires reasoning over knowledge graphs (KGs). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into KG encoder, e.g. graph neural networks (GNNs), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for question answering. In this paper, we propose a simple yet effective method to first retrieve the most relevant triplets from KGs and then rerank them, which are then concatenated with questions to be fed into language models. Extensive results on both CommonsenseQA and OpenbookQA datasets show that our method can outperform state-of-the-art up to 4.6{\%} absolute accuracy."
}

@inproceedings{he-etal-2023-language,
    title = "Language Model Analysis for Ontology Subsumption Inference",
	author = "He, Yuan  and
      Chen, Jiaoyan  and
      Jimenez-Ruiz, Ernesto  and
      Dong, Hang  and
      Horrocks, Ian",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.213/",
	doi = "10.18653/v1/2023.findings-acl.213",
	pages = "3439--3453",
	abstract = "Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM`s knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets."
}

@inproceedings{ji-etal-2023-rho,
    title = "{RHO}: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
	author = "Ji, Ziwei  and
      Liu, Zihan  and
      Lee, Nayeon  and
      Yu, Tiezheng  and
      Wilie, Bryan  and
      Zeng, Min  and
      Fung, Pascale",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.275/",
	doi = "10.18653/v1/2023.findings-acl.275",
	pages = "4504--4522",
	abstract = "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, which further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ({\ensuremath{\rho}}) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG (Moon et al., 2019) show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54{\%} in FeQA (Durmus et al., 2020))."
}

@inproceedings{hao-etal-2023-bertnet,
    title = "{B}ert{N}et: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models",
	author = "Hao, Shibo  and
      Tan, Bowen  and
      Tang, Kaiwen  and
      Ni, Bin  and
      Shao, Xiyan  and
      Zhang, Hengzhe  and
      Xing, Eric  and
      Hu, Zhiting",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.309/",
	doi = "10.18653/v1/2023.findings-acl.309",
	pages = "5000--5015",
	abstract = "It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations, from LMs of varying capacities such as RoBERTaNet. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., {\textquotedblleft}A is capable of but not good at B{\textquotedblright}). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs' knowledge capacities."
}

@inproceedings{liu-etal-2023-enhancing,
    title = "Enhancing Hierarchical Text Classification through Knowledge Graph Integration",
	author = "Liu, Ye  and
      Zhang, Kai  and
      Huang, Zhenya  and
      Wang, Kehang  and
      Zhang, Yanghai  and
      Liu, Qi  and
      Chen, Enhong",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.358/",
	doi = "10.18653/v1/2023.findings-acl.358",
	pages = "5797--5810",
	abstract = "Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations. Generally, when manually classifying a specific document to the taxonomic hierarchy, experts make inference based on their prior knowledge and experience. For machines to achieve this capability, we propose a novel Knowledge-enabled Hierarchical Text Classification model (K-HTC), which incorporates knowledge graphs into HTC. Specifically, K-HTC innovatively integrates knowledge into both the text representation and hierarchical label learning process, addressing the knowledge limitations of traditional methods. Additionally, a novel knowledge-aware contrastive learning strategy is proposed to further exploit the information inherent in the data. Extensive experiments on two publicly available HTC datasets show the efficacy of our proposed method, and indicate the necessity of incorporating knowledge graphs in HTC tasks."
}

@inproceedings{song-etal-2023-multilingual,
    title = "Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints",
	author = "Song, Ran  and
      He, Shizhu  and
      Gao, Shengxiang  and
      Cai, Li  and
      Liu, Kang  and
      Yu, Zhengtao  and
      Zhao, Jun",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.488/",
	doi = "10.18653/v1/2023.findings-acl.488",
	pages = "7709--7721",
	abstract = "Multilingual Knowledge Graph Completion (mKGC) aim at solving queries in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit a pronounced English-centric bias. This makes it difficult for mKGC to achieve good results, particularly in the context of low-resource languages. To overcome previous problems, this paper introduces global and local knowledge constraints for mKGC. The former is used to constrain the reasoning of answer entities , while the latter is used to enhance the representation of query contexts. The proposed method makes the pretrained model better adapt to the mKGC task. Experimental results on public datasets demonstrate that our method outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32{\%} and 16.03{\%}, which indicates that our proposed method has significant enhancement on mKGC."
}

@inproceedings{xu-etal-2023-pre,
    title = "Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion",
	author = "Xu, Wenjie  and
      Liu, Ben  and
      Peng, Miao  and
      Jia, Xu  and
      Peng, Min",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.493/",
	doi = "10.18653/v1/2023.findings-acl.493",
	pages = "7790--7803",
	abstract = "Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that our model has great competitiveness compared to other models with four metrics. Our model can effectively incorporate information from temporal knowledge graphs into the language models."
}

@inproceedings{zhao-etal-2023-alignment,
    title = "From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment",
	author = "Zhao, Yu  and
      Wu, Yike  and
      Cai, Xiangrui  and
      Zhang, Ying  and
      Zhang, Haiwei  and
      Yuan, Xiaojie",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.559/",
	doi = "10.18653/v1/2023.findings-acl.559",
	pages = "8795--8806",
	abstract = "Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explicitly models the fine-grained interaction between original entity information. The experiments on five cross-lingual EA datasets show that our approach outperforms the state-of-the-art EA methods and enables the mutual enhancement of the heterogeneous information. Codes are available at \url{https://github.com/OreOZhao/TEA}."
}

@inproceedings{jiang-etal-2023-text,
    title = "Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models",
	author = "Jiang, Pengcheng  and
      Agarwal, Shivam  and
      Jin, Bowen  and
      Wang, Xuan  and
      Sun, Jimeng  and
      Han, Jiawei",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.709/",
	doi = "10.18653/v1/2023.findings-acl.709",
	pages = "11161--11180",
	abstract = "The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TagReal that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TagReal achieves state-of-the-art performance on two benchmark datasets. We find that TagReal has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods."
}

@inproceedings{chen-etal-2023-dipping,
    title = "Dipping {PLM}s Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting",
	author = "Chen, Chen  and
      Wang, Yufei  and
      Sun, Aixin  and
      Li, Bing  and
      Lam, Kwok-Yan",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.729/",
	doi = "10.18653/v1/2023.findings-acl.729",
	pages = "11489--11503",
	abstract = "Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG, and (iii) the flexibility of CSProm-KG."
}

@inproceedings{banerjee-etal-2023-role,
    title = "The Role of Output Vocabulary in {T}2{T} {LM}s for {SPARQL} Semantic Parsing",
	author = "Banerjee, Debayan  and
      Nair, Pranav  and
      Usbeck, Ricardo  and
      Biemann, Chris",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.774/",
	doi = "10.18653/v1/2023.findings-acl.774",
	pages = "12219--12228",
	abstract = "In this work, we analyse the role of output vocabulary for text-to-text (T2T) models on the task of SPARQL semantic parsing. We perform experiments within the the context of knowledge graph question answering (KGQA), where the task is to convert questions in natural language to the SPARQL query language. We observe that the query vocabulary is distinct from human vocabulary. Language Models (LMs) are pre-dominantly trained for human language tasks, and hence, if the query vocabulary is replaced with a vocabulary more attuned to the LM tokenizer, the performance of models may improve. We carry out carefully selected vocabulary substitutions on the queries and find absolute gains in the range of 17{\%} on the GrailQA dataset."
}

@inproceedings{lefebvre-stoehr-2023-rethinking,
    title = "Rethinking the Event Coding Pipeline with Prompt Entailment",
	author = "Lefebvre, Cl{\'e}ment  and
      Stoehr, Niklas",
	editor = "Akhtar, Mubashara  and
      Aly, Rami  and
      Christodoulopoulos, Christos  and
      Cocarascu, Oana  and
      Guo, Zhijiang  and
      Mittal, Arpit  and
      Schlichtkrull, Michael  and
      Thorne, James  and
      Vlachos, Andreas",
	booktitle = "Proceedings of the Sixth Fact Extraction and VERification Workshop (FEVER)",
	month = may,
	year = "2023",
	address = "Dubrovnik, Croatia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.fever-1.1/",
	doi = "10.18653/v1/2023.fever-1.1",
	pages = "1--16",
	abstract = "For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as {\textquotedblleft}Military injured two civilians{\textquotedblright} by a template, e.g. {\textquotedblleft}People were [Z]{\textquotedblright} and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select suitable answer candidates Zstar = {\textquotedblleft}injured{\textquotedblright}, {\textquotedblleft}hurt{\textquotedblright}... by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. In a final step, the selected answer candidate can be mapped to its corresponding event type. This allows domain experts to draft the codebook directly as labeled prompts and interpretable answer candidates. This human-in-the-loop process is guided by our codebook design tool. We show that our approach is robust through several checks: perturbing the event description and prompt template, restricting the vocabulary and removing contextual information."
}

@inproceedings{lee-etal-2023-temporal,
    title = "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
	author = "Lee, Dong-Ho  and
      Ahrabian, Kian  and
      Jin, Woojeong  and
      Morstatter, Fred  and
      Pujara, Jay",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.36/",
	doi = "10.18653/v1/2023.emnlp-main.36",
	pages = "544--557",
	abstract = "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to predict future facts using knowledge of past facts. In this paper, we develop an approach to use in-context learning (ICL) with large language models (LLMs) for TKG forecasting. Our extensive evaluation compares diverse baselines, including both simple heuristics and state-of-the-art (SOTA) supervised models, against pre-trained LLMs across several popular benchmarks and experimental settings. We observe that naive LLMs perform on par with SOTA models, which employ carefully designed architectures and supervised training for the forecasting task, falling within the (-3.6{\%}, +1.5{\%}) Hits@1 margin relative to the median performance. To better understand the strengths of LLMs for forecasting, we explore different approaches for selecting historical facts, constructing prompts, controlling information propagation, and parsing outputs into a probability distribution. A surprising finding from our experiments is that LLM performance endures ($\pm$0.4{\%} Hit@1) even when semantic information is removed by mapping entities/relations to arbitrary numbers, suggesting that prior semantic knowledge is unnecessary; rather, LLMs can leverage the symbolic patterns in the context to achieve such a strong performance. Our analysis also reveals that ICL enables LLMs to learn irregular patterns from the historical context, going beyond frequency and recency biases"
}

@inproceedings{hwang-etal-2023-knowledge,
    title = "Knowledge Graph Compression Enhances Diverse Commonsense Generation",
	author = "Hwang, EunJeong  and
      Thost, Veronika  and
      Shwartz, Vered  and
      Ma, Tengfei",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.37/",
	doi = "10.18653/v1/2023.emnlp-main.37",
	pages = "558--572",
	abstract = "Generating commonsense explanations requires reasoning about commonsense knowledge beyond what is explicitly mentioned in the context. Existing models use commonsense knowledge graphs such as ConceptNet to extract a subgraph of relevant knowledge pertaining to concepts in the input. However, due to the large coverage and, consequently, vast scale of ConceptNet, the extracted subgraphs may contain loosely related, redundant and irrelevant information, which can introduce noise into the model. We propose to address this by applying a differentiable graph compression algorithm that focuses on the relevant knowledge for the task. The compressed subgraphs yield considerably more diverse outputs when incorporated into models for the tasks of generating commonsense and abductive explanations. Moreover, our model achieves better quality-diversity tradeoff than a large language model with 100 times the number of parameters. Our generic approach can be applied to additional NLP tasks that can benefit from incorporating external knowledge."
}

@inproceedings{conia-etal-2023-increasing,
    title = "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs",
	author = "Conia, Simone  and
      Li, Min  and
      Lee, Daniel  and
      Minhas, Umar  and
      Ilyas, Ihab  and
      Li, Yunyao",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.100/",
	doi = "10.18653/v1/2023.emnlp-main.100",
	pages = "1612--1634",
	abstract = "Recent work in Natural Language Processing and Computer Vision has been using textual information {--} e.g., entity names and descriptions {--} available in knowledge graphs to ground neural models to high-quality structured data. However, when it comes to non-English languages, the quantity and quality of textual information are comparatively scarce. To address this issue, we introduce the novel task of automatic Knowledge Graph Completion (KGE) and perform a thorough investigation on bridging the gap in both the quantity and quality of textual information between English and non-English languages. More specifically, we: i) bring to light the problem of increasing multilingual coverage and precision of entity names and descriptions in Wikidata; ii) demonstrate that state-of-the-art methods, namely, Machine Translation (MT), Web Search (WS), and Large Language Models (LLMs), struggle with this task; iii) present M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information; and, iv) study the impact of increasing multilingual coverage and precision of non-English textual information in Entity Linking, Knowledge Graph Completion, and Question Answering. As part of our effort towards better multilingual knowledge graphs, we also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE approaches in 10 languages across 7 language families."
}

@inproceedings{jiang-etal-2023-reasoninglm,
    title = "{R}easoning{LM}: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
	author = "Jiang, Jinhao  and
      Zhou, Kun  and
      Zhao, Xin  and
      Li, Yaliang  and
      Wen, Ji-Rong",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.228/",
	doi = "10.18653/v1/2023.emnlp-main.228",
	pages = "3721--3735",
	abstract = "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph (KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model (PLM) to model the question, and a graph neural network (GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at https://github.com/RUCAIBox/ReasoningLM."
}

@inproceedings{schulhoff-etal-2023-ignore,
    title = "Ignore This Title and {H}ack{AP}rompt: Exposing Systemic Vulnerabilities of {LLM}s Through a Global Prompt Hacking Competition",
	author = "Schulhoff, Sander  and
      Pinto, Jeremy  and
      Khan, Anaum  and
      Bouchard, Louis-Fran{\c{c}}ois  and
      Si, Chenglei  and
      Anati, Svetlina  and
      Tagliabue, Valen  and
      Kost, Anson  and
      Carnahan, Christopher  and
      Boyd-Graber, Jordan",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.302/",
	doi = "10.18653/v1/2023.emnlp-main.302",
	pages = "4945--4977",
	abstract = "Large Language Models (LLMs) are increasingly being deployed in interactive contexts that involve direct user engagement, such as chatbots and writing assistants. These deployments are increasingly plagued by prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and instead follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of a large-scale resource and quantitative study on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive ontology of the types of adversarial prompts."
}

@inproceedings{yang-etal-2023-rumor,
    title = "Rumor Detection on Social Media with Crowd Intelligence and {C}hat{GPT}-Assisted Networks",
	author = "Yang, Chang  and
      Zhang, Peng  and
      Qiao, Wenbo  and
      Gao, Hui  and
      Zhao, Jiaming",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.347/",
	doi = "10.18653/v1/2023.emnlp-main.347",
	pages = "5705--5717",
	abstract = "In the era of widespread dissemination through social media, the task of rumor detection plays a pivotal role in establishing a trustworthy and reliable information environment. Nonetheless, existing research on rumor detection confronts several challenges: the limited expressive power of text encoding sequences, difficulties in domain knowledge coverage and effective information extraction with knowledge graph-based methods, and insufficient mining of semantic structural information. To address these issues, we propose a Crowd Intelligence and ChatGPT-Assisted Network(CICAN) for rumor classification. Specifically, we present a crowd intelligence-based semantic feature learning module to capture textual content`s sequential and hierarchical features. Then, we design a knowledge-based semantic structural mining module that leverages ChatGPT for knowledge enhancement. Finally, we construct an entity-sentence heterogeneous graph and design Entity-Aware Heterogeneous Attention to effectively integrate diverse structural information meta-paths. Experimental results demonstrate that CICAN achieves performance improvement in rumor detection tasks, validating the effectiveness and rationality of using large language models as auxiliary tools."
}

@inproceedings{kumar-schockaert-2023-solving,
    title = "Solving Hard Analogy Questions with Relation Embedding Chains",
	author = "Kumar, Nitesh  and
      Schockaert, Steven",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.382/",
	doi = "10.18653/v1/2023.emnlp-main.382",
	pages = "6224--6236",
	abstract = "Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions."
}

@inproceedings{qi-etal-2023-investigation,
    title = "An Investigation of {LLM}s' Inefficacy in Understanding Converse Relations",
	author = "Qi, Chengwen  and
      Li, Bowen  and
      Hui, Binyuan  and
      Wang, Bailin  and
      Li, Jinyang  and
      Wu, Jinwang  and
      Laili, Yuanjun",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.429/",
	pages = "6932--6953",
	abstract = "Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark."
}

@inproceedings{zhao-etal-2023-structure,
    title = "Structure-aware Knowledge Graph-to-text Generation with Planning Selection and Similarity Distinction",
	author = "Zhao, Feng  and
      Zou, Hongzhi  and
      Yan, Cheng",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.537/",
	doi = "10.18653/v1/2023.emnlp-main.537",
	pages = "8693--8703",
	abstract = "The knowledge graph-to-text (KG-to-text) generation task aims to synthesize coherent and engaging sentences that accurately convey the complex information derived from an input knowledge graph. One of the primary challenges in this task is bridging the gap between the diverse structures of the KG and the target text, while preserving the details of the input KG. To address this, we propose a novel approach that efficiently integrates graph structure-aware modules with pre-trained language models. Unlike conventional techniques, which only consider direct connections between first-order neighbors, our method delves deeper by incorporating Relative Distance Encoding as a bias within the graph structure-aware module. This enables our model to better capture the intricate topology information present in the KG. To further elevate the fidelity of the generated text, Planning Selection and Similarity Distinction are introduced. Our approach filters the most relevant linearized sequences by employing a planning scorer, while simultaneously distinguishing similar input KGs through contrastive learning techniques. Experiments on two datasets demonstrate the superiority of our model."
}

@inproceedings{shi-etal-2023-hallucination,
    title = "Hallucination Mitigation in Natural Language Generation from Large-Scale Open-Domain Knowledge Graphs",
	author = "Shi, Xiao  and
      Zhu, Zhengyuan  and
      Zhang, Zeyu  and
      Li, Chengkai",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.770/",
	doi = "10.18653/v1/2023.emnlp-main.770",
	pages = "12506--12521",
	abstract = "In generating natural language descriptions for knowledge graph triples, prior works used either small-scale, human-annotated datasets or datasets with limited variety of graph shapes, e.g., those having mostly star graphs. Graph-to-text models trained and evaluated on such datasets are largely not assessed for more realistic large-scale, open-domain settings. We introduce a new dataset, GraphNarrative, to fill this gap. Fine-tuning transformer-based pre-trained language models has achieved state-of-the-art performance among graph-to-text models. However, this method suffers from information hallucination{---}the generated text may contain fabricated facts not present in input graphs. We propose a novel approach that, given a graph-sentence pair in GraphNarrative, trims the sentence to eliminate portions that are not present in the corresponding graph, by utilizing the sentence`s dependency parse tree. Our experiment results verify this approach using models trained on GraphNarrative and existing datasets. The dataset, source code, and trained models are released at https://github.com/idirlab/graphnarrator."
}

@inproceedings{kim-etal-2023-soda,
    title = "{SODA}: Million-scale Dialogue Distillation with Social Commonsense Contextualization",
	author = "Kim, Hyunwoo  and
      Hessel, Jack  and
      Jiang, Liwei  and
      West, Peter  and
      Lu, Ximing  and
      Yu, Youngjae  and
      Zhou, Pei  and
      Bras, Ronan  and
      Alikhani, Malihe  and
      Kim, Gunhee  and
      Sap, Maarten  and
      Choi, Yejin",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.799/",
	doi = "10.18653/v1/2023.emnlp-main.799",
	pages = "12930--12949",
	abstract = "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public."
}

@inproceedings{zeng-etal-2023-iekg,
    title = "{IEKG}: A Commonsense Knowledge Graph for Idiomatic Expressions",
	author = "Zeng, Ziheng  and
      Cheng, Kellen  and
      Nanniyur, Srihari  and
      Zhou, Jianing  and
      Bhat, Suma",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.881/",
	doi = "10.18653/v1/2023.emnlp-main.881",
	pages = "14243--14264",
	abstract = "Idiomatic expression (IE) processing and comprehension have challenged pre-trained language models (PTLMs) because their meanings are non-compositional. Unlike prior works that enable IE comprehension through fine-tuning PTLMs with sentences containing IEs, in this work, we construct IEKG, a commonsense knowledge graph for figurative interpretations of IEs. This extends the established ${ATOMIC}_{20}^{20}$ converting PTLMs into knowledge models (KMs) that encode and infer commonsense knowledge related to IE use. Experiments show that various PTLMs can be converted into KMs with IEKG. We verify the quality of IEKG and the ability of the trained KMs with automatic and human evaluation. Through applications in natural language understanding, we show that a PTLM injected with knowledge from IEKG exhibits improved IE comprehension ability and can generalize to IEs unseen during training."
}

@inproceedings{liang-etal-2023-hi,
    title = "Hi-{A}r{G}: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining",
	author = "Liang, Jingcong  and
      Ye, Rong  and
      Han, Meng  and
      Zhang, Qi  and
      Lai, Ruofei  and
      Zhang, Xinyu  and
      Cao, Zhao  and
      Huang, Xuanjing  and
      Wei, Zhongyu",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.902/",
	doi = "10.18653/v1/2023.emnlp-main.902",
	pages = "14606--14620",
	abstract = "The knowledge graph is a structure to store and represent knowledge, and recent studies have discussed its capability to assist language models for various applications. Some variations of knowledge graphs aim to record arguments and their relations for computational argumentation tasks. However, many must simplify semantic types to fit specific schemas, thus losing flexibility and expression ability. In this paper, we propose the **Hi**erarchical **Ar**gumentation **G**raph (Hi-ArG), a new structure to organize arguments. We also introduce two approaches to exploit Hi-ArG, including a text-graph multi-modal model GreaseArG and a new pre-training framework augmented with graph information. Experiments on two argumentation tasks have shown that after further pre-training and fine-tuning, GreaseArG supersedes same-scale language models on these tasks, while incorporating graph information during further pre-training can also improve the performance of vanilla language models. Code for this paper is available at {\ensuremath{<}}https://github.com/ljcleo/Hi-ArG{\ensuremath{>}}."
}

@inproceedings{zhang-etal-2023-learning-knowledge,
    title = "Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding",
	author = "Zhang, Taolin  and
      Xu, Ruyao  and
      Wang, Chengyu  and
      Duan, Zhongjie  and
      Chen, Cen  and
      Qiu, Minghui  and
      Cheng, Dawei  and
      He, Xiaofeng  and
      Qian, Weining",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.969/",
	doi = "10.18653/v1/2023.emnlp-main.969",
	pages = "15663--15676",
	abstract = "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the performance of various downstream NLP tasks by injecting knowledge facts from large-scale Knowledge Graphs (KGs). However, existing methods for pre-training KEPLMs with relational triples are difficult to be adapted to close domains due to the lack of sufficient domain graph semantics. In this paper, we propose a Knowledge-enhanced language representation learning framework for various closed domains (KANGAROO) via capturing the implicit graph structure among the entities. Specifically, since the entity coverage rates of closed-domain KGs can be relatively low and may exhibit the global sparsity phenomenon for knowledge injection, we consider not only the shallow relational representations of triples but also the hyperbolic embeddings of deep hierarchical entity-class structures for effective knowledge fusion. Moreover, as two closed-domain entities under the same entity-class often havel locally dense neighbor subgraphs counted by max point biconnected component, we further propose a data augmentation strategy based on contrastive learning over subgraphs to construct hard negative samples of higher quality. It makes the underlying KELPMs better distinguish the semantics of these neighboring entities to further complement the global semantic sparsity. In the experiments, we evaluate KANGAROO over various knowledge-aware and general NLP tasks in both full and few-shot learning settings, outperforming various KEPLM training paradigms performance in closed-domains significantly."
}

@inproceedings{zhao-etal-2023-anytod,
    title = "{A}ny{TOD}: A Programmable Task-Oriented Dialog System",
	author = "Zhao, Jeffrey  and
      Cao, Yuan  and
      Gupta, Raghav  and
      Lee, Harrison  and
      Rastogi, Abhinav  and
      Wang, Mingqiu  and
      Soltau, Hagen  and
      Shafran, Izhak  and
      Wu, Yonghui",
	editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-main.1006/",
	doi = "10.18653/v1/2023.emnlp-main.1006",
	pages = "16189--16204",
	abstract = "We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, AnyTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions AnyTOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ. In addition, we release STARv2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models."
}

@inproceedings{yu-etal-2023-documentnet,
    title = "{D}ocument{N}et: Bridging the Data Gap in Document Pre-training",
	author = "Yu, Lijun  and
      Miao, Jin  and
      Sun, Xiaoyu  and
      Chen, Jiayi  and
      Hauptmann, Alexander  and
      Dai, Hanjun  and
      Wei, Wei",
	editor = "Wang, Mingxuan  and
      Zitouni, Imed",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.emnlp-industry.66/",
	doi = "10.18653/v1/2023.emnlp-industry.66",
	pages = "707--722",
	abstract = "Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multimodal capabilities for VDER."
}

@inproceedings{jiang-etal-2023-combo,
    title = "{COMBO}: A Complete Benchmark for Open {KG} Canonicalization",
	author = "Jiang, Chengyue  and
      Jiang, Yong  and
      Wu, Weiqi  and
      Zheng, Yuting  and
      Xie, Pengjun  and
      Tu, Kewei",
	editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
	booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
	month = may,
	year = "2023",
	address = "Dubrovnik, Croatia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.eacl-main.26/",
	doi = "10.18653/v1/2023.eacl-main.26",
	pages = "340--357",
	abstract = "Open knowledge graph (KG) consists of (subject, relation, object) triples extracted from millions of raw text. The subject and object noun phrases and the relation in open KG have severe redundancy and ambiguity and need to be canonicalized. Existing datasets for open KG canonicalization only provide gold entity-level canonicalization for noun phrases. In this paper, we present COMBO, a Complete Benchmark for Open KG canonicalization. Compared with existing datasets, we additionally provide gold canonicalization for relation phrases, gold ontology-level canonicalization for noun phrases, as well as source sentences from which triples are extracted. We also propose metrics for evaluating each type of canonicalization. On the COMBO dataset, we empirically compare previously proposed canonicalization methods as well as a few simple baseline methods based on pretrained language models. We find that properly encoding the phrases in a triple using pretrained language models results in better relation canonicalization and ontology-level canonicalization of the noun phrase. We release our dataset, baselines, and evaluation scripts at path/to/url."
}

@inproceedings{kasner-etal-2023-mind,
    title = "Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models",
	author = "Kasner, Zden{\v{e}}k  and
      Konstas, Ioannis  and
      Dusek, Ondrej",
	editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
	booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
	month = may,
	year = "2023",
	address = "Dubrovnik, Croatia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.eacl-main.176/",
	doi = "10.18653/v1/2023.eacl-main.176",
	pages = "2398--2415",
	abstract = "Pretrained language models (PLMs) for data-to-text (D2T) generation can use human-readable data labels such as column headings, keys, or relation names to generalize to out-of-domain examples. However, the models are well-known in producing semantically inaccurate outputs if these labels are ambiguous or incomplete, which is often the case in D2T datasets. In this paper, we expose this issue on the task of descibing a relation between two entities. For our experiments, we collect a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We find that although PLMs for D2T generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains."
}

@inproceedings{ryu-etal-2023-minimal,
    title = "A Minimal Approach for Natural Language Action Space in Text-based Games",
	author = "Ryu, Dongwon  and
      Fang, Meng  and
      Haffari, Gholamreza  and
      Pan, Shirui  and
      Shareghi, Ehsan",
	editor = "Jiang, Jing  and
      Reitter, David  and
      Deng, Shumin",
	booktitle = "Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)",
	month = dec,
	year = "2023",
	address = "Singapore",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.conll-1.10/",
	doi = "10.18653/v1/2023.conll-1.10",
	pages = "138--154",
	abstract = "Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose $\epsilon$-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces."
}

@inproceedings{han-etal-2023-investigating,
    title = "Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning",
	author = "Han, Lifeng  and
      Erofeev, Gleb  and
      Sorokina, Irina  and
      Gladkoff, Serge  and
      Nenadic, Goran",
	editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna",
	booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.clinicalnlp-1.5/",
	doi = "10.18653/v1/2023.clinicalnlp-1.5",
	pages = "31--40",
	abstract = "Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI`s MMPLMs {\textquotedblleft}wmt21-dense-24-wide-en-X and X-en (WMT21fb){\textquotedblright} which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language pair which \textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly.We prepare carefully aligned \textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge.Our experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain EN-ES segments for three sub-task translation testings: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training.To the best of our knowledge, this is the first work on using MMPLMs towards \textit{clinical domain transfer-learning NMT} successfully for totally unseen languages during pre-training."
}

@inproceedings{naseem-etal-2023-reducing,
    title = "Reducing Knowledge Noise for Improved Semantic Analysis in Biomedical Natural Language Processing Applications",
	author = "Naseem, Usman  and
      Thapa, Surendrabikram  and
      Zhang, Qi  and
      Hu, Liang  and
      Masood, Anum  and
      Nasim, Mehwish",
	editor = "Naumann, Tristan  and
      Ben Abacha, Asma  and
      Bethard, Steven  and
      Roberts, Kirk  and
      Rumshisky, Anna",
	booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.clinicalnlp-1.32/",
	doi = "10.18653/v1/2023.clinicalnlp-1.32",
	pages = "272--277",
	abstract = "Graph-based techniques have gained traction for representing and analyzing data in various natural language processing (NLP) tasks. Knowledge graph-based language representation models have shown promising results in leveraging domain-specific knowledge for NLP tasks, particularly in the biomedical NLP field. However, such models have limitations, including knowledge noise and neglect of contextual relationships, leading to potential semantic errors and reduced accuracy. To address these issues, this paper proposes two novel methods. The first method combines knowledge graph-based language model with nearest-neighbor models to incorporate semantic and category information from neighboring instances. The second method involves integrating knowledge graph-based language model with graph neural networks (GNNs) to leverage feature information from neighboring nodes in the graph. Experiments on relation extraction (RE) and classification tasks in English and Chinese language datasets demonstrate significant performance improvements with both methods, highlighting their potential for enhancing the performance of language models and improving NLP applications in the biomedical domain."
}

@inproceedings{chen-etal-2023-da,
    title = "大模型与知识图谱(Large Language Models and Knowledge Graphs)",
	author = "Chen, Yubo  and
      Guo, Shaoru  and
      Liu, Kang  and
      Zhao, Jun",
	editor = "Zhang, Jiajun",
	booktitle = "Proceedings of the 22nd Chinese National Conference on Computational Linguistics (Volume 2: Frontier Forum)",
	month = aug,
	year = "2023",
	address = "Harbin, China",
	publisher = "Chinese Information Processing Society of China",
	url = "https://aclanthology.org/2023.ccl-2.6/",
	pages = "67--76",
	language = "zho",
	abstract = "{\textquotedblleft}知识图谱作为一种重要的知识组织形式,常被视为下一代人工智能技术的基础设施之一,引起了工业界和学术界的广泛关注。传统知识图谱表示方法主要使用符号显式地描述概念及其之间的结构关系,具有语义清晰和可解释性好等特点,但其知识类型有限,难以应对开放域应用场景。随着大规模预训练语言模型(大模型)的发展,将参数化的大模型视为知识图谱成为研究热点。在这一背景下,本文聚焦于大模型在知识图谱生命周期中的研究,总结分析了大模型在知识建模、知识获取、知识融合、知识管理、知识推理和知识应用等环节中的研究进展。最后,对大模型与知识图谱未来发展趋势予以展望。{\textquotedblright}"
}

@inproceedings{yashen-etal-2023-terl,
    title = "{TERL}: Transformer Enhanced Reinforcement Learning for Relation Extraction",
	author = "Yashen, Wang  and
      Tuo, Shi  and
      Xiaoye, Ouyang  and
      Dayu, Guo",
	editor = "Sun, Maosong  and
      Qin, Bing  and
      Qiu, Xipeng  and
      Jiang, Jing  and
      Han, Xianpei",
	booktitle = "Proceedings of the 22nd Chinese National Conference on Computational Linguistics",
	month = aug,
	year = "2023",
	address = "Harbin, China",
	publisher = "Chinese Information Processing Society of China",
	url = "https://aclanthology.org/2023.ccl-1.58/",
	pages = "677--688",
	language = "eng",
	abstract = "{\textquotedblleft}Relation Extraction (RE) task aims to discover the semantic relation that holds between two entitiesand contributes to many applications such as knowledge graph construction and completion. Reinforcement Learning (RL) has been widely used for RE task and achieved SOTA results, whichare mainly designed with rewards to choose the optimal actions during the training procedure,to improve RE`s performance, especially for low-resource conditions. Recent work has shownthat offline or online RL can be flexibly formulated as a sequence understanding problem andsolved via approaches similar to large-scale pre-training language modeling. To strengthen theability for understanding the semantic signals interactions among the given text sequence, thispaper leverages Transformer architecture for RL-based RE methods, and proposes a genericframework called Transformer Enhanced RL (TERL) towards RE task. Unlike prior RL-basedRE approaches that usually fit value functions or compute policy gradients, TERL only outputsthe best actions by utilizing a masked Transformer. Experimental results show that the proposedTERL framework can improve many state-of-the-art RL-based RE methods.{\textquotedblright}"
}

@inproceedings{xiong-etal-2023-enhancing,
    title = "Enhancing Ontology Knowledge for Domain-Specific Joint Entity and Relation Extraction",
	author = "Xiong, Xiong  and
      Chen, Wang  and
      Yunfei, Liu  and
      Shengyang, Li",
	editor = "Sun, Maosong  and
      Qin, Bing  and
      Qiu, Xipeng  and
      Jiang, Jing  and
      Han, Xianpei",
	booktitle = "Proceedings of the 22nd Chinese National Conference on Computational Linguistics",
	month = aug,
	year = "2023",
	address = "Harbin, China",
	publisher = "Chinese Information Processing Society of China",
	url = "https://aclanthology.org/2023.ccl-1.61/",
	pages = "713--725",
	language = "eng",
	abstract = "{\textquotedblleft}Pre-trained language models (PLMs) have been widely used in entity and relation extractionmethods in recent years. However, due to the semantic gap between general-domain text usedfor pre-training and domain-specific text, these methods encounter semantic redundancy anddomain semantics insufficiency when it comes to domain-specific tasks. To mitigate this issue,we propose a low-cost and effective knowledge-enhanced method to facilitate domain-specificsemantics modeling in joint entity and relation extraction. Precisely, we use ontology and entitytype descriptions as domain knowledge sources, which are encoded and incorporated into thedownstream entity and relation extraction model to improve its understanding of domain-specificinformation. We construct a dataset called SSUIE-RE for Chinese entity and relation extractionin space science and utilization domain of China Manned Space Engineering, which contains awealth of domain-specific knowledge. The experimental results on SSUIE-RE demonstrate theeffectiveness of our method, achieving a 1.4{\%} absolute improvement in relation F1 score overprevious best approach. Introduction{\textquotedblright}"
}

@inproceedings{mbouadeu-etal-2023-evaluation,
    title = "An Evaluation Framework for Mapping News Headlines to Event Classes in a Knowledge Graph",
	author = "Mbouadeu, Steve Fonin  and
      Lorenzo, Martin  and
      Barker, Ken  and
      Hassanzadeh, Oktie",
	editor = {H{\"u}rriyeto{\u{g}}lu, Ali  and
      Tanev, Hristo  and
      Zavarella, Vanni  and
      Yeniterzi, Reyyan  and
      Y{\"o}r{\"u}k, Erdem  and
      Slavcheva, Milena},
	booktitle = "Proceedings of the 6th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text",
	month = sep,
	year = "2023",
	address = "Varna, Bulgaria",
	publisher = "INCOMA Ltd., Shoumen, Bulgaria",
	url = "https://aclanthology.org/2023.case-1.6/",
	pages = "44--52",
	abstract = "Mapping ongoing news headlines to event-related classes in a rich knowledge base can be an important component in a knowledge-based event analysis and forecasting solution. In this paper, we present a methodology for creating a benchmark dataset of news headlines mapped to event classes in Wikidata, and resources for the evaluation of methods that perform the mapping. We use the dataset to study two classes of unsupervised methods for this task: 1) adaptations of classic entity linking methods, and 2) methods that treat the problem as a zero-shot text classification problem. For the first approach, we evaluate off-the-shelf entity linking systems. For the second approach, we explore a) pre-trained natural language inference (NLI) models, and b) pre-trained large generative language models. We present the results of our evaluation, lessons learned, and directions for future work. The dataset and scripts for evaluation are made publicly available."
}

@inproceedings{zanella-toussaint-2023-much,
    title = "How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?",
	author = "Zanella, Laura  and
      Toussaint, Yannick",
	editor = "Demner-fushman, Dina  and
      Ananiadou, Sophia  and
      Cohen, Kevin",
	booktitle = "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.bionlp-1.12/",
	doi = "10.18653/v1/2023.bionlp-1.12",
	pages = "145--155",
	abstract = "Biomedical event extraction can be divided into three main subtasks; (1) biomedical event trigger detection, (2) biomedical argument identification and (3) event construction. This work focuses in the two first subtasks. For the first subtask we analyze a set of transformer language models that are commonly used in the biomedical domain to evaluate and compare their capacity for event trigger detection. We fine-tune the models using seven manually annotated corpora to assess their performance in different biomedical subdomains. SciBERT emerged as the highest performing model, presenting a slight improvement compared to baseline models. Then, for the second subtask we construct a knowledge graph (KG) from the biomedical corpora and integrate its KG embeddings to SciBERT to enrich its semantic information. We demonstrate that adding the KG embeddings to the model improves the argument identification performance by around 20 {\%}, and by around 15 {\%} compared to two baseline models. Our results suggest that fine-tuning a transformer model that is pretrained from scratch with biomedical and general data allows to detect event triggers and identify arguments covering different biomedical subdomains, and therefore improving its generalization. Furthermore, the integration of KG embeddings into the model can significantly improve the performance of biomedical event argument identification, outperforming the results of baseline models."
}

@inproceedings{wang-etal-2023-prompt,
    title = "Prompt-based Zero-shot Text Classification with Conceptual Knowledge",
	author = "Wang, Yuqi  and
      Wang, Wei  and
      Chen, Qi  and
      Huang, Kaizhu  and
      Nguyen, Anh  and
      De, Suparna",
	editor = "Padmakumar, Vishakh  and
      Vallejo, Gisela  and
      Fu, Yao",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-srw.4/",
	doi = "10.18653/v1/2023.acl-srw.4",
	pages = "30--38",
	abstract = "In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of language-related issues, prompt-based approaches have been introduced, which are particularly useful in low-resource scenarios. However, existing approaches mostly rely on verbalizers to translate the predicted vocabulary to task-specific labels. The major limitations of this approach are the ignorance of potentially relevant domain-specific words and being biased by the pre-training data. To address these limitations, we propose a framework that incorporates conceptual knowledge for text classification in the extreme zero-shot setting. The framework includes prompt-based keyword extraction, weight assignment to each prompt keyword, and final representation estimation in the knowledge graph embedding space. We evaluated the method on four widely-used datasets for sentiment analysis and topic detection, demonstrating that it consistently outperforms recently-developed prompt-based approaches in the same experimental settings."
}

@inproceedings{he-etal-2023-buca,
    title = "{BUCA}: A Binary Classification Approach to Unsupervised Commonsense Question Answering",
	author = "He, Jie  and
      U, Simon  and
      Gutierrez-Basulto, Victor  and
      Pan, Jeff",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-short.33/",
	doi = "10.18653/v1/2023.acl-short.33",
	pages = "376--387",
	abstract = "Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry."
}

@inproceedings{feng-etal-2023-kalm,
    title = "{KALM}: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
	author = "Feng, Shangbin  and
      Tan, Zhaoxuan  and
      Zhang, Wenqian  and
      Lei, Zhenyu  and
      Tsvetkov, Yulia",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.118/",
	doi = "10.18653/v1/2023.acl-long.118",
	pages = "2116--2138",
	abstract = "With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts {---} from local (e.g., sentence), document-level, to global knowledge, to enable knowledge-rich and interpretable exchange across contexts. In addition, incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length. In light of these challenges, we propose KALM, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM firstly encodes long documents and knowledge graphs into the three knowledge-aware context representations. KALM then processes each context with context-specific layers. These context-specific layers are followed by a ContextFusion layer that facilitates knowledge exchange to derive an overarching document representation. Extensive experiments demonstrate that KALM achieves state-of-the-art performance on three long document understanding tasks across 6 datasets/settings. Further analyses reveal that the three knowledge-aware contexts are complementary and they all contribute to model performance, while the importance and information exchange patterns of different contexts vary on different tasks and datasets."
}

@inproceedings{tang-etal-2023-enhancing,
    title = "Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation",
	author = "Tang, Chen  and
      Zhang, Hongbo  and
      Loakman, Tyler  and
      Lin, Chenghua  and
      Guerin, Frank",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.253/",
	doi = "10.18653/v1/2023.acl-long.253",
	pages = "4604--4616",
	abstract = "Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at \url{https://github.com/tangg555/SaBART}."
}

@inproceedings{wang-etal-2023-query,
    title = "Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs",
	author = "Wang, Siyuan  and
      Wei, Zhongyu  and
      Han, Meng  and
      Fan, Zhihao  and
      Shan, Haijun  and
      Zhang, Qi  and
      Huang, Xuanjing",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.259/",
	doi = "10.18653/v1/2023.acl-long.259",
	pages = "4706--4718",
	abstract = "Logical reasoning over incomplete knowledge graphs to answer complex logical queries is a challenging task. With the emergence of new entities and relations in constantly evolving KGs, inductive logical reasoning over KGs has become a crucial problem. However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure. In this paper, we propose a structure-modeled textual encoding framework for inductive logical reasoning over KGs. It encodes linearized query structures and entities using pre-trained language models to find answers. For structure modeling of complex queries, we design stepwise instructions that implicitly prompt PLMs on the execution order of geometric operations in each query. We further separately model different geometric operations (i.e., projection, intersection, and union) on the representation space using a pre-trained encoder with additional attention and maxout layers to enhance structured modeling. We conduct experiments on two inductive logical reasoning datasets and three transductive datasets. The results demonstrate the effectiveness of our method on logical reasoning over KGs in both inductive and transductive settings."
}

@inproceedings{xu-etal-2023-unsupervised,
    title = "Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model",
	author = "Xu, Yi  and
      Sheng, Shuqian  and
      Qi, Jiexing  and
      Fu, Luoyi  and
      Lin, Zhouhan  and
      Wang, Xinbing  and
      Zhou, Chenghu",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.281/",
	doi = "10.18653/v1/2023.acl-long.281",
	pages = "5130--5144",
	abstract = "Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are two essential tasks for knowledge graphs. Existing unsupervised approaches become suitable candidates for jointly learning the two tasks due to their avoidance of using graph-text parallel data. However, they adopt multiple complex modules and still require entity information or relation type for training. To this end, we propose INFINITY, a simple yet effective unsupervised method with a unified pretrained language model that does not introduce external annotation tools or additional parallel information. It achieves fully unsupervised graph-text mutual conversion for the first time. Specifically, INFINITY treats both G2T and T2G as a bidirectional sequence generation task by fine-tuning only one pretrained seq2seq model. A novel back-translation-based framework is then designed to generate synthetic parallel data automatically. Besides, we investigate the impact of graph linearization and introduce the structure-aware fine-tuning strategy to alleviate possible performance deterioration via retaining structural information in graph sequences. As a fully unsupervised framework, INFINITY is empirically verified to outperform state-of-the-art baselines for G2T and T2G tasks. Additionally, we also devise a new training setting called cross learning for low-resource unsupervised information extraction."
}

@inproceedings{gao-etal-2023-peacok,
    title = "{P}ea{C}o{K}: Persona Commonsense Knowledge for Consistent and Engaging Narratives",
	author = "Gao, Silin  and
      Borges, Beatriz  and
      Oh, Soyoung  and
      Bayazit, Deniz  and
      Kanno, Saya  and
      Wakaki, Hiromi  and
      Mitsufuji, Yuki  and
      Bosselut, Antoine",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.362/",
	doi = "10.18653/v1/2023.acl-long.362",
	pages = "6569--6591",
	abstract = "Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understandhow the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also learn to maintain consistent speaker personas for themselves throughout the narrative, so that their counterparts feel involved in a realistic conversation or story. However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems (e.g., a singer is good at singing, and may have attended conservatoire). In this work, we construct a new large-scale persona commonsense knowledge graph, PeaCoK, containing {\textasciitilde}100K human-validated persona facts. Our knowledge graph schematizes five dimensions of persona knowledge identified in previous studies of human interactive behaviours, and distils facts in this schema from both existing commonsense knowledge graphs and large-scale pretrained language models. Our analysis indicates that PeaCoK contains rich and precise world persona inferences that help downstream systems generate more consistent and engaging narratives."
}

@inproceedings{kobayashi-etal-2023-pairspanbert,
    title = "{P}air{S}pan{BERT}: An Enhanced Language Model for Bridging Resolution",
	author = "Kobayashi, Hideo  and
      Hou, Yufang  and
      Ng, Vincent",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.383/",
	doi = "10.18653/v1/2023.acl-long.383",
	pages = "6931--6946",
	abstract = "We present PairSpanBERT, a SpanBERT-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either heuristically or via distance supervision with a knowledge graph. Despite the noise inherent in the automatically generated data, we achieve the best results reported to date on three evaluation datasets for bridging resolution when replacing SpanBERT with PairSpanBERT in a state-of-the-art resolver that jointly performs entity coreference resolution and bridging resolution."
}

@inproceedings{yuan-etal-2023-causality,
    title = "Causality-aware Concept Extraction based on Knowledge-guided Prompting",
	author = "Yuan, Siyu  and
      Yang, Deqing  and
      Liu, Jinxi  and
      Tian, Shuyu  and
      Liang, Jiaqing  and
      Xiao, Yanghua  and
      Xie, Rui",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.514/",
	doi = "10.18653/v1/2023.acl-long.514",
	pages = "9255--9272",
	abstract = "Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt can effectively alleviate concept bias and improve the performance of PLM-based CE models."
}

@inproceedings{baek-etal-2023-direct,
    title = "Direct Fact Retrieval from Knowledge Graphs without Entity Linking",
	author = "Baek, Jinheon  and
      Aji, Alham Fikri  and
      Lehmann, Jens  and
      Hwang, Sung Ju",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.558/",
	doi = "10.18653/v1/2023.acl-long.558",
	pages = "10038--10055",
	abstract = "There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the KGs given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval (DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our DiFaR framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach."
}

@inproceedings{wang-etal-2023-dynamic,
    title = "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering",
	author = "Wang, Yujie  and
      Zhang, Hu  and
      Liang, Jiye  and
      Li, Ru",
	editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
	booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.acl-long.785/",
	doi = "10.18653/v1/2023.acl-long.785",
	pages = "14048--14063",
	abstract = "Recently, knowledge graphs (KGs) have won noteworthy success in commonsense question answering. Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks. However, they ignore (i) optimizing the knowledge representation and structure of subgraphs and (ii) deeply fusing heterogeneous QA context with subgraphs. In this paper, we propose a dynamic heterogeneous-graph reasoning method with LMs and knowledge representation learning (DHLK), which constructs a heterogeneous knowledge graph (HKG) based on multiple knowledge sources and optimizes the structure and knowledge representation of the HKG using a two-stage pruning strategy and knowledge representation learning (KRL). It then performs joint reasoning by LMs and Relation Mask Self-Attention (RMSA). Specifically, DHLK filters key entities based on the dictionary vocabulary to achieve the first-stage pruning while incorporating the paraphrases in the dictionary into the subgraph to construct the HKG. Then, DHLK encodes and fuses the QA context and HKG using LM, and dynamically removes irrelevant KG entities based on the attention weights of LM for the second-stage pruning. Finally, DHLK introduces KRL to optimize the knowledge representation and perform answer reasoning on the HKG by RMSA.We evaluate DHLK at CommonsenseQA and OpenBookQA, and show its improvement on existing LM and LM+KG methods."
}

@inproceedings{deshpande-etal-2022-stereokg,
    title = "{S}tereo{KG}: Data-Driven Knowledge Graph Construction For Cultural Knowledge and Stereotypes",
	author = "Deshpande, Awantee  and
      Ruiter, Dana  and
      Mosbach, Marius  and
      Klakow, Dietrich",
	editor = "Narang, Kanika  and
      Mostafazadeh Davani, Aida  and
      Mathias, Lambert  and
      Vidgen, Bertie  and
      Talat, Zeerak",
	booktitle = "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)",
	month = jul,
	year = "2022",
	address = "Seattle, Washington (Hybrid)",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.woah-1.7/",
	doi = "10.18653/v1/2022.woah-1.7",
	pages = "67--78",
	abstract = "Analyzing ethnic or religious bias is important for improving fairness, accountability, and transparency of natural language processing models. However, many techniques rely on human-compiled lists of bias terms, which are expensive to create and are limited in coverage. In this study, we present a fully data-driven pipeline for generating a knowledge graph (KG) of cultural knowledge and stereotypes. Our resulting KG covers 5 religious groups and 5 nationalities and can easily be extended to more entities. Our human evaluation shows that the majority (59.2{\%}) of non-singleton entries are coherent and complete stereotypes. We further show that performing intermediate masked language model training on the verbalized KG leads to a higher level of cultural awareness in the model and has the potential to increase classification performance on knowledge-crucial samples on a related task, i.e., hate speech detection."
}

@inproceedings{han-etal-2022-examining,
    title = "Examining Large Pre-Trained Language Models for Machine Translation: What You Don`t Know about It",
	author = "Han, Lifeng  and
      Erofeev, Gleb  and
      Sorokina, Irina  and
      Gladkoff, Serge  and
      Nenadic, Goran",
	editor = {Koehn, Philipp  and
      Barrault, Lo{\"i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
	booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates (Hybrid)",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.wmt-1.84/",
	pages = "908--919",
	abstract = "Pre-trained language models (PLMs) often take advantage of the monolingual and multilingual dataset that is freely available online to acquire general or mixed domain knowledge before deployment into specific tasks. Extra-large PLMs (xLPLMs) are proposed very recently to claim supreme performances over smaller-sized PLMs such as in machine translation (MT) tasks. These xLPLMs include Meta-AI`s wmt21-dense-24-wide-en-X (2021) and NLLB (2022). In this work, we examine if xLPLMs are absolutely superior to smaller-sized PLMs in fine-tuning toward domain-specific MTs. We use two different in-domain data of different sizes: commercial automotive in-house data and clinical shared task data from the ClinSpEn2022 challenge at WMT2022. We choose the popular Marian Helsinki as smaller sized PLM and two massive-sized Mega-Transformers from Meta-AI as xLPLMs.Our experimental investigation shows that 1) on smaller-sized in-domain commercial automotive data, xLPLM wmt21-dense-24-wide-en-X indeed shows much better evaluation scores using SacreBLEU and hLEPOR metrics than smaller-sized Marian, even though its score increase rate is lower than Marian after fine-tuning; 2) on relatively larger-size well prepared clinical data fine-tuning, the xLPLM NLLB tends to lose its advantage over smaller-sized Marian on two sub-tasks (clinical terms and ontology concepts) using ClinSpEn offered metrics METEOR, COMET, and ROUGE-L, and totally lost to Marian on Task-1 (clinical cases) on all official metrics including SacreBLEU and BLEU; 3) metrics do not always agree with each other on the same tasks using the same model outputs; 4) clinic-Marian ranked No.2 on Task- 1 (via SacreBLEU/BLEU) and Task-3 (via METEOR and ROUGE) among all submissions."
}

@inproceedings{wold-2022-effectiveness,
    title = "The Effectiveness of Masked Language Modeling and Adapters for Factual Knowledge Injection",
	author = "Wold, Sondre",
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Valentino, Marco  and
      Thayaparan, Mokanarangan  and
      Nguyen, Thien Huu  and
      Penn, Gerald  and
      Ramesh, Arti  and
      Jana, Abhik",
	booktitle = "Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing",
	month = oct,
	year = "2022",
	address = "Gyeongju, Republic of Korea",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.textgraphs-1.6/",
	pages = "54--59",
	abstract = "This paper studies the problem of injecting factual knowledge into large pre-trained language models. We train adapter modules on parts of the ConceptNet knowledge graph using the masked language modeling objective and evaluate the success of the method by a series of probing experiments on the LAMA probe. Mean P@K curves for different configurations indicate that the technique is effective, increasing the performance on sub-sets of the LAMA probe for large values of k by adding as little as 2.1{\%} additional parameters to the original models."
}

@inproceedings{kovriguina-etal-2022-textgraphs,
    title = "{T}ext{G}raphs-16 Natural Language Premise Selection Task: Zero-Shot Premise Selection with Prompting Generative Language Models",
	author = "Kovriguina, Liubov  and
      Teucher, Roman  and
      Wardenga, Robert",
	editor = "Ustalov, Dmitry  and
      Gao, Yanjun  and
      Panchenko, Alexander  and
      Valentino, Marco  and
      Thayaparan, Mokanarangan  and
      Nguyen, Thien Huu  and
      Penn, Gerald  and
      Ramesh, Arti  and
      Jana, Abhik",
	booktitle = "Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing",
	month = oct,
	year = "2022",
	address = "Gyeongju, Republic of Korea",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.textgraphs-1.15/",
	pages = "127--132",
	abstract = "Automated theorem proving can benefit a lot from methods employed in natural language processing, knowledge graphs and information retrieval: this non-trivial task combines formal languages understanding, reasoning, similarity search. We tackle this task by enhancing semantic similarity ranking with prompt engineering, which has become a new paradigm in natural language understanding. None of our approaches requires additional training. Despite encouraging results reported by prompt engineering approaches for a range of NLP tasks, for the premise selection task vanilla re-ranking by prompting GPT-3 doesn`t outperform semantic similarity ranking with SBERT, but merging of the both rankings shows better results."
}

@article{liu-etal-2022-relational,
    title = "Relational Memory-Augmented Language Models",
	author = "Liu, Qi  and
      Yogatama, Dani  and
      Blunsom, Phil",
	editor = "Roark, Brian  and
      Nenkova, Ani",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "10",
	year = "2022",
	address = "Cambridge, MA",
	publisher = "MIT Press",
	url = "https://aclanthology.org/2022.tacl-1.32/",
	doi = "10.1162/tacl_a_00476",
	pages = "555--572",
	abstract = "We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms of perplexity and bits per character. We also show that relational memory improves coherence, is complementary to token-based memory, and enables causal interventions. Our model provides a simple yet effective way to combine an autoregressive language model and a knowledge graph for more coherent and logical generation."
}

@inproceedings{lin-etal-2022-gentus,
    title = "{G}en{TUS}: Simulating User Behaviour and Language in Task-oriented Dialogues with Generative Transformers",
	author = "Lin, Hsien-chin  and
      Geishauser, Christian  and
      Feng, Shutong  and
      Lubis, Nurul  and
      van Niekerk, Carel  and
      Heck, Michael  and
      Gasic, Milica",
	editor = "Lemon, Oliver  and
      Hakkani-Tur, Dilek  and
      Li, Junyi Jessy  and
      Ashrafzadeh, Arash  and
      Garcia, Daniel Hern{\'a}ndez  and
      Alikhani, Malihe  and
      Vandyke, David  and
      Du{\v{s}}ek, Ond{\v{r}}ej",
	booktitle = "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2022",
	address = "Edinburgh, UK",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.sigdial-1.28/",
	doi = "10.18653/v1/2022.sigdial-1.28",
	pages = "270--282",
	abstract = "User simulators (USs) are commonly used to train task-oriented dialogue systems via reinforcement learning. The interactions often take place on semantic level for efficiency, but there is still a gap from semantic actions to natural language, which causes a mismatch between training and deployment environment. Incorporating a natural language generation (NLG) module with USs during training can partly deal with this problem. However, since the policy and NLG of USs are optimised separately, these simulated user utterances may not be natural enough in a given context. In this work, we propose a generative transformer-based user simulator (GenTUS). GenTUS consists of an encoder-decoder structure, which means it can optimise both the user policy and natural language generation jointly. GenTUS generates both semantic actions and natural language utterances, preserving interpretability and enhancing language variation. In addition, by representing the inputs and outputs as word sequences and by using a large pre-trained language model we can achieve generalisability in feature representation. We evaluate GenTUS with automatic metrics and human evaluation. Our results show that GenTUS generates more natural language and is able to transfer to an unseen ontology in a zero-shot fashion. In addition, its behaviour can be further shaped with reinforcement learning opening the door to training specialised user simulators."
}

@inproceedings{vukovic-etal-2022-dialogue,
    title = "Dialogue Term Extraction using Transfer Learning and Topological Data Analysis",
	author = "Vukovic, Renato  and
      Heck, Michael  and
      Ruppik, Benjamin  and
      van Niekerk, Carel  and
      Zibrowius, Marcus  and
      Gasic, Milica",
	editor = "Lemon, Oliver  and
      Hakkani-Tur, Dilek  and
      Li, Junyi Jessy  and
      Ashrafzadeh, Arash  and
      Garcia, Daniel Hern{\'a}ndez  and
      Alikhani, Malihe  and
      Vandyke, David  and
      Du{\v{s}}ek, Ond{\v{r}}ej",
	booktitle = "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = sep,
	year = "2022",
	address = "Edinburgh, UK",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.sigdial-1.53/",
	doi = "10.18653/v1/2022.sigdial-1.53",
	pages = "564--581",
	abstract = "Goal oriented dialogue systems were originally designed as a natural language interface to a fixed data-set of entities that users might inquire about, further described by domain, slots and values. As we move towards adaptable dialogue systems where knowledge about domains, slots and values may change, there is an increasing need to automatically extract these terms from raw dialogues or related non-dialogue data on a large scale. In this paper, we take an important step in this direction by exploring different features that can enable systems to discover realisations of domains, slots and values in dialogues in a purely data-driven fashion. The features that we examine stem from word embeddings, language modelling features, as well as topological features of the word embedding space. To examine the utility of each feature set, we train a seed model based on the widely used MultiWOZ data-set. Then, we apply this model to a different corpus, the Schema-guided dialogue data-set. Our method outperforms the previously proposed approach that relies solely on word embeddings. We also demonstrate that each of the features is responsible for discovering different kinds of content. We believe our results warrant further research towards ontology induction, and continued harnessing of topological data analysis for dialogue and natural language processing research."
}

@inproceedings{gnehm-etal-2022-fine,
    title = "Fine-Grained Extraction and Classification of Skill Requirements in {G}erman-Speaking Job Ads",
	author = {Gnehm, Ann-sophie  and
      B{\"u}hlmann, Eva  and
      Buchs, Helen  and
      Clematide, Simon},
	editor = "Bamman, David  and
      Hovy, Dirk  and
      Jurgens, David  and
      Keith, Katherine  and
      O'Connor, Brendan  and
      Volkova, Svitlana",
	booktitle = "Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS)",
	month = nov,
	year = "2022",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.nlpcss-1.2/",
	doi = "10.18653/v1/2022.nlpcss-1.2",
	pages = "14--24",
	abstract = "Monitoring the development of labor market skill requirements is an information need that is more and more approached by applying text mining methods to job advertisement data. We present an approach for fine-grained extraction and classification of skill requirements from German-speaking job advertisements. We adapt pre-trained transformer-based language models to the domain and task of computing meaningful representations of sentences or spans. By using context from job advertisements and the large ESCO domain ontology we improve our similarity-based unsupervised multi-label classification results. Our best model achieves a mean average precision of 0.969 on the skill class level."
}

@inproceedings{li-etal-2022-knowledge,
    title = "Knowledge-Grounded Dialogue Generation with a Unified Knowledge Representation",
	author = "Li, Yu  and
      Peng, Baolin  and
      Shen, Yelong  and
      Mao, Yi  and
      Liden, Lars  and
      Yu, Zhou  and
      Gao, Jianfeng",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.naacl-main.15/",
	doi = "10.18653/v1/2022.naacl-main.15",
	pages = "206--218",
	abstract = "Knowledge-grounded dialogue systems are challenging to build due to the lack of training data and heterogeneous knowledge sources. Existing systems perform poorly on unseen topics due to limited topics covered in the training data. In addition, it is challenging to generalize to the domains that require different types of knowledge sources. To address the above challenges, we present PLUG, a language model that homogenizes different knowledge sources to a unified knowledge representation for knowledge-grounded dialogue generation tasks. We first retrieve relevant information from heterogeneous knowledge sources (e.g., wiki, dictionary, or knowledge graph); Then the retrieved knowledge is transformed into text and concatenated with dialogue history to feed into the language model for generating responses. PLUG is pre-trained on a large-scale knowledge-grounded dialogue corpus. The empirical evaluation on two benchmarks shows that PLUG generalizes well across different knowledge-grounded dialogue tasks. It achieves comparable performance with state-of-the-art methods in the fully-supervised setting and significantly outperforms other approaches in zero-shot and few-shot settings."
}

@inproceedings{bansal-etal-2022-cose,
    title = "{C}o{S}e-Co: Text Conditioned Generative {C}ommon{S}ense Contextualizer",
	author = "Bansal, Rachit  and
      Aggarwal, Milan  and
      Bhatia, Sumit  and
      Kaur, Jivat  and
      Krishnamurthy, Balaji",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.naacl-main.83/",
	doi = "10.18653/v1/2022.naacl-main.83",
	pages = "1128--1143",
	abstract = "Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task."
}

@inproceedings{moiseev-etal-2022-skill,
    title = "{SKILL}: Structured Knowledge Infusion for Large Language Models",
	author = "Moiseev, Fedor  and
      Dong, Zhe  and
      Alfonseca, Enrique  and
      Jaggi, Martin",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.naacl-main.113/",
	doi = "10.18653/v1/2022.naacl-main.113",
	pages = "1581--1588",
	abstract = "Large language models (LLMs) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into LLMs, by directly training T5 models on factual triples of knowledge graphs (KGs). We show that models pre-trained on Wikidata KG with our method outperform the T5 baselines on FreebaseQA and WikiHop, as well as the Wikidata-answerable subset of TriviaQA and NaturalQuestions. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size KG, WikiMovies, we saw 3x improvement of exact match score on MetaQA task. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs."
}

@inproceedings{west-etal-2022-symbolic,
    title = "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models",
	author = "West, Peter  and
      Bhagavatula, Chandra  and
      Hessel, Jack  and
      Hwang, Jena  and
      Jiang, Liwei  and
      Le Bras, Ronan  and
      Lu, Ximing  and
      Welleck, Sean  and
      Choi, Yejin",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.naacl-main.341/",
	doi = "10.18653/v1/2022.naacl-main.341",
	pages = "4602--4625",
	abstract = "The common practice for training commonsense models has gone from{--}human{--}to{--}corpus{--}to{--}machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from{--}machine{--}to{--}corpus{--}to{--}machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically{--}as text{--}in addition to the neural model. We distill only one aspect{--}the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model`s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models."
}

@inproceedings{sun-etal-2022-jointlk,
    title = "{J}oint{LK}: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering",
	author = "Sun, Yueqing  and
      Shi, Qi  and
      Qi, Le  and
      Zhang, Yu",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.naacl-main.372/",
	doi = "10.18653/v1/2022.naacl-main.372",
	pages = "5049--5060",
	abstract = "Existing KG-augmented models for commonsense question answering primarily focus on designing elaborate Graph Neural Networks (GNNs) to model knowledge graphs (KGs). However, they ignore (i) the effectively fusing and reasoning over question context representations and the KG representations, and (ii) automatically selecting relevant nodes from the noisy KGs during reasoning. In this paper, we propose a novel model, JointLK, which solves the above limitations through the joint reasoning of LM and GNN and the dynamic KGs pruning mechanism. Specifically, JointLK performs joint reasoning between LM and GNN through a novel dense bidirectional attention module, in which each question token attends on KG nodes and each KG node attends on question tokens, and the two modal representations fuse and update mutually by multi-step interactions. Then, the dynamic pruning module uses the attention weights generated by joint reasoning to prune irrelevant KG nodes recursively. We evaluate JointLK on the CommonsenseQA and OpenBookQA datasets, and demonstrate its improvements to the existing LM and LM+KG models, as well as its capability to perform interpretable reasoning."
}

@inproceedings{esmeir-etal-2022-entity,
    title = "Entity Retrieval from Multilingual Knowledge Graphs",
	author = "Esmeir, Saher  and
      C{\^a}mara, Arthur  and
      Meij, Edgar",
	editor = {Ataman, Duygu  and
      Gonen, Hila  and
      Ruder, Sebastian  and
      Firat, Orhan  and
      G{\"u}l Sahin, G{\"o}zde  and
      Mirzakhalov, Jamshidbek},
	booktitle = "Proceedings of the 2nd Workshop on Multi-lingual Representation Learning (MRL)",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates (Hybrid)",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.mrl-1.1/",
	doi = "10.18653/v1/2022.mrl-1.1",
	pages = "1--15",
	abstract = "Knowledge Graphs (KGs) are structured databases that capture real-world entities and their relationships. The task of entity retrieval from a KG aims at retrieving a ranked list of entities relevant to a given user query. While English-only entity retrieval has attracted considerable attention, user queries, as well as the information contained in the KG, may be represented in multiple{---}and possibly distinct{---}languages. Furthermore, KG content may vary between languages due to different information sources and points of view. Recent advances in language representation have enabled natural ways of bridging gaps between languages. In this paper, we therefore propose to utilise language models (LMs) and diverse entity representations to enable truly multilingual entity retrieval. We propose two approaches: (i) an array of monolingual retrievers and (ii) a single multilingual retriever, trained using queries and documents in multiple languages. We show that while our approach is on par with the significantly more complex state-of-the-art method for the English task, it can be successfully applied to virtually any language with a LM. Furthermore, it allows languages to benefit from one another, yielding significantly better performance, both for low- and high-resource languages."
}

@inproceedings{hudecek-etal-2022-unifying,
    title = "DIASER: A Unifying View On Task-oriented Dialogue Annotation",
	author = "Hude{\v{c}}ek, Vojt{\v{e}}ch  and
      Schaub, L{\'e}on-Paul  and
      Stancl, Daniel  and
      Paroubek, Patrick  and
      Du{\v{s}}ek, Ond{\v{r}}ej",
	editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
	booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
	month = jun,
	year = "2022",
	address = "Marseille, France",
	publisher = "European Language Resources Association",
	url = "https://aclanthology.org/2022.lrec-1.137/",
	pages = "1286--1296",
	abstract = "Every model is only as strong as the data that it is trained on. In this paper, we present a new dataset, obtained by merging four publicly available annotated corpora for task-oriented dialogues in several domains (MultiWOZ 2.2, CamRest676, DSTC2 and Schema-Guided Dialogue Dataset). This way, we assess the feasibility of providing a unified ontology and annotation schema covering several domains with a relatively limited effort. We analyze the characteristics of the resulting dataset along three main dimensions: language, information content and performance. We focus on aspects likely to be pertinent for improving dialogue success, e.g. dialogue consistency. Furthermore, to assess the usability of this new corpus, we thoroughly evaluate dialogue generation performance under various conditions with the help of two prominent recent end-to-end dialogue models: MarCo and GPT-2. These models were selected as popular open implementations representative of the two main dimensions of dialogue modelling. While we did not observe a significant gain for dialogue state tracking performance, we show that using more training data from different sources can improve language modelling capabilities and positively impact dialogue flow (consistency). In addition, we provide the community with one of the largest open dataset for machine learning experiments."
}

@inproceedings{hayashi-2022-towards,
    title = "Towards the Detection of a Semantic Gap in the Chain of Commonsense Knowledge Triples",
	author = "Hayashi, Yoshihiko",
	editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
	booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
	month = jun,
	year = "2022",
	address = "Marseille, France",
	publisher = "European Language Resources Association",
	url = "https://aclanthology.org/2022.lrec-1.424/",
	pages = "3984--3993",
	abstract = "A commonsense knowledge resource organizes common sense that is not necessarily correct all the time, but most people are expected to know or believe. Such knowledge resources have recently been actively built and utilized in artificial intelligence, particularly natural language processing. In this paper, we discuss an important but not significantly discussed the issue of semantic gaps potentially existing in a commonsense knowledge graph and propose a machine learning-based approach to detect a semantic gap that may inhibit the proper chaining of knowledge triples. In order to establish this line of research, we created a pilot dataset from ConceptNet, in which chains consisting of two adjacent triples are sampled, and the validity of each chain is human-annotated. We also devised a few baseline methods for detecting the semantic gaps and compared them in small-scale experiments. Although the experimental results suggest that the detection of semantic gaps may not be a trivial task, we achieved several insights to further push this research direction, including the potential efficacy of sense embeddings and contextualized word representations enabled by a pre-trained language model."
}

@inproceedings{papadopoulou-etal-2022-bootstrapping,
    title = "Bootstrapping Text Anonymization Models with Distant Supervision",
	author = "Papadopoulou, Anthi  and
      Lison, Pierre  and
      {\O}vrelid, Lilja  and
      Pil{\'a}n, Ildik{\'o}",
	editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
	booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
	month = jun,
	year = "2022",
	address = "Marseille, France",
	publisher = "European Language Resources Association",
	url = "https://aclanthology.org/2022.lrec-1.476/",
	pages = "4477--4487",
	abstract = "We propose a novel method to bootstrap text anonymization models based on distant supervision. Instead of requiring manually labeled training data, the approach relies on a knowledge graph expressing the background information assumed to be publicly available about various individuals. This knowledge graph is employed to automatically annotate text documents including personal data about a subset of those individuals. More precisely, the method determines which text spans ought to be masked in order to guarantee k-anonymity, assuming an adversary with access to both the text documents and the background information expressed in the knowledge graph. The resulting collection of labeled documents is then used as training data to fine-tune a pre-trained language model for text anonymization. We illustrate this approach using a knowledge graph extracted from Wikidata and short biographical texts from Wikipedia. Evaluation results with a RoBERTa-based model and a manually annotated collection of 553 summaries showcase the potential of the approach, but also unveil a number of issues that may arise if the knowledge graph is noisy or incomplete. The results also illustrate that, contrary to most sequence labeling problems, the text anonymization task may admit several alternative solutions."
}

@inproceedings{singh-etal-2022-massively,
    title = "Massively Multilingual Language Models for Cross Lingual Fact Extraction from Low Resource {I}ndian Languages",
	author = "Singh, Bhavyajeet  and
      Kandru, Siri Venkata Pavan Kumar  and
      Sharma, Anubhav  and
      Varma, Vasudeva",
	editor = "Akhtar, Md. Shad  and
      Chakraborty, Tanmoy",
	booktitle = "Proceedings of the 19th International Conference on Natural Language Processing (ICON)",
	month = dec,
	year = "2022",
	address = "New Delhi, India",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.icon-main.2/",
	pages = "11--18",
	abstract = "Massive knowledge graphs like Wikidata attempt to capture world knowledge about multiple entities. Recent approaches concentrate on automatically enriching these KGs from text. However a lot of information present in the form of natural text in low resource languages is often missed out. Cross Lingual Information Extraction aims at extracting factual information in the form of English triples from low resource Indian Language text. Despite its massive potential, progress made on this task is lagging when compared to Monolingual Information Extraction. In this paper, we propose the task of Cross Lingual Fact Extraction(CLFE) from text and devise an end-to-end generative approach for the same which achieves an overall F1 score of 77.46"
}

@inproceedings{agarwal-etal-2022-big,
    title = "There is No Big Brother or Small Brother:Knowledge Infusion in Language Models for Link Prediction and Question Answering",
	author = "Agarwal, Ankush  and
      Gawade, Sakharam  and
      Channabasavarajendra, Sachin  and
      Bhattacharya, Pushpak",
	editor = "Akhtar, Md. Shad  and
      Chakraborty, Tanmoy",
	booktitle = "Proceedings of the 19th International Conference on Natural Language Processing (ICON)",
	month = dec,
	year = "2022",
	address = "New Delhi, India",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.icon-main.26/",
	pages = "204--211",
	abstract = "The integration of knowledge graphs with deep learning is thriving in improving the performance of various natural language processing (NLP) tasks. In this paper, we focus on knowledge-infused link prediction and question answering using language models, T5, and BLOOM across three domains:Aviation, Movie, and Web. In this context, we infuse knowledge in large and small language models and study their performance, and find the performance to be similar. For the link prediction task on the Aviation Knowledge Graph, we obtain a 0.2 hits@1 score using T5-small, T5-base, T5-large, and BLOOM. Using template-based scripts, we create a set of 1 million synthetic factoid QA pairs in the aviation domain from National Transportation Safety Board (NTSB) reports. On our curated QA pairs, the three models of T5 achieve a 0.7 hits@1 score. We validate our findings with the paired student t test and Cohen`s kappa scores. For link prediction on Aviation Knowledge Graph using T5-small and T5-large, we obtain a Cohen`s kappa score of 0.76, showing substantial agreement between the models. Thus, we infer that small language models perform similar to large language models with the infusion of knowledge."
}

@inproceedings{yang-etal-2022-improving,
    title = "Improving Conversational Recommendation Systems' Quality with Context-Aware Item Meta-Information",
	author = "Yang, Bowen  and
      Han, Cong  and
      Li, Yu  and
      Zuo, Lei  and
      Yu, Zhou",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-naacl.4/",
	doi = "10.18653/v1/2022.findings-naacl.4",
	pages = "38--48",
	abstract = "A key challenge of Conversational Recommendation Systems (CRS) is to integrate the recommendation function and the dialog generation function smoothly. Previous works employ graph neural networks with external knowledge graphs (KG) to model individual recommendation items and integrate KGs with language models through attention mechanism for response generation. Although previous approaches prove effective, there is still room for improvement. For example, KG-based approaches only rely on entity relations and bag-of-words to recommend items and neglect the information in the conversational context. We propose to improve the usage of dialog context for both recommendation and response generation using an encoding architecture along with the self-attention mechanism of transformers. In this paper, we propose a simple yet effective architecture comprising a pre-trained language model (PLM) and an item metadata encoder to integrate the recommendation and the dialog generation better. The proposed item encoder learns to map item metadata to embeddings reflecting the rich information of the item, which can be matched with dialog context. The PLM then consumes the context-aware item embeddings and dialog context to generate high-quality recommendations and responses. Experimental results on the benchmark dataset ReDial show that our model obtains state-of-the-art results on both recommendation and response generation tasks."
}

@inproceedings{li-etal-2022-instilling,
    title = "Instilling Type Knowledge in Language Models via Multi-Task {QA}",
	author = "Li, Shuyang  and
      Sridhar, Mukund  and
      Satya Prakash, Chandana  and
      Cao, Jin  and
      Hamza, Wael  and
      McAuley, Julian",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-naacl.45/",
	doi = "10.18653/v1/2022.findings-naacl.45",
	pages = "594--603",
	abstract = "Understanding human language often necessitates understanding entities and their place in a taxonomy of knowledge{---}their \textit{types}.Previous methods to learn entity types rely on training classifiers on datasets with coarse, noisy, and incomplete labels. We introduce a method to instill fine-grained type knowledge in language models with text-to-text pre-training on type-centric questions leveraging knowledge base documents and knowledge graphs.We create the \textbf{WikiWiki} dataset: entities and passages from 10M Wikipedia articles linked to the Wikidata knowledge graph with 41K types.Models trained on WikiWiki achieve state-of-the-art performance in zero-shot dialog state tracking benchmarks, accurately infer entity types in Wikipedia articles, and can discover new types deemed useful by human judges."
}

@inproceedings{markowitz-etal-2022-statik,
    title = "{S}t{ATIK}: Structure and Text for Inductive Knowledge Graph Completion",
	author = "Markowitz, Elan  and
      Balasubramanian, Keshav  and
      Mirtaheri, Mehrnoosh  and
      Annavaram, Murali  and
      Galstyan, Aram  and
      Ver Steeg, Greg",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-naacl.46/",
	doi = "10.18653/v1/2022.findings-naacl.46",
	pages = "604--615",
	abstract = "Knowledge graphs (KGs) often represent knowledge bases that are incomplete. Machine learning models can alleviate this by helping automate graph completion. Recently, there has been growing interest in completing knowledge bases that are dynamic, where previously unseen entities may be added to the KG with many missing links. In this paper, we present \textbf{StATIK}{--}\textbf{St}ructure \textbf{A}nd \textbf{T}ext for \textbf{I}nductive \textbf{K}nowledge Completion. StATIK uses Language Models to extract the semantic information from text descriptions, while using Message Passing Neural Networks to capture the structural information. StATIK achieves state of the art results on three challenging inductive baselines. We further analyze our hybrid model through detailed ablation studies."
}

@inproceedings{liang-etal-2022-mwp,
    title = "{MWP}-{BERT}: Numeracy-Augmented Pre-training for Math Word Problem Solving",
	author = "Liang, Zhenwen  and
      Zhang, Jipeng  and
      Wang, Lei  and
      Qin, Wei  and
      Lan, Yunshi  and
      Shao, Jie  and
      Zhang, Xiangliang",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-naacl.74/",
	doi = "10.18653/v1/2022.findings-naacl.74",
	pages = "997--1009",
	abstract = "Math word problem (MWP) solving faces a dilemma in number representation learning. In order to avoid the number representation issue and reduce the search space of feasible solutions, existing works striving for MWP solving usually replace real numbers with symbolic placeholders to focus on logic reasoning. However, different from common symbolic reasoning tasks like program synthesis and knowledge graph reasoning, MWP solving has extra requirements in numerical reasoning. In other words, instead of the number value itself, it is the reusable numerical property that matters more in numerical reasoning. Therefore, we argue that injecting numerical properties into symbolic placeholders with contextualized representation learning schema can provide a way out of the dilemma in the number representation issue here. In this work, we introduce this idea to the popular pre-training language model (PLM) techniques and build MWP-BERT, an effective contextual number representation PLM. We demonstrate the effectiveness of our MWP-BERT on MWP solving and several MWP-specific understanding tasks on both English and Chinese benchmarks."
}

@inproceedings{jiang-etal-2022-great,
    title = "$Great~Truths~are ~Always ~Simple:$ A Rather Simple Knowledge Encoder for Enhancing the Commonsense Reasoning Capacity of Pre-Trained Models",
	author = "Jiang, Jinhao  and
      Zhou, Kun  and
      Wen, Ji-Rong  and
      Zhao, Xin",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-naacl.131/",
	doi = "10.18653/v1/2022.findings-naacl.131",
	pages = "1730--1741",
	abstract = "Commonsense reasoning in natural language is a desired ability of artificial intelligent systems. For solving complex commonsense reasoning tasks, a typical solution is to enhance pre-trained language models (PTMs) with a knowledge-aware graph neural network (GNN) encoder that models a commonsense knowledge graph (CSKG).Despite the effectiveness, these approaches are built on heavy architectures, and can`t clearly explain how external knowledge resources improve the reasoning capacity of PTMs. Considering this issue, we conduct a deep empirical analysis, and find that it is indeed \textit{relation features} from CSKGs (but not \textit{node features}) that mainly contribute to the performance improvement of PTMs. Based on this finding, we design a simple MLP-based knowledge encoder that utilizes statistical relation paths as features. Extensive experiments conducted on five benchmarks demonstrate the effectiveness of our approach, which also largely reduces the parameters for encoding CSKGs.Our codes and data are publicly available at \url{https://github.com/RUCAIBox/SAFE}."
}

@inproceedings{rony-etal-2022-dialokg,
    title = "{D}ialo{KG}: Knowledge-Structure Aware Task-Oriented Dialogue Generation",
	author = "Rony, Md Rashad Al Hasan  and
      Usbeck, Ricardo  and
      Lehmann, Jens",
	editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
	booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
	month = jul,
	year = "2022",
	address = "Seattle, United States",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-naacl.195/",
	doi = "10.18653/v1/2022.findings-naacl.195",
	pages = "2557--2571",
	abstract = "Task-oriented dialogue generation is challenging since the underlying knowledge is often dynamic and effectively incorporating knowledge into the learning process is hard. It is particularly challenging to generate both human-like and informative responses in this setting. Recent research primarily focused on various knowledge distillation methods where the underlying relationship between the facts in a knowledge base is not effectively captured. In this paper, we go one step further and demonstrate how the structural information of a knowledge graph can improve the system`s inference capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue system that effectively incorporates knowledge into a language model. Our proposed system views relational knowledge as a knowledge graph and introduces (1) a structure-aware knowledge embedding technique, and (2) a knowledge graph-weighted attention masking strategy to facilitate the system selecting relevant information during the dialogue generation. An empirical evaluation demonstrates the effectiveness of DialoKG over state-of-the-art methods on several standard benchmark datasets."
}

@inproceedings{hou-etal-2022-enhanced,
    title = "What Has Been Enhanced in my Knowledge-Enhanced Language Model?",
	author = "Hou, Yifan  and
      Fu, Guoji  and
      Sachan, Mrinmaya",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.102/",
	doi = "10.18653/v1/2022.findings-emnlp.102",
	pages = "1417--1438",
	abstract = "A number of knowledge integration (KI) methods have recently been proposed to incorporate external knowledge into pretrained language models (LMs). Even though knowledge-enhanced LMs (KELMs) outperform base LMs on knowledge-intensive tasks, the inner-workings of these KI methods are not well-understood. For instance, it is unclear which knowledge is effectively integrated into KELMs and which is not; and if such integration led to catastrophic forgetting of already learned knowledge. We show that existing model interpretation methods such as linear probes and prompts have some key limitations in answering these questions. Then, we revisit KI from an information-theoretic view and propose a new theoretically sound probe model called Graph Convolution Simulator (GCS) for KI interpretation. GCS is eventually quite simple {--} it uses graph attention on the corresponding knowledge graph for interpretation.We conduct various experiments to verify that GCS provides reasonable interpretation results for two well-known KELMs: ERNIE and K-Adapter. Our experiments reveal that only little knowledge is successfully integrated in these models, and simply increasing the size of the KI corpus may not lead to better KELMs."
}

@inproceedings{melnyk-etal-2022-knowledge,
    title = "Knowledge Graph Generation From Text",
	author = "Melnyk, Igor  and
      Dognin, Pierre  and
      Das, Payel",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.116/",
	doi = "10.18653/v1/2022.findings-emnlp.116",
	pages = "1610--1622",
	abstract = "In this work we propose a novel end-to-end multi-stage Knowledge Graph (KG) generation system from textual inputs, separating the overall process into two stages. The graph nodes are generated first using pretrained language model, followed by a simple edge construction head, enabling efficient KG extraction from the text. For each stage we consider several architectural choices that can be used depending on the available training resources. We evaluated the model on a recent WebNLG 2020 Challenge dataset, matching the state-of-the-art performance on text-to-RDF generation task, as well as on New York Times (NYT) and a large-scale TekGen datasets, showing strong overall performance, outperforming the existing baselines. We believe that the proposed system can serve as a viable KG construction alternative to the existing linearization or sampling-based graph generation approaches."
}

@inproceedings{li-etal-2022-multi-modal,
    title = "A Multi-Modal Knowledge Graph for Classical {C}hinese Poetry",
	author = "Li, Yuqing  and
      Zhang, Yuxin  and
      Wu, Bin  and
      Wen, Ji-Rong  and
      Song, Ruihua  and
      Bai, Ting",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.171/",
	doi = "10.18653/v1/2022.findings-emnlp.171",
	pages = "2318--2326",
	abstract = "Classical Chinese poetry has a long history and is a precious cultural heritage of humankind. Displaying the classical Chinese poetry in a visual way, helps to cross cultural barriers in different countries, making it enjoyable for all the people. In this paper, we construct a multi-modal knowledge graph for classical Chinese poetry (PKG), in which the visual information of words in the poetry are incorporated. Then a multi-modal pre-training language model, PKG-Bert, is proposed to obtain the poetry representation with visual information, which bridges the semantic gap between different modalities. PKG-Bert achieves the state-of-the-art performance on the poetry-image retrieval task, showing the effectiveness of incorporating the multi-modal knowledge. The large-scale multi-modal knowledge graph of classical Chinese poetry will be released to promote the researches in classical Chinese culture area."
}

@inproceedings{shen-etal-2022-palt,
    title = "{PALT}: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion",
	author = "Shen, Jianhao  and
      Wang, Chenguang  and
      Yuan, Ye  and
      Han, Jiawei  and
      Ji, Heng  and
      Sen, Koushik  and
      Zhang, Ming  and
      Song, Dawn",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.281/",
	doi = "10.18653/v1/2022.findings-emnlp.281",
	pages = "3833--3847",
	abstract = "This paper presents a parameter-lite transfer learning approach of pretrained language models (LM) for knowledge graph (KG) completion. Instead of finetuning, which modifies all LM parameters, we only tune a few new parameters while keeping the original LM parameters fixed. We establish this via reformulating KG completion as a {\textquotedblleft}fill-in-the-blank{\textquotedblright} task, and introducing a parameter-lite encoder on top of the original LMs. We show that, by tuning far fewer parameters than finetuning, LMs transfer non-trivially to most tasks and reach competitiveness with prior state-of-the-art approaches. For instance, we outperform the fully finetuning approaches on a KG completion benchmark by tuning only 1{\%} of the parameters."
}

@inproceedings{hou-etal-2022-adapters,
    title = "Adapters for Enhanced Modeling of Multilingual Knowledge and Text",
	author = "Hou, Yifan  and
      Jiao, Wenxiang  and
      Liu, Meizhen  and
      Allen, Carl  and
      Tu, Zhaopeng  and
      Sachan, Mrinmaya",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.287/",
	doi = "10.18653/v1/2022.findings-emnlp.287",
	pages = "3902--3917",
	abstract = "Large language models appear to learn facts from the large text corpora they are trained on. Such facts are encoded implicitly within their many parameters, making it difficult to verify or manipulate what knowledge has been learned. Language models have recently been extended to multilingual language models (MLLMs), enabling knowledge to be learned across hundreds of languages. Meanwhile, knowledge graphs contain facts in an explicit triple format, which require careful and costly curation and are only available in a few high-resource languages, restricting their research and application. To address these issues, we propose to enhance MLLMs with knowledge from multilingual knowledge graphs (MLKGs) so as to tackle language and knowledge graph tasks across many languages, including low-resource ones. Specifically, we introducea lightweight adapter set to enhance MLLMs with cross-lingual entity alignment and facts from MLKGs for many languages. Experiments on common benchmarks show that such enhancement benefits both MLLMs and MLKGs, achieving: (1) comparable or improved performance for knowledge graph completion and entity alignment relative to baselines, especially for low-resource languages (for which knowledge graphs are unavailable); and (2) improved MLLM performance on language understanding tasks that require multilingual factual knowledge; all while maintaining performance on other general language tasks."
}

@inproceedings{fang-zhang-2022-data,
    title = "Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation",
	author = "Fang, Yanbo  and
      Zhang, Yongfeng",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.433/",
	doi = "10.18653/v1/2022.findings-emnlp.433",
	pages = "5883--5893",
	abstract = "Predicting the key explanation concept is essential for generating commonsense explanations. This paper introduces a method to predict the concept from pre-trained language models for commonsense explanation generation. Our experiment found that adopting a language model as the concept extractor and fine-tuning it with 20{\%} training data can improve the quality and accuracy of the generated explanations over multiple evaluation metrics. Compared with conventional methods that search concepts over knowledge graphs, our method does not require the preparation and training models to search through knowledge graphs. To better understand the results from pre-trained language models, we also designed a metric to evaluate the retrieved concepts. Through analysis and experiments, we show the correlation between this metric and the performance of the generators, and we also show the importance of attaching concepts for generating high-quality sentences."
}

@inproceedings{yao-etal-2022-wordties,
    title = "{W}ord{T}ies: Measuring Word Associations in Language Models via Constrained Sampling",
	author = "Yao, Peiran  and
      Renwick, Tobias  and
      Barbosa, Denilson",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.440/",
	doi = "10.18653/v1/2022.findings-emnlp.440",
	pages = "5959--5970",
	abstract = "Word associations are widely used in psychology to provide insights on how humans perceive and understand concepts. Comparing word associations in language models (LMs) to those generated by human subjects can serve as a proxy to uncover embedded lexical and commonsense knowledge in language models. While much helpful work has been done applying direct metrics, such as cosine similarity, to help understand latent spaces, these metrics are symmetric, while human word associativity is asymmetric. We propose WordTies, an algorithm based on constrained sampling from LMs, which allows an asymmetric measurement of associated words, given a cue word as the input. Comparing to existing methods, word associations found by this method share more overlap with associations provided by humans, and observe the asymmetric property of human associations. To examine possible reasons behind associations, we analyze the knowledge and reasoning behind the word pairings as they are linked to lexical and commonsense knowledge graphs.When the knowledge about the nature of the word pairings is combined with a probability that the LM has learned that information, we have a new way to examine what information is captured in LMs."
}

@inproceedings{peng-etal-2022-guiding,
    title = "Guiding Neural Story Generation with Reader Models",
	author = "Peng, Xiangyu  and
      Xie, Kaige  and
      Alabdulkarim, Amal  and
      Kayam, Harshith  and
      Dani, Samihan  and
      Riedl, Mark",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-emnlp.526/",
	doi = "10.18653/v1/2022.findings-emnlp.526",
	pages = "7087--7111",
	abstract = "Automated storytelling has long captured the attention of researchers for the ubiquity of narratives in everyday life. However, it is challenging to maintain coherence and stay on-topictoward a specific ending when generating narratives with neural language models. In this paper, we introduce Story generation with ReaderModels (StoRM), a framework in which areader model is used to reason about the storyshould progress. A reader model infers whata human reader believes about the concepts,entities, and relations about the fictional storyworld. We show how an explicit reader modelrepresented as a knowledge graph affords the storycoherence and provides controllability in theform of achieving a given story world stategoal. Experiments show that our model produces significantly more coherent and on-topicstories, outperforming baselines in dimensionsincluding plot plausibility and staying on topic"
}

@inproceedings{huang-etal-2022-open,
    title = "Open Relation Modeling: Learning to Define Relations between Entities",
	author = "Huang, Jie  and
      Chang, Kevin  and
      Xiong, Jinjun  and
      Hwu, Wen-mei",
	editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-acl.26/",
	doi = "10.18653/v1/2022.findings-acl.26",
	pages = "297--308",
	abstract = "Relations between entities can be represented by different instances, e.g., a sentence containing both entities or a fact in a Knowledge Graph (KG). However, these instances may not well capture the general relations between entities, may be difficult to understand by humans, even may not be found due to the incompleteness of the knowledge source. In this paper, we introduce the Open Relation Modeling problem - given two entities, generate a coherent sentence describing the relation between them. To solve this problem, we propose to teach machines to generate definition-like relation descriptions by letting them learn from defining entities. Specifically, we fine-tune Pre-trained Language Models (PLMs) to produce definitions conditioned on extracted entity pairs. To help PLMs reason between entities and provide additional relational knowledge to PLMs for open relation modeling, we incorporate reasoning paths in KGs and include a reasoning path selection mechanism. Experimental results show that our model can generate concise but informative relation descriptions that capture the representative characteristics of entities."
}

@inproceedings{yu-etal-2022-cocolm,
    title = "{C}o{C}o{LM}: Complex Commonsense Enhanced Language Model with Discourse Relations",
	author = "Yu, Changlong  and
      Zhang, Hongming  and
      Song, Yangqiu  and
      Ng, Wilfred",
	editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-acl.93/",
	doi = "10.18653/v1/2022.findings-acl.93",
	pages = "1175--1187",
	abstract = "Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between {\textquotedblleft}Jim yells at Bob{\textquotedblright} and {\textquotedblleft}Bob is upset{\textquotedblright}). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities. Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM."
}

@inproceedings{jung-etal-2022-learning,
    title = "Learning from Missing Relations: Contrastive Learning with Commonsense Knowledge Graphs for Commonsense Inference",
	author = "Jung, Yong-Ho  and
      Park, Jun-Hyung  and
      Choi, Joon-Young  and
      Lee, Mingyu  and
      Kim, Junho  and
      Kim, Kang-Min  and
      Lee, SangKeun",
	editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-acl.119/",
	doi = "10.18653/v1/2022.findings-acl.119",
	pages = "1514--1523",
	abstract = "Commonsense inference poses a unique challenge to reason and generate the physical, social, and causal conditions of a given event. Existing approaches to commonsense inference utilize commonsense transformers, which are large-scale language models that learn commonsense knowledge graphs. However, they suffer from a lack of coverage and expressive diversity of the graphs, resulting in a degradation of the representation quality. In this paper, we focus on addressing missing relations in commonsense knowledge graphs, and propose a novel contrastive learning framework called SOLAR. Our framework contrasts sets of semantically similar and dissimilar events, learning richer inferential knowledge compared to existing approaches. Empirical results demonstrate the efficacy of SOLAR in commonsense inference of diverse commonsense knowledge graphs. Specifically, SOLAR outperforms the state-of-the-art commonsense transformer on commonsense inference with ConceptNet by 1.84{\%} on average among 8 automatic evaluation metrics. In-depth analysis of SOLAR sheds light on the effects of the missing relations utilized in learning commonsense knowledge graphs."
}

@inproceedings{tan-etal-2022-tegtok,
    title = "{T}eg{T}ok: Augmenting Text Generation via Task-specific and Open-world Knowledge",
	author = "Tan, Chao-Hong  and
      Gu, Jia-Chen  and
      Tao, Chongyang  and
      Ling, Zhen-Hua  and
      Xu, Can  and
      Hu, Huang  and
      Geng, Xiubo  and
      Jiang, Daxin",
	editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-acl.125/",
	doi = "10.18653/v1/2022.findings-acl.125",
	pages = "1597--1609",
	abstract = "Generating natural and informative texts has been a long-standing problem in NLP. Much effort has been dedicated into incorporating pre-trained language models (PLMs) with various open-world knowledge, such as knowledge graphs or wiki pages. However, their ability to access and manipulate the task-specific knowledge is still limited on downstream tasks, as this type of knowledge is usually not well covered in PLMs and is hard to acquire. To address the problem, we propose augmenting TExt Generation via Task-specific and Open-world Knowledge (TegTok) in a unified framework. Our model selects knowledge entries from two types of knowledge sources through dense retrieval and then injects them into the input encoding and output decoding stages respectively on the basis of PLMs. With the help of these two types of knowledge, our model can learn what and how to generate. Experiments on two text generation tasks of dialogue generation and question generation, and on two datasets show that our method achieves better performance than various baseline models."
}

@inproceedings{lv-etal-2022-pre,
    title = "Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach",
	author = "Lv, Xin  and
      Lin, Yankai  and
      Cao, Yixin  and
      Hou, Lei  and
      Li, Juanzi  and
      Liu, Zhiyuan  and
      Li, Peng  and
      Zhou, Jie",
	editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-acl.282/",
	doi = "10.18653/v1/2022.findings-acl.282",
	pages = "3570--3581",
	abstract = "In recent years, pre-trained language models (PLMs) have been shown to capture factual knowledge from massive texts, which encourages the proposal of PLM-based knowledge graph completion (KGC) models. However, these models are still quite behind the SOTA KGC models in terms of performance. In this work, we find two main reasons for the weak performance: (1) Inaccurate evaluation setting. The evaluation setting under the closed-world assumption (CWA) may underestimate the PLM-based KGC models since they introduce more external knowledge; (2) Inappropriate utilization of PLMs. Most PLM-based KGC models simply splice the labels of entities and relations as inputs, leading to incoherent sentences that do not take full advantage of the implicit knowledge in PLMs. To alleviate these problems, we highlight a more accurate evaluation setting under the open-world assumption (OWA), which manual checks the correctness of knowledge that is not in KGs. Moreover, motivated by prompt tuning, we propose a novel PLM-based KGC model named PKGC. The basic idea is to convert each triple and its support information into natural prompt sentences, which is further fed into PLMs for classification. Experiment results on two KGC datasets demonstrate OWA is more reliable for evaluating KGC, especially on the link prediction, and the effectiveness of our PKCG model on both CWA and OWA settings."
}

@inproceedings{qi-etal-2022-takg,
    title = "{T}a{KG}: A New Dataset for Paragraph-level Table-to-Text Generation Enhanced with Knowledge Graphs",
	author = "Qi, Qianqian  and
      Deng, Zhenyun  and
      Zhu, Yonghua  and
      Lee, Lia Jisoo  and
      Witbrock, Michael  and
      Liu, Jiamou",
	editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
	booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
	month = nov,
	year = "2022",
	address = "Online only",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.findings-aacl.17/",
	doi = "10.18653/v1/2022.findings-aacl.17",
	pages = "176--187",
	abstract = "We introduce TaKG, a new table-to-text generation dataset with the following highlights: (1) TaKG defines a long-text (paragraph-level) generation task as opposed to well-established short-text (sentence-level) generation datasets. (2) TaKG is the first large-scale dataset for this task, containing three application domains and {\textasciitilde}750,000 samples. (3) To address the divergence phenomenon, TaKG enhances table input using external knowledge graphs, extracted by a new Wikidata-based method. We then propose a new Transformer-based multimodal sequence-to-sequence architecture for TaKG that integrates two pretrained language models RoBERTa and GPT-2. Our model shows reliable performance on long-text generation across a variety of metrics, and outperforms existing models for short-text generation tasks."
}

@inproceedings{lal-etal-2022-using,
    title = "Using Commonsense Knowledge to Answer Why-Questions",
	author = "Lal, Yash Kumar  and
      Tandon, Niket  and
      Aggarwal, Tanvi  and
      Liu, Horace  and
      Chambers, Nathanael  and
      Mooney, Raymond  and
      Balasubramanian, Niranjan",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.79/",
	doi = "10.18653/v1/2022.emnlp-main.79",
	pages = "1204--1219",
	abstract = "Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the context of answering questions in the TellMeWhy dataset using COMET as a source of relevant commonsense relations. We analyze the effects of model size (T5 and GPT3) along with methods of injecting knowledge (COMET) into these models. Results show that the largest models, as expected, yield substantial improvements over base models. Injecting external knowledge helps models of various sizes, but the amount of improvement decreases with larger model size. We also find that the format in which knowledge is provided is critical, and that smaller models benefit more from larger amounts of knowledge. Finally, we develop an ontology of knowledge types and analyze the relative coverage of the models across these categories."
}

@inproceedings{zhang-etal-2022-drlk,
    title = "{DRLK}: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering",
	author = "Zhang, Miao  and
      Dai, Rufeng  and
      Dong, Ming  and
      He, Tingting",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.342/",
	doi = "10.18653/v1/2022.emnlp-main.342",
	pages = "5123--5133",
	abstract = "In recent years, Graph Neural Network (GNN) approaches with enhanced knowledge graphs (KG) perform well in question answering (QA) tasks. One critical challenge is how to effectively utilize interactions between the QA context and KG. However, existing work only adopts the identical QA context representation to interact with multiple layers of KG, which results in a restricted interaction. In this paper, we propose DRLK (Dynamic Hierarchical Reasoning with Language Model and Knowledge Graphs), a novel model that utilizes dynamic hierarchical interactions between the QA context and KG for reasoning. DRLK extracts dynamic hierarchical features in the QA context, and performs inter-layer and intra-layer interactions on each iteration, allowing the KG representation to be grounded with the hierarchical features of the QA context. We conduct extensive experiments on four benchmark datasets in medical QA and commonsense reasoning. The experimental results demonstrate that DRLK achieves state-of-the-art performances on two benchmark datasets and performs competitively on the others."
}

@inproceedings{lovelace-rose-2022-framework,
    title = "A Framework for Adapting Pre-Trained Language Models to Knowledge Graph Completion",
	author = "Lovelace, Justin  and
      Ros{\'e}, Carolyn",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.398/",
	doi = "10.18653/v1/2022.emnlp-main.398",
	pages = "5937--5955",
	abstract = "Recent work has demonstrated that entity representations can be extracted from pre-trained language models to develop knowledge graph completion models that are more robust to the naturally occurring sparsity found in knowledge graphs. In this work, we conduct a comprehensive exploration of how to best extract and incorporate those embeddings into knowledge graph completion models. We explore the suitability of the extracted embeddings for direct use in entity ranking and introduce both unsupervised and supervised processing methods that can lead to improved downstream performance. We then introduce supervised embedding extraction methods that can extract more informative representations. We then synthesize our findings and develop a knowledge graph completion model that significantly outperforms recent neural models."
}

@inproceedings{liu-etal-2022-enhancing-multilingual,
    title = "Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples",
	author = "Liu, Linlin  and
      Li, Xin  and
      He, Ruidan  and
      Bing, Lidong  and
      Joty, Shafiq  and
      Si, Luo",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.462/",
	doi = "10.18653/v1/2022.emnlp-main.462",
	pages = "6878--6890",
	abstract = "Knowledge-enhanced language representation learning has shown promising results across various knowledge-intensive NLP tasks. However, prior methods are limited in efficient utilization of multilingual knowledge graph (KG) data for language model (LM) pretraining. They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection. In this work, we explore methods to make better use of the multilingual annotation and language agnostic property of KG triples, and present novel knowledge based multilingual language models (KMLMs) trained directly on the knowledge triples. We first generate a large amount of multilingual synthetic sentences using the Wikidata KG triples. Then based on the intra- and inter-sentence structures of the generated data, we design pretraining tasks to enable the LMs to not only memorize the factual knowledge but also learn useful logical patterns. Our pretrained KMLMs demonstrate significant performance improvements on a wide range of knowledge-intensive cross-lingual tasks, including named entity recognition (NER), factual knowledge retrieval, relation classification, and a newly designed logical reasoning task."
}

@inproceedings{lee-etal-2022-efficient-pre,
    title = "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking",
	author = "Lee, Mingyu  and
      Park, Jun-Hyung  and
      Kim, Junho  and
      Kim, Kang-Min  and
      Lee, SangKeun",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.502/",
	doi = "10.18653/v1/2022.emnlp-main.502",
	pages = "7417--7427",
	abstract = "Self-supervised pre-training has achieved remarkable success in extensive natural language processing tasks. Masked language modeling (MLM) has been widely used for pre-training effective bidirectional representations but comes at a substantial training cost. In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model. CCM has two key differences from existing curriculum learning approaches to effectively reflect the nature of MLM. First, we introduce a novel curriculum that evaluates the MLM difficulty of each token based on a carefully-designed linguistic difficulty criterion. Second, we construct a curriculum that masks easy words and phrases first and gradually masks related ones to the previously masked ones based on a knowledge graph. Experimental results show that CCM significantly improves pre-training efficiency. Specifically, the model trained with CCM shows comparative performance with the original BERT on the General Language Understanding Evaluation benchmark at half of the training cost."
}

@inproceedings{hao-etal-2022-acenet,
    title = "{ACEN}et: Attention Guided Commonsense Reasoning on Hybrid Knowledge Graph",
	author = "Hao, Chuzhan  and
      Xie, Minghui  and
      Zhang, Peng",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.579/",
	doi = "10.18653/v1/2022.emnlp-main.579",
	pages = "8461--8471",
	abstract = "Augmenting pre-trained language models (PLMs) with knowledge graphs (KGs) has demonstrated superior performance on commonsense reasoning. Given a commonsense based QA context (question and multiple choices), existing approaches usually estimate the plausibility of candidate choices separately based on their respective retrieved KGs, without considering the interference among different choices. In this paper, we propose an Attention guided Commonsense rEasoning Network (ACENet) to endow the neural network with the capability of integrating hybrid knowledge. Specifically, our model applies the multi-layer interaction of answer choices to continually strengthen correct choice information and guide the message passing of GNN. In addition, we also design a mix attention mechanism of nodes and edges to iteratively select supporting evidence on hybrid knowledge graph. Experimental results demonstrate the effectiveness of our proposed model through considerable performance gains across CommonsenseQA and OpenbookQA datasets."
}

@inproceedings{wang-etal-2022-cn,
    title = "{CN}-{A}uto{MIC}: Distilling {C}hinese Commonsense Knowledge from Pretrained Language Models",
	author = "Wang, Chenhao  and
      Li, Jiachun  and
      Chen, Yubo  and
      Liu, Kang  and
      Zhao, Jun",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.628/",
	doi = "10.18653/v1/2022.emnlp-main.628",
	pages = "9253--9265",
	abstract = "Commonsense knowledge graphs (CKGs) are increasingly applied in various natural language processing tasks. However, most existing CKGs are limited to English, which hinders related research in non-English languages. Meanwhile, directly generating commonsense knowledge from pretrained language models has recently received attention, yet it has not been explored in non-English languages. In this paper, we propose a large-scale Chinese CKG generated from multilingual PLMs, named as **CN-AutoMIC**, aiming to fill the research gap of non-English CKGs. To improve the efficiency, we propose generate-by-category strategy to reduce invalid generation. To ensure the filtering quality, we develop cascaded filters to discard low-quality results. To further increase the diversity and density, we introduce a bootstrapping iteration process to reuse generated results. Finally, we conduct detailed analyses on CN-AutoMIC from different aspects. Empirical results show the proposed CKG has high quality and diversity, surpassing the direct translation version of similar English CKGs. We also find some interesting deficiency patterns and differences between relations, which reveal pending problems in commonsense knowledge generation. We share the resources and related models for further study."
}

@inproceedings{hu-etal-2022-empowering,
    title = "Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering",
	author = "Hu, Ziniu  and
      Xu, Yichong  and
      Yu, Wenhao  and
      Wang, Shuohang  and
      Yang, Ziyi  and
      Zhu, Chenguang  and
      Chang, Kai-Wei  and
      Sun, Yizhou",
	editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, United Arab Emirates",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-main.650/",
	doi = "10.18653/v1/2022.emnlp-main.650",
	pages = "9562--9581",
	abstract = "Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning empowered Language Model(OREO-LM), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based LMs to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, LM guides KG to walk towards the desired answer, while the retrieved knowledge improves LM.By adopting OREO-LM to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the Closed-Book setting. The performance enhancement is mainly from the KG reasoning`s capacity to infer missing relational facts. In addition, OREO-LM provides reasoning paths as rationales to interpret the model`s decision."
}

@inproceedings{koleva-etal-2022-named,
    title = "Named Entity Recognition in Industrial Tables using Tabular Language Models",
	author = "Koleva, Aneta  and
      Ringsquandl, Martin  and
      Buckley, Mark  and
      Hasan, Rakeb  and
      Tresp, Volker",
	editor = "Li, Yunyao  and
      Lazaridou, Angeliki",
	booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track",
	month = dec,
	year = "2022",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.emnlp-industry.35/",
	doi = "10.18653/v1/2022.emnlp-industry.35",
	pages = "348--356",
	abstract = "Specialized transformer-based models for encoding tabular data have gained interest in academia. Although tabular data is omnipresent in industry, applications of table transformers are still missing. In this paper, we study how these models can be applied to an industrial Named Entity Recognition (NER) problem where the entities are mentioned in tabular-structured spreadsheets. The highly technical nature of spreadsheets as well as the lack of labeled data present major challenges for fine-tuning transformer-based models. Therefore, we develop a dedicated table data augmentation strategy based on available domain-specific knowledge graphs. We show that this boosts performance in our low-resource scenario considerably. Further, we investigate the benefits of tabular structure as inductive bias compared to tables as linearized sequences. Our experiments confirm that a table transformer outperforms other baselines and that its tabular inductive bias is vital for convergence of transformer-based models."
}

@inproceedings{brayne-etal-2022-masked,
    title = "On Masked Language Models for Contextual Link Prediction",
	author = "Brayne, Angus  and
      Wiatrak, Maciej  and
      Corneil, Dane",
	editor = "Agirre, Eneko  and
      Apidianaki, Marianna  and
      Vuli{\'c}, Ivan",
	booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
	month = may,
	year = "2022",
	address = "Dublin, Ireland and Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.deelio-1.9/",
	doi = "10.18653/v1/2022.deelio-1.9",
	pages = "87--99",
	abstract = "In the real world, many relational facts require context; for instance, a politician holds a given elected position only for a particular timespan. This context (the timespan) is typically ignored in knowledge graph link prediction tasks, or is leveraged by models designed specifically to make use of it (i.e. n-ary link prediction models). Here, we show that the task of n-ary link prediction is easily performed using language models, applied with a basic method for constructing cloze-style query sentences. We introduce a pre-training methodology based around an auxiliary entity-linked corpus that outperforms other popular pre-trained models like BERT, even with a smaller model. This methodology also enables n-ary link prediction without access to any n-ary training set, which can be invaluable in circumstances where expensive and time-consuming curation of n-ary knowledge graphs is not feasible. We achieve state-of-the-art performance on the primary n-ary link prediction dataset WD50K and on WikiPeople facts that include literals - typically ignored by knowledge graph embedding methods."
}

@inproceedings{hosseini-etal-2022-knowledge,
    title = "Knowledge-Augmented Language Models for Cause-Effect Relation Classification",
	author = "Hosseini, Pedram  and
      Broniatowski, David A.  and
      Diab, Mona",
	editor = "Bosselut, Antoine  and
      Li, Xiang  and
      Lin, Bill Yuchen  and
      Shwartz, Vered  and
      Majumder, Bodhisattwa Prasad  and
      Lal, Yash Kumar  and
      Rudinger, Rachel  and
      Ren, Xiang  and
      Tandon, Niket  and
      Zouhar, Vil{\'e}m",
	booktitle = "Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.csrr-1.6/",
	doi = "10.18653/v1/2022.csrr-1.6",
	pages = "43--48",
	abstract = "Previous studies have shown the efficacy of knowledge augmentation methods in pretrained language models. However, these methods behave differently across domains and downstream tasks. In this work, we investigate the augmentation of pretrained language models with knowledge graph data in the cause-effect relation classification and commonsense causal reasoning tasks. After automatically verbalizing triples in ATOMIC2020, a wide coverage commonsense reasoning knowledge graph, we continually pretrain BERT and evaluate the resulting model on cause-effect pair classification and answering commonsense causal reasoning questions. Our results show that a continually pretrained language model augmented with commonsense reasoning knowledge outperforms our baselines on two commonsense causal reasoning benchmarks, COPA and BCOPA-CE, and a Temporal and Causal Reasoning (TCR) dataset, without additional improvement in model architecture or using quality-enhanced data for fine-tuning."
}

@inproceedings{shen-etal-2022-joint,
    title = "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion",
	author = "Shen, Jianhao  and
      Wang, Chenguang  and
      Gong, Linyuan  and
      Song, Dawn",
	editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
	booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
	month = oct,
	year = "2022",
	address = "Gyeongju, Republic of Korea",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2022.coling-1.171/",
	pages = "1965--1978",
	abstract = "The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via fine-tuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a low-resource regime, thanks to the better use of semantics. The code and datasets are available at \url{https://github.com/pkusjh/LASS}."
}

@inproceedings{amin-etal-2022-meddistant19,
    title = "{M}ed{D}istant19: Towards an Accurate Benchmark for Broad-Coverage Biomedical Relation Extraction",
	author = "Amin, Saadullah  and
      Minervini, Pasquale  and
      Chang, David  and
      Stenetorp, Pontus  and
      Neumann, Guenter",
	editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
	booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
	month = oct,
	year = "2022",
	address = "Gyeongju, Republic of Korea",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2022.coling-1.198/",
	pages = "2259--2277",
	abstract = "Relation extraction in the biomedical domain is challenging due to the lack of labeled data and high annotation costs, needing domain experts. Distant supervision is commonly used to tackle the scarcity of annotated data by automatically pairing knowledge graph relationships with raw texts. Such a pipeline is prone to noise and has added challenges to scale for covering a large number of biomedical concepts. We investigated existing broad-coverage distantly supervised biomedical relation extraction benchmarks and found a significant overlap between training and test relationships ranging from 26{\%} to 86{\%}. Furthermore, we noticed several inconsistencies in the data construction process of these benchmarks, and where there is no train-test leakage, the focus is on interactions between narrower entity types. This work presents a more accurate benchmark MedDistant19 for broad-coverage distantly supervised biomedical relation extraction that addresses these shortcomings and is obtained by aligning the MEDLINE abstracts with the widely used SNOMED Clinical Terms knowledge base. Lacking thorough evaluation with domain-specific language models, we also conduct experiments validating general domain relation extraction findings to biomedical relation extraction."
}

@inproceedings{tokuhisa-etal-2022-enhancing,
    title = "Enhancing Contextual Word Representations Using Embedding of Neighboring Entities in Knowledge Graphs",
	author = "Tokuhisa, Ryoko  and
      Kawano, Keisuke  and
      Nakamura, Akihiro  and
      Koide, Satoshi",
	editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
	booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
	month = oct,
	year = "2022",
	address = "Gyeongju, Republic of Korea",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2022.coling-1.281/",
	pages = "3175--3186",
	abstract = "Pre-trained language models (PLMs) such as BERT and RoBERTa have dramatically improved the performance of various natural language processing tasks. Although these models are trained on large amounts of raw text, they have no explicit grounding in real-world entities. Knowledge graphs (KGs) are manually annotated with factual knowledge and store the relations between nodes corresponding to entities as labeled edges. This paper proposes a mechanism called KG-attention, which integrates the structure of a KG into recent PLM architectures. Unlike the existing PLM+KG integration methods, KG-attention generalizes the embeddings of neighboring entities using the relation embeddings; accordingly, it can handle relations between unconnected entities in the KG. Experimental results demonstrated that our method achieved significant improvements in a relation classification task, an entity typing task, and several language comprehension tasks."
}

@inproceedings{colas-etal-2022-gap,
    title = "{GAP}: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation",
	author = "Colas, Anthony  and
      Alvandipour, Mehrdad  and
      Wang, Daisy Zhe",
	editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
	booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
	month = oct,
	year = "2022",
	address = "Gyeongju, Republic of Korea",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2022.coling-1.506/",
	pages = "5755--5769",
	abstract = "Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in a graph."
}

@inproceedings{zeng-etal-2022-automatic,
    title = "Automatic Biomedical Term Clustering by Learning Fine-grained Term Representations",
	author = "Zeng, Sihang  and
      Yuan, Zheng  and
      Yu, Sheng",
	editor = "Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi",
	booktitle = "Proceedings of the 21st Workshop on Biomedical Language Processing",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.bionlp-1.8/",
	doi = "10.18653/v1/2022.bionlp-1.8",
	pages = "91--96",
	abstract = "Term clustering is important in biomedical knowledge graph construction. Using similarities between terms embedding is helpful for term clustering. State-of-the-art term embeddings leverage pretrained language models to encode terms, and use synonyms and relation knowledge from knowledge graphs to guide contrastive learning. These embeddings provide close embeddings for terms belonging to the same concept. However, from our probing experiments, these embeddings are not sensitive to minor textual differences which leads to failure for biomedical term clustering. To alleviate this problem, we adjust the sampling strategy in pretraining term embeddings by providing dynamic hard positive and negative samples during contrastive learning to learn fine-grained representations which result in better biomedical term clustering. We name our proposed method as CODER++, and it has been applied in clustering biomedical concepts in the newly released Biomedical Knowledge Graph named BIOS."
}

@inproceedings{naseem-etal-2022-incorporating,
    title = "Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation",
	author = "Naseem, Usman  and
      Bandi, Ajay  and
      Raza, Shaina  and
      Rashid, Junaid  and
      Chakravarthi, Bharathi Raja",
	editor = "Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi",
	booktitle = "Proceedings of the 21st Workshop on Biomedical Language Processing",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.bionlp-1.10/",
	doi = "10.18653/v1/2022.bionlp-1.10",
	pages = "110--115",
	abstract = "Medical dialogue systems have the potential to assist doctors in expanding access to medical care, improving the quality of patient experiences, and lowering medical expenses. The computational methods are still in their early stages and are not ready for widespread application despite their great potential. Existing transformer-based language models have shown promising results but lack domain-specific knowledge. However, to diagnose like doctors, an automatic medical diagnosis necessitates more stringent requirements for the rationality of the dialogue in the context of relevant knowledge. In this study, we propose a new method that addresses the challenges of medical dialogue generation by incorporating medical knowledge into transformer-based language models. We present a method that leverages an external medical knowledge graph and injects triples as domain knowledge into the utterances. Automatic and human evaluation on a publicly available dataset demonstrates that incorporating medical knowledge outperforms several state-of-the-art baseline methods."
}

@inproceedings{wang-etal-2022-simkgc,
    title = "{S}im{KGC}: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models",
	author = "Wang, Liang  and
      Zhao, Wei  and
      Wei, Zhuoyu  and
      Liu, Jingming",
	editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.295/",
	doi = "10.18653/v1/2022.acl-long.295",
	pages = "4281--4294",
	abstract = "Knowledge graph completion (KGC) aims to reason over known facts and infer the missing links. Text-based methods such as KGBERT (Yao et al., 2019) learn entity representations from natural language descriptions, and have the potential for inductive KGC. However, the performance of text-based methods still largely lag behind graph embedding-based methods like TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b). In this paper, we identify that the key issue is efficient contrastive learning. To improve the learning efficiency, we introduce three types of negatives: in-batch negatives, pre-batch negatives, and self-negatives which act as a simple form of hard negatives. Combined with InfoNCE loss, our proposed model SimKGC can substantially outperform embedding-based methods on several benchmark datasets. In terms of mean reciprocal rank (MRR), we advance the state-of-the-art by +19{\%} on WN18RR, +6.8{\%} on the Wikidata5M transductive setting, and +22{\%} on the Wikidata5M inductive setting. Thorough analyses are conducted to gain insights into each component. Our code is available at \url{https://github.com/intfloat/SimKGC} ."
}

@inproceedings{dou-etal-2022-gpt,
    title = "Is {GPT}-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text",
	author = "Dou, Yao  and
      Forbes, Maxwell  and
      Koncel-Kedziorski, Rik  and
      Smith, Noah A.  and
      Choi, Yejin",
	editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
	booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
	month = may,
	year = "2022",
	address = "Dublin, Ireland",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.acl-long.501/",
	doi = "10.18653/v1/2022.acl-long.501",
	pages = "7250--7274",
	abstract = "Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation. We propose a new framework called Scarecrow for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of Scarecrow{---}such as redundancy, commonsense errors, and incoherence{---}are identified through several rounds of crowd annotation experiments without a predefined ontology. We then use Scarecrow to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at \url{https://yao-dou.github.io/scarecrow/}."
}

@inproceedings{wang-etal-2022-recindial,
    title = "{R}ec{I}n{D}ial: A Unified Framework for Conversational Recommendation with Pretrained Language Models",
	author = "Wang, Lingzhi  and
      Hu, Huang  and
      Sha, Lei  and
      Xu, Can  and
      Jiang, Daxin  and
      Wong, Kam-Fai",
	editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
	booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = nov,
	year = "2022",
	address = "Online only",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.aacl-main.37/",
	doi = "10.18653/v1/2022.aacl-main.37",
	pages = "489--500",
	abstract = "Conversational Recommender System (CRS), which aims to recommend high-quality items to users through interactive conversations, has gained great research interest recently. A CRS is usually composed of a recommendation module and a generation module. In the previous work, these two modules are loosely connected in the model training and are shallowly integrated during inference, where a simple switching or copy mechanism is adopted to incorporate recommended items into generated responses. Moreover, the current end-to-end neural models trained on small crowd-sourcing datasets (e.g., 10K dialogs in the ReDial dataset) tend to overfit and have poor chit-chat ability. In this work, we propose a novel unified framework that integrates recommendation into the dialog (RecInDial) generation by introducing a vocabulary pointer. To tackle the low-resource issue in CRS, we finetune the large-scale pretrained language models to generate fluent and diverse responses, and introduce a knowledge-aware bias learned from an entity-oriented knowledge graph to enhance the recommendation performance. Furthermore, we propose to evaluate the CRS models in an end-to-end manner, which can reflect the overall performance of the entire system rather than the performance of individual modules, compared to the separate evaluations of the two modules used in previous work. Experiments on the benchmark dataset ReDial show our RecInDial model significantly surpasses the state-of-the-art methods. More extensive analyses show the effectiveness of our model."
}

@inproceedings{li-zaki-2022-food,
    title = "Food Knowledge Representation Learning with Adversarial Substitution",
	author = "Li, Diya  and
      Zaki, Mohammed J",
	editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
	booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = nov,
	year = "2022",
	address = "Online only",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2022.aacl-main.50/",
	doi = "10.18653/v1/2022.aacl-main.50",
	pages = "653--664",
	abstract = "Knowledge graph embedding (KGE) has been well-studied in general domains, but has not been examined for food computing. To fill this gap, we perform knowledge representation learning over a food knowledge graph (KG). We employ a pre-trained language model to encode entities and relations, thus emphasizing contextual information in food KGs. The model is trained on two tasks {--} predicting a masked entity from a given triple from the KG and predicting the plausibility of a triple. Analysis of food substitutions helps in dietary choices for enabling healthier eating behaviors. Previous work in food substitutions mainly focuses on semantic similarity while ignoring the context. It is also hard to evaluate the substitutions due to the lack of an adequate validation set, and further, the evaluation is subjective based on perceived purpose. To tackle this problem, we propose a collection of adversarial sample generation strategies for different food substitutions over our learnt KGE. We propose multiple strategies to generate high quality context-aware recipe and ingredient substitutions and also provide generalized ingredient substitutions to meet different user needs. The effectiveness and efficiency of the proposed knowledge graph learning method and the following attack strategies are verified by extensive evaluations on a large-scale food KG."
}

@inproceedings{clark-etal-2021-integrating,
    title = "Integrating Transformers and Knowledge Graphs for {T}witter Stance Detection",
	author = "Clark, Thomas  and
      Conforti, Costanza  and
      Liu, Fangyu  and
      Meng, Zaiqiao  and
      Shareghi, Ehsan  and
      Collier, Nigel",
	editor = "Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin",
	booktitle = "Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)",
	month = nov,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.wnut-1.34/",
	doi = "10.18653/v1/2021.wnut-1.34",
	pages = "304--312",
	abstract = "Stance detection (SD) entails classifying the sentiment of a text towards a given target, and is a relevant sub-task for opinion mining and social media analysis. Recent works have explored knowledge infusion supplementing the linguistic competence and latent knowledge of large pre-trained language models with structured knowledge graphs (KGs), yet few works have applied such methods to the SD task. In this work, we first perform stance-relevant knowledge probing on Transformers-based pre-trained models in a zero-shot setting, showing these models' latent real-world knowledge about SD targets and their sensitivity to context. We then train and evaluate new knowledge-enriched stance detection models on two Twitter stance datasets, achieving state-of-the-art performance on both."
}

@inproceedings{behnamghader-etal-2021-mg,
    title = "{MG}-{BERT}: Multi-Graph Augmented {BERT} for Masked Language Modeling",
	author = "BehnamGhader, Parishad  and
      Zakerinia, Hossein  and
      Soleymani Baghshah, Mahdieh",
	editor = "Panchenko, Alexander  and
      Malliaros, Fragkiskos D.  and
      Logacheva, Varvara  and
      Jana, Abhik  and
      Ustalov, Dmitry  and
      Jansen, Peter",
	booktitle = "Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)",
	month = jun,
	year = "2021",
	address = "Mexico City, Mexico",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.textgraphs-1.12/",
	doi = "10.18653/v1/2021.textgraphs-1.12",
	pages = "125--131",
	abstract = "Pre-trained models like Bidirectional Encoder Representations from Transformers (BERT), have recently made a big leap forward in Natural Language Processing (NLP) tasks. However, there are still some shortcomings in the Masked Language Modeling (MLM) task performed by these models. In this paper, we first introduce a multi-graph including different types of relations between words. Then, we propose Multi-Graph augmented BERT (MG-BERT) model that is based on BERT. MG-BERT embeds tokens while taking advantage of a static multi-graph containing global word co-occurrences in the text corpus beside global real-world facts about words in knowledge graphs. The proposed model also employs a dynamic sentence graph to capture local context effectively. Experimental results demonstrate that our model can considerably enhance the performance in the MLM task."
}

@article{wang-etal-2021-kepler,
    title = "{KEPLER}: A Unified Model for Knowledge Embedding and Pre-trained Language Representation",
	author = "Wang, Xiaozhi  and
      Gao, Tianyu  and
      Zhu, Zhaocheng  and
      Zhang, Zhengyan  and
      Liu, Zhiyuan  and
      Li, Juanzi  and
      Tang, Jian",
	editor = "Roark, Brian  and
      Nenkova, Ani",
	journal = "Transactions of the Association for Computational Linguistics",
	volume = "9",
	year = "2021",
	address = "Cambridge, MA",
	publisher = "MIT Press",
	url = "https://aclanthology.org/2021.tacl-1.11/",
	doi = "10.1162/tacl_a_00360",
	pages = "176--194",
	abstract = "Pre-trained language representation models (PLMs) cannot well capture factual knowledge from text. In contrast, knowledge embedding (KE) methods can effectively represent the relational facts in knowledge graphs (KGs) with informative entity embeddings, but conventional KE models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained LanguagERepresentation (KEPLER), which can not only better integrate factual knowledge into PLMs but also produce effective text-enhanced KE with the strong PLMs. In KEPLER, we encode textual entity descriptions with a PLM as their embeddings, and then jointly optimize the KE and language modeling objectives. Experimental results show that KEPLER achieves state-of-the-art performances on various NLP tasks, and also works remarkably well as an inductive KE model on KG link prediction. Furthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M1 , a large-scale KG dataset with aligned entity descriptions, and benchmark state-of-the-art KE methods on it. It shall serve as a new KE benchmark and facilitate the research on large KG, inductive KE, and KG with text. The source code can be obtained from \url{https://github.com/THU-KEG/KEPLER}."
}

@inproceedings{balaraman-etal-2021-recent,
    title = "Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey",
	author = "Balaraman, Vevake  and
      Sheikhalishahi, Seyedmostafa  and
      Magnini, Bernardo",
	editor = "Li, Haizhou  and
      Levow, Gina-Anne  and
      Yu, Zhou  and
      Gupta, Chitralekha  and
      Sisman, Berrak  and
      Cai, Siqi  and
      Vandyke, David  and
      Dethlefs, Nina  and
      Wu, Yan  and
      Li, Junyi Jessy",
	booktitle = "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = jul,
	year = "2021",
	address = "Singapore and Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.sigdial-1.25/",
	doi = "10.18653/v1/2021.sigdial-1.25",
	pages = "239--251",
	abstract = "This paper aims at providing a comprehensive overview of recent developments in dialogue state tracking (DST) for task-oriented conversational systems. We introduce the task, the main datasets that have been exploited as well as their evaluation metrics, and we analyze several proposed approaches. We distinguish between static ontology DST models, which predict a fixed set of dialogue states, and dynamic ontology models, which can predict dialogue states even when the ontology changes. We also discuss the model`s ability to track either single or multiple domains and to scale to new domains, both in terms of knowledge transfer and zero-shot learning. We cover a period from 2013 to 2020, showing a significant increase of multiple domain methods, most of them utilizing pre-trained language models."
}

@inproceedings{shailabh-etal-2021-knowgraph,
    title = "{K}now{G}raph@{IITK} at {S}em{E}val-2021 Task 11: Building Knowledge Graph for {NLP} Research",
	author = "Shailabh, Shashank  and
      Chaurasia, Sajal  and
      Modi, Ashutosh",
	editor = "Palmer, Alexis  and
      Schneider, Nathan  and
      Schluter, Natalie  and
      Emerson, Guy  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan",
	booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.semeval-1.57/",
	doi = "10.18653/v1/2021.semeval-1.57",
	pages = "467--477",
	abstract = "Research in Natural Language Processing is making rapid advances, resulting in the publication of a large number of research papers. Finding relevant research papers and their contribution to the domain is a challenging problem. In this paper, we address this challenge via the SemEval 2021 Task 11: NLPContributionGraph, by developing a system for a research paper contributions-focused knowledge graph over Natural Language Processing literature. The task is divided into three sub-tasks: extracting contribution sentences that show important contributions in the research article, extracting phrases from the contribution sentences, and predicting the information units in the research article together with triplet formation from the phrases. The proposed system is agnostic to the subject domain and can be applied for building a knowledge graph for any area. We found that transformer-based language models can significantly improve existing techniques and utilized the SciBERT-based model. Our first sub-task uses Bidirectional LSTM (BiLSTM) stacked on top of SciBERT model layers, while the second sub-task uses Conditional Random Field (CRF) on top of SciBERT with BiLSTM. The third sub-task uses a combined SciBERT based neural approach with heuristics for information unit prediction and triplet formation from the phrases. Our system achieved F1 score of 0.38, 0.63 and 0.76 in end-to-end pipeline testing, phrase extraction testing and triplet extraction testing respectively."
}

@inproceedings{lin-etal-2021-ecnuica,
    title = "{ECNUICA} at {S}em{E}val-2021 Task 11: Rule based Information Extraction Pipeline",
	author = "Lin, Jiaju  and
      Ling, Jing  and
      Wang, Zhiwei  and
      Liu, Jiawei  and
      Chen, Qin  and
      He, Liang",
	editor = "Palmer, Alexis  and
      Schneider, Nathan  and
      Schluter, Natalie  and
      Emerson, Guy  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan",
	booktitle = "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.semeval-1.185/",
	doi = "10.18653/v1/2021.semeval-1.185",
	pages = "1295--1302",
	abstract = "This paper presents our endeavor for solving task11, NLPContributionGraph, of SemEval-2021. The purpose of the task was to extract triples from a paper in the Nature Language Processing field for constructing an Open Research Knowledge Graph. The task includes three sub-tasks: detecting the contribution sentences in papers, identifying scientific terms and predicate phrases from the contribution sentences; and inferring triples in the form of (subject, predicate, object) as statements for Knowledge Graph building. In this paper, we apply an ensemble of various fine-tuned pre-trained language models (PLM) for tasks one and two. In addition, self-training methods are adopted for tackling the shortage of annotated data. For the third task, rather than using classic neural open information extraction (OIE) architectures, we generate potential triples via manually designed rules and develop a binary classifier to differentiate positive ones from others. The quantitative results show that we obtain the 4th, 2nd, and 2nd rank in three evaluation phases."
}

@inproceedings{thai-etal-2021-simultaneously,
    title = "Simultaneously Self-Attending to Text and Entities for Knowledge-Informed Text Representations",
	author = "Thai, Dung  and
      Thirukovalluru, Raghuveer  and
      Bansal, Trapit  and
      McCallum, Andrew",
	editor = "Rogers, Anna  and
      Calixto, Iacer  and
      Vuli{\'c}, Ivan  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Camburu, Oana-Maria  and
      Bansal, Trapit  and
      Shwartz, Vered",
	booktitle = "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.repl4nlp-1.25/",
	doi = "10.18653/v1/2021.repl4nlp-1.25",
	pages = "241--247",
	abstract = "Pre-trained language models have emerged as highly successful methods for learning good text representations. However, the amount of structured knowledge retained in such models, and how (if at all) it can be extracted, remains an open question. In this work, we aim at directly learning text representations which leverage structured knowledge about entities mentioned in the text. This can be particularly beneficial for downstream tasks which are knowledge-intensive. Our approach utilizes self-attention between words in the text and knowledge graph (KG) entities mentioned in the text. While existing methods require entity-linked data for pre-training, we train using a mention-span masking objective and a candidate ranking objective {--} which doesn`t require any entity-links and only assumes access to an alias table for retrieving candidates, enabling large-scale pre-training. We show that the proposed model learns knowledge-informed text representations that yield improvements on the downstream tasks over existing methods."
}

@inproceedings{ribeiro-etal-2021-investigating,
    title = "Investigating Pretrained Language Models for Graph-to-Text Generation",
	author = {Ribeiro, Leonardo F. R.  and
      Schmitt, Martin  and
      Sch{\"u}tze, Hinrich  and
      Gurevych, Iryna},
	editor = "Papangelis, Alexandros  and
      Budzianowski, Pawe{\l}  and
      Liu, Bing  and
      Nouri, Elnaz  and
      Rastogi, Abhinav  and
      Chen, Yun-Nung",
	booktitle = "Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI",
	month = nov,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.nlp4convai-1.20/",
	doi = "10.18653/v1/2021.nlp4convai-1.20",
	pages = "211--227",
	abstract = "Graph-to-text generation aims to generate fluent texts from graph-based data. In this paper, we investigate two recent pretrained language models (PLMs) and analyze the impact of different task-adaptive pretraining strategies for PLMs in graph-to-text generation. We present a study across three graph domains: meaning representations, Wikipedia knowledge graphs (KGs) and scientific KGs. We show that approaches based on PLMs BART and T5 achieve new state-of-the-art results and that task-adaptive pretraining strategies improve their performance even further. We report new state-of-the-art BLEU scores of 49.72 on AMR-LDC2017T10, 59.70 on WebNLG, and 25.66 on AGENDA datasets - a relative improvement of 31.8{\%}, 4.5{\%}, and 42.4{\%}, respectively, with our models generating significantly more fluent texts than human references. In an extensive analysis, we identify possible reasons for the PLMs' success on graph-to-text tasks. Our findings suggest that the PLMs benefit from similar facts seen during pretraining or fine-tuning, such that they perform well even when the input graph is reduced to a simple bag of node and edge labels."
}

@inproceedings{yasunaga-etal-2021-qa,
    title = "{QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering",
	author = "Yasunaga, Michihiro  and
      Ren, Hongyu  and
      Bosselut, Antoine  and
      Liang, Percy  and
      Leskovec, Jure",
	editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.naacl-main.45/",
	doi = "10.18653/v1/2021.naacl-main.45",
	pages = "535--546",
	abstract = "The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions."
}

@inproceedings{agarwal-etal-2021-knowledge,
    title = "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
	author = "Agarwal, Oshin  and
      Ge, Heming  and
      Shakeri, Siamak  and
      Al-Rfou, Rami",
	editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.naacl-main.278/",
	doi = "10.18653/v1/2021.naacl-main.278",
	pages = "3554--3565",
	abstract = "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe."
}

@inproceedings{calixto-etal-2021-wikipedia,
    title = "{W}ikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting {W}ikipedia Hyperlinks",
	author = "Calixto, Iacer  and
      Raganato, Alessandro  and
      Pasini, Tommaso",
	editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
	booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.naacl-main.286/",
	doi = "10.18653/v1/2021.naacl-main.286",
	pages = "3651--3661",
	abstract = "Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages."
}

@inproceedings{alberts-etal-2021-visualsem,
    title = "{V}isual{S}em: a high-quality knowledge graph for vision and language",
	author = "Alberts, Houda  and
      Huang, Ningyuan  and
      Deshpande, Yash  and
      Liu, Yibo  and
      Cho, Kyunghyun  and
      Vania, Clara  and
      Calixto, Iacer",
	editor = "Ataman, Duygu  and
      Birch, Alexandra  and
      Conneau, Alexis  and
      Firat, Orhan  and
      Ruder, Sebastian  and
      Sahin, Gozde Gul",
	booktitle = "Proceedings of the 1st Workshop on Multilingual Representation Learning",
	month = nov,
	year = "2021",
	address = "Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.mrl-1.13/",
	doi = "10.18653/v1/2021.mrl-1.13",
	pages = "138--152",
	abstract = "An exciting frontier in natural language understanding (NLU) and generation (NLG) calls for (vision-and-) language models that can efficiently access external structured knowledge repositories. However, many existing knowledge bases only cover limited domains, or suffer from noisy data, and most of all are typically hard to integrate into neural language pipelines. To fill this gap, we release VisualSem: a high-quality knowledge graph (KG) which includes nodes with multilingual glosses, multiple illustrative images, and visually relevant relations. We also release a neural multi-modal retrieval model that can use images or sentences as inputs and retrieves entities in the KG. This multi-modal retrieval model can be integrated into any (neural network) model pipeline. We encourage the research community to use VisualSem for data augmentation and/or as a source of grounding, among other possible uses. VisualSem as well as the multi-modal retrieval models are publicly available and can be downloaded in this URL: \url{https://github.com/iacercalixto/visualsem}."
}

@inproceedings{duan-etal-2021-learning-numeracy,
    title = "Learning Numeracy: A Simple Yet Effective Number Embedding Approach Using Knowledge Graph",
	author = "Duan, Hanyu  and
      Yang, Yi  and
      Tam, Kar Yan",
	editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
	month = nov,
	year = "2021",
	address = "Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-emnlp.221/",
	doi = "10.18653/v1/2021.findings-emnlp.221",
	pages = "2597--2602",
	abstract = "Numeracy plays a key role in natural language understanding. However, existing NLP approaches, not only traditional word2vec approach or contextualized transformer-based language models, fail to learn numeracy. As the result, the performance of these models is limited when they are applied to number-intensive applications in clinical and financial domains. In this work, we propose a simple number embedding approach based on knowledge graph. We construct a knowledge graph consisting of number entities and magnitude relations. Knowledge graph embedding method is then applied to obtain number vectors. Our approach is easy to implement, and experiment results on various numeracy-related NLP tasks demonstrate the effectiveness and efficiency of our method."
}

@inproceedings{he-etal-2021-klmo-knowledge,
    title = "{KLM}o: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships",
	author = "He, Lei  and
      Zheng, Suncong  and
      Yang, Tao  and
      Zhang, Feng",
	editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
	month = nov,
	year = "2021",
	address = "Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-emnlp.384/",
	doi = "10.18653/v1/2021.findings-emnlp.384",
	pages = "4536--4542",
	abstract = "Interactions between entities in knowledge graph (KG) provide rich knowledge for language representation learning. However, existing knowledge-enhanced pretrained language models (PLMs) only focus on entity information and ignore the fine-grained relationships between entities. In this work, we propose to incorporate KG (including both entities and relations) into the language learning process to obtain KG-enhanced pretrained Language Model, namely KLMo. Specifically, a novel knowledge aggregator is designed to explicitly model the interaction between entity spans in text and all entities and relations in a contextual KG. An relation prediction objective is utilized to incorporate relation information by distant supervision. An entity linking objective is further utilized to link entity spans in text to entities in KG. In this way, the structured knowledge can be effectively integrated into language representations. Experimental results demonstrate that KLMo achieves great improvements on several knowledge-driven tasks, such as entity typing and relation classification, comparing with the state-of-the-art knowledge-enhanced PLMs."
}

@inproceedings{li-etal-2021-shot-knowledge,
    title = "Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models",
	author = "Li, Junyi  and
      Tang, Tianyi  and
      Zhao, Wayne Xin  and
      Wei, Zhicheng  and
      Yuan, Nicholas Jing  and
      Wen, Ji-Rong",
	editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
	booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-acl.136/",
	doi = "10.18653/v1/2021.findings-acl.136",
	pages = "1558--1568"
}

@inproceedings{aspillaga-etal-2021-inspecting,
    title = "Inspecting the concept knowledge graph encoded by modern language models",
	author = "Aspillaga, Carlos  and
      Mendoza, Marcelo  and
      Soto, Alvaro",
	editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
	booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-acl.263/",
	doi = "10.18653/v1/2021.findings-acl.263",
	pages = "2984--3000"
}

@inproceedings{glass-etal-2021-robust,
    title = "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
	author = "Glass, Michael  and
      Rossiello, Gaetano  and
      Chowdhury, Md Faisal Mahbub  and
      Gliozzo, Alfio",
	editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.148/",
	doi = "10.18653/v1/2021.emnlp-main.148",
	pages = "1939--1949",
	abstract = "Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to {\textquoteleft}fill' the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models."
}

@inproceedings{dziri-etal-2021-neural,
    title = "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
	author = {Dziri, Nouha  and
      Madotto, Andrea  and
      Za{\"i}ane, Osmar  and
      Bose, Avishek Joey},
	editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.168/",
	doi = "10.18653/v1/2021.emnlp-main.168",
	pages = "2197--2214",
	abstract = "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35{\%} based on FeQA (Durmus et al., 2020). The code is available at \url{https://github.com/nouhadziri/Neural-Path-Hunter}."
}

@inproceedings{ushio-etal-2021-distilling,
    title = "Distilling Relation Embeddings from Pretrained Language Models",
	author = "Ushio, Asahi  and
      Camacho-Collados, Jose  and
      Schockaert, Steven",
	editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
	booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2021",
	address = "Online and Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.emnlp-main.712/",
	doi = "10.18653/v1/2021.emnlp-main.712",
	pages = "9044--9062",
	abstract = "Pre-trained language models have been found to capture a surprisingly rich amount of lexical knowledge, ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities. Among others, this makes it possible to distill high-quality word vectors from pre-trained language models. However, it is currently unclear to what extent it is possible to distill relation embeddings, i.e. vectors that characterize the relationship between two words. Such relation embeddings are appealing because they can, in principle, encode relational knowledge in a more fine-grained way than is possible with knowledge graphs. To obtain relation embeddings from a pre-trained language model, we encode word pairs using a (manually or automatically generated) prompt, and we fine-tune the language model such that relationally similar word pairs yield similar output vectors. We find that the resulting relation embeddings are highly competitive on analogy (unsupervised) and relation classification (supervised) benchmarks, even without any task-specific fine-tuning. Source code to reproduce our experimental results and the model checkpoints are available in the following repository: \url{https://github.com/asahi417/relbert}"
}

@inproceedings{li-etal-2021-zero,
    title = "Zero-shot Generalization in Dialog State Tracking through Generative Question Answering",
	author = "Li, Shuyang  and
      Cao, Jin  and
      Sridhar, Mukund  and
      Zhu, Henghui  and
      Li, Shang-Wen  and
      Hamza, Wael  and
      McAuley, Julian",
	editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
	booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
	month = apr,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.eacl-main.91/",
	doi = "10.18653/v1/2021.eacl-main.91",
	pages = "1063--1074",
	abstract = "Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in task-oriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports natural language queries for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative question-answering using a conditional language model pre-trained on substantive English sentences. Our model improves joint goal accuracy in zero-shot domain adaptation settings by up to 9{\%} (absolute) over the previous state-of-the-art on the MultiWOZ 2.1 dataset."
}

@inproceedings{hanyu-etal-2021-chinese,
    title = "A {C}hinese Machine Reading Comprehension Dataset Automatic Generated Based on Knowledge Graph",
	author = "Hanyu, Zhao  and
      Sha, Yuan  and
      Jiahong, Leng  and
      Xiang, Pan  and
      Zhao, Xue  and
      Quanyue, Ma  and
      Yangxiao, Liang",
	editor = "Li, Sheng  and
      Sun, Maosong  and
      Liu, Yang  and
      Wu, Hua  and
      Liu, Kang  and
      Che, Wanxiang  and
      He, Shizhu  and
      Rao, Gaoqi",
	booktitle = "Proceedings of the 20th Chinese National Conference on Computational Linguistics",
	month = aug,
	year = "2021",
	address = "Huhhot, China",
	publisher = "Chinese Information Processing Society of China",
	url = "https://aclanthology.org/2021.ccl-1.95/",
	pages = "1066--1075",
	language = "eng",
	abstract = "Machine reading comprehension (MRC) is a typical natural language processing (NLP)task and has developed rapidly in the last few years. Various reading comprehension datasets have been built to support MRC studies. However large-scale and high-quality datasets are rare due to the high complexity and huge workforce cost of making sucha dataset. Besides most reading comprehension datasets are in English and Chinesedatasets are insufficient. In this paper we propose an automatic method for MRCdataset generation and build the largest Chinese medical reading comprehension dataset presently named CMedRC. Our dataset contains 17k questions generated by our auto-matic method and some seed questions. We obtain the corresponding answers from amedical knowledge graph and manually check all of them. Finally we test BiLSTM andBERT-based pre-trained language models (PLMs) on our dataset and propose a base-line for the following studies. Results show that the automatic MRC dataset generation method is considerable for future model improvements."
}

@inproceedings{wang-etal-2021-stage,
    title = "Stage-wise Fine-tuning for Graph-to-Text Generation",
	author = "Wang, Qingyun  and
      Yavuz, Semih  and
      Lin, Xi Victoria  and
      Ji, Heng  and
      Rajani, Nazneen",
	editor = "Kabbara, Jad  and
      Lin, Haitao  and
      Paullada, Amandalynne  and
      Vamvas, Jannis",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-srw.2/",
	doi = "10.18653/v1/2021.acl-srw.2",
	pages = "16--22",
	abstract = "Graph-to-text generation has benefited from pre-trained language models (PLMs) in achieving better performance than structured graph encoders. However, they fail to fully utilize the structure information of the input graph. In this paper, we aim to further improve the performance of the pre-trained language model by proposing a structured graph-to-text model with a two-step fine-tuning mechanism which first fine-tunes model on Wikipedia before adapting to the graph-to-text generation. In addition to using the traditional token and position embeddings to encode the knowledge graph (KG), we propose a novel tree-level embedding method to capture the inter-dependency structures of the input graph. This new approach has significantly improved the performance of all text generation metrics for the English WebNLG 2017 dataset."
}

@inproceedings{xing-etal-2021-km,
    title = "{KM}-{BART}: Knowledge Enhanced Multimodal {BART} for Visual Commonsense Generation",
	author = "Xing, Yiran  and
      Shi, Zai  and
      Meng, Zhao  and
      Lakemeyer, Gerhard  and
      Ma, Yunpu  and
      Wattenhofer, Roger",
	editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-long.44/",
	doi = "10.18653/v1/2021.acl-long.44",
	pages = "525--535",
	abstract = "We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts. We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs. We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task. In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs. To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task. Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks."
}

@inproceedings{zhang-etal-2021-smedbert,
    title = "{SM}ed{BERT}: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining",
	author = "Zhang, Taolin  and
      Cai, Zerui  and
      Wang, Chengyu  and
      Qiu, Minghui  and
      Yang, Bite  and
      He, Xiaofeng",
	editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
	booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = aug,
	year = "2021",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.acl-long.457/",
	doi = "10.18653/v1/2021.acl-long.457",
	pages = "5882--5893",
	abstract = "Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbours of linked-entity. In SMedBERT, the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, question matching and natural language inference."
}

@inproceedings{yang-etal-2020-improving-text,
    title = "Improving Text-to-Text Pre-trained Models for the Graph-to-Text Task",
	author = "Yang, Zixiaofan  and
      Einolghozati, Arash  and
      Inan, Hakan  and
      Diedrick, Keith  and
      Fan, Angela  and
      Donmez, Pinar  and
      Gupta, Sonal",
	editor = "Castro Ferreira, Thiago  and
      Gardent, Claire  and
      Ilinykh, Nikolai  and
      van der Lee, Chris  and
      Mille, Simon  and
      Moussallem, Diego  and
      Shimorina, Anastasia",
	booktitle = "Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)",
	month = "12",
	year = "2020",
	address = "Dublin, Ireland (Virtual)",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.webnlg-1.11/",
	pages = "107--116",
	abstract = "Converting a knowledge graph or sub-graph to natural text is useful when answering questions based on a knowledge base. High-capacity language models pre-trained on large-scale text corpora have recently been shown to be powerful when fine-tuned for the knowledge-graph-to-text (KG-to-text) task. In this paper, we propose two classes of methods to improve such pre-trained models for this task. First, we improve the structure awareness of the model by organizing the input as well as learning optimal ordering via multitask learning. Second, we bridge the domain gap between text-to-text and KG-to-text tasks via a second-phase KG-to-text pre-training on similar datasets and extra lexicalization supervision to make the input more similar to natural text. We demonstrate the efficacy of our methods on the popular WebNLG dataset. Our best model achieves an almost 3 point BLEU improvement on a strong baseline while lowering the relative slot-error-rate by around 35{\%}. We also validate our results via human evaluation."
}

@inproceedings{liu-etal-2020-commonsense,
    title = "Commonsense Evidence Generation and Injection in Reading Comprehension",
	author = "Liu, Ye  and
      Yang, Tao  and
      You, Zeyu  and
      Fan, Wei  and
      Yu, Philip S.",
	editor = "Pietquin, Olivier  and
      Muresan, Smaranda  and
      Chen, Vivian  and
      Kennington, Casey  and
      Vandyke, David  and
      Dethlefs, Nina  and
      Inoue, Koji  and
      Ekstedt, Erik  and
      Ultes, Stefan",
	booktitle = "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
	month = jul,
	year = "2020",
	address = "1st virtual meeting",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.sigdial-1.9/",
	doi = "10.18653/v1/2020.sigdial-1.9",
	pages = "61--73",
	abstract = "Human tackle reading comprehension not only based on the given context itself but often rely on the commonsense beyond. To empower the machine with commonsense reasoning, in this paper, we propose a Commonsense Evidence Generation and Injection framework in reading comprehension, named CEGI. The framework injects two kinds of auxiliary commonsense evidence into comprehensive reading to equip the machine with the ability of rational thinking. Specifically, we build two evidence generators: one aims to generate textual evidence via a language model; the other aims to extract factual evidence (automatically aligned text-triples) from a commonsense knowledge graph after graph completion. Those evidences incorporate contextual commonsense and serve as the additional inputs to the reasoning model. Thereafter, we propose a deep contextual encoder to extract semantic relationships among the paragraph, question, option, and evidence. Finally, we employ a capsule network to extract different linguistic units (word and phrase) from the relations, and dynamically predict the optimal option based on the extracted units. Experiments on the CosmosQA dataset demonstrate that the proposed CEGI model outperforms the current state-of-the-art approaches and achieves the highest accuracy (83.6{\%}) on the leaderboard."
}

@inproceedings{teo-2020-tr,
    title = "{TR} at {S}em{E}val-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation",
	author = "Teo, Don",
	editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
	booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
	month = dec,
	year = "2020",
	address = "Barcelona (online)",
	publisher = "International Committee for Computational Linguistics",
	url = "https://aclanthology.org/2020.semeval-1.76/",
	doi = "10.18653/v1/2020.semeval-1.76",
	pages = "601--608",
	abstract = "In this paper, we present our submission for subtask A of the Common Sense Validation and Explanation (ComVE) shared task. We examine the ability of large-scale pre-trained language models to distinguish commonsense from non-commonsense statements. We also explore the utility of external resources that aim to supplement the world knowledge inherent in such language models, including commonsense knowledge graph embedding models, word concreteness ratings, and text-to-image generation models. We find that such resources provide insignificant gains to the performance of fine-tuned language models. We also provide a qualitative analysis of the limitations of the language model fine-tuned to this task."
}

@inproceedings{zhang-etal-2020-pretrain,
    title = "Pretrain-{KGE}: Learning Knowledge Representation from Pretrained Language Models",
	author = "Zhang, Zhiyuan  and
      Liu, Xiaoqian  and
      Zhang, Yi  and
      Su, Qi  and
      Sun, Xu  and
      He, Bin",
	editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.findings-emnlp.25/",
	doi = "10.18653/v1/2020.findings-emnlp.25",
	pages = "259--266",
	abstract = "Conventional knowledge graph embedding (KGE) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich knowledge representation via pretrained language models by leveraging world knowledge from pretrained models. Specifically, we present a universal training framework named \textit{Pretrain-KGE} consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and KGE training phase. Extensive experiments show that our proposed Pretrain-KGE can improve results over KGE models, especially on solving the low-resource problem."
}

@inproceedings{huang-etal-2020-biomedical,
    title = "Biomedical Event Extraction with Hierarchical Knowledge Graphs",
	author = "Huang, Kung-Hsiang  and
      Yang, Mu  and
      Peng, Nanyun",
	editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.findings-emnlp.114/",
	doi = "10.18653/v1/2020.findings-emnlp.114",
	pages = "1277--1285",
	abstract = "Biomedical event extraction is critical in understanding biomolecular interactions described in scientific corpus. One of the main challenges is to identify nested structured events that are associated with non-indicative trigger words. We propose to incorporate domain knowledge from Unified Medical Language System (UMLS) to a pre-trained language model via Graph Edge-conditioned Attention Networks (GEANet) and hierarchical graph representation. To better recognize the trigger words, each sentence is first grounded to a sentence graph based on a jointly modeled hierarchical knowledge graph from UMLS. The grounded graphs are then propagated by GEANet, a novel graph neural networks for enhanced capabilities in inferring complex events. On BioNLP 2011 GENIA Event Extraction task, our approach achieved 1.41{\%} F1 and 3.19{\%} F1 improvements on all events and complex events, respectively. Ablation studies confirm the importance of GEANet and hierarchical KG."
}

@inproceedings{he-etal-2020-bert,
    title = "{BERT}-{MK}: Integrating Graph Contextualized Knowledge into Pre-trained Language Models",
	author = "He, Bin  and
      Zhou, Di  and
      Xiao, Jinghui  and
      Jiang, Xin  and
      Liu, Qun  and
      Yuan, Nicholas Jing  and
      Xu, Tong",
	editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.findings-emnlp.207/",
	doi = "10.18653/v1/2020.findings-emnlp.207",
	pages = "2281--2290",
	abstract = "Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model subgraphs in a medical KG. Then, the learned knowledge is integrated with a pre-trained language model to do the knowledge generalization. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial."
}

@inproceedings{wang-etal-2020-connecting,
    title = "Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering",
	author = "Wang, Peifeng  and
      Peng, Nanyun  and
      Ilievski, Filip  and
      Szekely, Pedro  and
      Ren, Xiang",
	editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.findings-emnlp.369/",
	doi = "10.18653/v1/2020.findings-emnlp.369",
	pages = "4129--4140",
	abstract = "Commonsense question answering (QA) requires background knowledge which is not explicitly stated in a given context. Prior works use commonsense knowledge graphs (KGs) to obtain this knowledge for reasoning. However, relying entirely on these KGs may not suffice, considering their limited coverage and the contextual dependence of their knowledge. In this paper, we augment a general commonsense QA framework with a knowledgeable path generator. By extrapolating over existing paths in a KG with a state-of-the-art language model, our generator learns to connect a pair of entities in text with a dynamic, and potentially novel, multi-hop relational path. Such paths can provide structured evidence for solving commonsense questions without fine-tuning the path generator. Experiments on two datasets show the superiority of our method over previous works which fully rely on knowledge from KGs (with up to 6{\%} improvement in accuracy), across various amounts of training data. Further evaluation suggests that the generated paths are typically interpretable, novel, and relevant to the task."
}

@inproceedings{banerjee-baral-2020-self,
    title = "Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering",
	author = "Banerjee, Pratyay  and
      Baral, Chitta",
	editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.11/",
	doi = "10.18653/v1/2020.emnlp-main.11",
	pages = "151--162",
	abstract = "The aim of all Question Answering (QA) systems is to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task. This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models."
}

@inproceedings{ji-etal-2020-language,
    title = "Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph",
	author = "Ji, Haozhe  and
      Ke, Pei  and
      Huang, Shaohan  and
      Wei, Furu  and
      Zhu, Xiaoyan  and
      Huang, Minlie",
	editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.54/",
	doi = "10.18653/v1/2020.emnlp-main.54",
	pages = "725--736",
	abstract = "Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation."
}

@inproceedings{feng-etal-2020-scalable,
    title = "Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering",
	author = "Feng, Yanlin  and
      Chen, Xinyue  and
      Lin, Bill Yuchen  and
      Wang, Peifeng  and
      Yan, Jun  and
      Ren, Xiang",
	editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.99/",
	doi = "10.18653/v1/2020.emnlp-main.99",
	pages = "1295--1309",
	abstract = "Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model`s prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released."
}

@inproceedings{li-etal-2020-towards,
    title = "Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text",
	author = "Li, Dongfang  and
      Hu, Baotian  and
      Chen, Qingcai  and
      Peng, Weihua  and
      Wang, Anqi",
	editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.111/",
	doi = "10.18653/v1/2020.emnlp-main.111",
	pages = "1427--1438",
	abstract = "Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models. However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect. As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China. It is a challenging medical examination with a passing rate of less than 14.2{\%} in 2018. Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books). The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8{\%} accuracy rate on the test set."
}

@inproceedings{zhao-etal-2020-learning,
    title = "Learning {P}hysical {C}ommon {S}ense as {K}nowledge {G}raph {C}ompletion via {BERT} {D}ata {A}ugmentation and {C}onstrained {T}ucker {F}actorization",
	author = "Zhao, Zhenjie  and
      Papalexakis, Evangelos  and
      Ma, Xiaojuan",
	editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.266/",
	doi = "10.18653/v1/2020.emnlp-main.266",
	pages = "3293--3298",
	abstract = "Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction. Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization. In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples. Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small. To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships. We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance."
}

@inproceedings{shen-etal-2020-exploiting,
    title = "Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning",
	author = "Shen, Tao  and
      Mao, Yi  and
      He, Pengcheng  and
      Long, Guodong  and
      Trischler, Adam  and
      Chen, Weizhu",
	editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.722/",
	doi = "10.18653/v1/2020.emnlp-main.722",
	pages = "8980--8994",
	abstract = "In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion."
}

@inproceedings{chang-etal-2020-incorporating,
    title = "Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks",
	author = "Chang, Ting-Yun  and
      Liu, Yang  and
      Gopalakrishnan, Karthik  and
      Hedayatnia, Behnam  and
      Zhou, Pei  and
      Hakkani-Tur, Dilek",
	editor = "Agirre, Eneko  and
      Apidianaki, Marianna  and
      Vuli{\'c}, Ivan",
	booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.deelio-1.9/",
	doi = "10.18653/v1/2020.deelio-1.9",
	pages = "74--79",
	abstract = "Pretrained language models have excelled at many NLP tasks recently; however, their social intelligence is still unsatisfactory. To enable this, machines need to have a more general understanding of our complicated world and develop the ability to perform commonsense reasoning besides fitting the specific downstream tasks. External commonsense knowledge graphs (KGs), such as ConceptNet, provide rich information about words and their relationships. Thus, towards general commonsense learning, we propose two approaches to implicitly and explicitly infuse such KGs into pretrained language models. We demonstrate our proposed methods perform well on SocialIQA, a social commonsense reasoning task, in both limited and full training data regimes."
}

@inproceedings{patel-ferraro-2020-complementary,
    title = "On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling",
	author = "Patel, Rajat  and
      Ferraro, Francis",
	editor = "Agirre, Eneko  and
      Apidianaki, Marianna  and
      Vuli{\'c}, Ivan",
	booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.deelio-1.11/",
	doi = "10.18653/v1/2020.deelio-1.11",
	pages = "89--99",
	abstract = "We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge graph embeddings and fine-grain entity type representations. Our work also shows that jointly modeling both structured knowledge tuples and language improves both."
}

@inproceedings{kim-etal-2020-multi,
    title = "Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models",
	author = "Kim, Bosung  and
      Hong, Taesuk  and
      Ko, Youngjoong  and
      Seo, Jungyun",
	editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
	booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
	month = dec,
	year = "2020",
	address = "Barcelona, Spain (Online)",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2020.coling-main.153/",
	doi = "10.18653/v1/2020.coling-main.153",
	pages = "1737--1743",
	abstract = "As research on utilizing human knowledge in natural language processing has attracted considerable attention in recent years, knowledge graph (KG) completion has come into the spotlight. Recently, a new knowledge graph completion method using a pre-trained language model, such as KG-BERT, is presented and showed high performance. However, its scores in ranking metrics such as Hits@k are still behind state-of-the-art models. We claim that there are two main reasons: 1) failure in sufficiently learning relational information in knowledge graphs, and 2) difficulty in picking out the correct answer from lexically similar candidates. In this paper, we propose an effective multi-task learning method to overcome the limitations of previous works. By combining relation prediction and relevance ranking tasks with our target link prediction, the proposed model can learn more relational properties in KGs and properly perform even when lexical similarity occurs. Experimental results show that we not only largely improve the ranking performances compared to KG-BERT but also achieve the state-of-the-art performances in Mean Rank and Hits@10 on the WN18RR dataset."
}

@inproceedings{yadav-etal-2020-medical,
    title = "Medical Knowledge-enriched Textual Entailment Framework",
	author = "Yadav, Shweta  and
      Pallagani, Vishal  and
      Sheth, Amit",
	editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
	booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
	month = dec,
	year = "2020",
	address = "Barcelona, Spain (Online)",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2020.coling-main.161/",
	doi = "10.18653/v1/2020.coling-main.161",
	pages = "1795--1801",
	abstract = "One of the cardinal tasks in achieving robust medical question answering systems is textual entailment. The existing approaches make use of an ensemble of pre-trained language models or data augmentation, often to clock higher numbers on the validation metrics. However, two major shortcomings impede higher success in identifying entailment: (1) understanding the focus/intent of the question and (2) ability to utilize the real-world background knowledge to capture the con-text beyond the sentence. In this paper, we present a novel Medical Knowledge-Enriched Textual Entailment framework that allows the model to acquire a semantic and global representation of the input medical text with the help of a relevant domain-specific knowledge graph. We evaluate our framework on the benchmark MEDIQA-RQE dataset and manifest that the use of knowledge-enriched dual-encoding mechanism help in achieving an absolute improvement of 8.27{\%} over SOTA language models."
}

@inproceedings{sun-etal-2020-colake,
    title = "{C}o{LAKE}: Contextualized Language and Knowledge Embedding",
	author = "Sun, Tianxiang  and
      Shao, Yunfan  and
      Qiu, Xipeng  and
      Guo, Qipeng  and
      Hu, Yaru  and
      Huang, Xuanjing  and
      Zhang, Zheng",
	editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
	booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
	month = dec,
	year = "2020",
	address = "Barcelona, Spain (Online)",
	publisher = "International Committee on Computational Linguistics",
	url = "https://aclanthology.org/2020.coling-main.327/",
	doi = "10.18653/v1/2020.coling-main.327",
	pages = "3660--3670",
	abstract = "With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing models consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these models. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the knowledge context of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and language context, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation."
}

@inproceedings{logan-iv-etal-2020-importance,
    title = "On Importance Sampling-Based Evaluation of Latent Language Models",
	author = "Logan IV, Robert L.  and
      Gardner, Matt  and
      Singh, Sameer",
	editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.196/",
	doi = "10.18653/v1/2020.acl-main.196",
	pages = "2171--2176",
	abstract = "Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models. However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space. Existing works avoid this issue by using importance sampling. Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates. In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM. In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique."
}

@inproceedings{huang-etal-2020-knowledge,
    title = "Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward",
	author = "Huang, Luyang  and
      Wu, Lingfei  and
      Wang, Lu",
	editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.457/",
	doi = "10.18653/v1/2020.acl-main.457",
	pages = "5094--5107",
	abstract = "Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders{---}a sequential document encoder and a graph-structured encoder{---}to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors."
}

@inproceedings{joshi-etal-2019-comparison,
    title = "A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics",
	author = "Joshi, Aditya  and
      Karimi, Sarvnaz  and
      Sparks, Ross  and
      Paris, Cecile  and
      MacIntyre, C Raina",
	editor = "Demner-Fushman, Dina  and
      Cohen, Kevin Bretonnel  and
      Ananiadou, Sophia  and
      Tsujii, Junichi",
	booktitle = "Proceedings of the 18th BioNLP Workshop and Shared Task",
	month = aug,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W19-5015/",
	doi = "10.18653/v1/W19-5015",
	pages = "135--141",
	abstract = "Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4{\%} in the accuracy when these context-based representations are used instead of word-based representations."
}

@inproceedings{zhang-etal-2019-ernie,
    title = "{ERNIE}: Enhanced Language Representation with Informative Entities",
	author = "Zhang, Zhengyan  and
      Han, Xu  and
      Liu, Zhiyuan  and
      Jiang, Xin  and
      Sun, Maosong  and
      Liu, Qun",
	editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P19-1139/",
	doi = "10.18653/v1/P19-1139",
	pages = "1441--1451",
	abstract = "Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future."
}

@inproceedings{bosselut-etal-2019-comet,
    title = "{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction",
	author = "Bosselut, Antoine  and
      Rashkin, Hannah  and
      Sap, Maarten  and
      Malaviya, Chaitanya  and
      Celikyilmaz, Asli  and
      Choi, Yejin",
	editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P19-1470/",
	doi = "10.18653/v1/P19-1470",
	pages = "4762--4779",
	abstract = "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5{\%} (ATOMIC) and 91.7{\%} (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods."
}

@inproceedings{logan-etal-2019-baracks,
    title = "{B}arack`s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling",
	author = "Logan, Robert  and
      Liu, Nelson F.  and
      Peters, Matthew E.  and
      Gardner, Matt  and
      Singh, Sameer",
	editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
	booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2019",
	address = "Florence, Italy",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P19-1598/",
	doi = "10.18653/v1/P19-1598",
	pages = "5962--5971",
	abstract = "Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model (KGLM), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked WikiText-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular WikiText-2 benchmark. In experiments, we demonstrate that the KGLM achieves significantly better performance than a strong baseline language model. We additionally compare different language model`s ability to complete sentences requiring factual knowledge, showing that the KGLM outperforms even very large language models in generating facts."
}

@inproceedings{li-etal-2019-pingan,
    title = "Pingan Smart Health and {SJTU} at {COIN} - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks",
	author = "Li, Xiepeng  and
      Zhang, Zhexi  and
      Zhu, Wei  and
      Li, Zheng  and
      Ni, Yuan  and
      Gao, Peng  and
      Yan, Junchi  and
      Xie, Guotong",
	editor = "Ostermann, Simon  and
      Zhang, Sheng  and
      Roth, Michael  and
      Clark, Peter",
	booktitle = "Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-6011/",
	doi = "10.18653/v1/D19-6011",
	pages = "93--98",
	abstract = "To solve the shared tasks of COIN: COmmonsense INference in Natural Language Processing) Workshop in , we need explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The first is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a model to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply concatenation or multi-head attention. We find out that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches achieve the state-of-the-art results on both shared task`s official test data, outperforming all the other submissions."
}

@inproceedings{sharma-etal-2019-incorporating,
    title = "Incorporating Domain Knowledge into Medical {NLI} using Knowledge Graphs",
	author = "Sharma, Soumya  and
      Santra, Bishal  and
      Jana, Abhik  and
      T.y.s.s, Santosh  and
      Ganguly, Niloy  and
      Goyal, Pawan",
	editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1631/",
	doi = "10.18653/v1/D19-1631",
	pages = "6092--6097",
	abstract = "Recently, biomedical version of embeddings obtained from language models such as BioELMo have shown state-of-the-art results for the textual inference task in the medical domain. In this paper, we explore how to incorporate structured domain knowledge, available in the form of a knowledge graph (UMLS), for the Medical NLI task. Specifically, we experiment with fusing embeddings obtained from knowledge graph with the state-of-the-art approaches for NLI task (ESIM model). We also experiment with fusing the domain-specific sentiment information for the task. Experiments conducted on MedNLI dataset clearly show that this strategy improves the baseline BioELMo architecture for the Medical NLI task."
}
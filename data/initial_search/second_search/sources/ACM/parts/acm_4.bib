@article{10.1145/3711682,
author = {Li, Tong and Long, Qingyue and Chai, Haoye and Zhang, Shiyuan and Jiang, Fenyu and Liu, Haoqiang and Huang, Wenzhen and Jin, Depeng and Li, Yong},
title = {Generative AI Empowered Network Digital Twins: Architecture, Technologies, and Applications},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3711682},
doi = {10.1145/3711682},
abstract = {The rapid advancement of mobile networks highlights the limitations of traditional network planning and optimization methods, particularly in modeling, evaluation, and application. Network Digital Twins, which simulate networks in the digital domain for evaluation, offer a solution to these challenges. This concept is further enhanced by generative AI technology, which promises more efficient and accurate AI-driven data generation for network simulation and optimization. This survey provides insights into generative AI-empowered network digital twins. We begin by outlining the architecture of a network digital twin, which encompasses both digital and physical domains. This architecture involves four key steps: data processing and network monitoring, digital replication and network simulation, designing and training network optimizers, Sim2Real, and network control. Next, we systematically discuss the related studies in each step and make a detailed taxonomy of the problem studied, the methods used, and the key designs leveraged. Each step is examined with a focus on the role of generative AI, from estimating missing data and simulating network behaviors to designing control strategies and bridging the gap between digital and physical domains. Finally, we discuss the open issues and challenges of generative AI-based network digital twins.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {157},
numpages = {43},
keywords = {Generative AI, digital twins, mobile networks, network monitoring, network simulation, network operation}
}

@article{10.1145/3689372,
author = {Jaidka, Kokil and Chen, Tsuhan and Chesterman, Simon and Hsu, Wynne and Kan, Min-Yen and Kankanhalli, Mohan and Lee, Mong Li and Seres, Gyula and Sim, Terence and Taeihagh, Araz and Tung, Anthony and Xiao, Xiaokui and Yue, Audrey},
title = {Misinformation, Disinformation, and Generative AI: Implications for Perception and Policy},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3689372},
doi = {10.1145/3689372},
abstract = {The emergence of generative artificial intelligence (GenAI) has exacerbated the challenges of misinformation, disinformation, and mal-information (MDM) within digital ecosystems. These multi-faceted challenges demand a re-evaluation of the digital information lifecycle and a deep understanding of its social impact. An interdisciplinary strategy integrating insights from technology, social sciences, and policy analysis is crucial to address these issues effectively. This article introduces a three-tiered framework to scrutinize the lifecycle of GenAI-driven content from creation to consumption, emphasizing the consumer perspective. We examine the dynamics of consumer behavior that drive interactions with MDM, pinpoints vulnerabilities in the information dissemination process, and advocates for adaptive, evidence-based policies. Our interdisciplinary methodology aims to bolster information integrity and fortify public trust, equipping digital societies to manage the complexities of GenAI and proactively address the evolving challenges of digital misinformation. We conclude by discussing how GenAI can be leveraged to combat MDM, thereby creating a reflective cycle of technological advancement and mitigation.},
journal = {Digit. Gov.: Res. Pract.},
month = feb,
articleno = {11},
numpages = {15},
keywords = {Misinformation, disinformation, trust, resilience, generative AI, social media}
}

@inproceedings{10.1145/3704289.3704300,
author = {Xiong, Xiao-Gang and Zeng, Meng-Ting},
title = {Research hotspots and path evolution of generative AI development--A Bibliometric Analysis Based on CiteSpace},
year = {2025},
isbn = {9798400716980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704289.3704300},
doi = {10.1145/3704289.3704300},
booktitle = {Proceedings of the 2024 7th International Conference on Big Data and Education},
pages = {22–28},
numpages = {7},
keywords = {Citespace, bibliometrics, generative AI},
location = {
},
series = {ICBDE '24}
}

@article{10.1145/3713080,
author = {Li, Xiangyang and Chen, Bo and Hou, Lu and TANG, Ruiming},
title = {CTRL: Connect Collaborative and Language Model for CTR Prediction},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713080},
doi = {10.1145/3713080},
abstract = {Traditional click-through rate (CTR) prediction models convert the tabular data into one-hot vectors and leverage the collaborative relations among features for inferring the user’s preference over items. This modeling paradigm discards essential semantic information. Though some works like P5 and KAR have explored the potential of using Pre-trained Language Models (PLMs) to extract semantic signals for CTR prediction, they are computationally expensive and suffer from low efficiency. Besides, the beneficial collaborative relations are not considered, hindering the recommendation performance. To solve these problems, in this paper, we propose a novel framework CTRL, which is industrial-friendly and model-agnostic with superior inference efficiency. Specifically, the original tabular data is first converted into textual data. Both tabular data and converted textual data are regarded as two different modalities and are separately fed into the collaborative CTR model and pre-trained language model. A cross-modal knowledge alignment procedure is performed to fine-grained align and integrate the collaborative and semantic signals, and the lightweight collaborative model can be deployed online for efficient serving after fine-tuned with supervised signals. Experimental results on three public datasets show that CTRL outperforms the state-of-the-art (SOTA) CTR models significantly. Moreover, we further verify its effectiveness on a large-scale industrial recommender system.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = feb,
keywords = {Click-Through Rate Prediction, Language Model, Recommender System}
}

@inproceedings{10.1145/3707292.3707356,
author = {Zhao, Pei and Zhang, Longxing and Zhao, Jiawen},
title = {Complete the exploration of low-resource knowledge graph completion based on large model technology},
year = {2025},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707292.3707356},
doi = {10.1145/3707292.3707356},
abstract = {In the construction and application of knowledge graph, it is a realistic research problem to complete the knowledge in the low resource field. Traditional methods that rely on manual annotation and rules are not only costly, but also have limitations in coverage and scalability. To solve this problem, this paper proposes a large model technique, combining fine-tuning and knowledge transfer strategies. Firstly, to improve the ability of fine-tuning of the large model, the rich knowledge learned by the large model in the high resource field to assist the completion of the low resource knowledge graph through knowledge transfer technology to make up for the shortage of direct extraction. The experimental results show that this method can effectively improve the completion rate and accuracy of the knowledge graph, especially in the completion of entity relations and attribute filling. Furthermore, we explore the impact of different fine-tuning strategies and knowledge transfer methods on the completion effect, providing experimental empirical and theoretical support for future studies on similar issues.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
pages = {140–145},
numpages = {6},
keywords = {Fine-tuning, Knowledge graph, Large model technology, Low resource, Transfer learning},
location = {
},
series = {AIIIP '24}
}

@article{10.1145/3714429,
author = {Zhang, Hongbin and Wang, Tao and Wang, Zhuowei and Lin, Nankai and Chen, Chong and Cheng, Lianglun},
title = {A GPT-assisted Multi-Granularity Contrastive Learning approach for Knowledge Graph Entity Typing},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3714429},
doi = {10.1145/3714429},
abstract = {Knowledge graph entity typing (KGET) is an efficient way to infer possible missing types for entities, which has become a key instrument to enhance the construction of knowledge graphs (KGs). Existing models to KGET have mainly focused on a single granularity information such as distinct entity information, but other granularity information including entity-to-type-clusters, the same cluster and interaction information have not been fully explored, resulting in inferring incorrect types in KGs. To address this, we propose a GPT-assisted Multi-Granularity Contrastive Learning (GMGCL) approach to acquire entity-to-type-clusters, entity, type-cluster and relation information by GPT-assisted entity-to-type-clusters clustering, entity-based, cluster-based and relation-based contrastive learning, respectively. Our approach is evaluated on FB15kET and YAGO43kET datasets, outperforming other baselines and obtaining a 1.35% average improvement at least on MRR.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
keywords = {Knowledge Graph Entity Typing, Generative Pre-trained Transformer, Contrastive Learning, Knowledge Representation}
}

@article{10.1145/3696662,
author = {Peng, Yingtao and Gao, Chen and Zhang, Yu and Dan, Tangpeng and Du, Xiaoyi and Luo, Hengliang and Li, Yong and Meng, Xiaofeng},
title = {Denoising Alignment with Large Language Model for Recommendation},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3696662},
doi = {10.1145/3696662},
abstract = {The mainstream approach of GNN-based recommendation aggregates high-order ID information associated with the node in the user-item graph. The aggregation pattern using ID as signal has two disadvantages: lack of textual semantics and the impact of interaction noise. These disadvantages pose a threat to effectively learn user preferences, especially in capturing intricate user-item semantic relationships. Although large language models (LLMs) allow the integration of rich textual information into recommenders and have had groundbreaking applications in recommender systems, current works need to bridge the gap between different representation spaces. This is because LLM-based methods align the representations of GNN-based models only by using text embedding of LLM, leading to unsatisfactory results. To address this challenge, we propose a denoising alignment framework with LLMs for GNN-based recommenders (DALR), which aims to align structural representation with textual representation and mitigate the effects of noise. Specifically, we propose a modeling framework that integrates the representation of graph structure with textual information from LLMs to capture intricate user-item interactions. We also suggest an alignment paradigm to enhance representation performance by aligning semantic signals from LLMs and structural features from GNN models. Additionally, we introduce a contrastive learning scheme to relieve the impact of noise and improve model performance. Extensive experiments on public datasets demonstrate that our model consistently outperforms the state-of-the-art methods. DALR achieves improvements ranging from 2.82% to 12.20% in Recall@5 and from 1.04% to 3.48% in NDCG@5 compared to the strongest baseline model, using the Steam dataset as an example.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {32},
numpages = {35},
keywords = {Recommender system, graph neural network, large language models, contrastive learning}
}

@inproceedings{10.1145/3708036.3708272,
author = {Yang, Guangyuan and Xie, Quanying and Chen, Lei},
title = {A Scientometrics Analysis and Visualization of Large Language Model in China's Library},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708272},
doi = {10.1145/3708036.3708272},
abstract = {Large Language Model has been researched in the field of library from the following aspects:space reproduction, service reform, library construction and so on. In order to clarify the current research situation of Large Language Model's application research in the field of library, and provide some reference for the further development of research fields related to Large Language Model empowering library in the future. This paper utilizes two methods of scientometrics and data visualization to analyze and study the journal papers on the application of Large Language Model in the field of Chinese libraries from the aspects of the degree of academic focus, the way of creating academic achievements and research topics of academic achievements, and puts forward the research practice of strengthening the application of Large Language Model in library from the aspects of ’Strengthen the practical research of Large Language Model empowering Chinese library’ and ‘Broaden the field of research related to Large Language Model empowering Chinese library’, in order to promote the all-round development of Large Language Model in the field of library.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {1403–1407},
numpages = {5},
keywords = {Chinese libraries, Data Visualization, Large Language Model, Library Service, Scientometrics},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.1145/3706890.3706997,
author = {Nan, Beier and Gu, Jinguang and Qiu, Chen and Wu, Jingyun},
title = {Construction and application of medical history knowledge graph based on UIE model fine-tuning},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706997},
doi = {10.1145/3706890.3706997},
abstract = {Objective: To achieve the automated extraction of complex medical history knowledge from Chinese electronic medical records, a fine-tuned UIE extraction model is utilized to automatically obtain medical history knowledge and construct a knowledge graph (KG) of medical histories. Method: Taking the current medical history as an example, a fundamental knowledge base of Chinese medical history was first built. Then, based on this knowledge base, a training set was annotated, and the UIE model was fine-tuned using this training set. The fine-tuned UIE model was then used to extract medical history knowledge, which was processed and stored to generate a KG of medical histories. Results: The fine-tuned UIE model achieved entity, relationship, and event extraction tasks. Subsequently, the extracted medical history information was processed and stored, successfully constructing a KG of the current medical history. Conclusion: This method realizes the completion of entity, relation, and event extraction tasks by training only one model, efficiently achieving automatic extraction of specified medical history knowledge from Chinese electronic medical records and constructing a KG of medical histories. It helps organize and analyze complex medical history knowledge for clinical use, offering practical value.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {621–626},
numpages = {6},
keywords = {Constructing KG, Information extraction, Medical history knowledge, Model fine-tuning},
location = {
},
series = {ISAIMS '24}
}

@inproceedings{10.1145/3706890.3706892,
author = {Duan, Yidan and Lin, Shaofu and Liu, Xiliang and Huang, Zhisheng and Su, Haoru and Bai, Yingfan},
title = {ElderQA-GPT: A Large Language Model for Online Q&amp;A on Geriatric Diseases Based on BGE Semantic Vector Knowledge Base and LangChain Architecture},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706892},
doi = {10.1145/3706890.3706892},
abstract = {Scientific guidance of disease Q&amp;A for the elderly is of distinct significance for improving the health level of the elderly, which is the immediate need of the current aging society. The traditional question answering system for geriatric diseases based on knowledge graph relies heavily on manual annotation and has some problems such as difficulty in knowledge fusion. The rapid development of large models provides a new opportunity for geriatric disease Q&amp;A system, but it also has problems such as interpretability, applicability and multi-round Q&amp;A illusion. In this paper, we propose a large language model ElderQA-GPT for online Q&amp;A of geriatric diseases based on BGE semantic vector knowledge base and LangChain architecture. First, in order to improve the interpretability of Q&amp;A dialogues, this paper adopts the SELF-QA framework and combines the current professional websites of geriatric diseases (including Dr. Ding Xiang, Seeking Medical Care, and the Chinese government website) to construct a vector database of geriatric diseases based on the BGE semantic vector model; secondly, in order to improve the accuracy and applicability of the system, the lightweight open source ChatGLM3 model is privately deployed based on the LangChain architecture, which enhances the retrieval capability of the localized inference knowledge base; lastly, in order to better accumulate and comprehend the information in the complex contexts and multiple rounds of dialogues, and to maintain the semantic consistency, a LoRA fine-tuning technique is used to enhance the ElderQA-GPT multi-round dialogue capability. Two sets of quantitative experiments were conducted to explore two dimensions: the interpretability of model responses and the resolution of the illusion problem in multi-round Q&amp;A sessions. Additionally, an ablation study was carried out by omitting the knowledge base to comparatively evaluate the impact on the system's performance. The validation results demonstrate that ElderQA-GPT exhibits improved answer interpretability and applicability, effectively maintaining semantic consistency across multi-round dialogues without introducing false information. The findings of this study significantly advance the field of online Q&amp;A for geriatric diseases by addressing critical challenges such as interpretability, accuracy, and multi-round dialogue. Our proposed model ElderQA-GPT not only improves public knowledge and understanding of geriatric diseases but also optimizes medical resource allocation and enhances health management for the elderly population. By providing interpretable and accurate responses across multi-round dialogues, ElderQA-GPT ensures that users receive reliable medical information tailored to their needs. These advancements have profound implications for healthcare delivery and society, paving the way for more efficient and effective healthcare services for elderly individuals.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {9–15},
numpages = {7},
keywords = {BGE, ElderQA-GPT, LangChain, SELF-QA},
location = {
},
series = {ISAIMS '24}
}

@article{10.1145/3704262,
author = {Cao, Yihan and Li, Siyu and Liu, Yixin and Yan, Zhiling and Dai, Yutong and Yu, Philip and Sun, Lichao},
title = {A Survey of AI-Generated Content (AIGC)},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3704262},
doi = {10.1145/3704262},
abstract = {Recently, Artificial Intelligence Generated Content (AIGC) has gained significant attention from society, especially with the rise of Generative AI (GAI) techniques such as ChatGPT, GPT-4 [165], DALL-E-3 [184], and Sora [137]. AIGC involves using AI models to create digital content, such as images, music, and natural language, with the goal of making the content creation process more efficient and accessible. Large-scale models have become increasingly important in AIGC as they provide better intent extraction and generation results. This survey provides a comprehensive review of the history of generative models and recent advances in AIGC, focusing on both unimodal and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, the survey discusses the existing open problems and future challenges in AIGC. Overall, this survey serves as a valuable resource for individuals interested in understanding the background and secrets behind the impressive performance of AIGC techniques.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {125},
numpages = {38},
keywords = {Generative AI, AI-generated content, multimodal machine learning}
}

@article{10.1145/3711857,
author = {Hu, Linmei and Zhang, Xinyu and Song, Dandan and Zhou, Changzhi and He, Hongyu and Nie, Liqiang},
title = {Efficient and Effective Role Player: A Compact Knowledge-grounded Persona-based Dialogue Model Enhanced by LLM Distillation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3711857},
doi = {10.1145/3711857},
abstract = {Incorporating explicit personas into dialogue models is critical for generating responses that fulfill specific user needs and preferences, creating a more personalized and engaging interaction. Early works on persona-based dialogue generation directly concatenate the persona descriptions and dialogue history into relatively small pre-trained language models (PLMs) for response generation, which leads to uninformative and inferior results due to the sparse persona information and the limited model generation capabilities. Recently, large language models (LLMs) have shown their surprising capabilities in language generation. Prompting the LLMs with the persona descriptions for role-playing dialogue generation has also achieved promising results. However, deploying LLMs is challenging for practical applications due to their large scale, spurring efforts to distill the generation capabilities into more concise and compact models through teacher-student learning. In this paper, we propose an efficient compact Knowledge-grounded Persona-based Dialogue model enhanced by LLM Distillation (KPDD). Specifically, first, we propose to enrich the annotated persona descriptions by integrating external knowledge graphs (KGs) with a mixed encoding network, coupled with a mixture of experts (MoE) module for both informative and diverse response generation. The mixed encoding network contains multiple layers of modality interaction operations, enabling information from both modalities propagates to the other. Second, to fully exploit the generation capabilities of LLMs, we turn to the distillation technique to improve the generation capabilities of our model, facilitated by a natural language inference (NLI) based filtering mechanism to extract high-quality information from LLMs. In addition, we employ a curriculum learning strategy to train our model on the high-quality filtered distilled data and progressively on the relatively noisy original data, enhancing its adaptability and performance. Extensive experiments show that KPDD outperforms state-of-the-art baselines in terms of both automatic and human evaluation.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = jan,
keywords = {Persona-based Dialogue Generation, Knowledge Graph, MoE, Large Language Model, Distillation, Curriculum Learning}
}

@article{10.1145/3715909,
author = {Wu, Chunlian and Chen, Sen and Li, Jiaming and Chai, Renchao and Fan, Lingling and Xie, Xiaofei and Feng, Ruitao},
title = {Beyond Decision: Android Malware Description Generation through Profiling Malicious Behavior Trajectory},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715909},
doi = {10.1145/3715909},
abstract = {Malware family labels and key features used for the decision-making of Android malware detection models fall short of precise comprehension of malicious behaviors due to their coarse granularity. To solve these problems, in this paper, we first introduce the concept of the malicious behavior trajectory (MBT) and propose an innovative approach called ProMal. ProMal aims to automatically generate malware descriptions with fine granularity through extracted MBTs from malware for users. Specifically, a labeled dataset of MBTs is constructed through substantial human efforts to build a behavioral knowledge graph (BxKG). The BxKG is scalable and can be automatically updated using two strategies to ensure its completeness and timeliness: 1) taking into consideration the evolution of Android SDKs, and 2) mining new MBTs by leveraging the widely-used malware datasets. We highlight that the knowledge graph is essential in ProMal, which can reason new MBTs based on existing MBTs because of its structured data representation and semantic relation modeling, and thus helps effectively extract real MBTs in Android malware. We evaluated ProMal on a recent malware dataset where researcher-crafted malware descriptions are available, and the Precision, Recall, and F1-Score of MBT identification based on BxKG reached 96.97%, 91.43%, and 0.94, respectively, outperforming the state-of-the-art approaches. Taking MBTs identified from Android malware as inputs, precise, fine-grained, and human-readable descriptions can be generated using the large language model, whose readability and usability are verified through a user study. The generated descriptions play a significant role in interpreting and comprehending malware behaviors.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Android Malware, Malicious Behavior Analysis, Knowledge Graph}
}

@inproceedings{10.1145/3707292.3707389,
author = {Li, Yanjun and Yang, Ruiting and Guo, Donghao and Song, Yu},
title = {Research on the Construction of Digital Knowledge Graphs Based on Resources of National First-Class Undergraduate Programs},
year = {2025},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707292.3707389},
doi = {10.1145/3707292.3707389},
abstract = {[Purpose/Significance]: The digitalization of education is an essential path to advancing higher education. The construction of knowledge graphs is a key approach to achieving the digitalization and intelligence of education. [Method/Process]: This paper leverages the rich video resources of existing national first-class undergraduate programs and, based on the teaching orientations of different universities, independently designs customized ontologies and extraction principles. These are then integrated into the LLM knowledge graph builder to ensure the hierarchical structure of the overall course framework. The course video content is transformed into text form, and large language models (LLMS) and word segmentation tools are used for core content extraction, text cleaning, and lexical analysis. The structured text is then converted into SPO (Subject-Predicate-Object) triplets database. [Results/Conclusions]: Finally, the database is imported into the LLM knowledge graph builder, which is pre-configured with extraction rules. It will automatically generate the knowledge graph. After the text is imported into the LLM knowledge graph builder, it will be manually checked to ensure it better meets the actual needs of the students. [Innovation/Limitations]: The research team plans to apply the knowledge graph to train a specialized knowledge-based Q&amp;A assistant. This will support students' understanding and self-assessment of knowledge points in an online learning community. Student feedback will be used to improve and enrich the knowledge graph. Compared to existing methods, this approach better aligns with the constantly evolving digital teaching resources available online, offering more comprehensive and higher-level automation.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
pages = {353–359},
numpages = {7},
keywords = {Knowledge graph, course resources, intelligent Q&amp;A, ontology construction, personalized learning},
location = {
},
series = {AIIIP '24}
}

@inproceedings{10.1145/3704323.3704357,
author = {Zhang, Lu and Liu, Yu and Luo, Yitian and Gao, Feng and Gu, Jinguang},
title = {Qwen-IG: A Qwen-based Instruction Generation Model for LLM Fine-tuning},
year = {2025},
isbn = {9798400717482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704323.3704357},
doi = {10.1145/3704323.3704357},
abstract = {The quality of instructions is crucial for LLM (large language model) fine-tuning. The most compelling data for instruction tuning exhibit not only high complexity, low perplexity, high faithfulness, and high answer relevancy, but also diversity, naturalness, coherence, and understandability. However, traditional QAG (question and answer generation) and LLMs face challenges regarding instruction complexity, answer relevancy and diversity. To overcome these issues, we have developed the Qwen-IG instruction generation model for LLM fine-tuning. Firstly, we collect high-quality instruction sets, such as alpaca, GPT4-LLM, and TigerBot, and utilize them to build a train set for LLM fine-tuning. Secondly, Qwen is trained with the train set just generated through three approaches, namely C2IO (context to instruction and output), C2I2O (context to instruction to output), and C2O2I (context to output to instruction). Finally, we employ the eight metrics mentioned above to evaluate the effectiveness of Qwen-IG in the task of generating instructions. The experimental results demonstrate Qwen-IG has increased at least 9.6% on complexity, 23% on answer relevancy and 17% on diversity.},
booktitle = {Proceedings of the 2024 13th International Conference on Computing and Pattern Recognition},
pages = {295–302},
numpages = {8},
keywords = {Instruction Generation, Large Language Model, Instruction tuning},
location = {
},
series = {ICCPR '24}
}

@article{10.1145/3708883,
author = {Qu, Zekai and Xie, Ruobing and Xiao, Chaojun and Yao, Yuan and Liu, Zhiyuan and Lian, Fengzong and Kang, Zhanhui and Zhou, Jie},
title = {Thoroughly Modeling Multi-domain Pre-trained Recommendation as Language},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3708883},
doi = {10.1145/3708883},
abstract = {With the thriving of the pre-trained language model (PLM) widely verified in various NLP tasks, pioneer efforts attempt to explore the possible cooperation of the general textual information in PLM with the personalized behavioral information in user historical behavior sequences to enhance sequential recommendation (SR). However, despite the commonalities of input format and task goal, there are huge gaps between the behavioral and textual information, which obstruct thoroughly modeling SR as language modeling via PLM. To bridge the gap, we propose a novel unified pre-trained language model enhanced sequential recommendation (UPSR) that thoroughly transfers the next item prediction task to a text generation task, aiming to build a unified pre-trained recommendation model for multi-domain recommendation tasks. We formally design five key indicators, namely naturalness, domain consistency, informativeness, noise and ambiguity, and text length, to guide the text (rightarrow) item adaptation (selecting appropriate text to form the item textual representation) and behavior sequence (rightarrow) text sequence adaptation (transferring the sequence of item textual representations into a text sequence) differently for pre-training and fine-tuning stages, which are essential but under-explored by previous works. In experiments, we conduct extensive evaluations on seven datasets with both supervised and zero-shot settings and achieve the overall best performance. Comprehensive model analyses also provide valuable insights for behavior modeling via PLM, shedding light on large pre-trained recommendation models. The source codes will be released in the future.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {54},
numpages = {28},
keywords = {Recommendation, Language model, Pre-training}
}

@article{10.1145/3712600,
author = {Chen, Jiayue and Wang, Xiaomeng and Xu, Tong and Wu, Shiwei},
title = {Towards Scene-Centric Multi-Level Interest Mining for Video Recommendation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3712600},
doi = {10.1145/3712600},
abstract = {Knowledge-aware video recommendation requires the ability of associating external knowledge to capture high-order connectivities between users and videos. One limitation of existing methods is that they only extract user interests at a granular level of relational paths by modeling high-order connectivities, which are coarse-grained in user interest modeling, failing to identify user-video relations at a finer-grained level of semantics. In this paper, we investigate the utility of semantic scene graphs in video recommendation scenario, which provide detailed, graph-based annotations of social situations depicted in video clips. We propose a new method named Scene-Centric multi-level Interest Miner (SCIMiner) which explicitly models multi-level user interests. Specifically, we construct user-video graph, knowledge graph and semantic scene graphs as hierarchical heterogeneous graphs. On top of the hierarchical graph representation, we propose two information aggregation strategies to capture user interests from different levels. Knowledge-aware aggregation scheme extracts coarse-grained user interests by aggregating relational paths in high-order connectivities, while scene-aware aggregation scheme models fine-grained user interests by capturing clip-level semantic commonality of user favorite videos. Furthermore, we adaptively distill complementary information about multi-level user interests extracted by different aggregation schemes and encode them into the representations of users and videos. Empirical results show that SCIMiner significantly outperforms the state-of-the-art methods. Further studies verify the complementary benefits of knowledge graph with semantic scene graphs in video recommendation scenario and the finer-grained explainability for predictions.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
keywords = {Video Recommendation, Graph Neural Networks, Knowledge Graph, Scene Graph, Multi-Level Interests}
}

@article{10.1145/3695995,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Wei, Moshi and Shi, Lin and Liu, Zhe and Wang, Qing},
title = {Deep API Sequence Generation via Golden Solution Samples and API Seeds},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3695995},
doi = {10.1145/3695995},
abstract = {Automatic API recommendation can accelerate developers’ programming and has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information retrieval-based (IR-based) approaches and sequence to sequence (seq2seq) model-based approaches. Although these approaches were reported to have remarkable performance, our observation finds two major drawbacks, i.e., IR-based approaches lack the consideration of relations among the recommended APIs, and seq2seq models do not model the API’s semantic meaning. To alleviate the above two problems, we propose APIGens, which is a retrieval-enhanced large language model (LLM)-based API recommendation approach to recommend an API sequence for a natural language query. The approach first retrieves similar programming questions in history based on the input natural language query, and then scores the results based on API documents via a scorer model. Finally, these results are used as samples for few-shot learning of LLM. To reduce the risk of encountering local optima, we also extract API seeds from the retrieved results to increase the search scope during the LLM generation process. The results show that our approach can achieve 48.41% ROUGE@10 on API sequence recommendation and the 82.61% MAP on API set recommendation, largely outperforming the state-of-the-art baselines.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {44},
numpages = {21},
keywords = {API recommendation, deep learning, information retrieval, sequence generation, large language model}
}

@inproceedings{10.5555/3716662.3716801,
author = {Wolfe, Robert and Mitra, Tanushree},
title = {The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations},
year = {2025},
publisher = {AAAI Press},
abstract = {Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1595–1607},
numpages = {13},
location = {San Jose, California, USA},
series = {AIES '24}
}

@inproceedings{10.1145/3705754.3705790,
author = {Zhu, Sitong and Xia, Baobing and Duan, Fangwei and Zhao, Zhenyang and Zhao, Zhenxia and Xiao, Teliang and Liu, Zhicheng and Liu, Xia},
title = {Automated Framework for Constructing Knowledge Graphs Oriented for Standard Analysis Using Large Language Models},
year = {2025},
isbn = {9798400710193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705754.3705790},
doi = {10.1145/3705754.3705790},
abstract = {In an era characterized by rapid technological growth and digital transformation, the necessity for efficient and structured knowledge representation has grown more crucial. Standards serve as fundamental cornerstones that offer guidelines, specifications, and frameworks to ensure the quality and interoperability of products, services, as well as systems. Nonetheless, the complexity and extensive nature of standard documents present significant challenges in extraction, alignment, and organization. Traditional manual processing methods frequently prove labor-intensive and susceptible to errors, impeding the capturing of intricate relationships and hierarchies within these standards. Knowledge Graphs (KGs) provide a robust methodology for organizing information, facilitating improved functionalities for advanced search, reasoning, and analytics. Despite their potential, structuring KGs from standard documents continues to be challenging because of unstructured text, domain-specific terminology, and the intricacies in accurate information extraction. Recent advancements in Natural Language Processing (NLP), particularly the emergence of Large Language Models (LLMs) like GPT-3, have opened new avenues for automating the extraction and structuring of information from unstructured content. These models exhibit exceptional proficiency in comprehending and producing human-like text, positioning them as feasible solutions for addressing the complexities associated with standard documents. This paper presents an automated framework named StandardKG Builder, which utilizes LLMs to construct knowledge graphs tailored for standard analysis from multiple perspectives for complex information extraction. Our evaluation on a comprehensive dataset of standard documents highlights the framework’s effectiveness and scalability. By merging sophisticated knowledge representation with advanced NLP techniques, our work significantly enhances the accessibility and analysis of standard documents, paving the way for more efficient and intelligent standard management systems.},
booktitle = {Proceedings of the 2024 2nd International Conference on Electronics, Computers and Communication Technology},
pages = {183–189},
numpages = {7},
keywords = {Knowledge Graph, Large Language Models, Standard Analysis},
location = {
},
series = {CECCT '24}
}

@article{10.1145/3709727,
author = {Luoma, Kyle and Kumar, Arun},
title = {SNAILS: Schema Naming Assessments for Improved LLM-Based SQL Inference},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3709727},
doi = {10.1145/3709727},
abstract = {Large Language Models (LLMs) have revolutionized Natural Language to SQL (NL-to-SQL), dominating most NL-to-SQL benchmarks. But LLMs still face limitations due to hallucinations, semantic ambiguity, and lexical mismatches between an NL query and the database schema. Naturally, a lot of work in the ML+DB intersection aims to mitigate such LLM limitations. In this work, we shine the light on a complementary data-centric question: How should DB schemas evolve in this era of LLMs to boost NL-to-SQL? The intuition is that more NL-friendly schema identifiers can help LLMs work better with DBs. We dive deeper into this seemingly obvious, but hitherto underexplored and important, connection between schema identifier ''naturalness'' and the behavior of LLM-based NL-to-SQL by creating a new integrated benchmark suite we call SNAILS. SNAILS has 4 novel artifacts: (1) A collection of real-world DB schemas not present in prior NL-to-SQL benchmarks; (2) A set of labeled NL-SQL query pairs on our collection not seen before by public LLMs; (3) A notion of naturalness level for schema identifiers and a novel labeled dataset of modified identifiers; and (4) AI artifacts to automatically modify identifier naturalness. Using SNAILS, we perform a comprehensive empirical evaluation of the impact of schema naturalness on LLM-based NL-to-SQL accuracy, and present a method for improving LLM-based NL-to-SQL with natural views. Our results reveal statistically significant correlations across multiple public LLMs from OpenAI, Meta, and Google on multiple databases using both zero-shot prompting as well as more complex NL-to-SQL workflows: DIN SQL, and CodeS. We present several fine-grained insights and discuss pathways for DB practitioners to better exploit LLMs for NL-to-SQL.},
journal = {Proc. ACM Manag. Data},
month = feb,
articleno = {77},
numpages = {26},
keywords = {benchmark, database, llm, natural language to sql, relational database schema, schema design, schema linking, schema naturalness}
}

@inproceedings{10.1145/3708036.3708165,
author = {Huang, Ling and Deng, Wanqiu and Jiang, Yiling and Zhong, Qinghua},
title = {Development trends of large language models and their applications in green digital intelligence of supply chains},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708165},
doi = {10.1145/3708036.3708165},
abstract = {In recent years, large language models (LLM) have developed rapidly and been widely used in the field of natural language processing, and have made significant progress and been widely used in both academia and industry. Large language models have also shown great potential in promoting the green digital intelligence of the supply chain. The development of language models provides technical support for the green digital intelligence of the supply chain. The model's text analysis capabilities can help companies analyze information such as suppliers' renewable energy usage, carbon footprint, environmental and social responsibility reports, and thus make more informed sourcing and supplier selection decisions. At the same time, the large language model can analyze supply chain data, provide suggestions and optimization plans for energy conservation and emission reduction, predict and manage environmental risks, and promote enterprises to transform into a more sustainable and environmentally friendly supply chain.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {770–774},
numpages = {5},
keywords = {Green digital intelligence of supply chain, Natural Language Processing, large language model},
location = {
},
series = {ICCSMT '24}
}

@article{10.1145/3707650,
author = {Korn, Daniel and Hou, Pei-Yu and Schatz, Kara and Beasley, Jon-Michael and Tropsha, Alexander and Chirkova, Rada},
title = {Towards Improving the Efficiency of Drug Repurposing by Leveraging Node Promiscuity in Biomedical Knowledge Graphs},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3707650},
doi = {10.1145/3707650},
abstract = {To accelerate the time- and labor-intensive processes of drug discovery and repurposing, it is increasingly common to mine knowledge sources for connections between diseases and the drugs that can treat them. In this article we address the scalability challenge in the connection mining, by introducing algorithms that can be used to find plausible mechanistic connections between drugs and the potentially associated diseases in biomedical knowledge graphs. These connections are then presented to biomedical experts as candidate hypotheses for further studies of whether the drugs can be repurposed to treat the diseases.One challenge that has to be addressed in this effort is the processing of promiscuous knowledge-graph nodes, that is, nodes associated with numerous relationships that may not be unique or indicative of the node properties. As it turns out, the multiplicity of relationships involving promiscuous graph nodes may prevent the aforementioned path-finding algorithms from aiding in drug repurposing. To address the promiscuous-node challenge, we introduce promiscuity scores for nodes and paths in knowledge graphs, and incorporate the scores in the proposed path-finding algorithms. We report experimental results that indicate that paths with low-promiscuity scores could be meaningful and of interest to biomedical experts in drug repurposing.},
journal = {ACM Trans. Comput. Healthcare},
month = jan,
articleno = {8},
numpages = {32},
keywords = {Biomedical knowledge graphs, drug repurposing, path-finding knowledge-graph algorithms}
}

@inproceedings{10.1145/3704522.3704537,
author = {Lee, Hyun and Islam, Maminur and Yi, Chris and Chakraborttii, Chandranil (Nil)},
title = {Enhancing Graph Representation Learning with WalkLM for Effective Community Detection},
year = {2025},
isbn = {9798400711589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704522.3704537},
doi = {10.1145/3704522.3704537},
abstract = {Embeddings in deep neural networks are essential for processing high-dimensional and categorical data by converting it into compact, low-dimensional vectors. This conversion enables the model to capture complex semantic relationships and improves its generalization across diverse tasks. Although embeddings have demonstrated significant effectiveness in natural language processing and applications involving large language models (LLMs), their utility decreases when handling graph-based data. In this study, we address this challenge by enhancing WalkLM, a cutting-edge language model tailored for generating graph embeddings. We refine the random walk algorithm to better capture both local and global contexts within graphs. Additionally, we implement a k-means algorithm for community detection in these graphs. Our experiments across various benchmark datasets confirm the efficacy of our approach.},
booktitle = {Proceedings of the 11th International Conference on Networking, Systems, and Security},
pages = {41–47},
numpages = {7},
keywords = {Graph Clustering, Graph Embedding, Community Detection, Network, Machine Learning, LM(Language Model)},
location = {
},
series = {NSysS '24}
}

@article{10.1145/3652952,
author = {Degbelo, Auriol},
title = {Prolegomena to a Description Language for GenAI Tools in Cities},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3652952},
doi = {10.1145/3652952},
abstract = {The potential of generative AI has been recently demonstrated through different applications. The open government and smart city initiatives can leverage this potential to produce innovations that improve government workflows and the lives of citizens. This commentary makes the case for a description language enabling the structured documentation of these upcoming innovations. The description language would facilitate the communication between governments, citizens, and innovators. The key elements of the description language are briefly sketched and its usefulness is shown by the generation of ideas for GenAI tools related to interactive maps in cities.},
journal = {Digit. Gov.: Res. Pract.},
month = feb,
articleno = {8},
numpages = {8},
keywords = {Smart cities, open government, GenAI tools, metadata generation, interactive maps, human-computer interaction}
}

@inproceedings{10.1145/3706890.3706998,
author = {Wang, Qi and Li, Qiyuan and Gao, Chao and Liu, Haijiang and Xu, Fangfang and Gu, Jinguang},
title = {A Medical Knowledge Management Mechanism with Knowledge Hypergraph Theory},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706998},
doi = {10.1145/3706890.3706998},
abstract = {The construction of medical knowledge graphs(KGs) structures complex medical data to support precise clinical decision-making and knowledge discovery. A core challenge in building medical KGs is effectively representing and processing complex clinical data. Existing triple-based medical knowledge representation methods often fail to fully capture and express the complexity of the data, particularly when dealing with higher-order relationships involving multiple entities. Additionally, the current "Attribute-Concept-Event" three-layer knowledge organization framework shows limitations in representing procedural knowledge in medical scenarios, falling short of the hierarchical and dynamic requirements of evidence-based medicine. To address these issues, we propose a four-layer medical knowledge organization method based on hypergraph theory. By incorporating hypergraph theory and a narrative layer into the KG, we provide a more flexible and dynamic framework for representing complex clinical information. Based on this approach, we construct a four-layer KG, RJUA-HKG, using real QA clinical data from urology as an example. Experiments show that, compared to traditional KGs, RJUA-HKG significantly reduces relational complexity and retrieval time while accurately capturing changes in diagnostic and treatment procedures.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {627–633},
numpages = {7},
keywords = {Knowledge graph, Knowledge hypergraph theory, Knowledge organization, Medical knowledge management mechanism},
location = {
},
series = {ISAIMS '24}
}

@inproceedings{10.1145/3706890.3706906,
author = {Zhang, Zhizheng and Yang, Qian and Du, Yan},
title = {A Question-and-Answer Scheme of Clinical Practice Guidelines Based on SDA*},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706906},
doi = {10.1145/3706890.3706906},
abstract = {This paper proposes a medical diagnosis and treatment knowledge question and answer scheme based on clinical practice guidelines, aiming at the need of standardization of medical process and popularization of medical knowledge. The scheme realizes computer intelligent diagnosis and treatment assistance through diagnosis and treatment knowledge base construction, problem analysis, information retrieval and answer generation. The SDA*(State-Decision-Action) modeling method and question formal definition for hypertension diagnose and treatment are proposed, and the answers are generated based on the answer set program (ASP). For the transformation of natural language and ASP rules, large language model (LLM) technology is adopted to automatically identify question terms, generate ASP rules and process the generated answers. To verify the scheme's ability to generate reliable answers, accuracy for problem classification and ASP rule generations are confirmed. The experimental results show that the scheme could provide the ability to solve medical problems based on the knowledge of clinical practice guidelines.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {96–101},
numpages = {6},
keywords = {Clinical practice guideline, SDA* model, answer set programming, hypertension, large language model, question and answer},
location = {
},
series = {ISAIMS '24}
}

@article{10.1145/3715007,
author = {Chen, Xiang and Gao, Chaoyang and Chen, Chunyang and Zhang, Guangbei and Liu, Yong},
title = {An Empirical Study on Challenges for LLM Application Developers},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715007},
doi = {10.1145/3715007},
abstract = {In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI’s ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development.Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Mining Software Repository, Empirical Study, LLM Developer, Development Challenges, Prompt Engineering}
}

@inproceedings{10.1145/3708036.3708061,
author = {Zhang, Chao and Li, Donghao and Dong, Bin},
title = {On-site smart maintenance based on collaboration between network large model and AI models},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708061},
doi = {10.1145/3708036.3708061},
abstract = {On-site maintenance is the end of telecom operators' network maintenance, and it is an important means of ensuring the quality of telecom network operation and user experience. Therefore, improving the quality and efficiency of on-site maintenance is essential. This paper designs an on-site smart maintenance scheme based on the network large model, according to the actual maintenance processes of telecom operators. It also proposes practical use cases such as maintenance Q&amp;A assistant, on-site troubleshooting and analysis assistant, and on-site maintenance agent. By giving full play to generative AI and orchestrating the ability of small models, the network large model helps maintenance personnel solve practical problems in production. It not only improves the maintenance efficiency but also effectively guarantees the safe operation of the network. While improving customer satisfaction, it can also reduce the maintenance costs of operators to a certain extent.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {145–150},
numpages = {6},
keywords = {Efficiency improvement, Network large model, On-site maintenance},
location = {
},
series = {ICCSMT '24}
}

@article{10.1145/3715069,
author = {Varshney, Deeksha and Behera, Niranshu and Katari, Prajeet and Ekbal, Asif},
title = {MedProm: Bridging Dialogue Gaps in Healthcare with Knowledge-Enhanced Generative Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715069},
doi = {10.1145/3715069},
abstract = {In medical dialogue systems, recent advancements underscore the critical role of incorporating relevant medical knowledge to enhance performance. However, existing knowledge bases often lack completeness, posing a challenge in sourcing pertinent information. We present MedProm, a novel generative model tailored for medical dialogue generation to address this gap. Motivated by the need for comprehensive and contextually relevant responses, MedProm leverages state-of-the-art language models such as BioGPT. Our model is designed to integrate extensive medical knowledge into conversations, facilitating effective communication between patients and healthcare providers. At the core of MedProm lies the MediConnect Graph, a meticulously constructed knowledge graph capturing intricate relationships among medical entities extracted from dialogue contexts. By employing a KnowFusion encoder with a pretraining objective and masked multi-head self-attention, MedProm effectively processes the MediConnect graph, enabling precise control over information flow to capture its underlying structure. Furthermore, MedProm incorporates a sophisticated Curriculum Knowledge Decoder, leveraging transformer-based decoding to generate response utterances conditioned on input representations from the KnowFusion Encoder. The training process is guided through curriculum learning, gradually increasing optimization difficulty based on a coherence-based criterion. Experimental results on two datasets demonstrate the efficacy of MedProm in generating accurate and contextually relevant responses compared to state-of-the-art models.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jan,
keywords = {Medical Dialogue Systems (MDS), Generative Neural Model, MediConnect Graph, KnowFusion Encoder, Curriculum Knowledge Decoder}
}

@proceedings{10.1145/3681772,
title = {IWCTS'24: Proceedings of the 17th ACM SIGSPATIAL International Workshop on Computational Transportation Science GenAI and Smart Mobility Session},
year = {2024},
isbn = {9798400711510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 17th International Workshop on Computational Transportation Science (IWCTS 2024) will feature a Smart Mobility track, emphasizing the growing relevance of human mobility data from sources like cell phones, connected vehicles, and volunteered geographic information. This data integration is advancing smart city frameworks, intelligent transportation systems, and urban planning. Managing and analyzing large-scale datasets highlights the critical role of advanced computational and AI techniques, including Generative AI (GenAI), Large Language Models (LLMs), and Retrieval-Augmented Generation (RAG). The workshop builds on previous success, focusing on computational and informatics approaches for optimized urban mobility. We will build upon the success of previous workshops to continue to focus on the computational and informatics approaches for (not limited to):},
location = {Atlanta, GA, USA}
}

@inproceedings{10.5555/3716662.3716812,
author = {Zhang, Richard and van Liemt, Erin and Fischella, Tyler},
title = {Ontology of Belief Diversity: A Community-Based Epistemological Approach},
year = {2025},
publisher = {AAAI Press},
abstract = {AI applications across classification, fairness, and human interaction often implicitly require ontologies of social concepts. Constructing these well, especially when there are many relevant categories, is a controversial task but is crucial for achieving meaningful inclusivity. Here, we focus on developing a pragmatic ontology of belief systems, which is a complex and often controversial space. By iterating on our community-based design until mutual agreement is reached, we found that epistemological methods were best for categorizing the fundamental ways beliefs differ, maximally respecting our principles of inclusivity and brevity. We demonstrate our methodology's utility and interpretability via user studies in term annotation and sentiment analysis experiments for belief fairness in language models.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1735–1743},
numpages = {9},
location = {San Jose, California, USA},
series = {AIES '24}
}

@inproceedings{10.1145/3708036.3708114,
author = {Qiu, Han and Sun, Chuanqiang and Chen, Hongyun and Zou, Baoyu and Dong, Zizheng},
title = {Design and Implementation of an Intelligent Document Extraction and Review System Based on Multimodal Large Language Models},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708114},
doi = {10.1145/3708036.3708114},
abstract = {This study aims to address the issues of low efficiency and high manual involvement in existing government affairs processes by proposing an automated system based on large model technology. The system integrates modules such as data collection, natural language processing, business rule engine, process generation, and intelligent applications. Innovations include using large models to transform unstructured government information into executable rules, automatically generating process models and documents, and implementing intelligent approval and automated verification at key nodes. Experimental results indicate that the system significantly enhances the automation level and execution efficiency of government affairs processes, providing a new solution for the intelligent transformation of government management.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {455–465},
numpages = {11},
keywords = {Government Affairs Process Automation, Intelligent Governance, Large Language Model, Natural Language Processing},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.1145/3708282.3708296,
author = {Qiu, Han and Chen, Hongyun and Zou, Baoyu and Dong, Zizheng},
title = {Intelligent Design and Implementation of Government Affairs Processes Driven by Large Language Models},
year = {2025},
isbn = {9798400709869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708282.3708296},
doi = {10.1145/3708282.3708296},
abstract = {This study aims to address the issues of low efficiency and high manual involvement in existing government affairs processes by proposing an automated system based on large model technology. The system integrates modules such as data collection, natural language processing, business rule engine, process generation, and intelligent applications. Innovations include using large models to transform unstructured government information into executable rules, automatically generating process models and documents, and implementing intelligent approval and automated verification at key nodes. Experimental results indicate that the system significantly enhances the automation level and execution efficiency of government affairs processes, providing a new solution for the intelligent transformation of government management.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence of Things and Computing},
pages = {70–79},
numpages = {10},
keywords = {Government Affairs Process Automation, Intelligent Governance, Large Language Model, Natural Language Processing},
location = {
},
series = {AITC '24}
}

@inproceedings{10.1145/3708394.3708450,
author = {Li, Zhuang and Chu, Zheng and Zhang, Qingwei},
title = {Artificial intelligence-based conversational exam model: a perspective for higher education},
year = {2025},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708394.3708450},
doi = {10.1145/3708394.3708450},
abstract = {By analyzing the current situation of the application of artificial intelligence in college education, based on the educational assessment mode in college education at the current stage, analyzing the characteristics of the conversational examination mode, and proposing a conversational examination mode based on artificial intelligence. This model includes the examination system architecture, the question bank design, and the construction of the knowledge graph, as well as the intelligent scoring mechanism. It shows great development potential and application in the change of the examination mode in college education. The model demonstrates considerable potential for further development and application in the context of evolving educational assessment practices within the college and university setting. It is particularly well-suited to the demands of the digital age.},
booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
pages = {325–330},
numpages = {6},
keywords = {Educational innovations, Examination mode, Multi-dimensional assessment, artificial intelligence (AI)},
location = {
},
series = {AIFE '24}
}

@article{10.1145/3712003,
author = {He, Junda and Treude, Christoph and Lo, David},
title = {LLM-Based Multi-Agent Systems for Software Engineering: Literature Review, Vision and the Road Ahead},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712003},
doi = {10.1145/3712003},
abstract = {Integrating Large Language Models (LLMs) into autonomous agents marks a significant shift in the research landscape by offering cognitive abilities that are competitive with human planning and reasoning. This paper explores the transformative potential of integrating Large Language Models into Multi-Agent (LMA) systems for addressing complex challenges in software engineering (SE). By leveraging the collaborative and specialized abilities of multiple agents, LMA systems enable autonomous problem-solving, improve robustness, and provide scalable solutions for managing the complexity of real-world software projects. In this paper, we conduct a systematic review of recent primary studies to map the current landscape of LMA applications across various stages of the software development lifecycle (SDLC). To illustrate current capabilities and limitations, we perform two case studies to demonstrate the effectiveness of state-of-the-art LMA frameworks. Additionally, we identify critical research gaps and propose a comprehensive research agenda focused on enhancing individual agent capabilities and optimizing agent synergy. Our work outlines a forward-looking vision for developing fully autonomous, scalable, and trustworthy LMA systems, laying the foundation for the evolution of Software Engineering 2.0.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Large Language Models, Autonomous Agents, Multi-Agent Systems, Software Engineering}
}

@article{10.1613/jair.1.16777,
author = {Sundaresan, Divya and Watson, Akhira and Bardaka, Eleni and Lee, Crystal Chen and Mayhorn, Christopher B. and Singh, Munindar P.},
title = {Prosociality in Microtransit},
year = {2025},
issue_date = {Feb 2025},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {82},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.16777},
doi = {10.1613/jair.1.16777},
abstract = {We study (public) microtransit, a type of transportation service wherein a municipality offers point-to-point rides to residents, for a fixed, nominal fare. Microtransit exemplifies practical resource allocation problems that are often over-constrained in that not all ride requests (pickup and dropoff locations at specified times) can be satisfied or satisfied only by violating soft goals such as sustainability, and where economic signals (e.g., surge pricing) are not applicable—they would lead to unethical outcomes by effectively coercing poor people.
We posit that instead of taking rider preferences as fixed, shaping them prosocially will lead to improved societal outcomes. Prosociality refers to an attitude or behavior that is intended to benefit others. This paper demonstrates a computational approach to prosociality in the context of a (public) microtransit service for disadvantaged riders. Prosociality appears as a willingness to adjust one’s pickup and dropoff times and locations to accommodate the schedules of others and to enable sharing rides (which increases the number of riders served with the same resources).
This paper describes an interdisciplinary study of prosociality in microtransit between a transportation researcher, psychologists, a social scientist, and AI researchers. Our contributions are these: (1) empirical support for the viability of prosociality in microtransit (and constraints on it) through interviews with drivers and focus groups of riders; (2) a prototype mobile app demonstrating how our prosocial intervention can be combined with the transportation backend; (3) a reinforcement learning approach to model a rider and determine the best interventions to persuade that rider toward prosociality; and (4) a cognitive model of rider personas to enable evaluation of alternative interventions.},
journal = {J. Artif. Int. Res.},
month = jan,
numpages = {34},
keywords = {Deep learning models, Artificial Intelligence, knowledge graph, Natural Language processing, healthcare services., machine learning}
}

@inproceedings{10.1145/3708036.3708151,
author = {Yan, Erkai and Gao, Mengxiao and Tang, Mei},
title = {Analysis and Research on Generative Artificial Intelligence in the Field of International Library and Information Science},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708151},
doi = {10.1145/3708036.3708151},
abstract = {Generative artificial intelligence is an artificial intelligence technology based on deep learning whose core lies in leveraging computer algorithms and training data to generate new, practically valuable content, encompassing text, images, audio, videos, etc. This technology is poised to exert profound impacts on the transformation and development of libraries. Drawing on generative artificial intelligence research publications in the field of international library and information science included in the Scopus database as the data source, this paper employs CiteSpace software and SciVal tools to conduct a visual analysis of literature outputs, core authors, journal sources, and keywords. The results show that generative artificial intelligence research in the international library and information science field is applied primarily in areas such as reference services, information literacy education, and smart libraries. Recommendations are made to promote the application and development of generative artificial intelligence technology in libraries by strengthening technological research and application, boosting data analysis and data sharing, emphasizing information security and privacy protection, promoting cross-boundary integration and ecological development, etc.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {679–686},
numpages = {8},
keywords = {ChatGPT, Generative artificial intelligence, library},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.5555/3716662.3716790,
author = {Varshney, Kush R.},
title = {Decolonial AI Alignment: Openness, Vi\'{s}eundefineda-Dharma, and Including Excluded Knowledges},
year = {2025},
publisher = {AAAI Press},
abstract = {Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment through mechanisms such as extractivism, automation, sociological essentialism, surveillance, and containment. However, that work has not engaged much with alignment: teaching behaviors to a large language model (LLM) in line with desired values, and has not considered a mechanism that arises within that process: moral absolutism---a part of the coloniality of knowledge. Colonialism has a history of altering the beliefs and values of colonized peoples; in this paper, I argue that this history is recapitulated in current LLM alignment practices and technologies. Furthermore, I suggest that AI alignment be decolonialized using three forms of openness: openness of models, openness to society, and openness to excluded knowledges. This suggested approach to decolonial AI alignment uses ideas from the argumentative moral philosophical tradition of Hinduism, which has been described as an open-source religion. One concept used is vi\'{s}eundefineda-dharma, or particular context-specific notions of right and wrong. At the end of the paper, I provide a suggested reference architecture to work toward the proposed framework.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1467–1481},
numpages = {15},
location = {San Jose, California, USA},
series = {AIES '24}
}

@proceedings{10.1145/3703619,
title = {VRCAI '24: Proceedings of the 19th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and its Applications in Industry},
year = {2024},
isbn = {9798400713484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, Guangdong Province, China}
}

@article{10.1145/3716131,
author = {Zhang, Haobo and Zhu, Qiannan and Dou, Zhicheng},
title = {A Unified Prompt-aware Framework for Personalized Search and Explanation Generation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3716131},
doi = {10.1145/3716131},
abstract = {Product search is crucial for users to find and purchase products they need. Personalized product search, which models users’ search intent and provides tailored results, has become a prominent research problem in industry and academia. Recent studies often leverage knowledge graphs (KGs) to improve search performance and generate explanations for search results. However, existing KG-based methods treat search and explanation tasks separately and explore paths in KGs as explanations, creating a gap between search results and generated explanations. Also, path-formed explanations in KGs are not flexible enough to build correlations with the user’s current query. To address these challenges, we propose P-PEG, a unified prompt-aware framework for personalized product search and explanation generation. P-PEG leverages a pre-trained language model (PLM) and search signal to enhance the generation of user-understandable explanations. We introduce a prompt learning technique and design prompt generators for search and explanation generation tasks based on a fixed PLM. By incorporating search results in explanation-based prompts, we bridge the gap between search results and explanations, facilitating better interaction. Additionally, we utilize the user’s current query, historical search log, and KGs to personalize the explanations and inject task knowledge into PLM. Experimental results show that P-PEG outperforms existing methods in the explanation generation task of the three datasets and the search task of the Electronics dataset, and achieves comparable performance in the search task of the Cellphones &amp; Accessories and CD &amp; Vinyl datasets.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = feb,
keywords = {Explanation generation, Product search, Prompt learning}
}

@article{10.1145/3713032,
author = {Chau, Michael and Xu, Jennifer},
title = {An IS Research Agenda on Large Language Models: Development, Applications, and Impacts on Business and Management},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3713032},
doi = {10.1145/3713032},
abstract = {Large language models have been advancing very rapidly and are making substantial impacts on all areas of business and management. We review the development of large language models and their applications in business and management, and identify the major issues and challenges faced by both practitioners and researchers. Based on our review, we propose an agenda for information systems researchers on large language models and discuss some of the potential directions for future research. Lastly, we present the articles in the special issue as exemplary research on large language models and discuss their implications.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {1},
numpages = {11},
keywords = {Large language models, artificial intelligence, information systems research, business and management}
}

@proceedings{10.1145/3704289,
title = {ICBDE '24: Proceedings of the 2024 7th International Conference on Big Data and Education},
year = {2024},
isbn = {9798400716980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3702386.3702400,
author = {Wang, Xianchuang and Fang, Haiguang and Shu, Lili and Li, Zeyu},
title = {Creative Accessibility in the Era of Artificial Intelligence and Its Applied Technology Research},
year = {2025},
isbn = {9798400710131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702386.3702400},
doi = {10.1145/3702386.3702400},
abstract = {In the era of artificial intelligence, an AI dual-teacher classroom environment constructed using technologies such as knowledge graphs, large models, and educational robots will facilitate students in achieving the cognitive goals related to creativity in Bloom's Taxonomy of educational objectives. The use of artificial intelligence technology, especially large model technology, can promote students' inclination towards creativity and creative thinking. However, it is essential to guide students to engage in deep thinking before designing prompts based on their ideas, thereby fostering their creative and critical thinking through human-machine collaboration. This study takes a certain experimental school as a case to explore the effectiveness of creative accessibility in the AI dual-teacher classroom environment and its application technology in primary and secondary schools.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence and Teacher Education},
pages = {50–56},
numpages = {7},
keywords = {AI Dual-Teacher Classroom, Artificial Intelligence Education, Creative Accessibility, Educational Prompt Engineering, Large Models},
location = {
},
series = {ICAITE '24}
}

@article{10.1145/3678004,
author = {Lin, Jianghao and Dai, Xinyi and Xi, Yunjia and Liu, Weiwen and Chen, Bo and Zhang, Hao and Liu, Yong and Wu, Chuhan and Li, Xiangyang and Zhu, Chenxu and Guo, Huifeng and Yu, Yong and Tang, Ruiming and Zhang, Weinan},
title = {How Can Recommender Systems Benefit from Large Language Models: A Survey},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3678004},
doi = {10.1145/3678004},
abstract = {With the rapid development of online services and web applications, recommender systems (RS) have become increasingly indispensable for mitigating information overload and matching users’ information needs by providing personalized suggestions over items. Although the RS research community has made remarkable progress over the past decades, conventional recommendation models (CRM) still have some limitations, e.g., lacking open-domain world knowledge, and difficulties in comprehending users’ underlying preferences and motivations. Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities for various natural language processing (NLP) tasks, which mainly stem from their extensive open-world knowledge, logical and commonsense reasoning abilities, as well as their comprehension of human culture and society. Consequently, the emergence of LLM is inspiring the design of RS and pointing out a promising research direction, i.e., whether we can incorporate LLM and benefit from their common knowledge and capabilities to compensate for the limitations of CRM. In this article, we conduct a comprehensive survey on this research direction, and draw a bird’s-eye view from the perspective of the whole pipeline in real-world RS. Specifically, we summarize existing research works from two orthogonal aspects: where and how to adapt LLM to RS. For the “WHERE” question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, user interaction, and pipeline controller. For the “HOW” question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLM or not during training, and whether to involve CRM for inference. Detailed analysis and general development paths are provided for both “WHERE” and “HOW” questions, respectively. Then, we highlight the key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {28},
numpages = {47},
keywords = {Recommender systems, large language models}
}

@article{10.1145/3706119,
author = {Fatemi, Sorouralsadat and Hu, Yuheng and Mousavi, Maryam},
title = {A Comparative Analysis of Instruction Fine-Tuning Large Language Models for Financial Text Classification},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3706119},
doi = {10.1145/3706119},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model’s accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {6},
numpages = {30},
keywords = {Large language models, parameter-efficient fine-tuning, instruction fine-tuning, text classification, finance}
}

@article{10.1145/3708344,
author = {Yuan, Wei and Yang, Chaoqun and Qu, Liang and Hung, Nguyen Quoc Viet and Ye, Guanhua and Yin, Hongzhi},
title = {PTF-FSR: A Parameter Transmission-Free Federated Sequential Recommender System},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3708344},
doi = {10.1145/3708344},
abstract = {Sequential recommender systems, as a specialized branch of recommender systems that can capture users’ dynamic preferences for more accurate and timely recommendations, have made significant progress. Recently, due to increasing concerns about user data privacy, some researchers have implemented federated learning for sequential recommendation, a.k.a., Federated Sequential Recommender Systems (FedSeqRecs), in which a public sequential recommender model is shared and frequently transmitted between a central server and clients to achieve collaborative learning. Although these solutions mitigate user privacy to some extent, they present two significant limitations that affect their practical usability: (1) They require a globally shared sequential recommendation model. However, in real-world scenarios, the recommendation model constitutes a critical intellectual property for platform and service providers. Therefore, service providers may be reluctant to disclose their meticulously developed models. (2) The communication costs are high as they correlate with the number of model parameters. This becomes particularly problematic as the current FedSeqRec will be inapplicable when sequential recommendation marches into a large language model era.To overcome the above challenges, this article proposes a parameter transmission-free federated sequential recommendation framework (PTF-FSR), which ensures both model and data privacy protection to meet the privacy needs of service providers and system users alike. Furthermore, since PTF-FSR only transmits prediction results under privacy protection, which are independent of model sizes, this new federated learning architecture can accommodate more complex and larger sequential recommendation models. Extensive experiments conducted on three widely used recommendation datasets, employing various sequential recommendation models from both ID-based and ID-free paradigms, demonstrate the effectiveness and generalization capability of our proposed framework. To facilitate future research in this direction, we release our code at .},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {52},
numpages = {24},
keywords = {Sequential Recommendation, Federated Learning, Contrastive Learning, Model Intellectual Property}
}

@proceedings{10.1145/3702163,
title = {ICETC '24: Proceedings of the 2024 16th International Conference on Education Technology and Computers},
year = {2024},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3712059,
author = {Qasim, Iqra and Horsch, Alexander and Prasad, Dilip},
title = {Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation Protocols},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3712059},
doi = {10.1145/3712059},
abstract = {Untrimmed videos have interrelated events, dependencies, context, overlapping events, object-object interactions, domain specificity, and other semantics that are worth highlighting while describing a video in natural language. Owing to such a vast diversity, a single sentence can only correctly describe a portion of the video. Dense Video Captioning (DVC) aims to detect and describe different events in a given video. The term DVC originated in the 2017 ActivityNet challenge, after which considerable effort has been made to address the challenge. DVC is divided into three sub-tasks: (1) Video Feature Extraction, (2) Temporal Event Localization, and (3) Dense Caption Generation. In this survey, we discuss all of the studies that claim to perform DVC along with its sub-tasks and summarize their results. We also discuss all of the datasets that have been used for DVC. Last, current challenges in the field are highlighted along with observatory remarks and future trends in the field.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {154},
numpages = {36},
keywords = {Dense video captioning models, video feature extraction, event localization, ActivityNet challenge, deep learning, artificial intelligence}
}

@proceedings{10.1145/3702386,
title = {ICAITE '24: Proceedings of the 2024 International Conference on Artificial Intelligence and Teacher Education},
year = {2024},
isbn = {9798400710131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3708597.3708606,
author = {Han, Xiaotian},
title = {Generative Artificial Intelligence for Future Education: Current Research Status, Hot Spots, and Research Trends},
year = {2025},
isbn = {9798400718304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708597.3708606},
doi = {10.1145/3708597.3708606},
abstract = {Generative artificial intelligence (GenAI), leveraging deep learning techniques such as neural networks, is revolutionizing education. This study explores the current research status, key areas, and emerging trends by analyzing 2,856 papers from 2014 to 2023 using CiteSpace software. Key findings reveal that: 1. The focus of GenAI research has shifted towards large language models, such as ChatGPT, especially since 2022. 2. Major research contributions come from China, the U.S., and South Korea, with China leading in institutional involvement. 3. Research trends indicate growing interest in AI applications for immersive education, deep learning models, and medical education. 4. Ethical considerations and data processing methodologies, including anomaly detection and data augmentation, are crucial emerging topics in the field. This paper outlines the most active research clusters and provides future directions for interdisciplinary applications and ethical AI.},
booktitle = {Proceedings of the 2024 8th International Conference on Algorithms, Computing and Systems},
pages = {56–61},
numpages = {6},
keywords = {Generative artificial intelligence 1, current research status 3, future educationn2, hot spots 4, research trends 5},
location = {
},
series = {ICACS '24}
}

@article{10.1145/3705313,
author = {Dang, Xiaochao and Ding, Guozhen and Dong, Xiaohui and Li, Fenfang and Gao, Shiwei and Wang, Yue},
title = {UIE-Based Relational Extraction Task for Mine Hoist Fault Data},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3705313},
doi = {10.1145/3705313},
abstract = {Information extraction is pivotal in natural language processing, where the goal is to convert unstructured text into structured information. A significant challenge in this domain is the diversity and specific needs of various processing tasks. Traditional approaches typically utilize separate frameworks for different information extraction tasks, such as named entity recognition and relationship extraction, which hampers their uniformity and scalability. In this study, this study introduce a Universal Information Extraction (UIE) framework combined with a cue learning strategy, significantly improving the efficiency and accuracy of extracting mine hoist fault data. Initially, domain-specific data is manually labeled to fine-tune the model, and the accuracy is further enhanced by constructing negative examples during this fine-tuning process. The model then focuses on faults using the Structured Extraction Language (SEL) and a schema-based prompt syntax, the Structural Schema Instructor (SSI), which targets and extracts key information from the fault data to meet specific domain requirements. Experimental results show that UIE substantially improves the processing efficiency and the F1 accuracy of the extracted mine hoist fault data, with the fine-tuned F1 score increasing from 23.59% to 92.51%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {4},
numpages = {23},
keywords = {Joint extraction, mechanical problem, mining sector, prompt learning}
}

@inbook{10.5555/3712729.3712835,
author = {Tolk, Andreas},
title = {Hybrid Modeling Integrating Artificial Intelligence and Modeling &amp; Simulation Paradigms},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This paper discusses the complementary relationship between Modeling and Simulation (M&amp;S) and Artificial Intelligence (AI) methods like machine learning. While M&amp;S uses algorithms to model system behavior from input parameters, AI learns patterns from correlation in data. The paper argues that hybrid models combining M&amp;S and AI can be more powerful than either alone. It provides a conceptual framework showing how M&amp;S and AI can be integrated in sequential, parallel, complementary or competitive configurations. Several example applications are given where AI enhances M&amp;S and vice versa, such as using AI to optimize simulation parameters, generate synthetic training data for AI from simulations, interpret AI model behavior through simulation, and automate aspects of simulation development with AI assistance. The potential benefits of hybrid AI/M&amp;S modeling span improved accuracy, efficiency, trustworthiness and cross-disciplinary collaboration. The paper calls for further research developing a solid theoretical foundation for merging these complementary paradigms.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1271–1280},
numpages = {10}
}

@inproceedings{10.1145/3702386.3702397,
author = {Jian, Xiaoyan and Li, Feilong and Liu, Zhaohong},
title = {A Study on the Application of AIGC Context in the Development of Digital Resources for Online Courses},
year = {2025},
isbn = {9798400710131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702386.3702397},
doi = {10.1145/3702386.3702397},
abstract = {This study explores the application of AIGC in the development of digital resources for online programming courses, pointing out that AI technology brings new opportunities for educational resource development, but faces problems such as technical limitations and uneven resource quality. Cracking this dilemma can be started from course research, PPT development, text and video resource production, course portal design and auxiliary programming software application, etc. It also puts forward the deficiencies of specific AI in digital resource construction. It is concluded that the complementary models of AI and artificial intelligence need to be explored in the context of teaching and learning in order to maximize the potential of AI in education.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence and Teacher Education},
pages = {136–141},
numpages = {6},
keywords = {aigc, digital resource development, online course, programming course},
location = {
},
series = {ICAITE '24}
}

@article{10.1145/3714464,
author = {Williams, Laurie and Benedetti, Giacomo and Hamer, Sivana and Paramitha, Ranindya and Rahman, Imranur and Tamanna, Mahzabin and Tystahl, Greg and Zahan, Nusrat and Morrison, Patrick and Acar, Yasemin and Cukier, Michel and K\"{a}stner, Christian and Kapravelos, Alexandros and Wermke, Dominik and Enck, William},
title = {Research Directions in Software Supply Chain Security},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3714464},
doi = {10.1145/3714464},
abstract = {Reusable software libraries, frameworks, and components, such as those provided by open-source ecosystems and third-party suppliers, accelerate digital innovation. However, recent years have shown almost exponential growth in attackers leveraging these software artifacts to launch software supply chain attacks. Past well-known software supply chain attacks include the SolarWinds, log4j, and xz utils incidents. Supply chain attacks are considered to have three major attack vectors: through vulnerabilities and malware accidentally or intentionally injected into open-source and third-party dependencies/components/containers; by infiltrating the build infrastructure during the build and deployment processes; and through targeted techniques aimed at the humans involved in software development, such as through social engineering. Plummeting trust in the software supply chain could decelerate digital innovation if the software industry reduces its use of open-source and third-party artifacts to reduce risks. This paper contains perspectives and knowledge obtained from intentional outreach with practitioners to understand their practical challenges and from extensive research efforts. We then provide an overview of current research efforts to secure the software supply chain. Finally, we propose a future research agenda to close software supply chain attack vectors and support the software industry.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan
}

@proceedings{10.1145/3708036,
title = {ICCSMT '24: Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
year = {2024},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3715156,
author = {Varagnolo, Davide and Melo, Dora and Pimenta Rodrigues, Irene},
title = {Translating Natural Language questions into CIDOC-CRM SPARQL queries to access Cultural Heritage knowledge bases},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3715156},
doi = {10.1145/3715156},
abstract = {To explore information on the Semantic Web, SPARQL queries or DL-queries are suitable tools. However, users interested in exploring the content of such knowledge bases often find it challenging to employ formal query languages, as this requires familiarity with the target domain’s representation model. To address these challenges, a Question-Answering System that automatically translates natural language questions into SPARQL queries, over the Smithsonian American Art Museum CIDOC-CRM representation is presented. The proposed approach uses an ontology, named Query Ontology, defined to represent the natural language concepts and relations specific to the question’s domain. This system’s architecture uses a traditional natural language processing symbolic approach, with a pipeline of modules for the syntactic, semantic, and pragmatic analysis. An evaluation of the proposed system is presented and shows very promising results.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = jan,
keywords = {datasets, SPARQL queries, CIDOC-CRM representation, SAAM’s knowledge base}
}

@proceedings{10.1145/3688828,
title = {GROUP '25: Companion Proceedings of the 2025 ACM International Conference on Supporting Group Work},
year = {2025},
isbn = {9798400711879},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hilton Head, New Jersey, USA}
}

@article{10.1145/3713079,
author = {Halder, Sajal and Lim, Kwan Hui and Chan, Jeffrey and Zhang, Xiuzhen},
title = {Deep Learning of Dynamic POI Generation and Optimisation for Itinerary Recommendation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3713079},
doi = {10.1145/3713079},
abstract = {Itinerary recommendation involves suggesting a sequence of Points of Interests (POIs) that users obtain maximum satisfaction under a time budget. Existing models have three challenges. First, they model user interest as non-time dependent, which can not capture user interest appropriately because user interest can be contextual on time, e.g., interest in restaurants are likely higher during typical meal times. Second, they model the distance dependency of user interest as a linear one, which does not always adequately capture this relationship, e.g., could be a cubic decay relationship. Finally, existing studies treat POI recommendation and itinerary optimisation as two separate problems, which can result in sub-optimal itinerary recommendations. In this paper, we propose a deep learning model that recommend POIs and construct the itinerary simultaneously and in an integrated manner. It captures user dynamic interest and non-linear spatial dependencies in itinerary recommendations. The proposed model has two steps, where the candidate selection policy generates a set of personalised candidate POIs based on user interest and the itinerary construction step maximises user interest within budget time. To recommend an appropriate candidate set, we propose a multi-head, attention-based transformer to leverage periodic trends and recent activities to capture user dynamic preferences. We also introduce a new co-visiting patterns-based graph convolutional network (GCN) model to capture user non-linear spatial dependencies. To construct the full itinerary from the dynamic candidate sets, we apply greedy policy that incrementally constructs itineraries within the budget time which aims to maximise user interest and minimise queuing time. Experimental results show that the proposed deep learning model outperforms state-of-the-art baselines in itinerary recommendation in four theme parks and four cities datasets The proposed model outperforms the baselines in itinerary recommendation from 7.79% to 26.28% on various dataset in terms of F1-score value. We also show that the proposed candidate generation approach outperforms the state-of-the-art next POI recommendation models in eight real datasets. The proposed model outperforms the baselines on average by 11.29 % in terms of F1-score@5 values and 9.08% in terms of F1-score@10 values. We have publicly shared our source code at GitHub for the reproducibility of our proposed model.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = jan,
keywords = {Itinerary Recommendation, User Interest, Deep Learning, Budget Time, Periodic Interest, Transformer}
}

@article{10.1145/3715098,
author = {Wang, Hao and Guo, Bin and Zeng, Yating and Chen, Mengqi and Ding, Yasan and Zhang, Ying and Yao, Lina and Yu, Zhiwen},
title = {Enabling Harmonious Human-Machine Interaction with Visual-Context Augmented Dialogue System: A Review},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3715098},
doi = {10.1145/3715098},
abstract = {The intelligent dialogue system, aiming at communicating with humans harmoniously with natural language, is brilliant for promoting the advancement of human-machine interaction in the era of artificial intelligence. With the gradually complex human-computer interaction requirements, it is difficult for traditional text-based dialogue system to meet the demands for more vivid and convenient interaction. Consequently, Visual-Context Augmented Dialogue System (VAD), which has the potential to communicate with humans by perceiving and understanding multimodal information (i.e., visual context in images or videos, textual dialogue history), has become a predominant research paradigm. Benefiting from the consistency and complementarity between visual and textual context, VAD possesses the potential to generate engaging and context-aware responses.  To depict the development of VAD, we first characterize the concept model of VAD and then present its generic system architecture to illustrate the system workflow, followed by a summary of multimodal fusion techniques. Subsequently, several research challenges and representative works are investigated, followed by the summary of authoritative benchmarks and real-world application of VAD. We conclude this paper by putting forward some open issues and promising research trends for VAD, e.g., the cognitive mechanisms of human-machine dialogue under cross-modal dialogue context, mobile and lightweight deployment of VAD.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = jan,
keywords = {Human-machine interaction, dialogue system, visual context, vision and language, large language models}
}

@article{10.1145/3708888,
author = {Wenbo, Zhang and Hongbo, Dang and Zhenshan, Bao and Bingyan, Song},
title = {LAMGCN:Traditional Chinese Medicine Herb Recommendation via LSTMs with Attention Mechanisms and Graph Convolutional Networks},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3708888},
doi = {10.1145/3708888},
abstract = {Herb recommendation plays a crucial role in the therapeutic process of Traditional Chinese Medicine (TCM), which aims to recommend a set of herbs to treat patients with different symptoms. Previous works used many methods to discover regularities in prescriptions but rarely considered the actual therapeutic process in TCM and the information of herbs was ignored. In this work, we propose LAMGCN(Herb Recommendation via LSTMs with Attention Mechanisms and Graph Convolutional Networks), which takes the syndrome induction process and the herb descriptions into account. We utilize attention mechanisms and graph neural networks to capture the correlation between symptoms and herbs. Extensive experiments have been done and the results demonstrate the effectiveness of our proposed method.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
keywords = {Traditional chinese medicine, Text classification, Herb recommendation, Attention mechanism, Graph neural network}
}

@article{10.1145/3711680,
author = {Kuang, Jiayi and Shen, Ying and Xie, Jingyou and Luo, Haohao and Xu, Zhe and Li, Ronghao and Li, Yinghui and Cheng, Xianfeng and Lin, Xika and Han, Yu},
title = {Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3711680},
doi = {10.1145/3711680},
abstract = {Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques and gradually becomes a benchmark test task in multimodal large language models (MLLMs). The goal of our survey is to provide an overview of the development of VQA and a detailed description of the latest models with high timeliness. This survey gives an up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on image-question information on the core VQA tasks. In addition, we elaborate on recent advances in extracting and fusing modal information with vision-language pretraining models and multimodal large language models in VQA. We also exhaustively review the progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge. Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = jan,
keywords = {visual question answering, multimodal representation and reasoning, multimodal large language models}
}

@proceedings{10.1145/3707292,
title = {AIIIP '24: Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
year = {2024},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3709134,
author = {Wang, Wenjie and Liu, Zheng and Feng, Fuli and Dou, Zhicheng and Ai, Qingyao and Yang, Grace Hui and Lian, Defu and Hou, Lu and Sun, Aixin and Zamani, Hamed and Metzler, Donald and de Rijke, Maarten},
title = {Pre-Trained Models for Search and Recommendation: Introduction to the Special Issue—Part 1},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3709134},
doi = {10.1145/3709134},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {27},
numpages = {6},
keywords = {Pre-trained Models, Search, Recommendation, Large Language Models, Trustworthiness}
}

@article{10.1145/3715318,
author = {Zhang, Qiang and Ding, Keyan and Lv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and Zhuang, Xiang and Wang, Zeyuan and Qin, Ming and Zhang, Mengyao and Zhang, Jinlu and Cui, Jiyu and Xu, Renjun and Chen, Hongyang and Fan, Xiaohui and Xing, Huabin and Chen, Huajun},
title = {Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3715318},
doi = {10.1145/3715318},
abstract = {Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this article, we endeavor to methodically delineate the concept of “scientific language,” whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {161},
numpages = {38},
keywords = {Scientific domain, large language models, protein, molecule, genome}
}

@inproceedings{10.1145/3712623.3712637,
author = {Tao, Jie and Cui, Peizhang and Wang, Xin},
title = {Natural Language Dialogue Model Based on Multi-Head Graph Attention Neural Network},
year = {2025},
isbn = {9798400712883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712623.3712637},
doi = {10.1145/3712623.3712637},
abstract = {The practical application of artificial intelligence in the field of natural language processing is becoming more and more extensive, and the technology is changing day by day. With the rapid update and development of artificial intelligence technology, more and more researchers find that the use of graph neural networks to construct a natural language dialog model can obtain better results than other artificial intelligence techniques, due to the fact that the deep neural network model is too large and complex, which hinders the practical application of artificial intelligence technology in the field of natural language dialog, in this paper, we propose a natural language dialog model based on the multi-head graph attention neural network This paper proposes a natural language dialog model based on multi-head graph attention neural network, which verifies the practicability and robustness of the method after simulation experiments.},
booktitle = {Proceedings of the 2024 2nd International Conference on Advances in Artificial Intelligence and Applications},
pages = {98–101},
numpages = {4},
keywords = {Artificial Intelligence, Multi-Head Graph Attention Neural Network, Natural Language Dialogue Model},
location = {
},
series = {AAIA '24}
}

@inproceedings{10.5555/3716662.3716788,
author = {Thais, Savannah},
title = {Misrepresented Technological Solutions in Imagined Futures: The Origins and Dangers of AI Hype in the Research Community},
year = {2025},
publisher = {AAAI Press},
abstract = {Technology does not exist in a vacuum; technological development, media representation, public perception, and governmental regulation cyclically influence each other to produce the collective understanding of a technology's capabilities, utilities, and risks. When these capabilities are overestimated, there is an enhanced risk of subjecting the public to dangerous or harmful technology, artificially restricting research and development directions, and enabling misguided or detrimental policy. The dangers of technological hype are particularly relevant in the rapidly evolving space of AI. Centering the research community as a key player in the development and proliferation of hype, we examine the origins and risks of AI hype to the research community and society more broadly and propose a set of measures that researchers, regulators, and the public can take to mitigate these risks and reduce the prevalence of unfounded claims about the technology.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1455–1465},
numpages = {11},
location = {San Jose, California, USA},
series = {AIES '24}
}

@article{10.1145/3712588,
author = {Liu, Yuanxing and Pei, Jiahuan and Zhang, Wei-Nan and Li, Ming and Che, Wanxiang and de Rijke, Maarten},
title = {Augmentation with Neighboring Information for Conversational Recommendation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3712588},
doi = {10.1145/3712588},
abstract = {Conversational recommender systems (CRSs) suggest items to users by understanding their needs and preferences from natural language conversations. While users can freely express preferences, modeling needs and preferences solely from users’ conversations is challenging due to the sparsity of the available information. Prior work introduces external resources to enrich information expressed in conversations. Obtaining such resources is challenging and not always effective. Can learning intrinsic relations among conversations and items enhance information without the use of external resources? Inspired by collaborative filtering, we propose to use so-called neighboring relations within training data, i.e., relations between conversations, items, and similar conversations and items, to enhance our algorithmic understanding of CRSs.We propose a neighboring relations enhanced conversational recommender system (NR-CRS) and study how neighboring relations improve CRSs from two angles: (i) We mine preference information from neighboring conversations to enhance the modeling of user representations and learning of user preferences. (ii) We generate negative samples based on neighboring items to extend the data available for training CRSs. Experiments on the ReDial dataset show that NR-CRS outperforms the state-of-the-art baseline by 11.3%–20.6% regarding recommendation performance while generating informative and diverse responses. We also assess the capabilities of large language models (LLMs) (i.e., Llama 2, Llama 3, and Chinese-Alpaca2) for CRSs. While the generated responses exhibit enhanced fluency and informativeness, recommending target items with LLMs remains challenging; we recommend that LLMs be used as a decoding base for NR-CRS to generate relevant and informative responses.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = jan,
keywords = {Conversational recommendation, Neighboring relations}
}

@proceedings{10.1145/3708394,
title = {AIFE '24: Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
year = {2024},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3715097,
author = {Deng, Yang and Liao, Lizi and Lei, Wenqiang and Yang, Grace and Lam, Wai and Chua, Tat-Seng},
title = {Proactive Conversational AI: A Comprehensive Survey of Advancements and Opportunities},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3715097},
doi = {10.1145/3715097},
abstract = {Dialogue systems are designed to offer human users social support or functional services through natural language interactions. Traditional conversation research has put significant emphasis on a system’s response-ability, including its capacity to understand dialogue context and generate appropriate responses. However, the key element of proactive behavior – a crucial aspect of intelligent conversations – is often overlooked in these studies. Proactivity empowers conversational agents to lead conversations towards achieving pre-defined targets or fulfilling specific goals on the system side. Proactive dialogue systems are equipped with advanced techniques to handle complex tasks, requiring strategic and motivational interactions, thus representing a significant step towards artificial general intelligence. Motivated by the necessity and challenges of building proactive dialogue systems, we provide a comprehensive review of various prominent problems and advanced designs for implementing proactivity into different types of dialogue systems, including open-domain dialogues, task-oriented dialogues, and information-seeking dialogues. We also discuss real-world challenges that require further research attention to meet application needs in the future, such as proactivity in dialogue systems that are based on large language models, proactivity in hybrid dialogues, evaluation protocols and ethical considerations for proactive dialogue systems. By providing a quick access and overall picture of the proactive dialogue systems domain, we aim to inspire new research directions and stimulate further advancements towards achieving the next level of conversational AI capabilities, paving the way for more dynamic and intelligent interactions within various application domains.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = jan,
keywords = {Dialogue Systems, Proactivity, Open-domain Dialogue, Task-oriented Dialogue, Conversational Information Seeking}
}

@article{10.1145/3711826,
author = {Taji, Mobina and Ghafouri, Arash and Naderi, Hasan and Minaei-Bidgoli, Behrouz},
title = {PersianMHQA: A Dataset for Open Domain Persian Multi-hop Question Answering Based on Wikipedia Encyclopedia},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3711826},
doi = {10.1145/3711826},
abstract = {Today, one of the most important tasks in natural language processing is answering user questions. Especially, users' questions nowadays moved from simple questions to complex questions. In recent years, several question answering datasets have been produced for Persian language, but none of them support complex open-domain and explainable questions. In this article, the PersianMHQA dataset is introduced which is the first open-domain question answering dataset for complex questions based on the unstructured Persian Wikipedia encyclopedia. This dataset contains 7,000 complex questions and sentence-level supporting facts are provided for each question that allows question answering systems to explain the predictions. The questions in this dataset are diverse and explainable and are not limited to any previous knowledge base. Various types of complexity are provided in this dataset, and the questions are designed in such a way that answering them requires reasoning over more than one paragraph.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {18},
numpages = {17},
keywords = {Question Answering Systems, multi-hop question, dataset, low resources languages, persian encyclopedia}
}

@proceedings{10.1145/3712623,
title = {AAIA '24: Proceedings of the 2024 2nd International Conference on Advances in Artificial Intelligence and Applications},
year = {2024},
isbn = {9798400712883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.5555/3716662.3716680,
author = {Bommasani, Rishi and Soylu, Dilara and Liao, Thomas I. and Creel, Kathleen A. and Liang, Percy},
title = {Ecosystem Graphs: Documenting the Foundation Model Supply Chain},
year = {2025},
publisher = {AAAI Press},
abstract = {Foundation models (e.g. GPT-4, Gemini, Llama 3) pervasively influence society, warranting greater understanding. While the models garner much attention, accurately characterizing their impact requires considering the broader sociotechnical ecosystem in which they are created and deployed. We propose Ecosystem Graphs as a documentation framework to centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical and social relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata, such as the model's estimated training emissions or licensing guidelines. Since its release in March 2023, Ecosystem Graphs represents an ongoing effort to document 568 assets (112 datasets, 359 models, 97 applications) from 117 organizations. Ecosystem Graphs functions as a multifunctional resource: we discuss two major uses by the 2024 AI Index and the UK's Competition and Markets Authority that demonstrate the value of Ecosystem Graphs.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {196–209},
numpages = {14},
location = {San Jose, California, USA},
series = {AIES '24}
}

@inproceedings{10.1145/3708394.3708455,
author = {Lv, Jiayan and Yao, Jinfang and Zhu, He},
title = {Research on the Cultivation of Teacher Candidates from the Perspective of AI Empowerment with Sentiment analysis},
year = {2025},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708394.3708455},
doi = {10.1145/3708394.3708455},
abstract = {Over the past decade, with the advancement of technology, chatbots have become a hotspot in the field of artificial intelligence (AI) and are widely used in consumer services, education, search engines, marketing, and other fields. Among them, the Chat Generative Pre-Trained Transformer (ChatGPT), composed of language models and optimization techniques, is leading a transformation in human-computer interaction methods. This study selects the social media platforms "REDnote" and "Weibo" as the research field to explore the role and impact of ChatGPT in education and industry ecosystems. This work examines public sentiment regarding the application of AI in educational ecosystems and talent development by analyzing social media discussions. The sentiment analysis conducted using advanced machine learning models, highlights the prevalence of positive emotions toward ChatGPT's role in enhancing teaching and learning experiences. Furthermore, this study introduces an AI-based dynamic talent cultivation model, rooted in the "3H" (Head, Hand, Heart) framework, which emphasizes cognitive skills, practical capabilities, and emotional intelligence.},
booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
pages = {358–364},
numpages = {7},
keywords = {Artificial Intelligence, ChatGPT, Deep Learning, Machine Learning, Normal Education, Social Media, Talent Cultivation, User Experience},
location = {
},
series = {AIFE '24}
}

@inproceedings{10.1145/3706890.3706951,
author = {Zhang, Tao and Zhao, Likun},
title = {MMR: Math Multi-step Reasoning in Medical Dialogue Generation},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706951},
doi = {10.1145/3706890.3706951},
abstract = {With the advancements of large language models in text and visual tasks, researchers are increasingly exploring their applications in medical scenarios. While existing studies have successfully applied these models to medical dialogue systems, challenges remain in accurately calculating medication dosages and handling multi-turn reasoning due to the low tolerance for error in medical contexts. To address this, we propose Math Multi-step Reasoning in Medical Dialogue Generation (MMR), which enhances reasoning by iteratively breaking down complex problems into simpler questions using a “Least to Most Prompting” (LMP)strategy. MMR integrates Chain of Thought, React mechanisms, and Retrieval-Augmented Generation (RAG) with a domain-specific knowledge base to improve reasoning accuracy. Supported by the MedDGQA dataset, MMR outperforms state-of-the-art methods in both objective and subjective evaluations.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {348–351},
numpages = {4},
keywords = {Large Language Models, Medical Dialogue Generation, Multi-step Reasoning},
location = {
},
series = {ISAIMS '24}
}

@inproceedings{10.1145/3706890.3706930,
author = {Weng, Suxiang and Gong, Weibin and Chen, Qinyin and Yan, Yiwei and Zheng, Yingbin and Zhuang, Jiaying and Liu, Yishan and Guo, Xiaoyun and Zhao, Min},
title = {The Research of Disease Trend Early Warning Model Based on Artificial Intelligence},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706930},
doi = {10.1145/3706890.3706930},
abstract = {By analyzing the word frequency and semantic parsing of patient medical records, the system structures unstructured data such as patient complaints, physical examination data and reports, admission diagnoses, medical orders, and medication plans to generate dynamic medical records. This process automatically populates nursing system assessment forms, identifies nursing problems, formulates nursing tasks, and recommends nursing decisions. This disease trend warning model leverages optimized algorithm to predict disease occurrence and progression, providing a solid foundation fobasis for personalized and precise nursing care. This system have been implemented in the cardiology department at the First Affiliated Hospital of Xiamen University for over 2 years, resulting in a 22.34% increase in nursing diagnosis accuracy and significant labor cost savings. Its deployment demonstrates real-world effectiveness in enhancing healthcare outcomes and operational efficiencies.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {229–234},
numpages = {6},
keywords = {Dynamic medical records, disease trend warning models, nursing assessment forms},
location = {
},
series = {ISAIMS '24}
}

@inproceedings{10.1145/3704323.3704363,
author = {Li, Lulu and Li, Zhengtao and Hu, Peng and Zhang, Hao},
title = {A formal corpus of mathematical theorems: a bridge from natural language to first-order logic},
year = {2025},
isbn = {9798400717482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704323.3704363},
doi = {10.1145/3704323.3704363},
abstract = {Currently, there is a significant gap in the conversion of mathematical theorems from natural language to logical expressions, specifically in the form of first-order predicate logic. To address this issue, this paper presents a corpus of mathematical theorems expressed in first-order predicate logic and explores its necessity and practical applications. The primary objective of this work is to introduce a new paradigm for the logical transformation of mathematical theorems by detailing specific the construction process. The paper aims to demonstrate the potential functions and roles of this corpus, providing solid data support for automated proofs, formal verification, and other related fields. This corpus facilitates the automated processing and understanding of mathematical theorems. The contribution of this paper lies in proposing an innovative solution to bridge the gap between natural language processing and formal logic research. This study aims to enhance the performance and accuracy of algorithms and tools for processing mathematical theorems by constructing a corpus of theorems in first-order predicate logic.},
booktitle = {Proceedings of the 2024 13th International Conference on Computing and Pattern Recognition},
pages = {394–398},
numpages = {5},
keywords = {First-order Logic, Mathematical Theorems, Corpus Construction, Natural Language Processing},
location = {
},
series = {ICCPR '24}
}

@proceedings{10.1145/3706890,
title = {ISAIMS '24: Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
year = {2024},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3706890.3707016,
author = {Wang, Juan and Hou, Li and Li, Yunhan and Sun, Yueping and Li, Jiaming and Yang, Li},
title = {Classifying Public Health Questions Using Large Language Models},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3707016},
doi = {10.1145/3706890.3707016},
abstract = {With the booming development of online medical communities, a large amount of valuable medical and health question-and-answer data emerges, providing the possibility for the extraction of public health information and the improvement of question and answer system efficiency. To classify public health questions more effectively, this paper proposes a model ensemble approach that utilizes BERT and two large language models to accurately predict the categories of public health questions. It integrates the prediction results from each model through a voting mechanism to derive the final classification outcome. To verify the effectiveness of this method, this paper conducts experiments based on the Chinese Medical Intent Dataset (CMID) and compares it with various other methods. The experimental results demonstrate that the method proposed in this paper has achieved significant improvement in accuracy, effectively proving the effectiveness of the approach combining model integration and voting mechanism in the classification task of public health questions.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {735–740},
numpages = {6},
keywords = {Large language models, Model integration, Public health questions, Voting mechanism},
location = {
},
series = {ISAIMS '24}
}

@article{10.1145/3712184,
author = {Fan, Wenqi and Zhao, Shu and Tang, Jiliang},
title = {Introduction of the Special Issue on Trustworthy Artificial Intelligence},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4681},
url = {https://doi.org/10.1145/3712184},
doi = {10.1145/3712184},
note = {Just Accepted},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan
}

@article{10.1145/3708886,
author = {Wang, Yuanlong and Ma, Qiang and Li, Ru and Zhang, Hu},
title = {Visual Story Generation Model Guided by Multi Granularity Image Information},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3708886},
doi = {10.1145/3708886},
abstract = {Visual story generation, which involves generating short stories from sequential images, has become a core task at the intersection of computer vision and natural language processing. However, existing methods suffer from a bias in the concept predicates predicted, leading to a semantic gap between the generated stories and the images. This article proposes a novel visual story generation model that utilizes multi granularity image information to guide the generation process and correct the bias in concept predicates, resulting in more image-consistent stories. The proposed model consists of two stages: In the first stage, a set of concepts predicates is predicted from the image and enriched with external knowledge, and the most suitable concepts for story generation are selected. In the second stage, fine-grained image information are utilized to integrate image information into the story generation module, improving the bias in concept predicates. The image theme information and the generated results of previous moments are used as prompts to guide the story generation module. Experimental results show that the proposed model outperforms baseline models in all evaluation metrics. Specifically, the Bilingual Evaluation Understudy 1 (BLEU-1), BLEU-2, BLEU-3, and BLEU-4 metrics are improved by 4.0, 3.8, 3.02, and 1.98 percentage points, respectively, and the METEOR metric is improved by 1.4 percentage points. The generated stories are more consistent with the image content, maintain a consistent theme, and enhance coherence between contexts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {8},
numpages = {13},
keywords = {Visual story generation, image information, concept predicate bias, fine-grained visual features}
}

@inproceedings{10.1145/3701100.3701155,
author = {Jia, Quanye and Liang, Xiao and Zhang, Qiang and Zheng, Huamingzhou and Fan, Xiaoxuan},
title = {Data Augmentation for Technical Standard Relation Extraction},
year = {2025},
isbn = {9798400718120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701100.3701155},
doi = {10.1145/3701100.3701155},
abstract = {The paper introduces a method for fine-grained relation extraction in grid technology standards, addressing challenges in manual annotation due to complex guidelines and large-scale dataset requirements. Data augmentation techniques, specifically word-level perturbation and sentence template methods, are applied to a limited annotated dataset to improve model generalization and efficiency. Experiments on Q/GDW 1168-2013 annotated data show that the GPLinker model has demonstrated its higher efficiency and accuracy in relation extraction, with the integrated use of word-level perturbation and sentence template augmentation significantly boosts the model's F1-score to 0.582925, indicating improved precision and recall in relation extraction from technical standards.},
booktitle = {Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology},
pages = {265–269},
numpages = {5},
keywords = {Data Augmentation, Deep Learning, Relation Extraction, Technical Standard},
location = {
},
series = {ADMIT '24}
}

@article{10.1145/3712005,
author = {Gao, Cuiyun and Hu, Xing and Gao, Shan and Xia, Xin and Jin, Zhi},
title = {The Current Challenges of Software Engineering in the Era of Large Language Models},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3712005},
doi = {10.1145/3712005},
abstract = {With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape’s challenges and opportunities.The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement &amp; design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Large Language Models, Challenges, LLM4SE}
}

@inproceedings{10.1145/3702163.3702174,
author = {Xu, Ye and Mao, Decheng and Wang, Chengliang},
title = {XR Technologies in vocational education and training research (2000-2024): A Bibliometric Review},
year = {2025},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702163.3702174},
doi = {10.1145/3702163.3702174},
abstract = {Extended reality (XR) technology plays a crucial role in vocational education and training in the smart era, and it has a significant impact on the cultivation of talents with innovative and comprehensive skills. We can identify current research hotspots and assess the depth and breadth of existing research so that we can fill the gaps in research in a timely manner. To achieve this goal, we used two software tools, Vosviewer and Citespace, to conduct a detailed multidimensional visualisation and analysis of 240 documents in the Web of Science (WoS) database since the 21st century. Our study reveals the following key findings: 1) The publication volume of the literature has shown a steady growth trend since 2000 to 2019, and has witnessed a significant increase between 2019 and 2024. Meanwhile, author groups such as Cattaneo, Alberto and Hamalainen, Raija are gradually becoming more representative scholars producing more articles.2) The field has clearly fostered three dominant research foci, which are: exploring VET based on an educational psychology perspective, approaching VET from a computer science and technology aspect, and focusing on the use of XR technology devices. 3) XR technology in vocational education and training shows a clear stage-by-stage evolution and is moving towards a deeper level of technological application. Some researchers have begun to focus on the construction of student-oriented educational models as well as teachers' acceptance and use of the technology.},
booktitle = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
pages = {76–83},
numpages = {8},
keywords = {Bibliometrics, Extended Reality, visual analysis, vocational education and training},
location = {
},
series = {ICETC '24}
}

@proceedings{10.1145/3705754,
title = {CECCT '24: Proceedings of the 2024 2nd International Conference on Electronics, Computers and Communication Technology},
year = {2024},
isbn = {9798400710193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3703594,
author = {Paschalides, Demetris and Pallis, George and Dikaiakos, Marios},
title = {A Framework for the Unsupervised Modeling and Extraction of Polarization Knowledge from News Media},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {1–2},
url = {https://doi.org/10.1145/3703594},
doi = {10.1145/3703594},
abstract = {Polarization poses global concerns for social cohesion and stability, making its understanding crucial for effective mitigation measures. In this paper, we introduce an unsupervised, domain-agnostic framework for computationally modeling, extracting, and measuring polarization in digital media. By leveraging Natural Language Processing and Graph Analysis techniques, the proposed framework creates a Polarization Data Model (PDM) that encompasses key elements of Polarization Knowledge (PK), such as entities, fellowships, dipoles, and discussion topics. To evaluate the effectiveness of the framework, we propose a multi-level PK evaluation methodology that assesses its ability to: (i) capture entities’ attitudes toward various topics, (ii) align politically cohesive fellowships with their respective party manifestos, and (iii) identify domain-specific topics along with their degree of polarization. We applied this evaluation methodology to the use cases of Abortion, Immigration, and Gun Control. The results demonstrate our framework’s robust performance across these case studies, yielding promising outcomes compared to state-of-the-art and baseline methods.},
journal = {Trans. Soc. Comput.},
month = jan,
articleno = {5},
numpages = {38},
keywords = {Polarization, Multi-level Polarization, Polarization Modeling, Polarization Extraction, Polarization Computational Evaluation}
}

@article{10.1145/3701181,
author = {Muralikumar, Meena Devii and McDonald, David W.},
title = {An Emerging Design Space of How Tools Support Collaborations in AI Design and Development},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3701181},
doi = {10.1145/3701181},
abstract = {Developing AI/ML models and incorporating them into a product is complex work. While AI models are generally non-deterministic and have high capability uncertainty, the advent of foundation models further exacerbates the complexity of working with AI and ensuring responsible innovation. This complex work is achieved by not just AI practitioners, but through coordination and collaboration of different groups of practitioners, not all of whom might be experts in AI. Our primary objective is to explore how the tools and systems used by practitioners help achieve this complex work. To that end, we conducted a design space analysis of 18 relevant tools using corresponding research publications and constructed a design space with the dimensions - User, Axis of AI work, Semantics of Use, Tool Architecture, Artifact Type and Availability, and Collaboration Goals. Using these dimensions we derive four spirits of the tool in supporting collaborations - groupware, core practice &amp; communication, community of practice, and visibility &amp; bridging. Through this work, we contribute a conceptual design space of how tools can be designed to support collaborations in AI development and discuss how our design space can be leveraged by system designers and researchers working at the intersection of HCI, CSCW, and AI.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {GROUP2},
numpages = {28},
keywords = {ai, collaboration, design space, tools}
}

@proceedings{10.1145/3705374,
title = {ICCDA '24: Proceedings of the 2024 8th International Conference on Computing and Data Analysis},
year = {2024},
isbn = {9798400710445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3712623.3712660,
author = {Shen, Meng and Luo, Ziheng and Liao, Yong},
title = {Comprehensive Out-of-context Misinformation Detection via Global Information Enhancement},
year = {2025},
isbn = {9798400712883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712623.3712660},
doi = {10.1145/3712623.3712660},
abstract = {False information that spreads rapidly on the Internet may have a negative impact on individuals, organizations, and society. Cheapfake, which refers to out-of-context misinformation, involves reusing the pairing of images and captions to support false descriptions. Out-of-context misinformation detection requires consideration of the global semantic understanding of images and intertwined contextual information. Currently, the baseline method don’t extract enough global information. This paper uses visual-semantic reasoning to extract the global features of images and texts, and proposes a new detection framework that employs fake statement checking and multimodal natural language inference to determine whether the input triples are cheapfake through multiple steps. The accuracy of our method is 3.4% higher than that of the baseline method, and it also has its own advantages compared with other improved methods.},
booktitle = {Proceedings of the 2024 2nd International Conference on Advances in Artificial Intelligence and Applications},
pages = {138–142},
numpages = {5},
keywords = {Content Security, Out-of-context, Misinformation Detection, Multimodal Natural Language Inference},
location = {
},
series = {AAIA '24}
}

@article{10.1145/3716628,
author = {Deng, Zehang and Guo, Yongjian and Han, Changzhou and Ma, Wanlun and Xiong, Junwu and Wen, Sheng and Xiang, Yang},
title = {AI Agents Under Threat: A Survey of Key Security Challenges and Future Pathways},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3716628},
doi = {10.1145/3716628},
abstract = {An Artificial Intelligence (AI) agent is a software entity that autonomously performs tasks or makes decisions based on pre-defined objectives and data inputs. AI agents, capable of perceiving user inputs, reasoning and planning tasks, and executing actions, have seen remarkable advancements in algorithm development and task performance. However, the security challenges they pose remain under-explored and unresolved. This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities. By systematically reviewing these threats, this paper highlights both the progress made and the existing limitations in safeguarding AI agents. The insights provided aim to inspire further research into addressing the security threats associated with AI agents, thereby fostering the development of more robust and secure AI agent applications.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = feb,
keywords = {AI Agent, Trustworthiness, Security}
}

@article{10.1145/3706631,
author = {Liu, Qidong and Qiu, Zhaopeng and Zhao, Xiangyu and Wu, Xian and Zhang, Zijian and Xu, Tong and Tian, Feng},
title = {A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1046-8188},
url = {https://doi.org/10.1145/3706631},
doi = {10.1145/3706631},
abstract = {Medication recommendation is one of the most critical health-related applications, which has attracted extensive research interest recently. Most existing works focus on a single hospital with abundant medical data. However, many small hospitals only have a few records, which hinders applying existing medication recommendation works to the real world. Thus, we seek to explore a more practical setting, i.e., multi-center medication recommendation. In this setting, most hospitals have few records, but the total number of records is large. Though small hospitals may benefit from total affluent records, it is also faced with the challenge that the data distributions between various hospitals are much different. In this work, we introduce a novel conTrastive prEtrain Model with Prompt Tuning (TEMPT) for multi-center medication recommendation, which includes two stages of pretraining and finetuning. We first design two self-supervised tasks for the pretraining stage to learn general medical knowledge. They are mask prediction and contrastive tasks, which extract the intra- and inter-relationships of input diagnosis and procedures. Furthermore, we devise a novel prompt tuning method to capture the specific information of each hospital rather than adopting the common finetuning. On the one hand, the proposed prompt tuning can better learn the heterogeneity of each hospital to fit various distributions. On the other hand, it can also relieve the catastrophic forgetting problem of finetuning. To validate the proposed model, we conduct extensive experiments on the public eICU, a multi-center medical dataset. The experimental results illustrate the effectiveness of our model. The implementation code is available to ease the reproducibility1.},
note = {Just Accepted},
journal = {ACM Trans. Inf. Syst.},
month = jan,
keywords = {Multi-Center, Medication Recommendation, Electronic Health Record, Contrastive Learning, Prompt Tuning}
}

@proceedings{10.1145/3704323,
title = {ICCPR '24: Proceedings of the 2024 13th International Conference on Computing and Pattern Recognition},
year = {2024},
isbn = {9798400717482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3717059,
author = {Liu, Huaping and Guo, Di and Cangelosi, Angelo},
title = {Embodied Intelligence: A Synergy of Morphology, Action, Perception and Learning},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3717059},
doi = {10.1145/3717059},
abstract = {Embodied intelligence emphasizes that the intelligence is affected by the tight coupling of brain, body and environment. It is continuously and dynamically generated through the process of information perception and physical interaction with the environment. During the past years, the research scope of embodied intelligence has also been expanding and it has attracted great attentions from various communities. At the same time, a huge number of works relevant to embodied intelligence have been proposed, especially in recent several years. In this paper, we present a comprehensive survey of embodied intelligence from the perspective that it is a synergy of morphology, action, perception and learning, providing a thorough summary and categorization of existing studies. Specifically, as the embodied intelligence is a synergy of all these components rather than themselves alone, we mainly focus on the connections across these four components (morphology, action, perception and learning) and identify areas where future research can benefit from their intrinsic connections.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = feb,
keywords = {Embodied intelligence, morphology, action, perception, learning.}
}

@article{10.1145/3701211,
author = {Ankenbauer, Sam Addison and Brewer, Robin N.},
title = {Time's Sublimest Target: Practices of Forgetting in HCI and CSCW},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3701211},
doi = {10.1145/3701211},
abstract = {In our contemporary moment, there exists a hegemonic design practice and a general social desire to retain information. With the help of sociotechnical platforms and other contemporary technologies, information has changed its temporal and spatial boundaries, creating unbounded, algorithmic, and emergent forms of retention. The consequences of such retention are numerous, ranging from an overabundance of autobiographical information that cannot be fully understood by the individual to the improper use and economization of such information by state and corporation alike. Within this context, this paper investigates a counter-hegemonic practice of forgetting, specifically from the perspective of human-computer interaction and computer-supported cooperative work research, with additional insight drawn from adjacent fields. In doing so, we present forgetting as a significant area of research with HCI and CSCW, a burgeoning and contradictory space that may offer solutions to issues we face within a moment of persistence by default. This paper also explores potential directions for future research and design on forgetting in HCI and CSCW through an investigation of an art piece by Chinese artist Song Dong.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {GROUP32},
numpages = {24},
keywords = {deletion, forgetting, intent, retention, spatial, temporal}
}

@inproceedings{10.1145/3704814.3704819,
author = {Pang, Haijie and Li, Chengji},
title = {A BERT and TextCNN integration-based Method for Public Complaints and Proposals Text Classification},
year = {2025},
isbn = {9798400718090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704814.3704819},
doi = {10.1145/3704814.3704819},
abstract = {With the wide use of Blockchain technology and online public complaints and proposals (hereinafter referred to as PCP) platforms, huge amounts of data are accordingly produced, which poses a challenge on the government's effectively choosing and classifying the key information. In view of this, this paper endeavors to conduct research on the PCP text classification for a smart government. Firstly, PCP texts are collected automatically and preprocessed, and then a hierarchical classification structure involving targeted government departments and PCP contents is proposed. Finally, a BERT and TextCNN model is adopted for PCP text classification. The experimental results show that the classification structure and model proposed in this paper can effectively categorize the PCP text, providing a scientific analysis and effective technological method to improve the work efficiency and service quality of PCP.},
booktitle = {Proceedings of the 8th International Conference on Computer Science and Application Engineering},
pages = {15–18},
numpages = {4},
keywords = {BERT, Blockchain, Public complaints and proposals, Text classification, TextCNN},
location = {
},
series = {CSAE '24}
}

@article{10.1145/3704922,
author = {Garcia, Cristiano Mesquita and Abilio, Ramon and Koerich, Alessandro Lameiras and Britto, Alceu de Souza and Barddal, Jean Paul},
title = {Concept Drift Adaptation in Text Stream Mining Settings: A Systematic Review},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3704922},
doi = {10.1145/3704922},
abstract = {The society produces textual data online in several ways, e.g., via reviews and social media posts. Therefore, numerous researchers have been working on discovering patterns in textual data that can indicate peoples’ opinions, interests, and so on. Most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets. This setting can lead to several problems, e.g., outdated datasets and models, which degrade in performance over time. This is particularly true regarding concept drift, in which the data distribution changes over time. Furthermore, text streaming scenarios also exhibit further challenges, such as the high speed at which data arrive over time. Models for stream scenarios must adhere to the aforementioned constraints while learning from the stream, thus storing texts for limited periods and consuming low memory. This study presents a systematic literature review regarding concept drift adaptation in text stream scenarios. Considering well-defined criteria, we selected 48 papers published between 2018 and August 2024 to unravel aspects such as text drift categories, detection types, model update mechanisms, stream mining tasks addressed, and text representation methods and their update mechanisms. Furthermore, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers. Finally, we brought forward a discussion on existing works in the area, also highlighting open challenges and future research directions for the community.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {27},
numpages = {67},
keywords = {Concept drift, text stream mining, semantic shift, representation shift, drift detection}
}

@article{10.1145/3698193,
author = {Zhang, Dan and Zheng, Shaojie and Zhu, Yifan and Yuan, Huihui and Gong, Jibing and Tang, Jie},
title = {MCAP: Low-Pass GNNs with Matrix Completion for Academic Recommendations},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3698193},
doi = {10.1145/3698193},
abstract = {Graph neural networks (GNNs) are commonly used and have shown promising performance in recommendation systems. A major branch, heterogeneous GNNs, models heterogeneous information by leveraging side information for academic paper recommendations. These networks use message passing and high-order propagation to learn representations for users and items. However, existing recommendation methods perform high-order propagation, leading to sub-optimal representation learning. To address this issue, this article proposes a framework called MCAP, which uses relation-aware GNNs and executes low-pass propagation with matrix completion to enhance academic paper recommendations. The framework uses an attention mechanism to learn top- (U)  relationships by constructing a user–user relation graph based on common authors and venues from interacted items. To efficiently and effectively capture semantic-aware similar items, MCAP builds an item–item relation graph by fusing side information of papers using text embedding models (e.g., Mistral) and large language models (e.g., GPT-3.5-Turbo, GLM-4). Finally, the relation-aware user–user and item–item graphs are incorporated into existing GNN-based models to generate representations of users and papers to enhance academic paper recommendations. The effectiveness of the MCAP is validated using four academic datasets, AMiner-PC, AMiner-WeChat, CiteULike, and DBLP, with user–item interactions and side information of papers. Comprehensive experiments show that the MCAP outperforms state-of-the-art models in terms of Recall@5, NDCG@5, and HR@5 with 69.2%, 70.5%, and 77.6% on the AMiner-WeChat dataset. The code for MCAP is available at .},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {33},
numpages = {29},
keywords = {Graph Neural Networks, Low-Pass Propagation, Relation Graph, Matrix Completion, Academic Paper Recommendations}
}

@inproceedings{10.1145/3706890.3706971,
author = {Man, Jianping and Hu, Zhensheng and Liu, Hongze and Yang, Rui and Liu, Jingjing and Chen, Ziyi and Zhou, Yi},
title = {A Model for Epilepsy Named Entity Recognition Based on Chinese EEG Reports},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706971},
doi = {10.1145/3706890.3706971},
abstract = {It is critical for clinical diagnosis and research to extract and utilize the medical information from electroencephalogram (EEG) reports of epilepsy. With the widespread application of deep learning techniques in health care, particularly in natural language processing tasks, named entity recognition (NER) has emerged as an essential tool for information extraction from medical text. This study proposed an epilepsy NER model for EEG reports to improve the efficiency of epilepsy text processing. 17,606 paragraphs from real Chinese epilepsy EEG reports as data samples, were meticulously annotated with 17 entity types by clinical experts. The model used the BERT and the Global Pointer (GP) algorithm to identify nested and non-nested entities, achieved outstanding performance in the Precision, Recall and F1 score. The experimental results demonstrate that our method significantly enhances the effectiveness of epilepsy entity recognition, providing robust support for the automated extraction and analysis of medical information.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {467–472},
numpages = {6},
keywords = {BERT model, EEG reports, Epilepsy, Global Pointer algorithm, Named entity recognition},
location = {
},
series = {ISAIMS '24}
}

@article{10.1145/3712296,
author = {Sun, Wei and Li, Mingxiao and Sileo, Damien and Davis, Jesse and Moens, Marie-Francine},
title = {Generating Explanations in Medical Question-Answering by Expectation Maximization Inference over Evidence},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712296},
doi = {10.1145/3712296},
abstract = {Medical Question Answering (medical QA) systems play an essential role in assisting healthcare workers in finding answers to their questions. However, it is not sufficient to merely provide answers by medical QA systems because users might want explanations, that is, more analytic statements in natural language that describe the elements and context that support the answer. To do so, we propose a novel approach for generating natural language explanations for answers predicted by medical QA systems. As high-quality medical explanations require additional medical knowledge, so that our system extract knowledge from medical textbooks to enhance the quality of explanations during the explanation generation process. Concretely, we designed an Expectation-Maximization approach that makes inferences about the evidence found in these texts, offering an efficient way to focus attention on lengthy evidence passages. Experimental results, conducted on two datasets MQAE-diag and MQAE, demonstrate the effectiveness of our framework for reasoning with textual evidence. Our approach outperforms state-of-the-art models, achieving a significant improvement of 6.13 and 5.47 percentage points on the Rouge-L score; 6.49 and 5.28 percentage points on the Bleu-4 score on the MQAE-diag and MQAE datasets.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jan,
keywords = {Expectation Maximization, Medical Question Answering, Explanation Generation}
}

@article{10.1145/3714456,
author = {Haider Rizvi, Syed Mustafa and Imran, Ramsha and Mahmood, Arif},
title = {Text Classification Using Graph Convolutional Networks: A Comprehensive Survey},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3714456},
doi = {10.1145/3714456},
abstract = {Text classification is a quintessential and practical problem in natural language processing with applications in diverse domains such as sentiment analysis, fake news detection, medical diagnosis, and document classification. A sizable body of recent works exists where researchers have studied and tackled text classification from different angles with varying degrees of success. Graph convolution network (GCN)-based approaches have gained a lot of traction in this domain over the last decade with many implementations achieving state-of-the-art performance in more recent literature and thus, warranting the need for an updated survey. This work aims to summarize and categorize various GCN-based Text Classification approaches with regard to the architecture and mode of supervision. It identifies their strengths and limitations and compares their performance on various benchmark datasets. We also discuss future research directions and the challenges that exist in this domain.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = jan,
keywords = {GCN, Text Classification, Text Analysis, Text Categorization}
}

@article{10.1145/3715005,
author = {Zhang, Shenglin and Xia, Sibo and Fan, Wenzhao and Shi, Binpeng and Xiong, Xiao and Zhong, Zhenyu and Ma, Minghua and Sun, Yongqian and Pei, Dan},
title = {Failure Diagnosis in Microservice Systems: A Comprehensive Survey and Analysis},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715005},
doi = {10.1145/3715005},
abstract = {Widely adopted for their scalability and flexibility, modern microservice systems present unique failure diagnosis challenges due to their independent deployment and dynamic interactions. This complexity can lead to cascading failures that negatively impact operational efficiency and user experience. Recognizing the critical role of fault diagnosis in improving the stability and reliability of microservice systems, researchers have conducted extensive studies and achieved a number of significant results. This survey provides an exhaustive review of 98 scientific papers from 2003 to the present, including a thorough examination and elucidation of the fundamental concepts, system architecture, and problem statement. It also includes a qualitative analysis of the dimensions, providing an in-depth discussion of current best practices and future directions, aiming to further its development and application. In addition, this survey compiles publicly available datasets, toolkits, and evaluation metrics to facilitate the selection and validation of techniques for practitioners.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
keywords = {Microservice, failure diagnosis, root cause localization, failure classification, multimodal data}
}

@article{10.1145/3714430,
author = {Xu, Shuyuan and Ji, Jianchao and Li, Yunqi and Ge, Yingqiang and Tan, Juntao and Zhang, Yongfeng},
title = {Causal Inference for Recommendation: Foundations, Methods and Applications},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2157-6904},
url = {https://doi.org/10.1145/3714430},
doi = {10.1145/3714430},
abstract = {Recommender systems are important and powerful tools for various personalized services. Traditionally, these systems use data mining and machine learning techniques to make recommendations based on correlations found in the data. However, relying solely on correlation without considering the underlying causal mechanism may lead to various practical issues such as fairness, explainability, robustness, bias, echo chamber and controllability problems. Therefore, researchers in related area have begun incorporating causality into recommendation systems to address these issues. In this survey, we review the existing literature on causal inference in recommender systems. We discuss the fundamental concepts of both recommender systems and causal inference as well as their relationship, and review the existing work on causal methods for different problems in recommender systems. Finally, we discuss open problems and future directions in the field of causal inference for recommendations.},
note = {Just Accepted},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
keywords = {Recommender Systems, Causal Inference}
}

@article{10.1145/3673233,
author = {M\"{o}ller, Lucas and Pad\'{o}, Sebastian},
title = {Explaining Neural News Recommendation with Attributions onto Reading Histories},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3673233},
doi = {10.1145/3673233},
abstract = {An important aspect of responsible recommendation systems is the transparency of the prediction mechanisms. This is a general challenge for deep-learning-based systems such as the currently predominant neural news recommender architectures, which are optimized to predict clicks by matching candidate news items against users’ reading histories. Such systems achieve state-of-the-art click-prediction performance, but the rationale for their decisions is difficult to assess. At the same time, the economic and societal impact of these systems makes such insights very much desirable.In this article, we ask the question to what extent the recommendations of current news recommender systems are actually based on content-related evidence from reading histories. We approach this question from an explainability perspective. Building on the concept of integrated gradients, we present a neural news recommender that can accurately attribute individual recommendations to news items and words in input reading histories while maintaining a top scoring click-prediction performance.Using our method as a diagnostic tool, we find that: (a), a substantial number of users’ clicks on news are not explainable from reading histories, and many history-explainable items are actually skipped; (b), while many recommendations are based on content-related evidence in histories, for others the model does not attend to reasonable evidence, and recommendations stem from a spurious bias in user representations. Our code is publicly available at .},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {7},
numpages = {25},
keywords = {News recommendation, explainability, attribution, interpretability, diagnosis, neural recommender}
}

@proceedings{10.1145/3704522,
title = {NSysS '24: Proceedings of the 11th International Conference on Networking, Systems, and Security},
year = {2024},
isbn = {9798400711589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3701100,
title = {ADMIT '24: Proceedings of the 2024 3rd International Conference on Algorithms, Data Mining, and Information Technology},
year = {2024},
isbn = {9798400718120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3708036.3708077,
author = {Yang, Guangyuan and Bai, Meicheng and Xie, Quanying and Chen, Lu},
title = {Review on ChatGPT in Library of China},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708077},
doi = {10.1145/3708036.3708077},
abstract = {As an important application field of new technology, the field of library has conducted in-depth discussions on the application of ChatGPT, and have been achieved a certain number of research results. For providing some reference for the future application of ChatGPT in libraries, this paper analyzes the current research status of the application of ChatGPT in libraries from the aspects of Academic Attention, Research Subjects and Main Research Topics, and puts forward that it should strengthen the discussion of the application of ChatGPT in libraries from the aspects of “Improve the influence of the application of ChatGPT in Chinese libraries to help build the core research group” and “Deepen the exchange and cooperation of interdisciplinary talents to empower the practical of the application of ChatGPT in Chinese libraries”.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {238–241},
numpages = {4},
keywords = {AI chatbot, ChatGPT, data visualization, intelligent service, research progress},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.1109/SCW63240.2024.00193,
author = {Ta\c{s}yaran, Fatih and Yasal, Osman and Morgado, Jos\'{e} A. and Ilic, Aleksandar and Unat, Didem and Kaya, Kamer},
title = {P-MoVE: Performance Monitoring and Visualization with Encoded Knowledge},
year = {2025},
isbn = {9798350355543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SCW63240.2024.00193},
doi = {10.1109/SCW63240.2024.00193},
abstract = {P-MoVE is a modern, open-source framework designed to monitor and visualize live and/or recorded performance data with the ultimate goal of being a digital twin for HPC systems. Leveraging a Knowledge Base (KB), built upon an HPC-specific ontology with an intuitive encoding for comprehending the performance, it rigorously manages telemetry samplers, databases, and visualization frameworks. The KB is generated through an in-depth probing of the system. It enables the configuration and monitoring of performance metric samplers, the generation of real-time visualizations, the establishment of linked-data connections, and the generation of queries for advanced analysis. Furthermore, with an Abstraction Layer, P-MoVE can be used for low-level profiling even on components from different vendors. It is equipped with modern profiling capabilities, including live cache-aware roofline modeling, crafted to provide realtime insights without impeding system performance. P-MoVE's capabilities have been demonstrated on various architectures using microbenchmarks and a common kernel, sparse-matrix vector multiplication.},
booktitle = {Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1531–1542},
numpages = {12},
keywords = {HPC, digital twins for HPC, optimization, performance visualization, profiling},
location = {Atlanta, GA, USA},
series = {SC-W '24}
}

@proceedings{10.1145/3702191,
title = {ICDIS '24: Proceedings of the 2024 International Symposium on Integrated Circuit Design and Integrated Systems},
year = {2024},
isbn = {9798400718229},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3708778,
title = {CIIS '24: Proceedings of the 2024 7th International Conference on Computational Intelligence and Intelligent Systems},
year = {2024},
isbn = {9798400717437},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3709681,
author = {Omar, Reham and Mangukiya, Omij and Mansour, Essam},
title = {Dialogue Benchmark Generation from Knowledge Graphs with Cost-Effective Retrieval-Augmented LLMs},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3709681},
doi = {10.1145/3709681},
abstract = {Dialogue benchmarks are crucial in training and evaluating chatbots engaging in domain-specific conversations. Knowledge graphs (KGs) represent semantically rich and well-organized data spanning various domains, such as DBLP, DBpedia, and YAGO. Traditionally, dialogue benchmarks have been manually created from documents, neglecting the potential of KGs in automating this process. Some question-answering benchmarks are automatically generated using extensive preprocessing from KGs, but they do not support dialogue generation. This paper introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation platform for automatically generating high-quality dialogue benchmarks tailored to a specific domain using a KG. Chatty-Gen decomposes the generation process into manageable stages and uses assertion rules for automatic validation between stages. Our approach enables control over intermediate results to prevent time-consuming restarts due to hallucinations. It also reduces reliance on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront processing of the entire KG using efficient query-based retrieval to find representative subgraphs based on the dialogue context. Our experiments with several real and large KGs demonstrate that Chatty-Gen significantly outperforms state-of-the-art systems and ensures consistent model and system performance across multiple LLMs of diverse capabilities, such as GPT-4o, Gemini 1.5, Llama 3, and Mistral.},
journal = {Proc. ACM Manag. Data},
month = feb,
articleno = {31},
numpages = {26},
keywords = {assertion-based validation, benchmarking, conversational question answering, cost-effecive inference, graph serialization, knowledge graphs (kgs), large language models (llms), retrieval-augumented generation (rag)}
}

@proceedings{10.5555/3715674,
title = {SC-W '24: Proceedings of the SC '24 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
year = {2024},
isbn = {9798350355543},
publisher = {IEEE Press},
location = {Atlanta, GA, USA}
}

@article{10.1145/3711908,
author = {Garg, Piyush and Chakraborty, Roshni and Dandapat, Sourav},
title = {PORTRAIT: A Hybrid Approach to Create Extractive Ground-truth Summary for Disaster Event},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1559-1131},
url = {https://doi.org/10.1145/3711908},
doi = {10.1145/3711908},
abstract = {Nowadays, Twitter is an important source of information and latest updates during ongoing events, such as disaster events. However, the huge number of tweets posted during a disaster makes identification of relevant information highly challenging. Therefore, a summary of the tweets can help the decision-makers to ensure efficient allocation of resources among the affected population. There exist several automated summarization approaches which can generate a summary given the tweets related to a disaster. Development of these automated summarization approaches require availability of ground-truth summary of the dataset for verification. However, the number of publicly available datasets along with the ground-truth summary for disaster events are still inadequate. To improve this situation, we need to create more number of ground-truth summaries. Existing approaches for ground-truth summary generation rely on the annotators’ wisdom and intuition. This process requires immense human effort and significant time. Moreover, the selection of the important tweets from the humongous set of input tweets often results in sub-optimal choice of tweets in the final summary. Therefore, to handle these challenges, we propose a hybrid approach (PORTRAIT) for ground-truth summary generation, where we partly automate the procedure to improve the quality of ground-truth summary and reduce human effort and time. We validate the effectiveness of PORTRAIT on 9 disaster events through quantitative and qualitative analysis. We prepare and release the ground-truth summaries for 9 disaster events, which consist of both natural and man-made disaster events belonging to 5 different continents.},
note = {Just Accepted},
journal = {ACM Trans. Web},
month = jan,
keywords = {Disaster tweet summarization, Ground-truth summary, Social media, Hybrid approach}
}

@proceedings{10.1145/3708282,
title = {AITC '24: Proceedings of the 2024 International Conference on Artificial Intelligence of Things and Computing},
year = {2024},
isbn = {9798400709869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3637487,
author = {Hossain, Md Imran and Zamzmi, Ghada and Mouton, Peter R. and Salekin, Md Sirajus and Sun, Yu and Goldgof, Dmitry},
title = {Explainable AI for Medical Data: Current Methods, Limitations, and Future Directions},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3637487},
doi = {10.1145/3637487},
abstract = {With the power of parallel processing, large datasets, and fast computational resources, deep neural networks (DNNs) have outperformed highly trained and experienced human experts in medical applications. However, the large global community of healthcare professionals, many of whom routinely face potentially life-or-death outcomes with complex medicolegal consequences, have yet to embrace this powerful technology. The major problem is that most current AI solutions function as a metaphorical black-box positioned between input data and output decisions without a rigorous explanation for their internal processes. With the goal of enhancing trust and improving acceptance of artificial intelligence– (AI) based technology in clinical medicine, there is a large and growing effort to address this challenge using eXplainable AI (XAI), a set of techniques, strategies, and algorithms with an explicit focus on explaining the “hows and whys” of DNNs. Here, we provide a comprehensive review of the state-of-the-art XAI techniques concerning healthcare applications and discuss current challenges and future directions. We emphasize the strengths and limitations of each category, including image, tabular, and textual explanations, and explore a range of evaluation metrics for assessing the effectiveness of XAI solutions. Finally, we highlight promising opportunities for XAI research to enhance the acceptance of DNNs by the healthcare community.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {148},
numpages = {46},
keywords = {Explainability, medical data, responsible AI, deep neural networks, interpretable AI}
}

@proceedings{10.1145/3708597,
title = {ICACS '24: Proceedings of the 2024 8th International Conference on Algorithms, Computing and Systems},
year = {2024},
isbn = {9798400718304},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3707127,
title = {ICBBE '24: Proceedings of the 2024 11th International Conference on Biomedical and Bioinformatics Engineering},
year = {2024},
isbn = {9798400718274},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3700666,
title = {ICBRA '24: Proceedings of the 11th International Conference on Bioinformatics Research and Applications},
year = {2024},
isbn = {9798400717536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}


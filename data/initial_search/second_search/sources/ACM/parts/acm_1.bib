@inproceedings{10.1145/3397271.3401289,
author = {Shi, Yunzhou and Luo, Zhiling and Zhu, Pengcheng and Ji, Feng and Zhou, Wei and Chen, Haiqing and Yang, Yujiu},
title = {G2T: Generating Fluent Descriptions for Knowledge Graph},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401289},
doi = {10.1145/3397271.3401289},
abstract = {Generating natural language descriptions for knowledge graph (KG) is an important category for intelligent writing. Recent models on this task substitute the sequence encoder in a commonly used encoder-decoder framework with a graph encoder. However, these models suffer from entity missing and repetition. In this paper, we propose a novel end-to-end generation model named G2T, which integrates a novel Graph Structure Enhanced Mechanism (GSEM) and a Copy Coverage Loss (CCL). Instead of just considering graph structure in the encoding phase in most existing methods, our GSEM fully utilizes graph structure in the decoding phase and helps to mitigate entity missing problem. Moreover, our CCL can further improve performance by avoiding generating repeated entities. With their help, our model is capable of generating fluent description for KG. The results of automatic and human evaluations show that our model outperforms the state-of-the-art models.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1861–1864},
numpages = {4},
keywords = {knowledge graph, knowledge representation, natural language generation},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3340531.3412028,
author = {Leblay, Julien and Chekol, Melisachew Wudage and Liu, Xin},
title = {Towards Temporal Knowledge Graph Embeddings with Arbitrary Time Precision},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412028},
doi = {10.1145/3340531.3412028},
abstract = {Acknowledging the dynamic nature of knowledge graphs, the problem of learning temporal knowledge graph embeddings has recently gained attention. Essentially, the goal is to learn vector representation for the nodes and edges of a knowledge graph taking time into account. These representations must preserve certain properties of the original graph, so as to allow not only classification or clustering tasks, as for classical graph embeddings, but also approximate time-dependent query answering or link predictions over knowledge graphs. For instance, "who was the leader of Germany in 1994?'' or "when was Bonn the capital of Germany?''Several existing work in the area adapt existing knowledge graph embedding models, adding a time dimension, usually restricting to one time granularity, like years or days, or treating time as fixed labels. However, this is not adequate for many facts of life, for instance historical and sensory data. In this work, we introduce and evaluate an approach that gracefully adjusts to time validity of virtually any granularity. Our model is robust to non-contiguous validity periods. It is generic enough to adapt to many existing non-temporal models and its size (number of parameters) does not depend on the size of the graph (number of entities and relations).},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {685–694},
numpages = {10},
keywords = {learning representations, temporal knowledge graph},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3394486.3403143,
author = {Zhou, Kun and Zhao, Wayne Xin and Bian, Shuqing and Zhou, Yuanhang and Wen, Ji-Rong and Yu, Jingsong},
title = {Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403143},
doi = {10.1145/3394486.3403143},
abstract = {Conversational recommender systems (CRS) aim to recommend high-quality items to users through interactive conversations. Although several efforts have been made for CRS, two major issues still remain to be solved. First, the conversation data itself lacks of sufficient contextual information for accurately understanding users' preference. Second, there is a semantic gap between natural language expression and item-level user preference.To address these issues, we incorporate both word-oriented and entity-oriented knowledge graphs~(KG) to enhance the data representations in CRSs, and adopt Mutual Information Maximization to align the word-level and entity-level semantic spaces. Based on the aligned semantic representations, we further develop a KG-enhanced recommender component for making accurate recommendations, and a KG-enhanced dialog component that can generate informative keywords or entities in the response text. Extensive experiments have demonstrated the effectiveness of our approach in yielding better performance on both recommendation and conversation tasks.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1006–1014},
numpages = {9},
keywords = {conversational recommender system, knowledge graph, mutual information maximization},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3387904.3389281,
author = {Zhang, Jinglei and Xie, Rui and Ye, Wei and Zhang, Yuhan and Zhang, Shikun},
title = {Exploiting Code Knowledge Graph for Bug Localization via Bi-directional Attention},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389281},
doi = {10.1145/3387904.3389281},
abstract = {Bug localization automatic localize relevant source files given a natural language description of bug within a software project. For a large project containing hundreds and thousands of source files, developers need cost lots of time to understand bug reports generated by quality assurance and localize these buggy source files. Traditional methods are heavily depending on the information retrieval technologies which rank the similarity between source files and bug reports in lexical level. Recently, deep learning based models are used to extract semantic information of code with significant improvements for bug localization. However, programming language is a highly structural and logical language, which contains various relations within and cross source files. Thus, we propose KGBugLocator to utilize knowledge graph embeddings to extract these interrelations of code, and a keywords supervised bi-directional attention mechanism regularize model with interactive information between source files and bug reports. With extensive experiments on four different projects, we prove our model can reach the new the-state-of-art(SOTA) for bug localization.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {219–229},
numpages = {11},
keywords = {bug localization, code representation, deep learning, knowledge graph},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3366423.3380107,
author = {Zhang, Hongming and Liu, Xin and Pan, Haojie and Song, Yangqiu and Leung, Cane Wing-Ki},
title = {ASER: A Large-scale Eventuality Knowledge Graph},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380107},
doi = {10.1145/3366423.3380107},
abstract = {Understanding human’s language requires complex world knowledge. However, existing large-scale knowledge graphs mainly focus on knowledge about entities while ignoring knowledge about activities, states, or events, which are used to describe how entities or things act in the real world. To fill this gap, we develop ASER (activities, states, events, and their relations), a large-scale eventuality knowledge graph extracted from more than 11-billion-token unstructured textual data. ASER contains 15 relation types belonging to five categories, 194-million unique eventualities, and 64-million unique edges among them. Both intrinsic and extrinsic evaluations demonstrate the quality and effectiveness of ASER.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {201–211},
numpages = {11},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3366423.3380005,
author = {Lissandrini, Matteo and Mottin, Davide and Palpanas, Themis and Velegrakis, Yannis},
title = {Graph-Query Suggestions for Knowledge Graph Exploration},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380005},
doi = {10.1145/3366423.3380005},
abstract = {We consider the task of exploratory search through graph queries on knowledge graphs. We propose to assist the user by expanding the query with intuitive suggestions to provide a more informative (full) query that can retrieve more detailed and relevant answers. To achieve this result, we propose a model that can bridge graph search paradigms with well-established techniques for information-retrieval. Our approach does not require any additional knowledge from the user and builds on principled language modelling approaches. We empirically show the effectiveness and efficiency of our approach on a large knowledge graph and how our suggestions are able to help build more complete and informative queries.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {2549–2555},
numpages = {7},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3374587.3374603,
author = {Xiaohui, Chen and Yinzhen, Liu and Li, Xu and Lei, Ge and Yiwei, Ma},
title = {The Construction Method of Geographic Knowledge Graph Ontology Model Based on GML},
year = {2020},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374603},
doi = {10.1145/3374587.3374603},
abstract = {Geographic ontology model is the conceptual model of geographic knowledge graph and the logical basis for constructing the pattern layer of geographic knowledge graph. In the classification of geographic ontology research, geographic ontology model is in the category of domain ontology. It is a set of abstract structures to express ontology according to the spatial location, attribute characteristics and relational characteristics of geographic data. This paper discussed the logical components and architecture of geographic ontology, designed the geographic ontology model reference to GML, described the model using OWL language, and constructed the geographic ontology model based on GML. The geographic ontology model comprises three sub-models: element model, geometric model and spatial relation model. Finally, based on Prot\'{e}g\'{e} ontology construction tool, this paper designed the semantic description of geographic entity and realized the construction of geographic ontology system.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {138–143},
numpages = {6},
keywords = {GML, Geographic knowledge graph, geographic ontology, logical composition, ontology model construction},
location = {Normal, IL, USA},
series = {CSAI '19}
}

@inproceedings{10.1145/3184558.3191639,
author = {Leblay, Julien and Chekol, Melisachew Wudage},
title = {Deriving Validity Time in Knowledge Graph},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191639},
doi = {10.1145/3184558.3191639},
abstract = {Knowledge Graphs (KGs) are a popular means to represent knowledge on the Web, typically in the form of node/edge labelled directed graphs. We consider temporal KGs, in which edges are further annotated with time intervals, reflecting when the relationship between entities held in time. In this paper, we focus on the task of predicting time validity for unannotated edges. We introduce the problem as a variation of relational embedding. We adapt existing approaches, and explore the importance example selection and the incorporation of side information in the learning process. We present our experimental evaluation in details.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1771–1776},
numpages = {6},
keywords = {factorization machines, temporal knowledge graph},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3308560.3317708,
author = {Mehta, Aman and Singhal, Aashay and Karlapalem, Kamalakar},
title = {Scalable Knowledge Graph Construction over Text using Deep Learning based Predicate Mapping},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317708},
doi = {10.1145/3308560.3317708},
abstract = {Automatic extraction of information from text and its transformation into a structured format is an important goal in both Semantic Web Research and computational linguistics. Knowledge Graphs (KG) serve as an intuitive way to provide structure to unstructured text. A fact in a KG is expressed in the form of a triple which captures entities and their interrelationships (predicates). Multiple triples extracted from text can be semantically identical but they may have a vocabulary gap which could lead to an explosion in the number of redundant triples. Hence, to get rid of this vocabulary gap, there is a need to map triples to a homogeneous namespace. In this work, we present an end-to-end KG construction system, which identifies and extracts entities and relationships from text and maps them to the homogenous DBpedia namespace. For Predicate Mapping, we propose a Deep Learning architecture to model semantic similarity. This mapping step is computation heavy, owing to the large number of triples in DBpedia. We identify and prune unnecessary comparisons to make this step scalable. Our experiments show that the proposed approach is able to construct a richer KG at a significantly lower computation cost with respect to previous work.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {705–713},
numpages = {9},
keywords = {Deep Learning, Knowledge Graph, Predicate Mapping, Scalability, Sentence Simplification},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3038912.3052558,
author = {Xiong, Chenyan and Power, Russell and Callan, Jamie},
title = {Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052558},
doi = {10.1145/3038912.3052558},
abstract = {This paper introduces Explicit Semantic Ranking (ESR), a new ranking technique that leverages knowledge graph embedding. Analysis of the query log from our academic search engine, SemanticScholar.org, reveals that a major error source is its inability to understand the meaning of research concepts in queries. To addresses this challenge, ESR represents queries and documents in the entity space and ranks them based on their semantic connections from their knowledge graph embedding. Experiments demonstrate ESR's ability in improving Semantic Scholar's online production system, especially on hard queries where word-based ranking fails.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1271–1279},
numpages = {9},
keywords = {academic search, entity-based ranking, knowledge graph},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3234944.3234963,
author = {Lin, Xinshi and Lam, Wai and Lai, Kwun Ping},
title = {Entity Retrieval in the Knowledge Graph with Hierarchical Entity Type and Content},
year = {2018},
isbn = {9781450356565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234944.3234963},
doi = {10.1145/3234944.3234963},
abstract = {We investigate the task of ad-hoc entity retrieval from a knowledge graph with hierarchical entity types and entity descriptions. Our model directly encodes them into a Markov random field based framework via a path aware smoothing method. We conduct experiments on recent benchmark datasets and investigate the incorporation of the Wikipedia type and article information. The results show that our framework achieves improvements over the existing and state-of-the-art models.},
booktitle = {Proceedings of the 2018 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {211–214},
numpages = {4},
keywords = {entity retrieval, structure-aware smoothing, type taxonomy},
location = {Tianjin, China},
series = {ICTIR '18}
}

@inproceedings{10.1145/3340531.3412685,
author = {Li, Feng-Lin and Chen, Hehong and Xu, Guohai and Qiu, Tian and Ji, Feng and Zhang, Ji and Chen, Haiqing},
title = {AliMeKG: Domain Knowledge Graph Construction and Application in E-commerce},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412685},
doi = {10.1145/3340531.3412685},
abstract = {Pre-­sales customer service is of importance to E­-commerce plat­forms as it contributes to optimizing customers? buying process. To better serve users, we propose AliMe KG, a domain knowledge graph in E­-commerce that captures user problems, points of inter­est (POI), item information and relations thereof. It helps to under­ stand user needs, answer pre­-sales questions and generate explana­tion texts. We applied AliMe KG to several online business scenar­ios such as shopping guide, question answering over properties and selling point generation, and gained positive and beneficial business results. In the paper, we systematically introduce how we construct domain knowledge graph from free text, and demonstrate its busi­ness value with several applications. Our experience shows that min­ ing structured knowledge from free text in vertical domain is prac­ticable, and can be of substantial value in industrial settings.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2581–2588},
numpages = {8},
keywords = {domain knowledge graph, e-commerce, pre-sales customer service},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3292500.3330942,
author = {Jiang, Tianwen and Zhao, Tong and Qin, Bing and Liu, Ting and Chawla, Nitesh V. and Jiang, Meng},
title = {The Role of "Condition": A Novel Scientific Knowledge Graph Representation and Construction Model},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330942},
doi = {10.1145/3292500.3330942},
abstract = {Conditions play an essential role in scientific observations, hypotheses, and statements. Unfortunately, existing scientific knowledge graphs (SciKGs) represent factual knowledge as a flat relational network of concepts, as same as the KGs in general domain, without considering the conditions of the facts being valid, which loses important contexts for inference and exploration. In this work, we propose a novel representation of SciKG, which has three layers. The first layer has concept nodes, attribute nodes, as well as the attaching links from attribute to concept. The second layer represents both fact tuples and condition tuples. Each tuple is a node of the relation name, connecting to the subject and object that are concept or attribute nodes in the first layer. The third layer has nodes of statement sentences traceable to the original paper and authors. Each statement node connects to a set of fact tuples and/or condition tuples in the second layer. We design a semi-supervised Multi-Input Multi-Output sequence labeling model that learns complex dependencies between the sequence tags from multiple signals and generates output sequences for fact and condition tuples. It has a self-training module of multiple strategies to leverage the massive scientific data for better performance when manual annotation is limited. Experiments on a data set of 141M sentences show that our model outperforms existing methods and the SciKGs we constructed provide a good understanding of the scientific statements.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1634–1642},
numpages = {9},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3338533.3366552,
author = {Wei, Jiwei and Yang, Yang and Li, Jingjing and Zhu, Lei and Zuo, Lin and Shen, Heng Tao},
title = {Residual Graph Convolutional Networks for Zero-Shot Learning},
year = {2020},
isbn = {9781450368414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338533.3366552},
doi = {10.1145/3338533.3366552},
abstract = {Most existing Zero-Shot Learning (ZSL) approaches adopt the semantic space as a bridge to classify unseen categories. However, it is difficult to transfer knowledge from seen categories to unseen categories through semantic space, since the correlations among categories are uncertain and ambiguous in the semantic space. In this paper, we formulated zero-shot learning as a classifier weight regression problem. Specifically, we propose a novel Residual Graph Convolution Network (ResGCN) which takes word embeddings and knowledge graph as inputs and outputs a visual classifier for each category. ResGCN can effectively alleviate the problem of over-smoothing and over-fitting. During the test, an unseen image can be classified by ranking the inner product of its visual feature and predictive visual classifiers. Moreover, we provide a new method to build a better knowledge graph. Our approach not only further enhances the correlations among categories, but also makes it easy to add new categories to the knowledge graph. Experiments conducted on the large-scale ImageNet 2011 21K dataset demonstrate that our method significantly outperforms existing state-of-the-art approaches.},
booktitle = {Proceedings of the 1st ACM International Conference on Multimedia in Asia},
articleno = {9},
numpages = {6},
keywords = {Knowledge Graph, ResGCN, Zero-Shot Learning},
location = {Beijing, China},
series = {MMAsia '19}
}

@article{10.1145/3361738,
author = {Ai, Qingyao and Zhang, Yongfeng and Bi, Keping and Croft, W. Bruce},
title = {Explainable Product Search with a Dynamic Relation Embedding Model},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3361738},
doi = {10.1145/3361738},
abstract = {Product search is one of the most popular methods for customers to discover products online. Most existing studies on product search focus on developing effective retrieval models that rank items by their likelihood to be purchased. However, they ignore the problem that there is a gap between how systems and customers perceive the relevance of items. Without explanations, users may not understand why product search engines retrieve certain items for them, which consequentially leads to imperfect user experience and suboptimal system performance in practice. In this work, we tackle this problem by constructing explainable retrieval models for product search. Specifically, we propose to model the “search and purchase” behavior as a dynamic relation between users and items, and create a dynamic knowledge graph based on both the multi-relational product data and the context of the search session. Ranking is conducted based on the relationship between users and items in the latent space, and explanations are generated with logic inferences and entity soft matching on the knowledge graph. Empirical experiments show that our model, which we refer to as the Dynamic Relation Embedding Model (DREM), significantly outperforms the state-of-the-art baselines and has the ability to produce reasonable explanations for search results.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {4},
numpages = {29},
keywords = {Product search, explainable model, knowledge graph, relation embedding}
}

@inproceedings{10.1145/3109859.3109889,
author = {Palumbo, Enrico and Rizzo, Giuseppe and Troncy, Rapha\"{e}l},
title = {entity2rec: Learning User-Item Relatedness from Knowledge Graphs for Top-N Item Recommendation},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109889},
doi = {10.1145/3109859.3109889},
abstract = {Knowledge Graphs have proven to be extremely valuable to recommender systems, as they enable hybrid graph-based recommendation models encompassing both collaborative and content information. Leveraging this wealth of heterogeneous information for top-N item recommendation is a challenging task, as it requires the ability of effectively encoding a diversity of semantic relations and connectivity patterns. In this work, we propose entity2rec, a novel approach to learning user-item relatedness from knowledge graphs for top-N item recommendation. We start from a knowledge graph modeling user-item and item-item relations and we learn property-specific vector representations of users and items applying neural language models on the network. These representations are used to create property-specific user-item relatedness features, which are in turn fed into learning to rank algorithms to learn a global relatedness model that optimizes top-N item recommendations. We evaluate the proposed approach in terms of ranking quality on the MovieLens 1M dataset, outperforming a number of state-of-the-art recommender systems, and we assess the importance of property-specific relatedness scores on the overall ranking quality.},
booktitle = {Proceedings of the Eleventh ACM Conference on Recommender Systems},
pages = {32–36},
numpages = {5},
keywords = {hybrid recommender system, knowledge graph, knowledge graph embeddings, learning to rank, linked open data, neural language models, node2vec, word2vec},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3397271.3401067,
author = {Huang, Longtao and Yuan, Bo and Zhang, Rong and Lu, Quan},
title = {Towards Linking Camouflaged Descriptions to Implicit Products in E-commerce},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401067},
doi = {10.1145/3397271.3401067},
abstract = {As the emergence of E-commerce services, billions of products are sold online everyday. How to detect illegal products from the large-scale online products has become an important and practical research problem. In order to evade detection, malicious sellers usually utilize camouflaged text to describe their illegal products implicitly. Thus brings great challenges to the current detection systems since newly camouflaged text can hardly be learned from historical data and the distribution of illegal and normal products is extremely unbalanced. Rather than solving this problem as a classification task in most previous efforts, we reformulate the problem from a perspective of implicit entity linking, which targets at linking a camouflaged description to a known product. In this paper, we introduce three types of context that could help to infer implicit entity from camouflaged descriptions and propose an end-to-end contextual representation model to capture the effect of different context. Furthermore, we involve a symmetric metric to model the matching score of the input title to the product by learning the mutual effect among the context. The experimental results on the datasets collected from a real-world E-commerce site demonstrate the advantage of the proposed model against the state-of-the-art methods.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {901–910},
numpages = {10},
keywords = {implicit entity linking, knowledge graph, neural networks},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3343413.3378011,
author = {Torbati, Ghazaleh H. and Yates, Andrew and Weikum, Gerhard},
title = {Personalized Entity Search by Sparse and Scrutable User Profiles},
year = {2020},
isbn = {9781450368926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343413.3378011},
doi = {10.1145/3343413.3378011},
abstract = {Prior work on personalizing web search results has focused on considering query-and-click logs to capture users' individual interests. For product search, extensive user histories about purchases and ratings have been exploited. However, for general entity search, such as for books on specific topics or travel destinations with certain features, personalization is largely underexplored. In this paper, we address personalization of book search, as an exemplary case of entity search, by exploiting sparse user profiles obtained through online questionnaires. We devise and compare a variety of re-ranking methods based on language models or neural learning. Our experiments show that even very sparse information about individuals can enhance the effectiveness of the search results.},
booktitle = {Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
pages = {427–431},
numpages = {5},
keywords = {knowledge graph, personalized entity search, sparse user profile},
location = {Vancouver BC, Canada},
series = {CHIIR '20}
}

@inproceedings{10.1145/3433996.3433999,
author = {Li, Xiaolian and Zhang, Bo},
title = {Discussion on Natural Language Processing and AI Poetry},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3433999},
doi = {10.1145/3433996.3433999},
abstract = {Letting machines express and create like humans is an important vision of artificial intelligence, and one of the core technical fields to realize this vision is intelligent writing. In recent years, intelligent writing has not only developed rapidly in technology, but also has become more and more important in its application. This article starts with common application forms and examples of smart writing, combined with practical experience, introduces the core technology of smart writing, and discusses the way of human-computer collaboration and the future development direction of smart writing.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {10–13},
numpages = {4},
keywords = {AI poetry, Artificial intelligence, Language model, Natural language processing, Word sequence},
location = {Taiyuan, China},
series = {CAIH2020}
}

@inproceedings{10.1145/3331184.3331427,
author = {Firsov, Anton and Bugay, Vladimir and Karpenko, Anton},
title = {USEing Transfer Learning in Retrieval of Statistical Data},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331427},
doi = {10.1145/3331184.3331427},
abstract = {DSSM-like models showed good results in retrieval of short documents that semantically match the query. However, these models require large collections of click-through data that are not available in some domains. On the other hand, the recent advances in NLP demonstrated the possibility to fine-tune language models and models trained on one set of tasks to achieve a state of the art results on a multitude of other tasks or to get competitive results using much smaller training sets. Following this trend, we combined DSSM-like architecture with USE (Universal Sentence Encoder) and BERT (Bidirectional Encoder Representations from Transformers) models in order to be able to fine-tune them on a small amount of click-through data and use them for information retrieval. This approach allowed us to significantly improve our search engine for statistical data.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1391–1392},
numpages = {2},
keywords = {information retrieval, language model, transfer learning},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3341162.3349310,
author = {Chen, Fanglin and Hong, Jason I.},
title = {Personal bits: mining interaction traces for personalized task intelligence},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3349310},
doi = {10.1145/3341162.3349310},
abstract = {As we work, play, shop, and communicate in digital interfaces, we continuously generate traces of information. To turn such noisy sources of personal data into actual insight, my research introduces Personal Bits, a service that enables personalized task support in various kinds of information tasks such as instant message handling, information retrieval, and text entry. Personal Bits mines a user's interaction traces with web apps and native mobile apps and extracts task-centric entities. I present three example apps for Personal Bits: Deja Wu, MessageOnTap, and ContextBoard, to address inefficiencies presented in these information tasks. Personal Bits acts as the central nexus for intelligence between apps and interaction traces, making it easy for apps to acquire personally relevant task entities in fine granularity.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {358–362},
numpages = {5},
keywords = {information retrieval, intelligent user interface, interaction traces, language model, messaging, personal data, productivity, task intelligence},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/3209542.3209548,
author = {Bhatia, Sumit and Vishwakarma, Harit},
title = {Know Thy Neighbors, and More! Studying the Role of Context in Entity Recommendation},
year = {2018},
isbn = {9781450354271},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209542.3209548},
doi = {10.1145/3209542.3209548},
abstract = {Knowledge Graphs capture the semantic relations between real-world entities and can thus, allow end-users to explore different aspects of an entity of interest by traversing through the edges in the graph. Most of the state-of-the-art methods in entity recommendation are limited in the sense that they allow users to search only in the immediate neighborhood of the entity of interest. This is majorly due to efficiency reasons as the search space increases exponentially as we move further away from the entity of interest in the graph. Often, users perform the search task in the context of an information need and we investigate the role this context can play in overcoming the scalability issue and improving knowledge graph exploration. Intuitively, only a small subset of entities in the graph are relevant to a users' interest. We show how can we efficiently select this sub-set by utilizing contextual clues and using graph-theoretic measures to further re-rank this set to offer highly relevant graph exploration capabilities to end-users.},
booktitle = {Proceedings of the 29th on Hypertext and Social Media},
pages = {87–95},
numpages = {9},
keywords = {contextual entity recommendation, contextual exploration, entity recommendation, entity retrieval, entity search, information discovery, knowledge graph exploration},
location = {Baltimore, MD, USA},
series = {HT '18}
}

@inproceedings{10.1145/3397271.3401255,
author = {Lu, Junyu and Ren, Xiancong and Ren, Yazhou and Liu, Ao and Xu, Zenglin},
title = {Improving Contextual Language Models for Response Retrieval in Multi-Turn Conversation},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401255},
doi = {10.1145/3397271.3401255},
abstract = {As an important branch of current dialogue systems, retrieval-based chatbots leverage information retrieval to select proper predefined responses. Various promising architectures have been designed for boosting response retrieval, however, few researches exploit the effectiveness of the pre-trained contextual language models. In this paper, we propose two approaches to adapt contextual language models in dialogue response selection task. In detail, the Speaker Segmentation approach is designed to discriminate different speakers to fully utilize speaker characteristics. Besides, we propose the Dialogue Augmentation approach, i.e., cutting off real conversations at different time points, to enlarge the training corpora. Compared with previous works which use utterance-level representations, our augmented contextual language models are able to obtain top-hole contextual dialogue representations for deeper semantic understanding. Evaluation on three large-scale datasets has demonstrated that our proposed approaches yield better performance than existing models.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1805–1808},
numpages = {4},
keywords = {augmentation, pre-trained language model, response retrieval},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.5555/3455716.3455786,
author = {Kazemi, Seyed Mehran and Goel, Rishab and Jain, Kshitij and Kobyzev, Ivan and Sethi, Akshay and Forsyth, Peter and Poupart, Pascal},
title = {Representation learning for dynamic graphs: a survey},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {70},
numpages = {73},
keywords = {graph representation learning, dynamic graphs, knowledge graph embedding, heterogeneous information networks}
}

@inproceedings{10.1145/3318464.3386145,
author = {Liu, Bang and Guo, Weidong and Niu, Di and Luo, Jinwen and Wang, Chaoyue and Wen, Zhen and Xu, Yu},
title = {GIANT: Scalable Creation of a Web-scale Ontology},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386145},
doi = {10.1145/3318464.3386145},
abstract = {Understanding what online users may pay attention to on the web is key to content recommendation and search services. These services will benefit from a highly structured and web-scale ontology of entities, concepts, events, topics and categories. While existing knowledge bases and taxonomies embody a large volume of entities and categories, we argue that they fail to discover properly grained concepts, events and topics in the language style of online users. Neither is a logically structured ontology maintained among these notions. In this paper, we present GIANT, a mechanism to construct a user-centered, web-scale, structured ontology, containing a large number of natural language phrases conforming to user attentions at various granularities, mined from the vast volume of web documents and search click logs. Various types of edges are also constructed to maintain a hierarchy in the ontology. We present our detailed techniques used in GIANT, and evaluate the proposed models and methods as compared to a variety of baselines, as well as deploy the resulted Attention Ontology in real-world applications, involving over a billion users, to observe its effect on content recommendation. The online performance of the ontology built by GIANT proves that it can significantly improve the click-through rate in news feeds recommendation.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {393–409},
numpages = {17},
keywords = {concept mining, document understanding, event mining, ontology creation, user interest modeling},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3106426.3109052,
author = {Albukhitan, Saeed and Helmy, Tarek and Alnazer, Ahmed},
title = {Arabic ontology learning using deep learning},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109052},
doi = {10.1145/3106426.3109052},
abstract = {Ontology, the backbone of Semantic Web, is defined as the formal specification of conceptual hierarchy with relationships between concepts. Ontology Learning (OL) is a process to create an ontology from text automatically or semi-automatically. OL is an important topic in the Semantic Web field in the last two decades but it is still not mature in Arabic not like Latin languages. Currently, there is a limited support for using knowledge from Arabic literature automatically in semantically-enabled systems. Deep Learning (DL), an artificial neural networks learning based application, has proved a good improvement in multiple areas including text mining. By using DL, it is possible to have word embedding as distributed word representations from textual data. The application of DL to aid Arabic ontology development remains largely unexplored. This paper investigates the performance of implementing DL with Arabic ontology learning tasks using major models such as Continuous Bag of Words (CBOW) and Skip-gram. Initial performance results are promising as an effective application of Arabic ontology learning.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1138–1142},
numpages = {5},
keywords = {arabic ontology, deep learning, ontology learning},
location = {Leipzig, Germany},
series = {WI '17}
}

@article{10.1145/3399630,
author = {Tao, Jie and Zhou, Lina},
title = {A Weakly Supervised WordNet-Guided Deep Learning Approach to Extracting Aspect Terms from Online Reviews},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3399630},
doi = {10.1145/3399630},
abstract = {The unstructured nature of online reviews makes it inefficient and inconvenient for prospective consumers to research and use in support of purchase decision making. The aspects of products provide a fine-grained meaningful perspective for understanding and organizing review texts. Traditional aspect term extraction approaches rely on discrete language models that treat words in isolation. Despite that continuous-space language models have demonstrated promise in addressing a wide range of problems, their application in aspect term extraction faces significant challenges. For instance, existing continuous-space language models typically require large collections of labeled data, which remain difficult to obtain in many domains. More importantly, previous methods are largely data driven but overlook the role of human knowledge in guiding model development. To address these limitations, this study designs and develops weakly supervised WordNet-guided deep learning to aspect term extraction. The approach draws on deep-level semantic information from WordNet to guide not only the selection representative seed terms but also the pruning of aspect candidate terms. The weak supervision is provided by a very small set of labeled data. We conduct a comprehensive evaluation of the proposed method using both direct and indirect methods. The evaluation results with Yelp restaurant reviews demonstrate that our proposed method consistently outperforms all baseline methods including discrete models and the state-of-the-art continuous-space language models for aspect term extraction across both direct and indirect evaluations. The research findings have broad research, technical, and practical implications for various stakeholders of online reviews.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {13},
numpages = {22},
keywords = {Aspect term extraction, continuous-space language model, deep learning, semantic knowledge, text analytics}
}

@inproceedings{10.1145/3308558.3313511,
author = {Vedula, Nikhita and Maneriker, Pranav and Parthasarathy, Srinivasan},
title = {BOLT-K: Bootstrapping Ontology Learning via Transfer of Knowledge},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313511},
doi = {10.1145/3308558.3313511},
abstract = {Dynamically extracting and representing continually evolving knowledge entities is an essential scaffold for grounded intelligence and decision making. Creating knowledge schemas for newly emerging, unfamiliar, domain-specific ideas or events poses the following challenges: (i) detecting relevant, often previously unknown concepts associated with the new domain; and (ii) learning ontological, semantically accurate relationships among the new concepts, despite having severely limited annotated data. To this end, we propose a novel LSTM-based framework with attentive pooling, BOLT-K, to learn an ontology for a target subject or domain. We bootstrap our ontology learning approach by adapting and transferring knowledge from an existing, functionally related source domain. We also augment the inadequate labeled data available for the target domain with various strategies to minimize human expertise during model development and training. BOLT-K first employs semantic and graphical features to recognize the entity or concept pairs likely to be related to each other, and filters out spurious concept combinations. It is then jointly trained on knowledge from the target and source domains to learn relationships among the target concepts. The target concepts and their corresponding relationships are subsequently used to construct an ontology. We extensively evaluate our framework on several, real-world bio-medical and commercial product domain ontologies. We obtain significant improvements of 5-25% F1-score points over state-of-the-art baselines. We also examine the potential of BOLT-K in detecting the presence of novel kinds of relationships that were unseen during training.},
booktitle = {The World Wide Web Conference},
pages = {1897–1908},
numpages = {12},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3209219.3213598,
author = {Moon, DeKita G.},
title = {Modeling Learners' Interest with a Domain-Independent Ontology-Based Framework},
year = {2018},
isbn = {9781450355896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209219.3213598},
doi = {10.1145/3209219.3213598},
abstract = {Ontologies are recognized as a promising approach to support the reusability and interoperability of learners' preferences; which is useful for the optimization and flexibility of data and resources. However, little to no research on adaptive learning systems or semantic technologies explore personalized experiences based on the various out-of-school experiences and activities of the users. This research investigates the design, development, and evaluation of an ontology-based framework for students' interests in a math word problem generator that may be applied to various other learning systems and possibly other domains. The cohesiveness of the problems in addition to the usability, usefulness, and the short-term effectiveness of the derived technology will be investigated by comparing the generated questions to numerical and traditional Algebra I problems. We aim to better understand students' interests to identify the role that their interests can play in semantic technologies, further supporting the recent advances in ontology-based educational technologies and personalized math word problem generators.},
booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
pages = {345–348},
numpages = {4},
keywords = {domain ontologies, educational technologies, human-centered computing, knowledge graphs, semantic technologies},
location = {Singapore, Singapore},
series = {UMAP '18}
}

@inproceedings{10.1145/3366423.3380156,
author = {Zhao, Xiangyu and Wang, Longbiao and He, Ruifang and Yang, Ting and Chang, Jinxin and Wang, Ruifang},
title = {Multiple Knowledge Syncretic Transformer for Natural Dialogue Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380156},
doi = {10.1145/3366423.3380156},
abstract = {Knowledge is essential for intelligent conversation systems to generate informative responses. This knowledge comprises a wide range of diverse modalities such as knowledge graphs (KGs), grounding documents and conversation topics. However, limited abilities in understanding language and utilizing different types of knowledge still challenge existing approaches. Some researchers try to enhance models’ language comprehension ability by employing the pre-trained language models, but they neglect the importance of external knowledge in specific tasks. In this paper, we propose a novel universal transformer-based architecture for dialogue system, the Multiple Knowledge Syncretic Transformer (MKST), which fuses multi-knowledge in open-domain conversation. Firstly, the model is pre-trained on a large-scale corpus to learn commonsense knowledge. Then during fine-tuning, we divide the type of knowledge into two specific categories that are handled in different ways by our model. While the encoder is responsible for encoding dialogue contexts with multifarious knowledge together, the decoder with a knowledge-aware mechanism attentively reads the fusion of multi-knowledge to promote better generation. This is the first attempt that fuses multi-knowledge in one conversation model. The experimental results have been demonstrated that our model achieves significant improvement on knowledge-driven dialogue generation tasks than state-of-the-art baselines. Meanwhile, our new benchmark could facilitate the further study in this research area.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {752–762},
numpages = {11},
keywords = {Dialogue Generation, Multiple Knowledge, Pre-trained Model, Syncretic Transformer},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3308558.3313656,
author = {Bhowmik, Rajarshi and de Melo, Gerard},
title = {Be Concise and Precise: Synthesizing Open-Domain Entity Descriptions from Facts},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313656},
doi = {10.1145/3308558.3313656},
abstract = {Despite being vast repositories of factual information, cross-domain knowledge graphs, such as Wikidata and the Google Knowledge Graph, only sparsely provide short synoptic descriptions for entities. Such descriptions that briefly identify the most discernible features of an entity provide readers with a near-instantaneous understanding of what kind of entity they are being presented. They can also aid in tasks such as named entity disambiguation, ontological type determination, and answering entity queries. Given the rapidly increasing numbers of entities in knowledge graphs, a fully automated synthesis of succinct textual descriptions from underlying factual information is essential. To this end, we propose a novel fact-to-sequence encoder-decoder model with a suitable copy mechanism to generate concise and precise textual descriptions of entities. In an in-depth evaluation, we demonstrate that our method significantly outperforms state-of-the-art alternatives.},
booktitle = {The World Wide Web Conference},
pages = {116–126},
numpages = {11},
keywords = {knowledge graphs, open-domain factual knowledge, synoptic description generation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3404555.3404635,
author = {Gao, Shengxin and Du, Jinlian and Zhang, Xiao},
title = {Research on Relation Extraction Method of Chinese Electronic Medical Records Based on BERT},
year = {2020},
isbn = {9781450377089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404555.3404635},
doi = {10.1145/3404555.3404635},
abstract = {Relation extraction is a necessary step in obtaining information from electronic medical records. The deep learning methods for relation extraction are primarily based on word2vec and convolutional or recurrent neural network. However, word vectors generated by word2vec are static and cannot well reflect the different meanings of polysemy in different contexts and the feature extraction ability of RNN (Recurrent Neural Network) is not good enough. At the same time, the BERT (Bidirectional Encoder Representations from Transformers) pre-trained language model has achieved excellent results in many natural language processing tasks. In this paper, we propose a medical relation extraction model based on BERT. We combine the information of the whole sentence obtained from the pre-train language model with the corresponding information of two medical entities to complete relation extraction task. The experimental data were obtained from the Chinese electronic medical records provided by a hospital in Beijing. Experimental results on electronic medical records show that our model's accuracy, precision, recall, and F1-score reach 67.37%, 69.54%, 67.38%, 68.44%, which are higher than other three methods. Because named entity recognition task is the premise of relation extraction, we will combine the model with named entity recognition in the future work.},
booktitle = {Proceedings of the 2020 6th International Conference on Computing and Artificial Intelligence},
pages = {487–490},
numpages = {4},
keywords = {BERT, Chinese electronic medical records, Relationship extraction, deep learning},
location = {Tianjin, China},
series = {ICCAI '20}
}

@inproceedings{10.1145/3397271.3401265,
author = {Manotumruksa, Jarana and Dalton, Jeff and Meij, Edgar and Yilmaz, Emine},
title = {CrossBERT: A Triplet Neural Architecture for Ranking Entity Properties},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401265},
doi = {10.1145/3397271.3401265},
abstract = {Task-based Virtual Personal Assistants (VPAs) such as the Google Assistant, Alexa, and Siri are increasingly being adopted for a wide variety of tasks. These tasks are grounded in real-world entities and actions (e.g., book a hotel, organise a conference, or requesting funds). In this work we tackle the task of automatically constructing actionable knowledge graphs in response to a user query in order to support a wider variety of increasingly complex assistant tasks. We frame this as an entity property ranking task given a user query with annotated properties. We propose a new method for property ranking, CrossBERT. CrossBERT builds on the Bidirectional Encoder Representations from Transformers (BERT) and creates a new triplet network structure on cross query-property pairs that is used to rank properties. We also study the impact of using external evidence for query entities from textual entity descriptions. We perform experiments on two standard benchmark collections, the NTCIR-13 Actionable Knowledge Graph Generation (AKGG) task and Entity Property Identification (EPI) task. The results demonstrate that CrossBERT significantly outperforms the best performing runs from AKGG and EPI, as well as previous state-of-the-art BERT-based models. In particular, CrossBERT significantly improves Recall and NDCG by approximately 2-12% over the BERT models across the two used datasets.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2049–2052},
numpages = {4},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3397271.3401089,
author = {Lu, Shuqi and Dou, Zhicheng and Xiong, Chenyan and Wang, Xiaojie and Wen, Ji-Rong},
title = {Knowledge Enhanced Personalized Search},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401089},
doi = {10.1145/3397271.3401089},
abstract = {This paper presents a knowledge graph enhanced personalized search model, KEPS. For each user and her queries, KEPS first con- ducts personalized entity linking on the queries and forms better intent representations; then it builds a knowledge enhanced profile for the user, using memory networks to store the predicted search intents and linked entities in her search history. The knowledge enhanced user profile and intent representation are then utilized by KEPS for better, knowledge enhanced, personalized search. Furthermore, after providing personalized search for each query, KEPS leverages user's feedback (click on documents) to post-adjust the entity linking on previous queries. This fixes previous linking errors and improves ranking quality for future queries. Experiments on the public AOL search log demonstrate the advantage of knowledge in personalized search: personalized entity linking better reflects user's search intent, the memory networks better maintain user's subtle preferences, and the post linking adjustment fixes some linking errors with the received feedback signals. The three components together lead to a significantly better ranking accuracy of KEPS.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {709–718},
numpages = {10},
keywords = {entity-oriented search, knowledge base, personalized search},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3144457.3144458,
author = {Wu, Yao and Huang, Tao and Zhao, Dan and Chen, Hong and Li, Cuiping},
title = {PIN: Potential Wise Crowd From Million Grassroots},
year = {2017},
isbn = {9781450353687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144457.3144458},
doi = {10.1145/3144457.3144458},
abstract = {Crowdsourcing proves a viable approach to solve certain large-scale problems by posting tasks distributively to humans and harnessing their knowledge to get results effectively and efficiently. Unfortunately, crowdsourcing suffers from lack of available participants with domain knowledge or skills. In this paper, we propose potential wise crowd (i.e., a crowd with similarity and diversity in domain knowledge) find from million grassroots in social networks. We design and implement a distant-supervision framework to find potential crowdsourcers from existing social networks. A knowledge graph is used to assess the domain knowledge in terms of similarity and diversity. The wise crowd formation is a NP-hard problem and we propose greedy algorithms to approach it. Experimental results show the performance of our framework and algorithms in aspects of effectiveness and efficiency.},
booktitle = {Proceedings of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services},
pages = {186–195},
numpages = {10},
keywords = {crowd formation, distant supervision, mobile crowdsourcing, mobile recruitment framework},
location = {Melbourne, VIC, Australia},
series = {MobiQuitous 2017}
}

@inproceedings{10.1145/3077136.3080768,
author = {Xiong, Chenyan and Callan, Jamie and Liu, Tie-Yan},
title = {Word-Entity Duet Representations for Document Ranking},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080768},
doi = {10.1145/3077136.3080768},
abstract = {This paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval. In this work, the query and documents are modeled by word-based representations and entity-based representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed. With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet. Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the attention mechanism successfully steers the model away from noisy entities, and together they significantly outperform both word-based and entity-based learning to rank systems.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {763–772},
numpages = {10},
keywords = {document ranking, entity-based search, text representation},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3178876.3186029,
author = {Cannaviccio, Matteo and Barbosa, Denilson and Merialdo, Paolo},
title = {Towards Annotating Relational Data on the Web with Language Models},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186029},
doi = {10.1145/3178876.3186029},
abstract = {Tables and structured lists on Web pages are a potential source of valuable information, and several methods have been proposed to annotate them with semantics that can be leveraged for search, question answering and information extraction. This paper is concerned with the specific problem of finding and ranking relations from a given Knowledge Graph (KG) that hold over pairs of entities juxtaposed in a table or structured list. The state-of-the-art for this task is to attempt to link the entities mentioned in the table cells to objects in the KG and rank the relations that hold for those linked objects. As a result, these methods are hampered by the incompleteness and uneven coverage in even the best knowledge graphs available today. The alternative described here does not require entity linking, relying instead on ranking relations using generative language models derived from Web-scale corpora. As such, it can produce quality results even when the entities in the table are missing in the KG. The experimental validation, designed to expose the challenges posed by KG incompleteness, shows that our approach is robust and effective in practice.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {1307–1316},
numpages = {10},
keywords = {generative language models, knowledge graphs, web table understanding},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3267851.3267894,
author = {Paranjape, Bhargavi and Ge, Yubin and Bai, Zhen and Hammer, Jessica and Cassell, Justine},
title = {Towards Automatic Generation of Peer-Targeted Science Talk in Curiosity-Evoking Virtual Agent},
year = {2018},
isbn = {9781450360135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267851.3267894},
doi = {10.1145/3267851.3267894},
abstract = {Curiosity is a critical skill that spurs learning, but is often found to decline with age and schooling. Recent research has shown that peer interaction may serve a special role in inducing curiosity through increased uncertainty and conceptual conflicts, since peers have similar authority in knowledge. For a virtual agent to stimulate curiosity, it should be able to generate curiosity-eliciting verbal behaviors such as hypothesis verbalization and argumentation, in the manner that simulates peer-like cognitive and behavioral abilities. In this paper, we design and implement a virtual peer that can carry out key curiosity-eliciting science talk during a dialog-based multi-party board game. We propose a child-centered and data-driven approach to simulate the latent reasoning process of young children and age-appropriate language during open-ended game play. In particular, we use a combination of child knowledge-graph construction and child-child interaction driven modeling to generate game appropriate behaviors that are compatible with 9-14 year old children. Encouraging human evaluation of the generated behaviors and generalizability of the generation framework to other tasks opens up new directions in incorporating open-endedness and science talk in virtual agents that will make them truly play a peer role in learning.},
booktitle = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},
pages = {71–78},
numpages = {8},
keywords = {Behavior Generation, Board game play, Cognitive Architectures, Curiosity, Knowledge Base, Open-Ended Play, Semantic Memory, Virtual Peer},
location = {Sydney, NSW, Australia},
series = {IVA '18}
}

@inproceedings{10.1145/3292500.3330725,
author = {Chen, Qibin and Lin, Junyang and Zhang, Yichang and Yang, Hongxia and Zhou, Jingren and Tang, Jie},
title = {Towards Knowledge-Based Personalized Product Description Generation in E-commerce},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330725},
doi = {10.1145/3292500.3330725},
abstract = {Quality product descriptions are critical for providing competitive customer experience in an E-commerce platform. An accurate and attractive description not only helps customers make an informed decision but also improves the likelihood of purchase. However, crafting a successful product description is tedious and highly time-consuming. Due to its importance, automating the product description generation has attracted considerable interest from both research and industrial communities. Existing methods mainly use templates or statistical methods, and their performance could be rather limited. In this paper, we explore a new way to generate personalized product descriptions by combining the power of neural networks and knowledge base. Specifically, we propose a KnOwledge Based pErsonalized (or KOBE) product description generation model in the context of E-commerce.In KOBE, we extend the encoder-decoder framework, the Transformer, to a sequence modeling formulation using self-attention. In order to make the description both informative and personalized, KOBE considers a variety of important factors during text generation, including product aspects, user categories, and knowledge base. Experiments on real-world datasets demonstrate that the proposed method outperforms the baseline on various metrics. KOBE can achieve an improvement of 9.7% over state-of-the-arts in terms of BLEU. We also present several case studies as the anecdotal evidence to further prove the effectiveness of the proposed approach. The framework has been deployed in Taobao, the largest online E-commerce platform in China.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3040–3050},
numpages = {11},
keywords = {controllable text generation, knowledge base, personalization, product description generation},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3214708.3214712,
author = {Quamar, Abdul and Ozcan, Fatma and Xirogiannopoulos, Konstantinos},
title = {Discovery and Creation of Rich Entities for Knowledge Bases},
year = {2018},
isbn = {9781450358477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3214708.3214712},
doi = {10.1145/3214708.3214712},
abstract = {Businesses and professional organizations from a variety of different domains such as finance, weather, healthcare, social networks, etc., produce massive amounts of unstructured, semi-structured and structured data. Knowledge bases, enable querying and analysis of integrated content derived from such data available as open, third party and propriety data sets. Many knowledge bases today, provide an entity-centric view over the integrated content by using domain-specific ontologies. These entity-centric views enable querying individual real-world entities, as well as exploring exact information (such as address or net revenue of a company) through explicit querying using languages such as SQL or SPARQL. Although very useful for many business and commercial applications, this may not be sufficient for the exploration of relevant and context specific information associated with real-world entities stored in these knowledge bases. Users often need to resort to a manual and tedious process of exploration using ad-hoc queries to gather the required information.To enhance user experience and ameliorate the problem of relevant data exploration, we propose the concept of Rich Entities. These rich entities comprise of all the relevant and context specific information grouped together around real-world entities and served as efficient and meaningful responses to user queries against these entities in a knowledge base. These rich entities are created by grouping together information not only from a single entity represented as an ontology concept, but also related concepts and properties as specified by the domain ontology. In this paper we propose several novel techniques and algorithms to automatically detect, learn, and create domain-specific rich entities. We use inputs from query patterns in existing query workloads against knowledge bases, and leverage the structure and relationships between entities defined in the domain ontology. Our techniques are very effective and can be applied to a wide variety of application domains thus adding great value to data exploration and information extraction from entity-centric real-world knowledge bases.},
booktitle = {Proceedings of the 5th International Workshop on Exploratory Search in Databases and the Web},
articleno = {4},
numpages = {6},
keywords = {Knowledge Bases, Ontology, Rich Entities},
location = {Houston, TX, USA},
series = {ExploreDB 2018}
}

@inproceedings{10.1145/3357384.3357889,
author = {Zheng, Wen and Zhou, Ke},
title = {Enhancing Conversational Dialogue Models with Grounded Knowledge},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357889},
doi = {10.1145/3357384.3357889},
abstract = {Leveraging external knowledge to enhance conversational models has become a popular research area in recent years. Compared to vanilla generative models, the knowledge-grounded models may produce more informative and engaging responses. Although various approaches have been proposed in the past, how to effectively incorporate knowledge remains an open research question. It is unclear how much external knowledge should be retrieved and what is the optimal way to enhance the conversational model, trading off between relevant information and noise. Therefore, in this paper, we aim to bridge the gap by first extensively evaluating various types of state-of-the-art knowledge-grounded conversational models, including recurrent neural network based, memory networks based, and Transformer based models. We demonstrate empirically that those conversational models can only be enhanced with the right amount of external knowledge. To effectively leverage information originated from external knowledge, we propose a novel Transformer with Expanded Decoder (Transformer-ED or TED for short), which can automatically tune the weights for different sources of evidence when generating responses. Our experiments show that our proposed model outperforms state-of-the-art models in terms of both quality and diversity.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {709–718},
numpages = {10},
keywords = {copy-mechanism, generative model, knowledge-grounded, memory network, multi-task learning, sequence-to-sequence, ted, transformer, transformer-ed},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1145/3345517,
author = {Abdulhameed, Tiba Zaki and Zitouni, Imed and Abdel-Qader, Ikhlas},
title = {Wasf-Vec: Topology-based Word Embedding for Modern Standard Arabic and Iraqi Dialect Ontology},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3345517},
doi = {10.1145/3345517},
abstract = {Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word’s topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word’s nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {22},
numpages = {27},
keywords = {2D visualizing, Arabic language, Topology, Word embedding, class-based language modeling, dialect, morphology, orthographic, phonology, words classification, words features, words ontology}
}

@inproceedings{10.1145/3340531.3412777,
author = {Sakor, Ahmad and Singh, Kuldeep and Patel, Anery and Vidal, Maria-Esther},
title = {Falcon 2.0: An Entity and Relation Linking Tool over Wikidata},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412777},
doi = {10.1145/3340531.3412777},
abstract = {The Natural Language Processing (NLP) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in Knowledge Graphs (KGs). Considering Wikidata as the background KG, there are still limited tools to link knowledge within the text to Wikidata. In this paper, we present Falcon 2.0, the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in the English language and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata. The candidates are represented by their Internationalized Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts to the English language model for the recognition task (e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach for the linking task. We have empirically studied the performance of Falcon 2.0 on Wikidata and concluded that it outperforms all the existing baselines. Falcon 2.0 is open source and can be reused by the community; all the required instructions of Falcon 2.0 are well-documented at our GitHub repository (https://github.com/SDM-TIB/falcon2.0). We also demonstrate an online API, which can be run without any technical expertise. Falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3141–3148},
numpages = {8},
keywords = {background knowledge, dbpedia, english morphology, entity linking, nlp, relation linking, wikidata},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3340531.3411895,
author = {Yuan, Chenxi and Yuan, Chun and Bai, Yang and Li, Ziran},
title = {Logic Enhanced Commonsense Inference with Chain Transformer},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411895},
doi = {10.1145/3340531.3411895},
abstract = {We study the commonsense inference task that aims to reason and generate the causes and effects of a given event. Existing neural methods focus more on understanding and representing the event itself, but pay little attention to the relations between different commonsense dimensions (e.g. causes or effects) of the event, making the generated results logically inconsistent and unreasonable. To alleviate this issue, we propose Chain Transformer, a logic enhanced commonsense inference model that combines both direct and indirect inferences to construct a logical chain so as to reason in a more logically consistent way. First, we apply a self-attention based encoder to represent and encode the given event. Then a chain of decoders is implemented to reason and generate for different dimensions following the logical chain, where an attention module is designed to link different decoders and to make each decoder attend to the previous reasoned inferences. Experiments on two real-world datasets show that Chain Transformer outperforms previous methods on both automatic and human evaluation, and demonstrate that Chain Transformer can generate more reasonable and logically consistent inference results.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1763–1772},
numpages = {10},
keywords = {attention network, commonsense inference, commonsense knowledge, logical chain},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3342558.3345404,
author = {Yousefinaghani, Samira and Dara, Rozita and Sharif, Shayan},
title = {Impact of In-domain Vector Representations on the Classification of Disease-related Tweets: Avian Influenza Case Study},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345404},
doi = {10.1145/3342558.3345404},
abstract = {A number of methods have been proposed for the construction of vector representations for natural language processing (NLP) tasks. These methods have been applied to various domains and each has its own pros and cons. Despite their effectiveness, the proposed approaches usually ignore the sentiment information concerning specific tasks. In this paper, we examined various types of word vectors and their impact on the performance of a sentiment classification problem in the area of infectious diseases. Vectors were used in the embedding layer of a word-based convolutional neural network (CNN) to identify tweets pertaining to avian influenza. We proposed a new approach to build effective word embeddings for the sentiment analysis task. Furthermore, the performance of the language model was compared in terms of using various corpus sizes and vector dimensions. Our experiments indicated that initializing the sentiment learning network with domain-specific word embeddings outperforms general domain embeddings. We found that the proposed method leads to a considerable improvement in the classification performance.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {34},
numpages = {4},
keywords = {Convolutional neural network, Twitter, avian influenza, machine learning, sentiment analysis, word embedding},
location = {Berlin, Germany},
series = {DocEng '19}
}

@inproceedings{10.1145/3167020.3167033,
author = {Lakhanpal, Shilpa and Gupta, Ajay and Agrawal, Rajeev},
title = {Mining Domain Similarity to Enhance Digital Indexing},
year = {2017},
isbn = {9781450348959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167020.3167033},
doi = {10.1145/3167020.3167033},
abstract = {Indexing research articles in scientific publications can be arduous. The authors tag their articles by the topics or domains relevant to their research. A publication's organizers may tag them by the broad topics of the specific publication. A third-party may index or tag these articles based on their subject knowledge. Hence indexing of articles can be uneven due to inconsistencies in area knowledge by third-parties or the niche topic representation by the authors. Publications may have schemes in place for indexing or tagging the articles but such schemes cannot keep up with the continuously changing landscape of research. These schemes may need to be updated with newer topics or domains being churned out by the state of the art research. Our technique endeavors to address this problem. We present a methodology to find similarity among domains extracted from the content of research papers, and cluster related domains. Analysis of these clusters provides insights into how the existing indexing schemes may be enhanced by adding newer domains.},
booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
pages = {88–92},
numpages = {5},
keywords = {ACM CCS, Domain, K-Means, Ontology, WuP Similarity},
location = {Bangkok, Thailand},
series = {MEDES '17}
}

@inproceedings{10.1145/3018661.3018692,
author = {Ensan, Faezeh and Bagheri, Ebrahim},
title = {Document Retrieval Model Through Semantic Linking},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018692},
doi = {10.1145/3018661.3018692},
abstract = {This paper addresses the task of document retrieval based on the degree of document relatedness to the meanings of a query by presenting a semantic-enabled language model. Our model relies on the use of semantic linking systems for forming a graph representation of documents and queries, where nodes represent concepts extracted from documents and edges represent semantic relatedness between concepts. Based on this graph, our model adopts a probabilistic reasoning model for calculating the conditional probability of a query concept given values assigned to document concepts. We present an integration framework for interpolating other retrieval systems with the presented model in this paper. Our empirical experiments on a number of TREC collections show that the semantic retrieval has a synergetic impact on the results obtained through state of the art keyword-based approaches, and the consideration of semantic information obtained from entity linking on queries and documents can complement and enhance the performance of other retrieval models.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {181–190},
numpages = {10},
keywords = {information retrieval, language models, semantic linking, semantic relatedness, semantic search},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inproceedings{10.1145/3366424.3383539,
author = {Luo, Feng and Wang, Xiaoli and Wu, Qingfeng and Liang, Jiaying and Qiu, Xueliang and Bao, Zhifeng},
title = {HQADeepHelper: A Deep Learning System for Healthcare Question Answering},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383539},
doi = {10.1145/3366424.3383539},
abstract = {It is challenging to generate high quality answers for healthcare queries in online platforms. Recent studies proposed deep models for healthcare question answering (HQA) tasks. However, these models have not been thoroughly compared, and they were only tested on self-created datasets. This paper demonstrates a novel system, denoted by HQADeepHelper, to facilitate the learning and practicing of deep models for HQA. We have implemented a wide spectrum of state-of-the-art deep models for HQA retrieval. Users can upload self-collected HQA datasets and knowledge graphs, and do simple configurations by selecting datasets, knowledge graphs, neural network models, and evaluation metrics. Based on user’s configuration specified, the system can automatically train and test the model, conduct extensive experimental evaluation of the models selected, and report comprehensive findings. The reports provide new insights about the strengths and weaknesses of deep models that can guide practitioners to select appropriate models for various scenarios. Moreover, users can download the datasets, knowledge graphs, experimental reports and source codes of neural network models for their own practice and evaluations further.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {194–197},
numpages = {4},
keywords = {Healthcare question answering, Neural network models},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3357384.3357874,
author = {Bhutani, Nikita and Jagadish, H V},
title = {Online Schemaless Querying of Heterogeneous Open Knowledge Bases},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357874},
doi = {10.1145/3357384.3357874},
abstract = {Applications that depend on a deep understanding of natural language text have led to a renaissance of large knowledge bases (KBs). Some of these are curated manually and conform to an ontology. Many others, called open KBs, are derived automatically from unstructured text without any pre-specified ontology. These open KBs offer broad coverage of information but are far more heterogeneous than curated KBs, which themselves are more heterogeneous than traditional databases with a fixed schema. Due to the heterogeneity of information representation, querying KBs is a challenging task. Traditionally, query expansion is performed to cover all possible transformations and semantically equivalent structures. Such query expansion can be impractical for heterogeneous open KBs, particularly when complex queries lead to a combinatorial explosion of expansion possibilities. Furthermore, learning a query expansion model requires training examples, which is difficult to scale to diverse representations of facts in the KB. In this paper, we introduce an online schemaless querying method that does not require the query to exactly match the facts. Instead of exactly matching a query, it finds matches for individual query components and then identifies an answer by reasoning over the collective evidence. We devise an alignment-based algorithm for extracting answers based on textual and semantic similarity of query components and evidence fields. Thus, any representational mismatches between the query and evidence are handled online at query-time. Experiments show our approach is effective in handling multi-constraint queries.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {699–708},
numpages = {10},
keywords = {heterogeneity, open knowledge bases, schemaless querying},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3106426.3106465,
author = {Ristoski, Petar and Faralli, Stefano and Ponzetto, Simone Paolo and Paulheim, Heiko},
title = {Large-scale taxonomy induction using entity and word embeddings},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106465},
doi = {10.1145/3106426.3106465},
abstract = {Taxonomies are an important ingredient of knowledge organization, and serve as a backbone for more sophisticated knowledge representations in intelligent systems, such as formal ontologies. However, building taxonomies manually is a costly endeavor, and hence, automatic methods for taxonomy induction are a good alternative to build large-scale taxonomies. In this paper, we propose TIEmb, an approach for automatic unsupervised class subsumption axiom extraction from knowledge bases using entity and text embeddings. We apply the approach on the WebIsA database, a database of subsumption relations extracted from the large portion of the World Wide Web, to extract class hierarchies in the Person and Place domain.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {81–87},
numpages = {7},
keywords = {entity embeddings, ontology induction, text embeddings},
location = {Leipzig, Germany},
series = {WI '17}
}

@inproceedings{10.1145/3422852.3423484,
author = {Huang, Bin and Tang, Siao and Shen, Guangyao and Li, Guohao and Wang, Xin and Zhu, Wenwu},
title = {Commonsense Learning: An Indispensable Path towards Human-centric Multimedia},
year = {2020},
isbn = {9781450381512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422852.3423484},
doi = {10.1145/3422852.3423484},
abstract = {Learning commonsense knowledge and conducting commonsense reasoning are basic human ability to make presumptions about the type and essence of ordinary situation in daily life, which serve as very important goals in human-centric Artificial Intelligence (AI). With the increasing number of media types and quantities provided by various Internet services, commonsense learning and reasoning with no doubt are playing key roles in making progresses for human-centric multimedia analysis. Therefore, this paper first introduces the basic concept of commonsense knowledge and commonsense reasoning, then summarizes commonsense resources and benchmarks, gives an overview on recent commonsense learning and reasoning methods, and discusses several popular applications of commonsense knowledge in real-world scenarios. This work distinguishes itself from existing literature that merely pays attention to natural language processing in focusing more on multimedia which include both natural language processing and computer vision. Furthermore, we also present our insights and thinking on future research directions for commonsense.},
booktitle = {Proceedings of the 1st International Workshop on Human-Centric Multimedia Analysis},
pages = {91–100},
numpages = {10},
keywords = {commonsense knowledge, reasoning},
location = {Seattle, WA, USA},
series = {HuMA'20}
}

@inproceedings{10.1145/3340531.3417466,
author = {Huang, Shanshan and Zhu, Kenny Q. and Liao, Qianzi and Shen, Libin and Zhao, Yinggong},
title = {Enhanced Story Representation by ConceptNet for Predicting Story Endings},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3417466},
doi = {10.1145/3340531.3417466},
abstract = {Predicting endings for narrative stories is a grand challenge for machine commonsense reasoning. The task requires ac- curate representation of the story semantics and structured logic knowledge. Pre-trained language models, such as BERT, made progress recently in this task by exploiting spurious statistical patterns in the test dataset, instead of 'understanding' the stories per se. In this paper, we propose to improve the representation of stories by first simplifying the sentences to some key concepts and second modeling the latent relation- ship between the key ideas within the story. Such enhanced sentence representation, when used with pre-trained language models, makes substantial gains in prediction accuracy on the popular Story Cloze Test without utilizing the biased validation data.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3277–3280},
numpages = {4},
keywords = {commonsense knowledge, commonsense reasoning, story comprehension},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3340531.3417416,
author = {Romero, Julien and Razniewski, Simon},
title = {Inside Quasimodo: Exploring Construction and Usage of Commonsense Knowledge},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3417416},
doi = {10.1145/3340531.3417416},
abstract = {Quasimodo is an open-source commonsense knowledge base that significantly advanced the state of salient commonsense knowledge base construction. It introduced a pipeline that gathers, normalizes, validates and scores statements coming from query log and question answering forums. In this demonstration, we present a companion web portal which allows (i) to explore the data, (ii) to run and analyze the extraction pipeline live, and (iii) inspect the usage of Quasimodo's knowledge in several downstream use cases. The web portal is available at https://quasimodo.r2.enst.fr.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3445–3448},
numpages = {4},
keywords = {commonsense, datasets, knowledge base, visualisation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3383972.3384051,
author = {Zhao, Lin and Li, Minglei and Kou, Jinqiao and Zhang, Jian and Zhang, Yang},
title = {A Framework for Event-oriented Text Retrieval Based on Temporal Aspects: A Recent Review},
year = {2020},
isbn = {9781450376426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383972.3384051},
doi = {10.1145/3383972.3384051},
abstract = {Event, as an important carrier for users to understand the world, has become a special retrieval object. In contrast to traditional text retrieval, Event-oriented Text Retrieval (ETR) can search events by utilizing events knowledge and using events as proxies for information needs. Accordingly, ETR has become the preferred way for users to obtain their interested events from massive web collections. Moreover, it also has already aroused considerable attention from scholars in recent years. However, the retrieval effectiveness of ETR is still subject to the effect of temporal aspects (i.e., temporal dynamics). Thus, in this review, we first analyze three major temporal components in the framework of ETR. After that, we provide a comprehensive overview of state-of-the-art approaches corresponding to such three components. Finally, we summary some ETR-related resources and pinpoint several potential research directions.},
booktitle = {Proceedings of the 2020 12th International Conference on Machine Learning and Computing},
pages = {39–46},
numpages = {8},
keywords = {Event-oriented, Review, Temporal dynamics, Text retrieval},
location = {Shenzhen, China},
series = {ICMLC '20}
}

@inproceedings{10.1145/3366423.3380263,
author = {Kumar, Ramnath and Yadav, Shweta and Daniulaityte, Raminta and Lamy, Francois and Thirunarayan, Krishnaprasad and Lokala, Usha and Sheth, Amit},
title = {eDarkFind: Unsupervised Multi-view Learning for Sybil Account Detection},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380263},
doi = {10.1145/3366423.3380263},
abstract = {Darknet crypto markets are online marketplaces using crypto currencies (e.g., Bitcoin, Monero) and advanced encryption techniques to offer anonymity to vendors and consumers trading for illegal goods or services. The exact volume of substances advertised and sold through these crypto markets is difficult to assess, at least partially, because vendors tend to maintain multiple accounts (or Sybil accounts) within and across different crypto markets. Linking these different accounts will allow us to accurately evaluate the volume of substances advertised across the different crypto markets by each vendor. In this paper, we present a multi-view unsupervised framework (eDarkFind) that helps modeling vendor characteristics and facilitates Sybil account detection. We employ a multi-view learning paradigm to generalize and improve the performance by exploiting the diverse views from multiple rich sources such as BERT, stylometric, and location representation. Our model is further tailored to take advantage of domain-specific knowledge such as the Drug Abuse Ontology to take into consideration the substance information. We performed extensive experiments and demonstrated that the multiple views obtained from diverse sources can be effective in linking Sybil accounts. Our proposed eDarkFind model achieves an accuracy of 98% on three real-world datasets which shows the generality of the approach.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1955–1965},
numpages = {11},
keywords = {Correlation Analysis, Darknet Market, Drug Trafficker Identification, Multi-view Learning, Stylometry, Sybil Detection},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3331184.3331396,
author = {Nguyen, Vincent and Karimi, Sarvnaz and Jin, Brian},
title = {An Experimentation Platform for Precision Medicine},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331396},
doi = {10.1145/3331184.3331396},
abstract = {Precision medicine - where data from patients, their genes, their lifestyles and the available treatments and their combination are taken into account for finding a suitable treatment - requires searching the biomedical literature and other resources such as clinical trials with the patients' information. The retrieved information could then be used in curating data for clinicians for decision-making. We present information retrieval researchers with an on-line system which enables experimentation in search for precision medicine within the framework provided by the TREC Precision Medicine (PM) track. A number of query and document processing and ranking approaches are provided. These include some ofthe most promising gene mention expansion methods, as well as learning-to-rank using neural networks.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1357–1360},
numpages = {4},
keywords = {domain-specific search, health informatics, literature search},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.1145/3383465,
author = {Patil, Charulata and Patwardhan, Manasi},
title = {Visual Question Generation: The State of the Art},
year = {2020},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3383465},
doi = {10.1145/3383465},
abstract = {Visual question generation (VQG) is an interesting problem that has recently received attention. The task of VQG involves generating meaningful questions based on the input image. It is a multi-modal problem involving image understanding and natural language generation, especially using deep learning methods. VQG can be considered as complementary task of visual question answering. In this article, we review the current state of VQG in terms of methods to understand the problem, existing datasets to train the VQG model, evaluation metrics, and algorithms to handle the problem. Finally, we discuss the challenges that need to be conquered and the possible future directions for an effective VQG.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {47},
numpages = {22},
keywords = {Image understanding, question generation}
}

@inproceedings{10.1145/3394171.3414047,
author = {Vu, Xuan-Son and Le, Duc-Trong and Edlund, Christoffer and Jiang, Lili and Nguyen, Hoang D.},
title = {Privacy-Preserving Visual Content Tagging using Graph Transformer Networks},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3414047},
doi = {10.1145/3394171.3414047},
abstract = {With the rapid growth of Internet media, content tagging has become an important topic with many multimedia understanding applications, including efficient organisation and search. Nevertheless, existing visual tagging approaches are susceptible to inherent privacy risks in which private information may be exposed unintentionally. The use of anonymisation and privacy-protection methods is desirable, but with the expense of task performance. Therefore, this paper proposes an end-to-end framework (SGTN) using Graph Transformer and Convolutional Networks to significantly improve classification and privacy preservation of visual data. Especially, we employ several mechanisms such as differential privacy based graph construction and noise-induced graph transformation to protect the privacy of knowledge graphs. Our approach unveils new state-of-the-art on MS-COCO dataset in various semi-supervised settings. In addition, we showcase a real experiment in the education domain to address the automation of sensitive document tagging. Experimental results show that our approach achieves an excellent balance of model accuracy and privacy preservation on both public and private datasets.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2299–2307},
numpages = {9},
keywords = {dp-adjacency-graph, dp-embedding, graph-transformer, privacy-preservation, visual tagging},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{10.1109/TCBB.2020.2979959,
author = {Jiang, Tianwen and Zeng, Qingkai and Zhao, Tong and Qin, Bing and Liu, Ting and Chawla, Nitesh V. and Jiang, Meng},
title = {Biomedical Knowledge Graphs Construction From Conditional Statements},
year = {2020},
issue_date = {May-June 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.2979959},
doi = {10.1109/TCBB.2020.2979959},
abstract = {Conditions play an essential role in biomedical statements. However, existing biomedical knowledge graphs (BioKGs) only focus on factual knowledge, organized as a flat relational network of biomedical concepts. These BioKGs ignore the conditions of the facts being valid, which loses essential contexts for knowledge exploration and inference. We consider both facts and their conditions in biomedical statements and proposed a three-layered information-lossless representation of BioKG. The first layer has biomedical concept nodes, attribute nodes. The second layer represents both biomedical fact and condition tuples by nodes of the relation phrases, connecting to the subject and object in the first layer. The third layer has nodes of statements connecting to a set of fact tuples and/or condition tuples in the second layer. We transform the BioKG construction problem into a sequence labeling problem based on a novel designed tag schema. We design a Multi-Input Multi-Output sequence labeling model (MIMO) that learns from &lt;italic&gt;multiple input&lt;/italic&gt; signals and generates proper number of &lt;italic&gt;multiple output&lt;/italic&gt; sequences for tuple extraction. Experiments on a newly constructed dataset show that MIMO outperforms the existing methods. Further case study demonstrates that the BioKGs constructed provide a good understanding of the biomedical statements.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {823–835},
numpages = {13}
}

@inproceedings{10.1145/3281375.3281405,
author = {Bhattacharya, Sambit and Agrawal, Rajeev and Wagner, Neal},
title = {Application of deep learning and geo-knowledge bases to scene understanding},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281405},
doi = {10.1145/3281375.3281405},
abstract = {Humans can easily perform tasks that use vision and language jointly, such as describing a scene and answering questions about objects in the scene and how they are related. Image captioning and visual question &amp; answer are two popular research tasks that have emerged from advances in deep learning and the availability of datasets that specifically address these problems. However recent work has shown that deep learning based solutions to these tasks are just as brittle as solutions for only vision or only natural language tasks. Image captioning is vulnerable to adversarial perturbations; novel objects, which are not described in training data, and contextual biases in training data can degrade performance in surprising ways. For these reasons, it is important to find ways in which general-purpose knowledge can guide connectionist models. We investigate challenges to integrate existing ontologies and knowledge bases with deep learning solutions, and possible approaches for overcoming such challenges. We focus on geo-referenced data such as geo-tagged images and videos that capture outdoor scenery. Geo-knowledge bases are domain specific knowledge bases that contain concepts and relations that describe geographic objects. This work proposes to increase the robustness of automatic scene description and inference by leveraging geo-knowledge bases along with the strengths of deep learning for visual object detection and classification.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {74–79},
numpages = {6},
keywords = {commonsense knowledge, deep learning, knowledge base, scene understanding},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/3340531.3412072,
author = {Sahu, Sunil Kumar and Thomas, Derek and Chiu, Billy and Sengupta, Neha and Mahdy, Mohammady},
title = {Relation Extraction with Self-determined Graph Convolutional Network},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412072},
doi = {10.1145/3340531.3412072},
abstract = {Relation Extraction is a way of obtaining the semantic relationship between entities in text. The state-of-the-art methods use linguistic tools to build a graph for the text in which the entities appear and then a Graph Convolutional Network (GCN) is employed to encode the pre-built graphs. Although their performance is promising, the reliance on linguistic tools results in a non end-to-end process. In this work, we propose a novel model, the Self-determined Graph Convolutional Network (SGCN), which determines a weighted graph using a self-attention mechanism, rather using any linguistic tool. Then, the self-determined graph is encoded using a GCN. We test our model on the TACRED dataset and achieve the state-of-the-art result. Our experiments show that SGCN outperforms the traditional GCN, which uses dependency parsing tools to build the graph.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2205–2208},
numpages = {4},
keywords = {graph neural networks, information extraction, natural language processing, relation extraction},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3360901.3364451,
author = {Clark, Peter},
title = {Project Aristo: Towards Machines that Capture and Reason with Science Knowledge},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364451},
doi = {10.1145/3360901.3364451},
abstract = {AI2's Project Aristo seeks to build a system that has a deep understanding of science, using knowledge captured mainly from large-scale text. Recently, Aristo achieved surprising success on the Grade 8 New York Regents Science Exams, scoring over 90% on the exam's non-diagram, multiple choice (NDMC) questions, where even 3 years ago the best systems scored less than 60%. In this talk, I will describe the journey of Aristo through various knowledge capture technologies that have helped it, including acquiring if/then rules, tables, knowledge graphs, and latent neural representations. I will also discuss the growing tension between capturing structured knowledge vs. capturing knowledge latently using neural models, the latter proving highly effective but hard to interpret. Finally I will speculate on the larger quest towards knowledgable machines that can reason, explain, and discuss, and how structured and latent knowledge can interact to help reach this goal.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {1–2},
numpages = {2},
keywords = {knowledge acquisition, question answering},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3030024.3038282,
author = {Mauro, Noemi},
title = {Intelligent and Personalized Community Maps},
year = {2017},
isbn = {9781450348935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030024.3038282},
doi = {10.1145/3030024.3038282},
abstract = {My PhD project focuses on Participatory GIS (PGIS). In the project I analyze two methodologies to offer personalized search results in community maps and a natural interaction with the system. The first consists of automatically gathering the terms according to which the users express their information needs, in order to enrich the domain conceptualization of a PGIS, giving common definitions for places. The second concerns the creation of ontology-based user models that reflect the interests, lexicon and modality of expression adopted by each person, mapped to the domain ontology adopted by the PGIS. In the project I also analyze how these techniques may be jointly used during the query expansion process to retrieve more accurate and relevant search results.},
booktitle = {Companion Proceedings of the 22nd International Conference on Intelligent User Interfaces},
pages = {181–184},
numpages = {4},
keywords = {linked data, ontologies, ontology-based user model, participatory GIS, personalization, semantic search},
location = {Limassol, Cyprus},
series = {IUI '17 Companion}
}

@inproceedings{10.1145/3340531.3412783,
author = {Armitage, Jason and Kacupaj, Endri and Tahmasebzadeh, Golsa and Swati and Maleshkova, Maria and Ewerth, Ralph and Lehmann, Jens},
title = {MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412783},
doi = {10.1145/3340531.3412783},
abstract = {In this paper, we introduce the MLM (Multiple Languages and Modalities) dataset - a new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. The generation process and inclusion of semantic data provide a resource that further tests the ability for multitask systems to learn relationships between entities. The dataset is designed for researchers and developers who build applications that perform multiple tasks on data encountered on the web and in digital archives. A second version of MLM provides a geo-representative subset of the data with weighted samples for countries of the European Union. We demonstrate the value of the resource in developing novel applications in the digital humanities with a motivating use case and specify a benchmark set of tasks to retrieve modalities and locate entities in the dataset. Evaluation of baseline multitask and single task systems on the full and geo-representative versions of MLM demonstrate the challenges of generalising on diverse data. In addition to the digital humanities, we expect the resource to contribute to research in multimodal representation learning, location estimation, and scene understanding.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2967–2974},
numpages = {8},
keywords = {machine learning, multilingual data, multimodal data, multitask learning},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3394171.3413600,
author = {Liu, Ye and Yuan, Junsong and Chen, Chang Wen},
title = {ConsNet: Learning Consistency Graph for Zero-Shot Human-Object Interaction Detection},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413600},
doi = {10.1145/3394171.3413600},
abstract = {We consider the problem of Human-Object Interaction (HOI) Detection, which aims to locate and recognize HOI instances in the form of &lt;human, action, object&gt; in images. Most existing works treat HOIs as individual interaction categories, thus can not handle the problem of long-tail distribution and polysemy of action labels. We argue that multi-level consistencies among objects, actions and interactions are strong cues for generating semantic representations of rare or previously unseen HOIs. Leveraging the compositional and relational peculiarities of HOI labels, we propose ConsNet, a knowledge-aware framework that explicitly encodes the relations among objects, actions and interactions into an undirected graph called consistency graph, and exploits Graph Attention Networks (GATs) to propagate knowledge among HOI categories as well as their constituents. Our model takes visual features of candidate human-object pairs and word embeddings of HOI labels as inputs, maps them into visual-semantic joint embedding space and obtains detection results by measuring their similarities. We extensively evaluate our model on the challenging V-COCO and HICO-DET datasets, and results validate that our approach outperforms state-of-the-arts under both fully-supervised and zero-shot settings.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4235–4243},
numpages = {9},
keywords = {graph neural networks, human-object interaction detection, zero-shot learning},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3019612.3019833,
author = {Dragoni, Mauro and Rexha, Andi and Ziak, Hermann and Kern, Roman},
title = {A semantic federated search engine for domain-specific document retrieval},
year = {2017},
isbn = {9781450344869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3019612.3019833},
doi = {10.1145/3019612.3019833},
abstract = {Retrieval of domain-specific documents became attractive for the Semantic Web community due to the possibility of integrating classic Information Retrieval (IR) techniques with semantic knowledge. Unfortunately, the gap between the construction of a full semantic search engine and the possibility of exploiting a repository of ontologies covering all possible domains is far from being filled. Recent solutions focused on the aggregation of different domain-specific repositories managed by third-parties. In this paper, we present a semantic federated search engine developed in the context of the EEXCESS EU project. Through the developed platform, users are able to perform federated queries over repositories in a transparent way, i.e. without knowing how their original queries are transformed before being actually submitted. The platform implements a facility for plugging new repositories and for creating, with the support of general purpose knowledge bases, knowledge graphs describing the content of each connected repository. Such knowledge graphs are then exploited for enriching queries performed by users.},
booktitle = {Proceedings of the Symposium on Applied Computing},
pages = {303–308},
numpages = {6},
keywords = {federated search, information retrieval, knowledge-based query expansion, semantic web},
location = {Marrakech, Morocco},
series = {SAC '17}
}

@inproceedings{10.1145/3368089.3409731,
author = {Xie, Wenkai and Peng, Xin and Liu, Mingwei and Treude, Christoph and Xing, Zhenchang and Zhang, Xiaoxin and Zhao, Wenyun},
title = {API method recommendation via explicit matching of functionality verb phrases},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409731},
doi = {10.1145/3368089.3409731},
abstract = {Due to the lexical gap between functionality descriptions and user queries, documentation-based API retrieval often produces poor results.Verb phrases and their phrase patterns are essential in both describing API functionalities and interpreting user queries. Thus we hypothesize that API retrieval can be facilitated by explicitly recognizing and matching between the fine-grained structures of functionality descriptions and user queries. To verify this hypothesis, we conducted a large-scale empirical study on the functionality descriptions of 14,733 JDK and Android API methods. We identified 356 different functionality verbs from the descriptions, which were grouped into 87 functionality categories, and we extracted 523 phrase patterns from the verb phrases of the descriptions. Building on these findings, we propose an API method recommendation approach based on explicit matching of functionality verb phrases in functionality descriptions and user queries, called PreMA. Our evaluation shows that PreMA can accurately recognize the functionality categories (92.8%) and phrase patterns (90.4%) of functionality description sentences; and when used for API retrieval tasks, PreMA can help participants complete their tasks more accurately and with fewer retries compared to a baseline approach.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1015–1026},
numpages = {12},
keywords = {API Documentation, API Retrieval, Functionality Description},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3397271.3401442,
author = {Wei, Mengxi and He, YIfan and Zhang, Qiong},
title = {Robust Layout-aware IE for Visually Rich Documents with Pre-trained Language Models},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401442},
doi = {10.1145/3397271.3401442},
abstract = {Many business documents processed in modern NLP and IR pipelines are visually rich: in addition to text, their semantics can also be captured by visual traits such as layout, format, and fonts. We study the problem of information extraction from visually rich documents (VRDs) and present a model that combines the power of large pre-trained language models and graph neural networks to efficiently encode both textual and visual information in business documents. We further introduce new fine-tuning objectives to improve in-domain unsupervised fine-tuning to better utilize large amount of unlabeled in-domain data.We experiment on real world invoice and resume data sets and show that the proposed method outperforms strong text-based RoBERTa baselines by 6.3% absolute F1 on invoices and 4.7% absolute F1 on resumes. When evaluated in a few-shot setting, our method requires up to 30x less annotation data than the baseline to achieve the same level of performance at ~90% F1.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2367–2376},
numpages = {10},
keywords = {graph neural networks, structured information extraction, visually rich document},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inproceedings{10.1145/3132847.3133048,
author = {Xiong, Chenyan and Liu, Zhengzhong and Callan, Jamie and Hovy, Eduard},
title = {JointSem: Combining Query Entity Linking and Entity based Document Ranking},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133048},
doi = {10.1145/3132847.3133048},
abstract = {Entity-based ranking systems often employ entity linking systems to align entities to query and documents. Previously, entity linking systems were not designed specifically for search engines and were mostly used as a preprocessing step. This work presents JointSem, a joint semantic ranking system that combines query entity linking and entity-based document ranking. In JointSem, the spotting and linking signals are used to describe the importance of candidate entities in the query, and the linked entities are utilized to provide additional ranking features for the documents. The linking signals and the ranking signals are combined by a joint learning-to-rank model, and the whole system is fully optimized towards end-to-end ranking performance. Experiments on TREC Web Track datasets demonstrate the effectiveness of joint learning of entity linking and entity-based ranking.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2391–2394},
numpages = {4},
keywords = {document ranking, entity linking, entity-based search},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3371158.3371200,
author = {Kumar, Sumit and Ramena, Gopi and Goyal, Manoj and Mohanty, Debi and Agarwal, Ankur and Changmai, Benu and Moharana, Sukumar},
title = {On-Device Information Extraction from Screenshots in form of tags},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371200},
doi = {10.1145/3371158.3371200},
abstract = {We propose a method to make mobile Screenshots easily searchable. In this paper, we present the workflow in which we: 1) pre-processed a collection of screenshots, 2) identified script present in image, 3) extracted unstructured text from images, 4) identified language of the extracted text, 5) extracted keywords from the text, 6) identified tags based on image features, 7) expanded tag set by identifying related keywords, 8) inserted image tags with relevant images after ranking and indexed them to make it searchable on device. We made the pipeline which supports multiple languages and executed it on-device, which addressed privacy concerns. We developed novel architectures for components in the pipeline, optimized performance and memory for on-device computation. We observed from experimentation that the solution developed can reduce overall user effort and improve end user experience while searching, whose results are published.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {275–281},
numpages = {7},
keywords = {on-device search, on-device tag extraction, tag expansion, tag recommendation},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@article{10.1145/3383123,
author = {Huang, Minlie and Zhu, Xiaoyan and Gao, Jianfeng},
title = {Challenges in Building Intelligent Open-domain Dialog Systems},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3383123},
doi = {10.1145/3383123},
abstract = {There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI [33]. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This article reviews the recent work on neural approaches that are devoted to addressing three challenges in developing such systems: semantics, consistency, and interactiveness. Semantics requires a dialog system to not only understand the content of the dialog but also identify users’ emotional and social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users’ trust and gain their long-term confidence. Interactiveness refers to the system’s ability to generate interpersonal responses to achieve particular social goals such as entertainment and conforming. The studies we select to present in this survey are based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent open-domain dialog systems.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {21},
numpages = {32},
keywords = {Dialog system, chatbot, conversation generation, conversational AI, response generation, social bot}
}

@inproceedings{10.1145/3209978.3209982,
author = {Xiong, Chenyan and Liu, Zhengzhong and Callan, Jamie and Liu, Tie-Yan},
title = {Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3209982},
doi = {10.1145/3209978.3209982},
abstract = {This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents. KESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience. The whole model is learned end-to-end using entity salience labels. The salience model also improves ad hoc search accuracy, providing effective ranking features by modeling the salience of query entities in candidate documents. Our experiments on two entity salience corpora and two TREC ad hoc search datasets demonstrate the effectiveness of KESM over frequency-based and feature-based methods. We also provide examples showing how KESM conveys its text understanding ability learned from entity salience to search.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {575–584},
numpages = {10},
keywords = {entity salience, entity-oriented search, text representation, text understanding},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3397271.3401462,
author = {Feng, Fuli and Luo, Cheng and He, Xiangnan and Liu, Yiqun and Chua, Tat-Seng},
title = {FinIR 2020: The First Workshop on Information Retrieval in Finance},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401462},
doi = {10.1145/3397271.3401462},
abstract = {This half-day workshop explores challenges and potential research directions about Information Retrieval (IR) in finance. The focus will be on stimulating discussions around the accessing, searching, filtering, and analyzing financial documents in banking, insurance, and investment, such as the financial statements, analyst reports, filling forms, and news articles. We welcome theoretical, experimental, and methodological studies that aim to advance techniques of managing and understanding financial documents, as well as emphasize the applicability in practical applications. The workshop aims to bring together a diverse set of researchers and practitioners interested in investigating relevant topics. Besides, to facilitate developing and testing some relevant techniques, we hold a data challenge on quantifying analyst reports and news articles for the prediction of commodity prices.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2451–2454},
numpages = {4},
keywords = {finance, information retrieval, unstructured data},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3384675,
author = {Zhang, Yingying and Fang, Quan and Qian, Shengsheng and Xu, Changsheng},
title = {Knowledge-aware Attentive Wasserstein Adversarial Dialogue Response Generation},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3384675},
doi = {10.1145/3384675},
abstract = {Natural language generation has become a fundamental task in dialogue systems. RNN-based natural response generation methods encode the dialogue context and decode it into a response. However, they tend to generate dull and simple responses. In this article, we propose a novel framework, called KAWA-DRG (Knowledge-aware Attentive Wasserstein Adversarial Dialogue Response Generation) to model conversation-specific external knowledge and the importance variances of dialogue context in a unified adversarial encoder-decoder learning framework. In KAWA-DRG, a co-attention mechanism attends to important parts within and among context utterances with word-utterance-level attention. Prior knowledge is integrated into the conditional Wasserstein auto-encoder for learning the latent variable space. The posterior and prior distribution of latent variables are generated and trained through adversarial learning. We evaluate our model on Switchboard, DailyDialog, In-Car Assistant, and Ubuntu Dialogue Corpus. Experimental results show that KAWA-DRG outperforms the existing methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {37},
numpages = {20},
keywords = {Dialogue system, adversarial learning, co-attention, external knowledge}
}

@inproceedings{10.1145/3394171.3413880,
author = {Zhang, Shengyu and Tan, Ziqi and Yu, Jin and Zhao, Zhou and Kuang, Kun and Liu, Jie and Zhou, Jingren and Yang, Hongxia and Wu, Fei},
title = {Poet: Product-oriented Video Captioner for E-commerce},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413880},
doi = {10.1145/3394171.3413880},
abstract = {In e-commerce, a growing number of user-generated videos are used for product promotion. How to generate video descriptions that narrate the user-preferred product characteristics depicted in the video is vital for successful promoting. Traditional video captioning methods, which focus on routinely describing what exists and happens in a video, are not amenable for product-oriented video captioning. To address this problem, we propose a product-oriented video captioner framework, abbreviated as Poet. Poet firstly represents the videos as product-oriented spatial-temporal graphs. Then, based on the aspects of the video-associated product, we perform knowledge-enhanced spatial-temporal inference on those graphs for capturing the dynamic change of fine-grained product-part characteristics. The knowledge leveraging module in Poet differs from the traditional design by performing knowledge filtering and dynamic memory modeling. We show that Poet achieves consistent performance improvement over previous methods concerning generation quality, product aspects capturing, and lexical diversity. Experiments are performed on two product-oriented video captioning datasets, buyer-generated fashion video dataset (BFVD) and fan-generated fashion video dataset (FFVD), collected from Mobile Taobao. We will release the desensitized datasets to promote further investigations on both video captioning and general video analysis problems.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1292–1301},
numpages = {10},
keywords = {e-commerce, external knowledge, user-generated video analysis, video-to-text generation},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{10.1613/jair.1.11259,
author = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
title = {From word to sense embeddings: a survey on vector representations of meaning},
year = {2018},
issue_date = {September 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {63},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11259},
doi = {10.1613/jair.1.11259},
abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {743–788},
numpages = {46}
}

@inproceedings{10.1145/3383583.3398525,
author = {Ostendorff, Malte and Ruas, Terry and Schubotz, Moritz and Rehm, Georg and Gipp, Bela},
title = {Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398525},
doi = {10.1145/3383583.3398525},
abstract = {Many digital libraries recommend literature to their users considering the similarity between a query document and their repository. However, they often fail to distinguish what is the relationship that makes two documents alike. In this paper, we model the problem of finding the relationship between two documents as a pairwise document classification task. To find the semantic relation between documents, we apply a series of techniques, such as GloVe, Paragraph Vectors, BERT, and XLNet under different configurations (e.g., sequence length, vector concatenation scheme), including a Siamese architecture for the Transformer-based systems. We perform our experiments on a newly proposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that define the semantic document relations. Our results show vanilla BERT as the best performing system with an F1-score of 0.93, which we manually examine to better understand its applicability to other domains. Our findings suggest that classifying semantic relations between documents is a solvable task and motivates the development of a recommender system based on the evaluated techniques. The discussions in this paper serve as first steps in the exploration of documents through SPARQL-like queries such that one could find documents that are similar in one aspect but dissimilar in another.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {127–136},
numpages = {10},
keywords = {BERT, Siamese networks, Wikipedia, XLNet, document classification, document similarity, recommender systems, transformers},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3366423.3380175,
author = {Yu, Wenhao and Yu, Mengxia and Zhao, Tong and Jiang, Meng},
title = {Identifying Referential Intention with Heterogeneous Contexts},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380175},
doi = {10.1145/3366423.3380175},
abstract = {Citing, quoting, and forwarding &amp; commenting behaviors are widely seen in academia, news media, and social media. Existing behavior modeling approaches focused on mining content and describing preferences of authors, speakers, and users. However, behavioral intention plays an important role in generating content on the platforms. In this work, we propose to identify the referential intention which motivates the action of using the referred (e.g., cited, quoted, and retweeted) source and content to support their claims. We adopt a theory in sociology to develop a schema of four types of intentions. The challenge lies in the heterogeneity of observed contextual information surrounding the referential behavior, such as referred content (e.g., a cited paper), local context (e.g., the sentence citing the paper), neighboring context (e.g., the former and latter sentences), and network context (e.g., the academic network of authors, affiliations, and keywords). We propose a new neural framework with Interactive Hierarchical Attention (IHA) to identify the intention of referential behavior by properly aggregating the heterogeneous contexts. Experiments demonstrate that the proposed method can effectively identify the type of intention of citing behaviors (on academic data) and retweeting behaviors (on Twitter). And learning the heterogeneous contexts collectively can improve the performance. This work opens a door for understanding content generation from a fundamental perspective of behavior sciences.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {962–972},
numpages = {11},
keywords = {Heterogeneous Contexts, Interactive Hierarchical Attention, Referential Intention},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3331000,
author = {Gon\c{c}alves, Rodrigo and Dorneles, Carina Friedrich},
title = {Automated Expertise Retrieval: A Taxonomy-Based Survey and Open Issues},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3331000},
doi = {10.1145/3331000},
abstract = {Understanding people’s expertise is not a trivial task since it is time-consuming when manually executed. Automated approaches have become a topic of research in recent years in various scientific fields, such as information retrieval, databases, and machine learning. This article carries out a survey on automated expertise retrieval, i.e., finding data linked to a person that describes the person’s expertise, which allows tasks such as profiling or finding people with a certain expertise. A faceted taxonomy is introduced that covers many of the existing approaches and classifies them on the basis of features chosen from studying the state-of-the-art. A list of open issues, with suggestions for future research topics, is introduced as well. It is hoped that our taxonomy and review of related works on expertise retrieval will be useful when analyzing different proposals and will allow a better understanding of existing work and a systematic classification of future work on the topic.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {96},
numpages = {30},
keywords = {Expertise retrieval, expert finding, expertise profile}
}

@article{10.14778/3415478.3415559,
author = {Suri, Sahaana and Chanda, Raghuveer and Bulut, Neslihan and Narayana, Pradyumna and Zeng, Yemao and Bailis, Peter and Basu, Sugato and Narlikar, Girija and R\'{e}, Christopher and Sethi, Abishek},
title = {Leveraging organizational resources to adapt models to new data modalities},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415559},
doi = {10.14778/3415478.3415559},
abstract = {As applications in large organizations evolve, the machine learning (ML) models that power them must adapt the same predictive tasks to newly arising data modalities (e.g., a new video content launch in a social media application requires existing text or image models to extend to video). To solve this problem, organizations typically create ML pipelines from scratch. However, this fails to utilize the domain expertise and data they have cultivated from developing tasks for existing modalities. We demonstrate how organizational resources, in the form of aggregate statistics, knowledge bases, and existing services that operate over related tasks, enable teams to construct a common feature space that connects new and existing data modalities. This allows teams to apply methods for data curation (e.g., weak supervision and label propagation) and model training (e.g., forms of multi-modal learning) across these different data modalities. We study how this use of organizational resources composes at production scale in over 5 classification tasks at Google, and demonstrate how it reduces the time needed to develop models for new modalities from months to weeks or days.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3396–3410},
numpages = {15}
}

@inproceedings{10.1145/3397271.3401416,
author = {van Hulst, Johannes M. and Hasibi, Faegheh and Dercksen, Koen and Balog, Krisztian and de Vries, Arjen P.},
title = {REL: An Entity Linker Standing on the Shoulders of Giants},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401416},
doi = {10.1145/3397271.3401416},
abstract = {Entity linking is a standard component in modern retrieval system that is often performed by third-party toolkits. Despite the plethora of open source options, it is difficult to find a single system that has a modular architecture where certain components may be replaced, does not depend on external sources, can easily be updated to newer Wikipedia versions, and, most important of all, has state-of-the-art performance. The REL system presented in this paper aims to fill that gap. Building on state-of-the-art neural components from natural language processing research, it is provided as a Python package as well as a web API. We also report on an experimental comparison against both well-established systems and the current state-of-the-art on standard entity linking benchmarks.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2197–2200},
numpages = {4},
keywords = {NER, entity disambiguation, entity linking, toolkit},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1109/TASLP.2020.3013114,
author = {Wang, Yu and Li, Yun and Zhu, Ziye and Tong, Hanghang and Huang, Yue},
title = {Adversarial Learning for Multi-Task Sequence Labeling With Attention Mechanism},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3013114},
doi = {10.1109/TASLP.2020.3013114},
abstract = {With the requirements of natural language applications, multi-task sequence labeling methods have some immediate benefits over the single-task sequence labeling methods. Recently, many state-of-the-art multi-task sequence labeling methods were proposed, while still many issues to be resolved including (C1) exploring a more general relationship between tasks, (C2) extracting the task-shared knowledge purely and (C3) merging the task-shared knowledge for each task appropriately. To address the above challenges, we propose MTAA, a symmetric multi-task sequence labeling model, which performs an arbitrary number of tasks simultaneously. Furthermore, MTAA extracts the shared knowledge among tasks by adversarial learning and integrates the proposed multi-representation fusion attention mechanism for merging feature representations. We evaluate MTAA on two widely used data sets: CoNLL2003 and OntoNotes5.0. Experimental results show that our proposed model outperforms the latest methods on the named entity recognition and the syntactic chunking task by a large margin, and achieves state-of-the-art results on the part-of-speech tagging task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {2476–2488},
numpages = {13}
}

@article{10.14778/3372716.3372727,
author = {Karagiannis, Georgios and Trummer, Immanuel and Jo, Saehan and Khandelwal, Shubham and Wang, Xuezhi and Yu, Cong},
title = {Mining an "anti-knowledge base" from Wikipedia updates with applications to fact checking and beyond},
year = {2019},
issue_date = {December 2019},
publisher = {VLDB Endowment},
volume = {13},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3372716.3372727},
doi = {10.14778/3372716.3372727},
abstract = {We introduce the problem of anti-knowledge mining. Our goal is to create an "anti-knowledge base" that contains factual mistakes. The resulting data can be used for analysis, training, and benchmarking in the research domain of automated fact checking. Prior data sets feature manually generated fact checks of famous misclaims. Instead, we focus on the long tail of factual mistakes made by Web authors, ranging from erroneous sports results to incorrect capitals.We mine mistakes automatically, by an unsupervised approach, from Wikipedia updates that correct factual mistakes. Identifying such updates (only a small fraction of the total number of updates) is one of the primary challenges. We mine anti-knowledge by a multi-step pipeline. First, we filter out candidate updates via several simple heuristics. Next, we correlate Wikipedia updates with other statements made on the Web. Using claim occurrence frequencies as input to a probabilistic model, we infer the likelihood of corrections via an iterative expectation-maximization approach. Finally, we extract mistakes in the form of subject-predicate-object triples and rank them according to several criteria. Our end result is a data set containing over 110,000 ranked mistakes with a precision of 85% in the top 1% and a precision of over 60% in the top 25%. We demonstrate that baselines achieve significantly lower precision. Also, we exploit our data to verify several hypothesis on why users make mistakes. We finally show that the AKB can be used to find mistakes on the entire Web.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {561–573},
numpages = {13}
}

@article{10.1162/coli_a_00363,
author = {Laha, Anirban and Jain, Parag and Mishra, Abhijit and Sankaranarayanan, Karthik},
title = {Scalable Micro-planned Generation of Discourse from Structured Data},
year = {2020},
issue_date = {December 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {45},
number = {4},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00363},
doi = {10.1162/coli_a_00363},
abstract = {We present a framework for generating natural language description from structured data such as tables; the problem comes under the category of data-to-text natural language generation (NLG). Modern data-to-text NLG systems typically use end-to-end statistical and neural architectures that learn from a limited amount of task-specific labeled data, and therefore exhibit limited scalability, domain-adaptability, and interpretability. Unlike these systems, ours is a modular, pipeline-based approach, and does not require task-specific parallel data. Rather, it relies on monolingual corpora and basic off-the-shelf NLP tools. This makes our system more scalable and easily adaptable to newer domains.Our system utilizes a three-staged pipeline that: (i) converts entries in the structured data to canonical form, (ii) generates simple sentences for each atomic entry in the canonicalized representation, and (iii) combines the sentences to produce a coherent, fluent, and adequate paragraph description through sentence compounding and co-reference replacement modules. Experiments on a benchmark mixed-domain data set curated for paragraph description from tables reveals the superiority of our system over existing data-to-text approaches. We also demonstrate the robustness of our system in accepting other popular data sets covering diverse data types such as knowledge graphs and key-value maps.},
journal = {Comput. Linguist.},
month = jan,
pages = {737–763},
numpages = {27}
}

@inproceedings{10.1145/3340531.3412736,
author = {Moerchen, Fabian and Ernst, Patrick and Zappella, Giovanni},
title = {Personalizing Natural Language Understanding using Multi-armed Bandits and Implicit Feedback},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412736},
doi = {10.1145/3340531.3412736},
abstract = {Natural Language Understanding (NLU) models on voice-controlled speakers face several challenges. In particular, music streaming services have large catalogs, often containing millions of songs, artists, and albums and several thousands of custom playlists and stations. In many cases there is ambiguity and little structural difference between carrier phrases and entity names. In this work, we describe how we leveraged multi-armed bandits in combination with implicit customer feedback to improve accuracy and personalization of responses to voice request in the music domain. Our models are tested in a large-scale industrial system containing several other components. In particular, we focused on using this technology to correct errors made by upstream NLU models and personalize responses based on customer preferences and music provider functionality. The models resulted in significant improvement of playback rate for Amazon Music and are deployed in systems serving several countries and languages. We further used the implicit feedback of the customers to generate weakly labeled training data for the NLU models. This improved the experience for customers using other music providers on all Alexa devices.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2661–2668},
numpages = {8},
keywords = {multi-armed bandits, music, natural language understanding, personalization},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1109/TASLP.2020.3012060,
author = {Qin, Yujia and Qi, Fanchao and Ouyang, Sicong and Liu, Zhiyuan and Yang, Cheng and Wang, Yasheng and Liu, Qun and Sun, Maosong},
title = {Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3012060},
doi = {10.1109/TASLP.2020.3012060},
abstract = {Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at https://github.com/thunlp/SememeRNN.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2364–2373},
numpages = {10}
}

@inproceedings{10.1145/3372020.3391564,
author = {Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and Lee, Scott Uk-Jin},
title = {Semantic-based Architecture Smell Analysis},
year = {2020},
isbn = {9781450370714},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372020.3391564},
doi = {10.1145/3372020.3391564},
abstract = {Software smells have negative impacts on the reliability and modifiability of software systems. The smells in architecture design can be cascaded down to the implementation level and cause issues that require much effort to fix. Therefore, early detection of the architecture smells can benefit the overall quality of the software system. This paper presents an integration of methods that formally define the software architecture design towards architecture smell detection. Our approach serves as a framework that allows the architectural structures and behaviours to be formally analysed based on a coherent technique. We evaluated the accuracy and performance of our approach with the models generated from open source projects. The results show that our approach is effective and functions well.},
booktitle = {Proceedings of the 8th International Conference on Formal Methods in Software Engineering},
pages = {109–118},
numpages = {10},
keywords = {Architecture Smells, Model Checking, Ontology Web Language, Smell Detection, Software Architecture},
location = {Seoul, Republic of Korea},
series = {FormaliSE '20}
}

@inproceedings{10.1145/3102254.3102279,
author = {Cochez, Michael and Ristoski, Petar and Ponzetto, Simone Paolo and Paulheim, Heiko},
title = {Biased graph walks for RDF graph embeddings},
year = {2017},
isbn = {9781450352253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102254.3102279},
doi = {10.1145/3102254.3102279},
abstract = {Knowledge Graphs have been recognized as a valuable source for background information in many data mining, information retrieval, natural language processing, and knowledge extraction tasks. However, obtaining a suitable feature vector representation from RDF graphs is a challenging task. In this paper, we extend the RDF2Vec approach, which leverages language modeling techniques for unsupervised feature extraction from sequences of entities. We generate sequences by exploiting local information from graph substructures, harvested by graph walks, and learn latent numerical representations of entities in RDF graphs. We extend the way we compute feature vector representations by comparing twelve different edge weighting functions for performing biased walks on the RDF graph, in order to generate higher quality graph embeddings. We evaluate our approach using different machine learning, as well as entity and document modeling benchmark data sets, and show that the naive RDF2Vec approach can be improved by exploiting Biased Graph Walks.},
booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
articleno = {21},
numpages = {12},
keywords = {data mining, graph embeddings, linked open data},
location = {Amantea, Italy},
series = {WIMS '17}
}

@inproceedings{10.1145/3289600.3291030,
author = {Zhu, Qi and Ren, Xiang and Shang, Jingbo and Zhang, Yu and El-Kishky, Ahmed and Han, Jiawei},
title = {Integrating Local Context and Global Cohesiveness for Open Information Extraction},
year = {2019},
isbn = {9781450359405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289600.3291030},
doi = {10.1145/3289600.3291030},
abstract = {Extracting entities and their relations from text is an important task for understanding massive text corpora. Open information extraction (IE) systems mine relation tuples (i.e., entity arguments and a predicate string to describe their relation) from sentences. These relation tuples are not confined to a predefined schema for the relations of interests. However, current Open IE systems focus on modeling local context information in a sentence to extract relation tuples, while ignoring the fact that global statistics in a large corpus can be collectively leveraged to identify high-quality sentence-level extractions. In this paper, we propose a novel Open IE system, called ReMine, which integrates local context signals and global structural signals in a unified, distant-supervision framework. Leveraging facts from external knowledge bases as supervision, the new system can be applied to many different domains to facilitate sentence-level tuple extractions using corpus-level statistics. Our system operates by solving a joint optimization problem to unify (1) segmenting entity/relation phrases in individual sentences based on local context; and (2) measuring the quality of tuples extracted from individual sentences with a translating-based objective. Learning the two subtasks jointly helps correct errors produced in each subtask so that they can mutually enhance each other. Experiments on two real-world corpora from different domains demonstrate the effectiveness, generality, and robustness of ReMine when compared to state-of-the-art open IE systems.},
booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
pages = {42–50},
numpages = {9},
keywords = {distant supervision, entity recognition, open information extraction, relation extraction, weakly-supervised learning},
location = {Melbourne VIC, Australia},
series = {WSDM '19}
}

@inproceedings{10.1145/3077136.3080803,
author = {Jameel, Shoaib and Bouraoui, Zied and Schockaert, Steven},
title = {MEmbER: Max-Margin Based Embeddings for Entity Retrieval},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080803},
doi = {10.1145/3077136.3080803},
abstract = {We propose a new class of methods for learning vector space embeddings of entities. While most existing methods focus on modelling similarity, our primary aim is to learn embeddings that are interpretable, in the sense that query terms have a direct geometric representation in the vector space. Intuitively, we want all entities that have some property (i.e. for which a given term is relevant) to be located in some well-defined region of the space. This is achieved by imposing max-margin constraints that are derived from a bag-of-words representation of the entities. The resulting vector spaces provide us with a natural vehicle for identifying entities that have a given property (or ranking them according to how much they have the property), and conversely, to describe what a given set of entities have in common. As we show in our experiments, our models lead to a substantially better performance in a range of entity-oriented search tasks, such as list completion and entity ranking.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {783–792},
numpages = {10},
keywords = {entity embedding, entity ranking, list completion, maximum margin},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inbook{10.1145/2915031.2915035,
title = {Text Data Understanding},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915035},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@article{10.1145/3185663,
author = {Huang, Jizhou and Ding, Shiqiang and Wang, Haifeng and Liu, Ting},
title = {Learning to Recommend Related Entities With Serendipity for Web Search Users},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3185663},
doi = {10.1145/3185663},
abstract = {Entity recommendation, providing entity suggestions to assist users in discovering interesting information, has become an indispensable feature of today’s Web search engine. However, the majority of existing entity recommendation methods are not designed to boost the performance in terms of serendipity, which also plays an important role in the appreciation of users for a recommendation system. To keep users engaged, it is important to take into account serendipity when building an entity recommendation system. In this article, we propose a learning to recommend framework that consists of two components: related entity finding and candidate entity ranking. To boost serendipity performance, three different sets of features that correlate with the three aspects of serendipity are employed in the proposed framework. Extensive experiments are conducted on large-scale, real-world datasets collected from a widely used commercial Web search engine. The experiments show that our method significantly outperforms several strong baseline methods. An analysis on the impact of features reveals that the set of interestingness features is the most powerful feature set, and the set of unexpectedness features can significantly contribute to recommendation effectiveness. In addition, online controlled experiments conducted on a commercial Web search engine demonstrate that our method can significantly improve user engagement against multiple baseline methods. This further confirms the effectiveness of the proposed framework.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {25},
numpages = {22},
keywords = {Serendipity, Web search, entity recommendation, recommender system, serendipitous entities, serendipitous recommendations}
}

@inproceedings{10.1145/3394486.3403244,
author = {Huang, Jiaxin and Xie, Yiqing and Meng, Yu and Zhang, Yunyi and Han, Jiawei},
title = {CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and Relation Transferring},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403244},
doi = {10.1145/3394486.3403244},
abstract = {Taxonomy is not only a fundamental form of knowledge representation, but also crucial to vast knowledge-rich applications, such as question answering and web search. Most existing taxonomy construction methods extract hypernym-hyponym entity pairs to organize a "universal" taxonomy. However, these generic taxonomies cannot satisfy user's specific interest in certain areas and relations. Moreover, the nature of instance taxonomy treats each node as a single word, which has low semantic coverage for people to fully understand. In this paper, we propose a method for seed-guided topical taxonomy construction, which takes a corpus and a seed taxonomy described by concept names as input, and constructs a more complete taxonomy based on user's interest, wherein each node is represented by a cluster of coherent terms. Our framework, CoRel, has two modules to fulfill this goal. A relation transferring module learns and transfers the user's interested relation along multiple paths to expand the seed taxonomy structure in width and depth. A concept learning module enriches the semantics of each concept node by jointly embedding the taxonomy and text. Comprehensive experiments conducted on real-world datasets show that CoRel generates high-quality topical taxonomies and outperforms all the baselines significantly.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1928–1936},
numpages = {9},
keywords = {relation extraction, semantic computing, taxonomy construction, topic discovery},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1145/3313873,
author = {Yang, Xiaoshan and Xu, Changsheng},
title = {Image Captioning by Asking Questions},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3313873},
doi = {10.1145/3313873},
abstract = {Image captioning and visual question answering are typical tasks that connect computer vision and natural language processing. Both of them need to effectively represent the visual content using computer vision methods and smoothly process the text sentence using natural language processing skills. The key problem of these two tasks is to infer the target result based on the interactive understanding of the word sequence and the image. Though they practically use similar algorithms, they are studied independently in the past few years. In this article, we attempt to exploit the mutual correlation between these two tasks. We propose the first VQA-improved image-captioning method that transfers the knowledge learned from the VQA corpora to the image-captioning task. A VQA model is first pretrained on image--question--answer instances. Then, the pretrained VQA model is used to extract VQA-grounded semantic representations according to selected free-form open-ended visual question--answer pairs. The VQA-grounded features are complementary to the visual features, because they interpret images from a different perspective. We incorporate the VQA model into the image-captioning model by adaptively fusing the VQA-grounded feature and the attended visual feature. We show that such simple VQA-improved image-captioning (VQA-IIC) models perform better than conventional image-captioning methods on large-scale public datasets.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {55},
numpages = {19},
keywords = {Image captioning, attention networks, visual question answering}
}

@inproceedings{10.1145/3331184.3331257,
author = {Dietz, Laura},
title = {ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331257},
doi = {10.1145/3331184.3331257},
abstract = {Related work has demonstrated the helpfulness of utilizing information about entities in text retrieval; here we explore the converse: Utilizing information about text in entity retrieval. We model the relevance of Entity-Neighbor-Text (ENT) relations to derive a learning-to-rank-entities model.We focus on the task of retrieving (multiple) relevant entities in response to a topical information need such as "Zika fever". The ENT Rank model is designed to exploit semi-structured knowledge resources such as Wikipedia for entity retrieval. The ENT Rank model combines (1) established features of entity-relevance, with (2) information from neighboring entities (co-mentioned or mentioned-on-page) through (3) relevance scores of textual contexts through traditional retrieval models such as BM25 and RM3.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {215–224},
numpages = {10},
keywords = {context, edge weight prediction, entity links, entity retrieval, neighbor-relations},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3196321.3196335,
author = {Zhou, Cheng and Li, Bin and Sun, Xiaobing and Guo, Hongjing},
title = {Recognizing software bug-specific named entity in software bug repository},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196335},
doi = {10.1145/3196321.3196335},
abstract = {Software bug issues are unavoidable in software development and maintenance. In order to manage bugs effectively, bug tracking systems are developed to help to record, manage and track the bugs of each project. The rich information in the bug repository provides the possibility of establishment of entity-centric knowledge bases to help understand and fix the bugs. However, existing named entity recognition (NER) systems deal with text that is structured, formal, well written, with a good grammatical structure and few spelling errors, which cannot be directly used for bug-specific named entity recognition. For bug data, they are free-form texts, which include a mixed language studded with code, abbreviations and software-specific vocabularies. In this paper, we summarize the characteristics of bug entities, propose a classification method for bug entities, and build a baseline corpus on two open source projects (Mozilla and Eclipse). On this basis, we propose an approach for bug-specific entity recognition called BNER with the Conditional Random Fields (CRF) model and word embedding technique. An empirical study is conducted to evaluate the accuracy of our BNER technique, and the results show that the two designed baseline corpus are suitable for bug-specific named entity recognition, and our BNER approach is effective on cross-projects NER.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {108–119},
numpages = {12},
keywords = {CRF model, named entity recognition, software bug, software bug corpus, word embedding},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inbook{10.1145/2915031.2915055,
title = {Index},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915055},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inbook{10.1145/2915031.2915037,
title = {Overview of Text Data Access},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915037},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3302425.3302484,
author = {Cao, Juan and Gong, Junpeng and Zhang, Pengzhou},
title = {Open-Domain Table-to-Text Generation based on Seq2seq},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302484},
doi = {10.1145/3302425.3302484},
abstract = {Table-to-text generation involves using natural language to describe a table which has formal structure and valuable information. Open-domain table-to-text especially refers to table-to-text generation for open domain. This paper introduces a theme model based on seq2seq for open-domain table-to-text generation. To deal with the problem of out-of-vocabulary and make the most of the internal correlation within table and the relevance between table and text, this study adopts an improved encoder-decoder approach and a method associating table and text. In addition, this paper improves the beam search method for the inference of the model. The model is experimented on WIKITABLETEXT, and improves the current state-of-the-art BLEU-4 score from 38.23 to 38.71.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {72},
numpages = {5},
keywords = {Beam search, Open-domain, Seq2seq, Table-to-text generation},
location = {Sanya, China},
series = {ACAI '18}
}

@inproceedings{10.1145/3269206.3271668,
author = {Van Gysel, Christophe and de Rijke, Maarten and Kanoulas, Evangelos},
title = {Mix 'n Match: Integrating Text Matching and Product Substitutability within Product Search},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271668},
doi = {10.1145/3269206.3271668},
abstract = {Two products are substitutes if both can satisfy the same consumer need. Intrinsic incorporation of product substitutability - where substitutability is integrated within latent vector space models - is in contrast to the extrinsic re-ranking of result lists. The fusion of text matching and product substitutability objectives allows latent vector space models to mix and match regularities contained within text descriptions and substitution relations. We introduce a method for intrinsically incorporating product substitutability within latent vector space models for product search that are estimated using gradient descent; it integrates flawlessly with state-of-the-art vector space models. We compare our method to existing methods for incorporating structural entity relations, where product substitutability is incorporated extrinsically by re-ranking. Our method outperforms the best extrinsic method on four benchmarks. We investigate the effect of different levels of text matching and product similarity objectives, and provide an analysis of the effect of incorporating product substitutability on product search ranking diversity. Incorporating product substitutability information improves search relevance at the cost of diversity.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1373–1382},
numpages = {10},
keywords = {entity similarity, latent vector space models, product search},
location = {Torino, Italy},
series = {CIKM '18}
}

@article{10.1145/3368960,
author = {Sun, Xiao and Li, Jia and Wei, Xing and Li, Changliang and Tao, Jianhua},
title = {Emotional Conversation Generation Based on a Bayesian Deep Neural Network},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3368960},
doi = {10.1145/3368960},
abstract = {The field of conversation generation using neural networks has attracted increasing attention from researchers for several years. However, traditional neural language models tend to generate a generic reply with poor semantic logic and no emotion. This article proposes an emotional conversation generation model based on a Bayesian deep neural network that can generate replies with rich emotions, clear themes, and diverse sentences. The topic and emotional keywords of the replies are pregenerated by introducing commonsense knowledge in the model. The reply is divided into multiple clauses, and then a multidimensional generator based on the transformer mechanism proposed in this article is used to iteratively generate clauses from two dimensions: sentence granularity and sentence structure. Subjective and objective experiments prove that compared with existing models, the proposed model effectively improves the semantic logic and emotional accuracy of replies. This model also significantly enhances the diversity of replies, largely overcoming the shortcomings of traditional models that generate safe replies.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {8},
numpages = {24},
keywords = {Bayesian neural network, Emotional conversation generation, affective computing, deep learning, natural language processing}
}

@inbook{10.1145/2915031.2915033,
title = {Introduction},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915033},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3097983.3098200,
author = {Ahmed, Amr and Long, James and Silva, Daniel and Wang, Yuan},
title = {A Practical Algorithm for Solving the Incoherence Problem of Topic Models In Industrial Applications},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098200},
doi = {10.1145/3097983.3098200},
abstract = {Topic models are often applied in industrial settings to discover user profiles from activity logs where documents correspond to users and words to complex objects such as web sites and installed apps. Standard topic models ignore the content-based similarity structure between these objects largely because of the inability of the Dirichlet prior to capture such side information of word-word correlation. Several approaches were proposed to replace the Dirichlet prior with more expressive alternatives. However, this added expressivity comes with a heavy premium: inference becomes intractable and sparsity is lost which renders these alternatives not suitable for industrial scale applications. In this paper we take a radically different approach to incorporating word-word correlation in topic models by applying this side information at the posterior level rather than at the prior level. We show that this choice preserves sparsity and results in a graph-based sampler for LDA whose computational complexity is asymptotically on bar with the state of the art Alias base sampler for LDA cite{aliasLDA}. We illustrate the efficacy of our approach over real industrial datasets that span up to billion of users, tens of millions of words and thousands of topics. To the best of our knowledge, our approach provides the first practical and scalable solution to this important problem.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1713–1721},
numpages = {9},
keywords = {big data, interpretable models, knowledge representation, latent variable models, topic models, user modeling},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inbook{10.1145/2915031.2915054,
title = {References},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915054},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3340555.3356100,
author = {Li, Hao and Liu, Chen and Zhu, Su and Yu, Kai},
title = {Robust Spoken Language Understanding with Acoustic and Domain Knowledge},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3356100},
doi = {10.1145/3340555.3356100},
abstract = {Spoken language understanding (SLU) converts user utterances into structured semantic forms. There are still two main issues for SLU: robustness to ASR-errors and the data sparsity of new and extended domains. In this paper, we propose a robust SLU system by leveraging both acoustic and domain knowledge. We extract audio features by training ASR models on a large number of utterances without semantic annotations. For exploiting domain knowledge, we design lexicon features from the domain ontology and propose an error elimination algorithm to help predicted values recovered from ASR-errors. The results of CATSLU challenge show that our systems can outperform all of the other teams across four domains.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {531–535},
numpages = {5},
keywords = {Robustness, Spoken Language Understanding},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.5555/3291291.3291311,
author = {Boyer, John M.},
title = {Natural language question answering in the financial domain},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper describes a natural language question answering system focused on answering financial domain questions using a daily updated corpus of financial reports. Financial entity types of interest included company stocks, country bonds, currencies, industries, commodities, and diversified assets. Financial questions of interest included explanatory and factual questions about entities as well as financial outlook for entities.An important architectural divergence emerged between the approach required for answering financial outlook questions versus the approach for answering other financial information questions. The financial domain focus also introduced additional challenges to open domain natural language processing that were addressed in the areas of document ingestion, question classification accuracy, question analysis techniques, speed of machine learning, answer ranking by linguistic confidence versus temporality, and system accuracy assessment.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {189–200},
numpages = {12},
keywords = {financial domain, financial information retrieval, financial sentiment analysis, question analysis, question answering systems, question classification, text analytics},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3366423.3380158,
author = {Huang, Yen-Hao and Liu, Ting-Wei and Lee, Ssu-Rui and Calderon Alvarado, Fernando Henrique and Chen, Yi-Shin},
title = {Conquering Cross-source Failure for News Credibility: Learning Generalizable Representations beyond Content Embedding},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380158},
doi = {10.1145/3366423.3380158},
abstract = {False information on the Internet has caused severe damage to society. Researchers have proposed methods to determine the credibility of news and have obtained good results. As different media sources (publishers) have different content generators (writers) and may focus on different topics or aspects, the word/topic distribution for each media source is divergent from others. We expose a challenge in the generalizability of existing content-based methods to perform consistently when applied to news from media sources non-existing in the training set, namely the cross-source failure. A cross-source setting can cause a decrease beyond in accuracy for current methods; content-sensitive features are considered one of the major causes of cross-source failure for a content-based approach. To overcome this challenge, we propose a syntactic network for news credibility (SYNC), which focuses on function words and syntactic structure to learn generalizable representations for news credibility and further reinforce the cross-source robustness for different media. Experiments with cross-validation on 194 real-world media sources showed that the proposed method could learn the generalizable features and outperformed the state-of-the-art methods on unseen media sources. Extensive analysis on the embedding feature representation represents a strength of the proposed method compared to current content embedding feature approaches. We envision that the proposed method is more robust for real-life application with SYNC on account of its good generalizability.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {774–784},
numpages = {11},
keywords = {cross-source failure, fake news, generalizability, neural network},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3374217,
author = {Zhang, Wei Emma and Sheng, Quan Z. and Alhazmi, Ahoud and Li, Chenliang},
title = {Adversarial Attacks on Deep-learning Models in Natural Language Processing: A Survey},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3374217},
doi = {10.1145/3374217},
abstract = {With the development of high computational devices, deep neural networks (DNNs), in recent years, have gained significant popularity in many Artificial Intelligence (AI) applications. However, previous efforts have shown that DNNs are vulnerable to strategically modified samples, named adversarial examples. These samples are generated with some imperceptible perturbations, but can fool the DNNs to give false predictions. Inspired by the popularity of generating adversarial examples against DNNs in Computer Vision (CV), research efforts on attacking DNNs for Natural Language Processing (NLP) applications have emerged in recent years. However, the intrinsic difference between image (CV) and text (NLP) renders challenges to directly apply attacking methods in CV to NLP. Various methods are proposed addressing this difference and attack a wide range of NLP applications. In this article, we present a systematic survey on these works. We collect all related academic works since the first appearance in 2017. We then select, summarize, discuss, and analyze 40 representative works in a comprehensive way. To make the article self-contained, we cover preliminary knowledge of NLP and discuss related seminal works in computer vision. We conclude our survey with a discussion on open issues to bridge the gap between the existing progress and more robust adversarial attacks on NLP DNNs.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {24},
numpages = {41},
keywords = {Deep neural networks, adversarial examples, natural language processing, textual data}
}

@inproceedings{10.1145/3383583.3398521,
author = {Zhang, Jinsong and Guo, Chun and Liu, Xiaozhong},
title = {Characterize and Evaluate Scientific Domain and Domain Context Knowledge Map},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398521},
doi = {10.1145/3383583.3398521},
abstract = {Domain knowledge map, a.k.a., scholarly network, construction as an important method can describe the significant characters of a selected domain. In this research, we will address three fundamental problems for scholarly network generation. Firstly, two different methods will be investigated to associate keywords on the graph: Co-occur Domain Distance and Citation Probability Distribution Distance. Secondly, this paper will construct domain (core journals and conference proceedings) knowledge and domain referral (domain citation) scholarly networks, and propose a novel method to integrate those graphs by optimizing the nodes and their linkage. Finally, the paper will propose an innovative method to evaluate the accuracy and coverage of scholarly networks based on training keyword oriented Labeled-LDA model and validate different domain or domain referral graphs.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {187–196},
numpages = {10},
keywords = {domain characterization, domain context, knowledge map, scientific domain},
location = {Virtual Event, China},
series = {JCDL '20}
}

@article{10.1145/3345317,
author = {Folt\'{y}nek, Tom\'{a}\v{s} and Meuschke, Norman and Gipp, Bela},
title = {Academic Plagiarism Detection: A Systematic Literature Review},
year = {2019},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3345317},
doi = {10.1145/3345317},
abstract = {This article summarizes the research on computational methods to detect academic plagiarism by systematically reviewing 239 research papers published between 2013 and 2018. To structure the presentation of the research contributions, we propose novel technically oriented typologies for plagiarism prevention and detection efforts, the forms of academic plagiarism, and computational plagiarism detection methods. We show that academic plagiarism detection is a highly active research field. Over the period we review, the field has seen major advances regarding the automated detection of strongly obfuscated and thus hard-to-identify forms of academic plagiarism. These improvements mainly originate from better semantic text analysis methods, the investigation of non-textual content features, and the application of machine learning. We identify a research gap in the lack of methodologically thorough performance evaluations of plagiarism detection systems. Concluding from our analysis, we see the integration of heterogeneous analysis methods for textual and non-textual content features using machine learning as the most promising area for future research contributions to improve the detection of academic plagiarism further.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {112},
numpages = {42},
keywords = {Plagiarism detection, literature review, machine learning, semantic analysis, text-matching software}
}

@inproceedings{10.1145/3131704.3131713,
author = {Lin, Zeqi and Zhao, Junfeng and Zou, Yanzhen and Xie, Bing},
title = {Document Distance Estimation via Code Graph Embedding},
year = {2017},
isbn = {9781450353137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131704.3131713},
doi = {10.1145/3131704.3131713},
abstract = {Accurately representing the distance between two documents (i.e. pieces of textual information extracted from various software artifacts) has far-reaching applications in many automated software engineering approaches, such as concept location, bug location and traceability link recovery. This is a challenging task, since documents containing different words may have similar semantic meanings. In this paper, we propose a novel document distance estimation approach. This approach captures latent semantic associations between documents through analyzing structural information in software source code: first, we embed code elements as points in a shared representation space according to structural dependencies between them; then, we represent documents as weighted point clouds of code elements in the representation space and reduce the distance between two documents to an earth mover's distance transportation problem. We define a document classification task in StackOverflow dataset to evaluate the effectiveness of our approach. The empirical evaluation results show that our approach outperforms several state-of-the-art approaches.},
booktitle = {Proceedings of the 9th Asia-Pacific Symposium on Internetware},
articleno = {11},
numpages = {10},
keywords = {code graph, document distance, multi-relational data embedding},
location = {Shanghai, China},
series = {Internetware '17}
}

@inproceedings{10.1145/3394486.3403237,
author = {Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Chang, Kai-Wei and Sun, Yizhou},
title = {GPT-GNN: Generative Pre-Training of Graph Neural Networks},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403237},
doi = {10.1145/3394486.3403237},
abstract = {Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabelled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of graph generation into two components: 1) attribute generation and 2) edge generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale open academic graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks?},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1857–1867},
numpages = {11},
keywords = {generative pre-training, gnn pre-training, graph neural networks, graph representation learning, network embedding},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3308558.3313578,
author = {Zhao, Chen and He, Yeye},
title = {Auto-EM: End-to-end Fuzzy Entity-Matching using Pre-trained Deep Models and Transfer Learning},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313578},
doi = {10.1145/3308558.3313578},
abstract = {Entity matching (EM), also known as entity resolution, fuzzy join, and record linkage, refers to the process of identifying records corresponding to the same real-world entities from different data sources. It is an important and long-standing problem in data integration and data mining. So far progresses have been made mainly in the form of model improvements, where models with better accuracy are developed when large amounts of training data is available. In real-world applications we find that advanced approaches can often require too many labeled examples that is expensive to obtain, which has become a key obstacle to wider adoption. We in this work take a different tack, proposing a transfer-learning approach to EM, leveraging pre-trained EM models from large-scale, production knowledge bases (KB). Specifically, for each entity-type in KB, (e.g., location, organization, people, etc.), we use rich synonymous names of known entities in the KB as training data, to pre-train type-detection and EM models for each type, using a novel hierarchical neural network architecture we develop. Given a new EM task, with little or no training data, we can either fine-tune or directly leverage pre-trained EM models, to build end-to-end, high-quality EM systems. Experiments on a variety of real EM tasks suggest that the pre-trained approach is effective and outperforms existing EM methods.1.},
booktitle = {The World Wide Web Conference},
pages = {2413–2424},
numpages = {12},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3269206.3269245,
author = {Dargahi Nobari, Arash and Askari, Arian and Hasibi, Faegheh and Neshati, Mahmood},
title = {Query Understanding via Entity Attribute Identification},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269245},
doi = {10.1145/3269206.3269245},
abstract = {Understanding searchers' queries is an essential component of semantic search systems. In many cases, search queries involve specific attributes of an entity in a knowledge base (KB), which can be further used to find query answers. In this study, we aim to move forward the understanding of queries by identifying their related entity attributes from a knowledge base. To this end, we introduce the task of entity attribute identification and propose two methods to address it: (i) a model based on Markov Random Field, and (ii) a learning to rank model. We develop a human annotated test collection and show that our proposed methods can bring significant improvements over the baseline methods.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1759–1762},
numpages = {4},
keywords = {entity attributes, entity search, query understanding},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3299869.3314043,
author = {Korn, Flip and Wang, Xuezhi and Wu, You and Yu, Cong},
title = {Automatically Generating Interesting Facts from Wikipedia Tables},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3314043},
doi = {10.1145/3299869.3314043},
abstract = {Modern search engines provide contextual information surrounding query entities beyond ten blue links in the form of information cards. Among the various attributes displayed about entities there has been recent interest in providing fun facts. Obtaining such trivia at a large scale is, however, non-trivial: hiring professional content creators is expensive and extracting statements from the Web is prone to uninteresting, out-of-context and/or unreliable facts.In this paper we show how fun facts can be mined from superlative tables in Wikipedia, whose rows are ranked according to some statistics, to provide a large volume of reliable and interesting content. We employ a template-based approach to semi-automatically generate natural language statements as fun facts. We show how to bootstrap and streamline the process for faster and cheaper task completion. However, the content contained in these tables is dynamic. Therefore, we address the problem of automatically maintaining the pairing of templates to tables as the tables are updated over time. Fun facts produced by our work is now part of Google's production search results.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {349–361},
numpages = {13},
keywords = {dynamic maintenance, fun facts generation, superlative tables},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@inbook{10.1145/2915031.2915032,
title = {Preface},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915032},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3397271.3401173,
author = {Zheng, Jianming and Cai, Fei and Chen, Honghui},
title = {Incorporating Scenario Knowledge into A Unified Fine-tuning Architecture for Event Representation},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401173},
doi = {10.1145/3397271.3401173},
abstract = {Given an occurred event, human can easily predict the next event or reason the preceding event, yet which is difficult for machine to perform such event reasoning. Event representation bridges the connection and targets to model the process of event reasoning as a machine-readable format, which then can support a wide range of applications in information retrieval, e.g., question answering and information extraction. Existing work mainly resorts to a joint training to integrate all levels of training loss in event chains by a simple loss summation, which is easily trapped into a local optimum. In addition, the scenario knowledge in event chains is not well investigated for event representation. In this paper, we propose a unified fine-tuning architecture, incorporated with scenario knowledge for event representation, i.e., UniFA-S, which mainly consists of a unified fine-tuning architecture (UniFA) and a scenario-level variational auto-encoder (S-VAE). In detail, UniFA employs a multi-step fine-tuning to integrate all levels of training and S-VAE applies a stochastic variable to implicitly represent the scenario-level knowledge. We evaluate our proposal from two aspects, i.e., the representation and inference abilities. For the representation ability, our ensemble model UniFA-S can beat state-of-the-art baselines for two similarity tasks. For the inference ability, UniFA-S can outperform the best baseline, achieving 4.1%-8.2% improvements in terms of accuracy for various inference tasks.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {249–258},
numpages = {10},
keywords = {event representation, fine-tuning, pre-training, scenario knowledge},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@inbook{10.1145/2915031.2915052,
title = {Toward A Unified System for Text Management and Analysis},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915052},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3331184.3331254,
author = {Chen, Xu and Chen, Hanxiong and Xu, Hongteng and Zhang, Yongfeng and Cao, Yixin and Qin, Zheng and Zha, Hongyuan},
title = {Personalized Fashion Recommendation with Visual Explanations based on Multimodal Attention Network: Towards Visually Explainable Recommendation},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331254},
doi = {10.1145/3331184.3331254},
abstract = {Fashion recommendation has attracted increasing attention from both industry and academic communities. This paper proposes a novel neural architecture for fashion recommendation based on both image region-level features and user review information. Our basic intuition is that: for a fashion image, not all the regions are equally important for the users, i.e., people usually care about a few parts of the fashion image. To model such human sense, we learn an attention model over many pre-segmented image regions, based on which we can understand where a user is really interested in on the image, and correspondingly, represent the image in a more accurate manner. In addition, by discovering such fine-grained visual preference, we can visually explain a recommendation by highlighting some regions of its image. For better learning the attention model, we also introduce user review information as a weak supervision signal to collect more comprehensive user preference. In our final framework, the visual and textual features are seamlessly coupled by a multimodal attention network. Based on this architecture, we can not only provide accurate recommendation, but also can accompany each recommended item with novel visual explanations. We conduct extensive experiments to demonstrate the superiority of our proposed model in terms of Top-N recommendation, and also we build a collectively labeled dataset for evaluating our provided visual explanations in a quantitative manner.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {765–774},
numpages = {10},
keywords = {collaborative filtering, fashion recommendation, recommender system},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3290688.3290710,
author = {Huang, Haojie and Wong, Raymond},
title = {Web Service based Intelligent Search on Legal Documents},
year = {2019},
isbn = {9781450366038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290688.3290710},
doi = {10.1145/3290688.3290710},
abstract = {Web services such as RESTful APIs provide high flexibility for knowledge base systems in different domains. To apply it to the legal aspect, we can obtain much data of the case law efficiently enabling us to relate one case to another easily and even to compare the details of a plenty of the case laws simultaneously. Having said that, to ensure the performance of the web services and the accuracy of the data sourcing from them is onerous in the consideration of the backend system for the web services and relevant search engine.In this paper, we introduce a web service for the legal knowledge, LegalKB, enhancing a concept search, and implement a method, WMD ranking method, to speed up the data search through the enhanced system queries.On top of this, we propose a method to automate the optimisation of the system parameters to ensure that the system queries run in the most optimal manner - integration of multiple machine learning methods into our web service to facilitate third-party applications to interface with our web service enlarging the knowledge base to a great extent.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {50},
numpages = {9},
keywords = {Information extraction, Knowledge base, Legal domain knowledge base, Web service},
location = {Sydney, NSW, Australia},
series = {ACSW '19}
}

@inbook{10.1145/2915031.2915042,
title = {Web Search},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915042},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inbook{10.1145/2915031.2915048,
title = {Text Summarization},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915048},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3041048.3041054,
author = {Turner, Ronald C.},
title = {Proposed Model for Natural Language ABAC Authoring},
year = {2017},
isbn = {9781450349109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3041048.3041054},
doi = {10.1145/3041048.3041054},
abstract = {Authorization policy authoring has required tools from the start. With access policy governance now an executive-level responsibility, it is imperative that such a tool expose the policy to business users' with little or no IT intervention-as natural language. NIST SP 800-162 [1] first prescribes natural language policies (NLPs) as the preferred expression of policy and then implicitly calls for automated translation of NLP to machine-executable code. This paper therefore proposes an interoperable model for the NLP's human expression. It furthermore documents the research and development of a tool set for end-to-end authoring and translation. This R&amp;D journey-focusing constantly on end users' has debunked certain myths, has responded to steadily increasing market sophistication, has applied formal disciplines (e.g. ontologies, grammars and compiler design) and has motivated an informal demonstration of autonomic code generation. The lessons learned should be of practical value to the entire ABAC community. The research in progress' increasingly complex policies, proactive rule analytics, and expanded NLP authoring language support will require collaboration with an ever-expanding technical community from industry and academia.},
booktitle = {Proceedings of the 2nd ACM Workshop on Attribute-Based Access Control},
pages = {61–72},
numpages = {12},
keywords = {ABAC, RDF analytics, SPARQL queries, XACML authoring tool, business rules, natural language policies, policy semantics},
location = {Scottsdale, Arizona, USA},
series = {ABAC '17}
}

@inbook{10.1145/2915031.2915039,
title = {Feedback},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915039},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inbook{10.1145/2915031.2915044,
title = {Overview of Text Data Analysis},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915044},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3340531.3411937,
author = {Xu, Feifei and Wang, Xinpeng and Ma, Yunpu and Tresp, Volker and Wang, Yuyi and Zhou, Shanlin and Du, Haizhou},
title = {Controllable Multi-Character Psychology-Oriented Story Generation},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411937},
doi = {10.1145/3340531.3411937},
abstract = {Story generation, which aims to generate a long and coherent story automatically based on the title or an input sentence, is an important research area in the field of natural language generation. There is relatively little work on story generation with appointed emotions. Most existing works focus on using only one specific emotion to control the generation of a whole story and ignore the emotional changes in the characters in the course of the story. In our work, we aim to design an emotional line for each character that considers multiple emotions common in psychological theories, with the goal of generating stories with richer emotional changes in the characters. To the best of our knowledge, this work is first to focuses on characters' emotional lines in story generation. We present a novel model-based attention mechanism that we call SoCP (Storytelling of multi-Character Psychology). We show that the proposed model can generate stories considering the changes in the psychological state of different characters. To take into account the particularity of the model, in addition to commonly used evaluation indicators(BLEU, ROUGE, etc.), we introduce the accuracy rate of psychological state control as a novel evaluation metric. The new indicator reflects the effect of the model on the psychological state control of story characters. Experiments show that with SoCP, the generated stories follow the psychological state for each character according to both automatic and human evaluations.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1675–1684},
numpages = {10},
keywords = {attention, character, psychology, story generation},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inbook{10.1145/2915031.2915036,
title = {MeTA : A Unified Toolkit for Text Data Management and Analysis},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915036},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3357384.3358057,
author = {Xiao, Teng and Ren, Jiaxin and Meng, Zaiqiao and Sun, Huan and Liang, Shangsong},
title = {Dynamic Bayesian Metric Learning for Personalized Product Search},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358057},
doi = {10.1145/3357384.3358057},
abstract = {In this paper, we study the problem of personalized product search under streaming scenarios. We address the problem by proposing a Dynamic Bayesian Metric Learning model, abbreviated as DBML, which can collaboratively track the evolutions of latent semantic representations of different categories of entities (i.e., users, products and words) over time in a joint metric space. In particular, unlike previous work using inner-product metric to model the affinities between entities, our DBML is a novel probabilistic metric learning approach that is able to avoid the contradicts, keep the triangle inequality in the latent space, and correctly utilize implicit feedbacks. For inferring dynamic embeddings of the entities, we propose a scalable online inference algorithm, which can jointly learn the latent representations of entities and smooth their changes across time, based on amortized inference. The inferred dynamic semantic representations of entities collaboratively inferred in a unified form by our DBML can benefit not only for improving personalized product search, but also for capturing the affinities between users, products and words. Experimental results on large datasets over a number of applications demonstrate that our DBML outperforms the state-of-the-art algorithms, and can effectively capture the evolutions of semantic representations of different categories of entities over time.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1693–1702},
numpages = {10},
keywords = {metric learning, online learning, probabilistic model, product search},
location = {Beijing, China},
series = {CIKM '19}
}

@inbook{10.1145/2915031.2915053,
title = {Appendixes},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915053},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3162957.3162984,
author = {Zhang, Chao and Song, Hui and Liu, Zhenyu},
title = {MiSAS: a multi-domain feature-level sentiment analysis system on micro-blog},
year = {2017},
isbn = {9781450353656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3162957.3162984},
doi = {10.1145/3162957.3162984},
abstract = {Big data from micro-blog has been an important access to social groups' psychology, market feedback and so on. Unlike the review corpus which is usually related to the specific object (e.g. a product), the micro-blog content covers the opinion of many domains. It is less useful to extract the fine-grained feature-level opinion target without detect the domain. This paper proposed a systematic feature-level sentiment analysis approach on Micro-blog that recognize data related to the interesting topic automatically. Working with the big micro-blog data we figure out valuable text features to train the opinion targets extraction and sentimental polarity detection models that achieve better multi-domain adaption. We implement the MiSAS system, which crawls micro-blog raw data, outputs opinion targets and orientation summarization on the giving domains, offering valuable analytical tool for practical applications.},
booktitle = {Proceedings of the 3rd International Conference on Communication and Information Processing},
pages = {14–18},
numpages = {5},
keywords = {MiSAS, micro-blog, opinion target extraction, sentiment analysis, sentimental polarity detection},
location = {Tokyo, Japan},
series = {ICCIP '17}
}

@inproceedings{10.1145/3366423.3380193,
author = {Rosset, Corbin and Xiong, Chenyan and Song, Xia and Campos, Daniel and Craswell, Nick and Tiwary, Saurabh and Bennett, Paul},
title = {Leading Conversational Search by Suggesting Useful Questions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380193},
doi = {10.1145/3366423.3380193},
abstract = {This paper studies a new scenario in conversational search, conversational question suggestion, which leads search engine users to more engaging experiences by suggesting interesting, informative, and useful follow-up questions. We first establish a novel evaluation metric, usefulness, which goes beyond relevance and measures whether the suggestions provide valuable information for the next step of a user’s journey, and construct a public benchmark for useful question suggestion. Then we develop two suggestion systems, a BERT based ranker and a GPT-2 based generator, both trained with novel weak supervision signals that convey past users’ search behaviors in search sessions. The weak supervision signals help ground the suggestions to users’ information-seeking trajectories: we identify more coherent and informative sessions using encodings, and then weakly supervise our models to imitate how users transition to the next state of search. Our offline experiments demonstrate the crucial role our “next-turn” inductive training plays in improving usefulness over a strong online system. Our online A/B test in Bing shows that our more useful question suggestions receive 8% more user clicks than the previous system.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1160–1170},
numpages = {11},
keywords = {Conversational Search, Question Suggestion, Usefulness},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inbook{10.1145/2915031.2915040,
title = {Search Engine Implementation},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915040},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inbook{10.1145/2915031.2915047,
title = {Text Categorization},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915047},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inbook{10.1145/2915031.2915043,
title = {Recommender Systems},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915043},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inbook{10.1145/2915031.2915034,
title = {Background},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915034},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3018661.3018687,
author = {Wang, Pengwei and Zhang, Yong and Ji, Lei and Yan, Jun and Jin, Lianwen},
title = {Concept Embedded Convolutional Semantic Model for Question Retrieval},
year = {2017},
isbn = {9781450346757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018661.3018687},
doi = {10.1145/3018661.3018687},
abstract = {The question retrieval, which aims to find similar questions of a given question, is playing pivotal role in various question answering (QA) systems. This task is quite challenging mainly on three aspects: lexical gap, polysemy and word order. In this paper, we propose a unified framework to simultaneously handle these three problems. We use word combined with corresponding concept information to handle the polysemous problem. The concept embedding and word embedding are learned at the same time from both context-dependent and context-independent view. The lexical gap problem is handled since the semantic information has been encoded into the embedding. Then, we propose to use a high-level feature embedded convolutional semantic model to learn the question embedding by inputting the concept embedding and word embedding without manually labeling training data. The proposed framework nicely represent the hierarchical structures of word information and concept information in sentences with their layer-by-layer composition and pooling. Finally, the framework is trained in a weakly-supervised manner on question answer pairs, which can be directly obtained without manually labeling. Experiments on two real question answering datasets show that the proposed framework can significantly outperform the state-of-the-art solutions.},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
pages = {395–403},
numpages = {9},
keywords = {concept embedding, question embedding, question retrieval},
location = {Cambridge, United Kingdom},
series = {WSDM '17}
}

@inbook{10.1145/2915031.2915046,
title = {Text Clustering},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915046},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@proceedings{10.1145/3308560,
title = {WWW '19: Companion Proceedings of The 2019 World Wide Web Conference},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, USA}
}

@article{10.1613/jair.1.11640,
author = {Ruder, Sebastian and Vuli\'{c}, Ivan and S\o{}gaard, Anders},
title = {A survey of cross-lingual word embedding models},
year = {2019},
issue_date = {May 2019},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {65},
number = {1},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11640},
doi = {10.1613/jair.1.11640},
abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {569–630},
numpages = {62}
}

@article{10.1145/3106745,
author = {Goodwin, Travis R. and Harabagiu, Sanda M.},
title = {Knowledge Representations and Inference Techniques for Medical Question Answering},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3106745},
doi = {10.1145/3106745},
abstract = {Answering medical questions related to complex medical cases, as required in modern Clinical Decision Support (CDS) systems, imposes (1) access to vast medical knowledge and (2) sophisticated inference techniques. In this article, we examine the representation and role of combining medical knowledge automatically derived from (a) clinical practice and (b) research findings for inferring answers to medical questions. Knowledge from medical practice was distilled from a vast Electronic Medical Record (EMR) system, while research knowledge was processed from biomedical articles available in PubMed Central. The knowledge automatically acquired from the EMR system took into account the clinical picture and therapy recognized from each medical record to generate a probabilistic Markov network denoted as a Clinical Picture and Therapy Graph (CPTG). Moreover, we represented the background of medical questions available from the description of each complex medical case as a medical knowledge sketch. We considered three possible representations of medical knowledge sketches that were used by four different probabilistic inference methods to pinpoint the answers from the CPTG. In addition, several answer-informed relevance models were developed to provide a ranked list of biomedical articles containing the answers. Evaluations on the TREC-CDS data show which of the medical knowledge representations and inference methods perform optimally. The experiments indicate an improvement of biomedical article ranking by 49% over state-of-the-art results.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {14},
numpages = {26},
keywords = {Clinical decision support, medical information retrieval, medical knowledge representation, medical question answering, probabilistic inference}
}

@inbook{10.1145/2915031.2915050,
title = {Opinion Mining and Sentiment Analysis},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915050},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@article{10.1145/3299986.3299990,
author = {Karmaker Santu, Shubhra Kanti and Geigle, Chase and Ferguson, Duncan and Cope, William and Kalantzis, Mary and Searsmith, Duane and Zhai, Chengxiang},
title = {SOFSAT: Towards a Setlike Operator based Framework for Semantic Analysis of Text},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3299986.3299990},
doi = {10.1145/3299986.3299990},
abstract = {As data reported by humans about our world, text data play a very important role in all data mining applications, yet how to develop a general text analysis system to sup- port all text mining applications is a difficult challenge. In this position paper, we introduce SOFSAT, a new frame- work that can support set-like operators for semantic analy- sis of natural text data with variable text representations. It includes three basic set-like operators|TextIntersect, Tex- tUnion, and TextDi erence|that are analogous to the cor- responding set operators intersection, union, and di erence, respectively, which can be applied to any representation of text data, and di erent representations can be combined via transformation functions that map text to and from any rep- resentation. Just as the set operators can be exibly com- bined iteratively to construct arbitrary subsets or supersets based on some given sets, we show that the correspond- ing text analysis operators can also be combined exibly to support a wide range of analysis tasks that may require di erent work ows, thus enabling an application developer to program" a text mining application by using SOFSAT as an application programming language for text analysis. We discuss instantiations and implementation strategies of the framework with some speci c examples, present ideas about how the framework can be implemented by exploit- ing/extending existing techniques, and provide a roadmap for future research in this new direction.},
journal = {SIGKDD Explor. Newsl.},
month = dec,
pages = {21–30},
numpages = {10},
keywords = {Intelligent Text Analysis, Semantic Analysis, Semantic Operator for Text, Text Mining}
}

@inbook{10.1145/2915031.2915045,
title = {Word Association Mining},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915045},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inbook{10.1145/2915031.2915051,
title = {Joint Analysis of Text and Structured Data},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915051},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3361242.3361251,
author = {Sun, Zhiyu and Peng, Fang and Guan, Junrui and Sun, Yanchun},
title = {An approach to helping developers learn open source projects based on machine learning},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361251},
doi = {10.1145/3361242.3361251},
abstract = {Developers usually learn excellent coding methods and design patterns by reading the code from well-known open-source projects, and participate in the development of open-source projects to enhance their programming capabilities. When developers have just joined an existing open-source project development, the first thing to do is to read and understand the project code. However, almost no project will maintain design documentations. Developers can only understand code according to user guide (mainly focus on how to use code but not on how to develop code) or brief code comments, which is relatively difficult for new developers. To help developers learn open-source projects more quickly, we propose an approach to helping developers learn open-source projects based on machine learning. First, we build a code structure graph for the project code by static analysis. Second, we implement a project entries recommendation approach based on clustering and machine learning to recommend project entries suitable for developers to read. Third, we implement a learning path recommendation algorithm. The algorithm recommends learning paths based on function nodes in the code structure graph selected by the developers, helps developers understand open-source projects better. In experiments, we select two famous c++ open-source projects, Lua and Memcache, as examples to perform project learning path recommendation. The experimental results show that our approach save a lot of time for developers to learn open-source projects while maintaining the accuracy of the recommendations.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {13},
numpages = {10},
keywords = {Code structure graph, Learning path recommendation, Machine Learning, Software reverse engineering},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inbook{10.1145/2915031.2915041,
title = {Search Engine Evaluation},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915041},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@proceedings{10.1145/3308558,
title = {WWW '19: The World Wide Web Conference},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to The Web Conference 2019. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3397271.3401130,
author = {Voskarides, Nikos and Li, Dan and Ren, Pengjie and Kanoulas, Evangelos and de Rijke, Maarten},
title = {Query Resolution for Conversational Search with Limited Supervision},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397271.3401130},
doi = {10.1145/3397271.3401130},
abstract = {In this work we focus on multi-turn passage retrieval as a crucial component of conversational search. One of the key challenges in multi-turn passage retrieval comes from the fact that the current turn query is often underspecified due to zero anaphora, topic change, or topic return. Context from the conversational history can be used to arrive at a better expression of the current turn query, defined as the task of query resolution. In this paper, we model the query resolution task as a binary term classification problem: for each term appearing in the previous turns of the conversation decide whether to add it to the current turn query or not. We propose QuReTeC (Query Resolution by Term Classification), a neural query resolution model based on bidirectional transformers. We propose a distant supervision method to automatically generate training data by using query-passage relevance labels. Such labels are often readily available in a collection either as human annotations or inferred from user interactions. We show that QuReTeC outperforms state-of-the-art models, and furthermore, that our distant supervision method can be used to substantially reduce the amount of human-curated data required to train QuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval architecture and demonstrate its effectiveness on the TREC CAsT dataset.},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {921–930},
numpages = {10},
keywords = {conversational search, query resolution},
location = {Virtual Event, China},
series = {SIGIR '20}
}

@article{10.1145/3402179,
author = {Uprety, Sagar and Gkoumas, Dimitris and Song, Dawei},
title = {A Survey of Quantum Theory Inspired Approaches to Information Retrieval},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3402179},
doi = {10.1145/3402179},
abstract = {Since 2004, researchers have been using the mathematical framework of quantum theory in information retrieval (IR). Quantum theory offers a generalized probability and logic framework. Such a framework has been shown to be capable of unifying the representation, ranking, and user cognitive aspects of IR, and helpful in developing more dynamic, adaptive, and context-aware IR systems. Although quantum-inspired IR is still a growing area, a wide array of work in different aspects of IR has been done and produced promising results. This article presents a survey of the research done in this area, aiming to show the landscape of the field and draw a road map of future directions.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {98},
numpages = {39},
keywords = {Information retrieval, quantum theory, quantum-inspired models}
}

@inproceedings{10.1145/3340555.3356099,
author = {Tan, Chaohong and Ling, Zhenhua},
title = {Multi-Classification Model for Spoken Language Understanding},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3356099},
doi = {10.1145/3340555.3356099},
abstract = {The spoken language understanding (SLU) is an important part of spoken dialogue system (SDS). In the paper, we focus on how to extract a set of act-slot-value tuples from users’ utterances in the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU). This paper adopts the pretrained BERT model to encode users’ utterances and builds multiple classifiers to get the required tuples. In our framework, finding acts and values of slots are recognized as classification tasks respectively. Such multi-task training is expected to help the encoder to get better understanding of the utterance. Since the system is built on the transcriptions given by automatic speech recognition (ASR), some tricks are applied to correct the errors of the tuples. We also found that using the minimum edit distance (MED) between results and candidates to rebuild the tuples was beneficial in our experiments.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {526–530},
numpages = {5},
keywords = {BERT, Spoken language understanding, classification, multi-task learning, text tagging},
location = {Suzhou, China},
series = {ICMI '19}
}

@article{10.1145/3274784.3274788,
author = {Culpepper, J. Shane and Diaz, Fernando and Smucker, Mark D.},
title = {Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018)},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3274784.3274788},
doi = {10.1145/3274784.3274788},
abstract = {The purpose of the Strategic Workshop in Information Retrieval in Lorne is to explore the long-range issues of the Information Retrieval field, to recognize challenges that are on - or even over - the horizon, to build consensus on some of the key challenges, and to disseminate the resulting information to the research community. The intent is that this description of open problems will help to inspire researchers and graduate students to address the questions, and will provide funding agencies data to focus and coordinate support for information retrieval research.},
journal = {SIGIR Forum},
month = aug,
pages = {34–90},
numpages = {57}
}

@inbook{10.1145/2915031.2915038,
title = {Retrieval Models},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915038},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1145/3325291.3325357,
author = {Alibadi, Zaid and Du, Mingzhe and Vidal, Jose M.},
title = {Using Pre-trained Embeddings to Detect the Intent of an Email},
year = {2019},
isbn = {9781450371735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325291.3325357},
doi = {10.1145/3325291.3325357},
abstract = {This research studies the problem of email overload and proposes a system that automatically detects whether the email is "to read" or "to do". The goal of our research is to test if we could automate both the features extraction and sentence classification phases by using word embedding. We propose to achieve this goal using a simple feature-based adaptation approach, where email's sentences are represented as dense numeric vectors of reduced dimensionality using either word embeddings or sentence encodings. Given that several types of word embeddings and sentence encodings exist, we compare email's sentence representations corresponding to different word embeddings and sentence encodings with the goal of understanding what embeddings/encodings are more suitable for use in the task of detecting the intent of an email. Our experimental results using three different types of embeddings: context-free word embeddings (word2vec and GloVe), contextual word embeddings (ELMo and BERT), and sentence embeddings (DAN-based Universal Sentence Encoder and Transformer-based Universal Sentence Encoder) suggest that the email's sentences representations based on ELMo embeddings produce better results than the representations that use other embeddings. We achieved an accuracy of 90.10%, comparing with word2vec (82.02%), BERT (58.08%), DAN-based USE (86.66%), and Transformer-based USE (88.16%).},
booktitle = {Proceedings of the 7th ACIS International Conference on Applied Computing and Information Technology},
articleno = {2},
numpages = {7},
keywords = {email intent, email overload, email speech acts, embeddings, transfer learning},
location = {Honolulu, HI, USA},
series = {ACIT '19}
}

@inproceedings{10.1145/3269206.3271732,
author = {Gaur, Manas and Kursuncu, Ugur and Alambo, Amanuel and Sheth, Amit and Daniulaityte, Raminta and Thirunarayan, Krishnaprasad and Pathak, Jyotishman},
title = {"Let Me Tell You About Your Mental Health!": Contextualized Classification of Reddit Posts to DSM-5 for Web-based Intervention},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3271732},
doi = {10.1145/3269206.3271732},
abstract = {Social media platforms are increasingly being used to share and seek advice on mental health issues. In particular, Reddit users freely discuss such issues on various subreddits, whose structure and content can be leveraged to formally interpret and relate subreddits and their posts in terms of mental health diagnostic categories. There is prior research on the extraction of mental health-related information, including symptoms, diagnosis, and treatments from social media; however, our approach can additionally provide actionable information to clinicians about the mental health of a patient in diagnostic terms for web-based intervention. Specifically, we provide a detailed analysis of the nature of subreddit content from domain expert's perspective and introduce a novel approach to map each subreddit to the best matching DSM-5 (Diagnostic and Statistical Manual of Mental Disorders - 5th Edition) category using multi-class classifier. Our classification algorithm analyzes all the posts of a subreddit by adapting topic modeling and word-embedding techniques, and utilizing curated medical knowledge bases to quantify relationship to DSM-5 categories. Our semantic encoding-decoding optimization approach reduces the false-alarm-rate from 30% to 2.5% over a comparable heuristic baseline, and our mapping results have been verified by domain experts achieving a kappa score of 0.84.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {753–762},
numpages = {10},
keywords = {drug abuse ontology, dsm-5, medical knowledge bases, mental health, reddit, semantic encoding and decoding, semantic social computing},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3394486.3403357,
author = {Han, Fred X. and Niu, Di and Chen, Haolan and Guo, Weidong and Yan, Shengli and Long, Bowei},
title = {Meta-Learning for Query Conceptualization at Web Scale},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403357},
doi = {10.1145/3394486.3403357},
abstract = {Concepts naturally constitute an abstraction for fine-grained entities and knowledge in the open domain. They enable search engines and recommendation systems to enhance user experience by discovering high-level abstraction of a search query and the user intent behind it. In this paper, we study the problem of query conceptualization, which is to find the most appropriate matching concepts for any given search query from a large pool of pre-defined concepts. We propose a coarse-to-fine approach to first reduce the search space for each query through a shortlisting scheme and then identify the matching concepts using pre-trained language models, which are meta-tuned to our query-concept matching task. Our shortlisting scheme involves using a GRU-based Relevant Words Generator (RWG) to first expand and complete the context of the given query and then shortlisting the candidate concepts through a scoring mechanism based on word overlaps. To accurately identify the most appropriate matching concepts for a query, even when the concepts may have zero verbatim overlaps with the query, we meta-fine-tune a BERT pairwise text-matching model under the Reptile meta-learning algorithm, which achieves zero-shot transfer learning on the conceptualization problem. Our two-stage framework can be trained with data completely derived from a search click graph, without requiring any human labelling efforts. For evaluation, we have constructed a large click graph based on more than $7$ million instances of the click history recorded in Tencent QQ browser and performed the query conceptualization task based on a large ontology with $159,148$ unique concepts. Results from a range of evaluation methods, including an offline evaluation procedure on the click graph, human evaluation, online A/B testing and case studies, have demonstrated the superiority of our approach over a number of competitive pre-trained language models and fine-tuned neural network baselines.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3064–3073},
numpages = {10},
keywords = {conceptualization, information retrieval, meta-learning, query analysis},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1145/3410569,
author = {Laatar, Rim and Aloulou, Chafik and Belguith, Lamia Hadrich},
title = {Disambiguating Arabic Words According to Their Historical Appearance in the Document Based on Recurrent Neural Networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3410569},
doi = {10.1145/3410569},
abstract = {How can we determine the semantic meaning of a word in relation to its context of appearance? We eventually have to grabble with this difficult question, as one of the paramount problems of Natural Language Processing (NLP). In other words, this issue is commonly defined as Word Sense Disambiguation (WSD). The latter is one of the crucial difficulties within the NLP field. In this respect, word vectors extracted from a neural network model have been successfully applied for resolving the WSD problem. Accordingly, this article presents an unprecedented method to disambiguate Arabic words according to both their contextual appearance in a source text and the era in which they emerged. In fact, in the few previous decades, many researchers have been grabbling with Arabic Word Sense Disambiguation.It should be noted that the Arabic language can be divided into three major historical periods: old Arabic, middle-age Arabic, and contemporary Arabic. Actually, contemporary Arabic has proved to be the greatest concern of many researchers. The main gist of our work is to disambiguate Arabic words according to the historical period in which they appeared. To perform such a task, we suggest a method that deploys contextualized word embeddings to better gather valid syntactic and semantic information of the same word by taking into account its contextual uses. The preponderant thing is to convert both the senses and the contextual uses of an ambiguous item to vectors, then determine which of the possible conceptual meanings of the target word is closer to the given context.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {86},
numpages = {16},
keywords = {Natural language processing, contemporary arabic, contextualized word embeddings, historical dictionary, middle-age arabic, old arabic, recurrent neural networks, word sense disambiguation}
}

@inproceedings{10.1145/3038912.3052642,
author = {Kejriwal, Mayank and Szekely, Pedro},
title = {Information Extraction in Illicit Web Domains},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052642},
doi = {10.1145/3038912.3052642},
abstract = {Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have 'long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {997–1006},
numpages = {10},
keywords = {distributional semantics, feature-agnostic, illicit domains, information extraction, named entity recognition},
location = {Perth, Australia},
series = {WWW '17}
}

@inproceedings{10.1145/3438872.3439101,
author = {Yang, JinXiong and Bai, Liang and Guo, Yanming},
title = {A survey of text classification models},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439101},
doi = {10.1145/3438872.3439101},
abstract = {With the rapid development of artificial intelligence, text classification method based on deep learning model has surpassed traditional machine learning method in various aspects. This paper introduces dozens of deep learning models for text classification according to the different network structures of the models. In addition, this paper briefly introduces the evaluation indicators and application scenarios of text classification, summarizes and forecasts the current challenges and future development trend of text classification.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {327–334},
numpages = {8},
keywords = {Deep learning, Model, Text classification},
location = {Shanghai, China},
series = {RICAI '20}
}

@inproceedings{10.1145/3314221.3314594,
author = {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.},
title = {Genie: a generator of natural language semantic parsers for virtual assistant commands},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314594},
doi = {10.1145/3314221.3314594},
abstract = {To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62% accuracy on realistic user inputs. We demonstrate Genie’s generality by showing a 19% and 31% improvement over the previous state of the art on a music skill, aggregate functions, and access control.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {394–410},
numpages = {17},
keywords = {data augmentation, data engineering, semantic parsing, training data generation, virtual assistants},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@article{10.1145/3201407,
author = {Li, Peipei and Wang, Haixun and Li, Hongsong and Wu, Xindong},
title = {Employing Semantic Context for Sparse Information Extraction Assessment},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3201407},
doi = {10.1145/3201407},
abstract = {A huge amount of texts available on the World Wide Web presents an unprecedented opportunity for information extraction (IE). One important assumption in IE is that frequent extractions are more likely to be correct. Sparse IE is hence a challenging task because no matter how big a corpus is, there are extractions supported by only a small amount of evidence in the corpus. However, there is limited research on sparse IE, especially in the assessment of the validity of sparse IEs. Motivated by this, we introduce a lightweight, explicit semantic approach for assessing sparse IE.1 We first use a large semantic network consisting of millions of concepts, entities, and attributes to explicitly model the context of any semantic relationship. Second, we learn from three semantic contexts using different base classifiers to select an optimal classification model for assessing sparse extractions. Finally, experiments show that as compared with several state-of-the-art approaches, our approach can significantly improve the F-score in the assessment of sparse extractions while maintaining the efficiency.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {54},
numpages = {36},
keywords = {Sparse information extraction, classification, isA relationship, semantic network}
}

@inproceedings{10.1145/3264996.3265002,
author = {Anbarasan and Lee, Jeannie S.A.},
title = {Speech and Gestures for Smart-Home Control and Interaction for Older Adults},
year = {2018},
isbn = {9781450359825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264996.3265002},
doi = {10.1145/3264996.3265002},
abstract = {Older adults have been encountering difficulties in using modern technological devices to control home appliances as they are lacking in technology literacy and mobility. This led to the usage of remote controllers or requiring assistance from family members, which is not beneficial for older adults since there is less independence. To alleviate this problem, this project aims to develop a prototype system named "Genie" which caters for older adults ranging from 65 to 80 years old, allowing for easy control of smart home appliances through combination of speech and gesture interactions. An experiment was carried out with a total of 20 older adults on the prototype system where the initial results demonstrate a significant increase in usability. Based on the evaluation, such interaction methods show promise to be effective in replacing manual operations of home appliances through the use of simple speech or gesture commands.},
booktitle = {Proceedings of the 3rd International Workshop on Multimedia for Personal Health and Health Care},
pages = {49–57},
numpages = {9},
keywords = {gestures, human interactions, older adults, smart-home, speech},
location = {Seoul, Republic of Korea},
series = {HealthMedia'18}
}

@inbook{10.1145/2915031.2915049,
title = {Topic Analysis},
year = {2018},
isbn = {9781970001174},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/2915031.2915049},
abstract = {Recent years have seen a dramatic growth of natural language text data, including web pages, news articles, scientific literature, emails, enterprise documents, and social media (such as blog articles, forum posts, product reviews, and tweets). This has led to an increasing demand for powerful software tools to help people manage and analyze vast amounts of text data effectively and efficiently. Unlike data generated by a computer system or sensors, text data are usually generated directly by humans, and capture semantically rich content. As such, text data are especially valuable for discovering knowledge about human opinions and preferences, in addition to many other kinds of knowledge that we encode in text. In contrast to structured data, which conform to well-defined schemas (thus are relatively easy for computers to handle), text has less explicit structure, requiring computer processing toward understanding of the content encoded in text. The current technology of natural language processing has not yet reached a point to enable a computer to precisely understand natural language text, but a wide range of statistical and heuristic approaches to management and analysis of text data have been developed over the past few decades. They are usually very robust and can be applied to analyze and manage text data in any natural language, and about any topic.This book provides a systematic introduction to many of these approaches, with an emphasis on covering the most useful knowledge and skills required to build a variety of practically useful text information systems. Because humans can understand natural languages far better than computers can, effective involvement of humans in a text information system is generally needed and text information systems often serve as intelligent assistants for humans. Depending on how a text information system collaborates with humans, we distinguish two kinds of text information systems. The first is information retrieval systems which include search engines and recommender systems; they assist users in finding from a large collection of text data the most relevant text data that are actually needed for solving a specific application problem, thus effecively turning big raw text data into much smaller relevant text data that can be more easily processed by humans. The second is text mining application systems; they can assist users in analyzing patterns in text data to extract and discover useful actionable knowledge directly useful for task completion or decision making, thus providing more direct task support for users. This book covers the major concepts, techniques, and ideas in information retrieval and text data mining from a practical viewpoint, and includes many hands-on exercises designed with a companion software toolkit (i.e., MeTA) to help readers learn how to apply techniques of information retrieval and text mining to real-world text data and how to experiment with and improve some of the algorithms for interesting application tasks. This book can be used as a textbook for computer science undergraduates and graduates, library and information scientists, or as a reference book for practitioners working on relevant problems in managing and analyzing text data.},
booktitle = {Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining}
}

@inproceedings{10.1109/ICSE.2019.00057,
author = {Abad, Zahra Shakeri Hossein and Gervasi, Vincenzo and Zowghi, Didar and Far, Behrouz H.},
title = {Supporting analysts by dynamic extraction and classification of requirements-related knowledge},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00057},
doi = {10.1109/ICSE.2019.00057},
abstract = {In many software development projects, analysts are required to deal with systems' requirements from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents. Accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario. Our approach is to mine real-time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation. We propose to use both generative and discriminating methods. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modelling of natural language processing tasks. We used an extended version of Support Vector Machines (SVMs) with variable-sized feature vectors to efficiently and dynamically extract and classify requirements-related knowledge from the existing documents. To evaluate the performance of our approach intuitively and quantitatively, we used edit distance and precision/recall metrics. We show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate. Furthermore, we found that statistical and linguistic parameters such as smoothing methods, and words contiguity and order features can impact the performance of both extraction and classification tasks.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {442–453},
numpages = {12},
keywords = {dynamic language models, natural language processing, requirements classification, requirements elicitation, weighted finite state transducers},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3377325.3377486,
author = {Yaghoub-Zadeh-Fard, Mohammad-Ali and Benatallah, Boualem and Casati, Fabio and Barukh, Moshe Chai and Zamanirad, Shayan},
title = {Dynamic word recommendation to obtain diverse crowdsourced paraphrases of user utterances},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377486},
doi = {10.1145/3377325.3377486},
abstract = {Building task-oriented bots requires mapping a user utterance to an intent with its associated entities to serve the request. Doing so is not easy since it requires large quantities of high-quality and diverse training data to learn how to map all possible variations of utterances with the same intent. Crowdsourcing may be an effective, inexpensive, and scalable technique for collecting such large datasets. However, the diversity of the results suffers from the priming effect (i.e. workers are more likely to use the words in the sentence we are asking to paraphrase). In this paper, we leverage priming as an opportunity rather than a threat: we dynamically generate word suggestions to motivate crowd workers towards producing diverse utterances. The key challenge is to make suggestions that can improve diversity without resulting in semantically invalid paraphrases. To achieve this, we propose a probabilistic model that generates continuously improved versions of word suggestions that balance diversity and semantic relevance. Our experiments show that the proposed approach improves the diversity of crowdsourced paraphrases.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {55–66},
numpages = {12},
keywords = {bots, crowdsourcing, paraphrasing},
location = {Cagliari, Italy},
series = {IUI '20}
}

@inproceedings{10.1145/3292500.3330710,
author = {Weng, Wei-Hung and Chung, Yu-An and Szolovits, Peter},
title = {Unsupervised Clinical Language Translation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330710},
doi = {10.1145/3292500.3330710},
abstract = {As patients' access to their doctors' clinical notes becomes common, translating professional, clinical jargon to layperson-understandable language is essential to improve patient-clinician communication. Such translation yields better clinical outcomes by enhancing patients' understanding of their own health conditions, and thus improving patients' involvement in their own care. Existing research has used dictionary-based word replacement or definition insertion to approach the need. However, these methods are limited by expert curation, which is hard to scale and has trouble generalizing to unseen datasets that do not share an overlapping vocabulary. In contrast, we approach the clinical word and sentence translation problem in a completely unsupervised manner. We show that a framework using representation learning, bilingual dictionary induction and statistical machine translation yields the best precision at 10 of 0.827 on professional-to-consumer word translation, and mean opinion scores of 4.10 and 4.28 out of 5 for clinical correctness and layperson readability, respectively, on sentence translation. Our fully-unsupervised strategy overcomes the curation problem, and the clinically meaningful evaluation reduces biases from inappropriate evaluators, which are critical in clinical machine learning.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {3121–3131},
numpages = {11},
keywords = {consumer health, machine translation, representation learning, unsupervised learning},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3352411.3352452,
author = {Shuoqiu, Yang and Chaojun, Xu},
title = {Research on Constructing Sentiment Dictionary of Online Course Reviews based on Multi-source Combination},
year = {2019},
isbn = {9781450371414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352411.3352452},
doi = {10.1145/3352411.3352452},
abstract = {The construction of sentiment dictionary is an important task in text sentiment analysis. Using the sentiment words of specific fields to establish a domain-oriented sentiment dictionary can significantly improve the effect on sentiment recognition and classification in the specific field. A method of constructing a sentiment dictionary for online course reviews was proposed, which based on multi-source combination. In the first place, the sentiment words were identified and extracted using TextRank and the word2vec model, which combined with the general sentiment dictionary and the online course reviews corpus. Then, the label propagation algorithm was applied to discriminate the sentiment polarity of sentiment words, thus constructing a sentiment dictionary for online course reviews. Through the accuracy experiment on the determination of the sentiment polarity of words and the experiment on sentiment classification based on different dictionaries, the accuracy rate, the recall rate and the F value are calculated. The experimental results show that the proposed method was an accuracy and effective way to achieve the sentiment classification of online course reviews.},
booktitle = {Proceedings of the 2019 2nd International Conference on Data Science and Information Technology},
pages = {71–76},
numpages = {6},
keywords = {Label propagation, Online course reviews, Sentiment dictionary, TextRank, Word vector},
location = {Seoul, Republic of Korea},
series = {DSIT 2019}
}

@article{10.1109/TASLP.2018.2819941,
author = {Yu, Kai and Zhao, Zijian and Wu, Xueyang and Lin, Hongtao and Liu, Xuan},
title = {Rich Short Text Conversation Using Semantic-Key-Controlled Sequence Generation},
year = {2018},
issue_date = {August 2018},
publisher = {IEEE Press},
volume = {26},
number = {8},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2819941},
doi = {10.1109/TASLP.2018.2819941},
abstract = {With the recent advances of the sequence-to-sequence framework, generation approaches for the short text conversation STC become attractive. The traditional sequence-to-sequence approaches for the STC often suffer from poor diversity and general reply without substantiality. It is also hard to control the topic or semantics of the selected reply from multiple generated candidates. In this paper, a novel external-memory-driven sequence-to-sequence learning approach is proposed to address these problems. A tensor of the external memory is constructed to represent interpretable topics or semantics. During generation, a controllable memory trigger is extracted given the input sequence, and a reply is then generated using the memory trigger as well as the sequence-to-sequence model. Experiments show that the proposed approach can generate much richer diversity than the traditional sequence-to-sequence training with attention. Meanwhile, it achieves better quality score in human evaluation. It is also observed that by manually manipulating the memory trigger, it is possible to interpretably guide the topics or semantics of the reply.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {1359–1368},
numpages = {10}
}

@inproceedings{10.1145/3331184.3331198,
author = {Zhang, Hongfei and Song, Xia and Xiong, Chenyan and Rosset, Corby and Bennett, Paul N. and Craswell, Nick and Tiwary, Saurabh},
title = {Generic Intent Representation in Web Search},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331198},
doi = {10.1145/3331184.3331198},
abstract = {This paper presents GEneric iNtent Encoder (GEN Encoder) which learns a distributed representation space for user intent in search. Leveraging large scale user clicks from Bing search logs as weak supervision of user intent, GEN Encoder learns to map queries with shared clicks into similar embeddings end-to-end and then fine-tunes on multiple paraphrase tasks. Experimental results on an intrinsic evaluation task - query intent similarity modeling - demonstrate GEN Encoder's robust and significant advantages over previous representation methods. Ablation studies reveal the crucial role of learning from implicit user feedback in representing user intent and the contributions of multi-task learning in representation generality. We also demonstrate that GEN Encoder alleviates the sparsity of tail search traffic and cuts down half of the unseen queries by using an efficient approximate nearest neighbor search to effectively identify previous queries with the same search intent. Finally, we demonstrate distances between GEN encodings reflect certain information seeking behaviors in search sessions.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {65–74},
numpages = {10},
keywords = {generic intent representation, query embedding, user intent},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3038912.3052630,
author = {Chen, Long and Jose, Joemon M. and Yu, Haitao and Yuan, Fajie},
title = {A Semantic Graph-Based Approach for Mining Common Topics from Multiple Asynchronous Text Streams},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052630},
doi = {10.1145/3038912.3052630},
abstract = {In the age of Web 2.0, a substantial amount of unstructured content are distributed through multiple text streams in an asynchronous fashion, which makes it increasingly difficult to glean and distill useful information. An effective way to explore the information in text streams is topic modelling, which can further facilitate other applications such as search, information browsing, and pattern mining. In this paper, we propose a semantic graph based topic modelling approach for structuring asynchronous text streams. Our model integrates topic mining and time synchronization, two core modules for addressing the problem, into a unified model. Specifically, for handling the lexical gap issues, we use global semantic graphs of each timestamp for capturing the hidden interaction among entities from all the text streams. For dealing with the sources asynchronism problem, local semantic graphs are employed to discover similar topics of different entities that can be potentially separated by time gaps. Our experiment on two real-world datasets shows that the proposed model significantly outperforms the existing ones.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {1201–1209},
numpages = {9},
keywords = {knowledge repository, language modelling, topic modelling},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1145/3426723,
author = {Fang, Hui and Zhang, Danning and Shu, Yiheng and Guo, Guibing},
title = {Deep Learning for Sequential Recommendation: Algorithms, Influential Factors, and Evaluations},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3426723},
doi = {10.1145/3426723},
abstract = {In the field of sequential recommendation, deep learning--(DL) based methods have received a lot of attention in the past few years and surpassed traditional models such as Markov chain-based and factorization-based ones. However, there is little systematic study on DL-based methods, especially regarding how to design an effective DL model for sequential recommendation. In this view, this survey focuses on DL-based sequential recommender systems by taking the aforementioned issues into consideration. Specifically, we illustrate the concept of sequential recommendation, propose a categorization of existing algorithms in terms of three types of behavioral sequences, summarize the key factors affecting the performance of DL-based models, and conduct corresponding evaluations to showcase and demonstrate the effects of these factors. We conclude this survey by systematically outlining future directions and challenges in this field.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {10},
numpages = {42},
keywords = {Sequential recommendation, deep learning, evaluations, influential factors, session-based recommendation, survey}
}

@article{10.1109/TCBB.2018.2801303,
author = {Xu, Bo and Lin, Hongfei and Lin, Yuan},
title = {Learning to Refine Expansion Terms for Biomedical Information Retrieval Using Semantic Resources},
year = {2019},
issue_date = {May 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2801303},
doi = {10.1109/TCBB.2018.2801303},
abstract = {With the rapid development of biomedicine, the number of biomedical articles has increased accordingly, which presents a great challenge for biologists trying to keep up with the latest research. Information retrieval seeks to meet this challenge by searching among a large number of articles based on given queries and providing the most relevant ones to fulfill information needs. As an effective information retrieval technique, query expansion has some room for improvement to achieve the desired performance when directly applied for biomedical information retrieval because there exist many domain-related terms both in users' queries and in related articles. To solve this problem, we propose a biomedical query expansion framework based on learning-to-rank methods, in which we refine candidate expansion terms by training term-ranking models to select the most relevant terms. To train the term-ranking models, we first propose a pseudo-relevance feedback method based on MeSH to select candidate expansion terms and then represent the candidate terms as feature vectors by defining both the corpus-based term features and the resource-based term features. Experimental results obtained for TREC genomics datasets show that our method can capture more relevant terms to expand the original query and effectively improve biomedical information retrieval performance.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {954–966},
numpages = {13}
}

@article{10.1145/3378414,
author = {Zarnoufi, Randa and Jaafar, Hamid and Abik, Mounia},
title = {Machine Normalization: Bringing Social Media Text from Non-Standard to Standard Form},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3378414},
doi = {10.1145/3378414},
abstract = {User-generated text in social media communication (SMC) is mainly characterized by non-standard form. It may contain code switching (CS) text, a widespread phenomenon in SMC, in addition to noisy elements used, especially in written conversations (use of abbreviations, symbols, emoticons) or misspelled words. All of these factors constitute a wall in front of text mining applications. Common text mining tools are dedicated to standard use of standard languages but cannot deal with other forms, especially written text in social media. To overcome these problems, in this work we present our solution for the normalization of non-standard use of standard and non-standard languages (dialects) in SMC text with the use of existent resources and tools. The main processing in our solution consists of CS normalization from multiple to one language by the use of a machine translation--like approach. This processing relies on a linguistic approach of CS, which aims at identifying automatically the translation source and target languages (without human intervention). The remaining processing operations concern the normalization of SMC special expressions and spelling correction of out-of-vocabulary words. To preserve the coded-switched sentence meaning across translation, we adopt a knowledge-based approach for word sense translation disambiguation reinforced with a multi-lingual vertical context. All of these processes are embedded in what we refer to as the machine normalization system. Our solution can be used as a front-end of text mining processing, enabling the analysis of SMC noisy text. The conducted experiments show that our system performs better than considered baselines.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {49},
numpages = {30},
keywords = {Text normalization, automatic language identification, code switching normalization, dialects, matrix language, multilingual vertical context, social media, standard languages, word sense disambiguation}
}

@inproceedings{10.1145/3408066.3408101,
author = {Chew, Lit-Jie and Haw, Su-Cheng and Subramaniam, Samini},
title = {Recommender System for Retail Domain: An Insight on Techniques and Evaluations},
year = {2020},
isbn = {9781450377034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408066.3408101},
doi = {10.1145/3408066.3408101},
abstract = {Recommender system has been developed as a useful tool especially when we reached the era of big data and in the meanwhile the internet has been overwhelming with lots of choices. There is a need for people to filter the information to search for their needs and wants efficiently. E-commerce website such as Amazon and Netflix have been using recommender system to build and boost their sales through the personalization recommendation. With the success in the e-commerce area, researchers are keen on finding a method to boost traditional offline retailer sales thru the recommender system. Therefore, in this paper, we introduced the existing recommender system and discuss the method of filtering of each method. Then, we provide the overview of the recent paper in retailer and e-commerce domain to provide the insight and trends such as the filtering techniques and evaluation metric used. Several possible research direction has been discussed based on the current trends and problems.},
booktitle = {Proceedings of the 12th International Conference on Computer Modeling and Simulation},
pages = {9–13},
numpages = {5},
keywords = {Collaborative filtering, Content-based filtering, Hybrid filtering, Recommender system},
location = {Brisbane, QLD, Australia},
series = {ICCMS '20}
}

@article{10.1145/3394592,
author = {Ren, Xuhui and Yin, Hongzhi and Chen, Tong and Wang, Hao and Hung, Nguyen Quoc Viet and Huang, Zi and Zhang, Xiangliang},
title = {CRSAL: Conversational Recommender Systems with Adversarial Learning},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3394592},
doi = {10.1145/3394592},
abstract = {Recommender systems have been attracting much attention from both academia and industry because of their ability to capture user interests and generate personalized item recommendations. As the life pace in contemporary society speeds up, traditional recommender systems are inevitably limited by their disconnected interaction styles and low adaptivity to users’ evolving demands. Consequently, conversational recommender systems emerge as a prospective research area, where an intelligent dialogue agent is integrated with a recommender system. Conversational recommender systems possess the ability to accurately understand end-users’ intent or request and generate human-like dialogue responses when performing recommendations. However, existing conversational recommender systems only allow the systems to ask users for more preference information, while users’ further questions and concerns about the recommended items (e.g., enquiring the location of a recommended restaurant) can hardly be addressed. Though the recent task-oriented dialogue systems allow for two-way communications, they are not easy to train because of their high dependence on human guidance in terms of user intent recognition and system response generation. Hence, to enable two-way human-machine communications and tackle the challenges brought by manually crafted rules, we propose Conversational Recommender System with Adversarial Learning (CRSAL), a novel end-to-end system to tackle the task of conversational recommendation. In CRSAL, we innovatively design a fully statistical dialogue state tracker coupled with a neural policy agent to precisely capture each user’s intent from limited dialogue data and generate conversational recommendation actions. We further develop an adversarial Actor-Critic reinforcement learning approach to adaptively refine the quality of generated system actions, thus ensuring coherent human-like dialogue responses. Extensive experiments on two benchmark datasets fully demonstrate the superiority of CRSAL on conversational recommendation tasks.},
journal = {ACM Trans. Inf. Syst.},
month = jun,
articleno = {34},
numpages = {40},
keywords = {Conversational recommender systems, adversarial learning, deep neural networks, dialogue systems}
}

@inproceedings{10.1145/3290605.3300805,
author = {Chen, Fanglin and Xia, Kewei and Dhabalia, Karan and Hong, Jason I.},
title = {MessageOnTap: A Suggestive Interface to Facilitate Messaging-related Tasks},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300805},
doi = {10.1145/3290605.3300805},
abstract = {Text messages are sometimes prompts that lead to information related tasks, e.g. checking one's schedule, creating reminders, or sharing content. We introduce MessageOnTap, a suggestive inter-face for smartphones that uses the text in a conversation to suggest task shortcuts that can streamline likely next actions. When activated, MessageOnTap uses word embeddings to rank relevant external apps, and parameterizes associated task shortcuts using key phrases mentioned in the conversation, such as times, persons, or events. MessageOnTap also tailors the auto-complete dictionary based on text in the conversation, to streamline any text input.We first conducted a month-long study of messaging behaviors(N=22) that informed our design. We then conducted a lab study to evaluate the effectiveness of MessageOnTap's suggestive interface, and found that participants can complete tasks 3.1x faster withMessageOnTap than their typical task flow.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {contextual computing, information seeking &amp; search, messaging/communication, personal data/tracking, productivity, text/speech/language, user experience design},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3219819.3219956,
author = {Li, Yan and Ye, Jieping},
title = {Learning Adversarial Networks for Semi-Supervised Text Classification via Policy Gradient},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219956},
doi = {10.1145/3219819.3219956},
abstract = {Semi-supervised learning is a branch of machine learning techniques that aims to make fully use of both labeled and unlabeled instances to improve the prediction performance. The size of modern real world datasets is ever-growing so that acquiring label information for them is extraordinarily difficult and costly. Therefore, deep semi-supervised learning is becoming more and more popular. Most of the existing deep semi-supervised learning methods are built under the generative model based scheme, where the data distribution is approximated via input data reconstruction. However, this scheme does not naturally work on discrete data, e.g., text; in addition, learning a good data representation is sometimes directly opposed to the goal of learning a high performance prediction model. To address the issues of this type of methods, we reformulate the semi-supervised learning as a model-based reinforcement learning problem and propose an adversarial networks based framework. The proposed framework contains two networks: a predictor network for target estimation and a judge network for evaluation. The judge network iteratively generates proper reward to guide the training of predictor network, and the predictor network is trained via policy gradient. Based on the aforementioned framework, we propose a recurrent neural network based model for semi-supervised text classification. We conduct comprehensive experimental analysis on several real world benchmark text datasets, and the results from our evaluations show that our method outperforms other competing state-of-the-art methods.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1715–1723},
numpages = {9},
keywords = {adversarial networks, policy gradients, semi-supervised learning, text classification},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3077136.3080796,
author = {Zhang, Shuo and Balog, Krisztian},
title = {EntiTables: Smart Assistance for Entity-Focused Tables},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080796},
doi = {10.1145/3077136.3080796},
abstract = {Tables are among the most powerful and practical tools for organizing and working with data. Our motivation is to equip spreadsheet programs with smart assistance capabilities. We concentrate on one particular family of tables, namely, tables with an entity focus. We introduce and focus on two specifc tasks: populating rows with additional instances (entities) and populating columns with new headings. We develop generative probabilistic models for both tasks. For estimating the components of these models, we consider a knowledge base as well as a large table corpus. Our experimental evaluation simulates the various stages of the user entering content into an actual table. A detailed analysis of the results shows that the models' components are complimentary and that our methods outperform existing approaches from the literature.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {255–264},
numpages = {10},
keywords = {intelligent table assistance, semantic search, table completion},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3423334.3431450,
author = {Wang, Zhangyu and Moosavi, Vahid},
title = {From PIace2Vec to Multi-Scale Built-Environment Representation: A General-Purpose Distributional Embedding for Urban Data Analysis},
year = {2020},
isbn = {9781450381604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423334.3431450},
doi = {10.1145/3423334.3431450},
abstract = {Built environments like cities, roads, communities are rich sources of urban data. Many downstream applications require comprehensive analysis like geographic information retrieval, recommender systems, geographic knowledge graphs, and in general, understanding urban spaces [28]. Points of Interests (POI), as one of the most researched aspects of urban data, has been successfully modeled using concepts borrowed from Machine Learning (ML) and Natural Language Processing (NLP). In the work of Place2Vec [28], a Word2Vec-like statistical model is proposed to represent spatial adjacency with a continuous embedding space. This method successfully models the functional semantics of POIs with regard to several human-assessment based evaluations. However, though the Place2Vec model addresses the distributional heterogeneity within a given spatial context with ITDL augmentation, it does not address the spatial heterogeneity among different regions. To solve this problem, we propose to introduce a hierarchical, density-based, self-adjusting clustering mechanism. The boundary of relatedness and unrelatedness is learned from the given context, where denser areas have tighter bounds while sparser areas have looser ones. We train our model on both the baseline Yelp hierarchical dataset [28] and our OpenStreetMap dataset. We demonstrate that 1) our model significantly improves the performance on 2 of the 3 baseline tasks and the stability of training, and 2) our model generalizes excellently across 112 cities of radically different scales (minimum 1725 POIs, maximum 2694070 POIs), regions (North America, Europe, Asia, Africa) and types (commercial, touristy, industrial, etc.) without the need of adjusting or tuning any hyperparameters. We also demonstrate that our model can be used to discover interesting facts about cities like inter-city semantic analogy and intra-city connectivity, which can be very useful in urban planning, social computing and public policy making.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks, and Geoadvertising},
articleno = {1},
numpages = {12},
keywords = {Geo-Semantics, Machine Learning, Points of Interest, Similarity},
location = {Seattle, WA, USA},
series = {LocalRec'20}
}

@article{10.1145/3379443,
author = {Li, Guangjie and Liu, Hui and Nyamawe, Ally S.},
title = {A Survey on Renamings of Software Entities},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3379443},
doi = {10.1145/3379443},
abstract = {More than 70% of characters in the source code are used to label identifiers. Consequently, identifiers are one of the most important source for program comprehension. Meaningful identifiers are crucial to understand and maintain programs. However, for reasons like constrained schedule, inexperience, and unplanned evolution, identifiers may fail to convey the semantics of the entities associated with them. As a result, such entities should be renamed to improve software quality. However, manual renaming and recommendation are fastidious, time consuming, and error prone, whereas automating the process of renamings is challenging: (1) It involves complex natural language processing to understand the meaning of identifers; (2) It also involves difficult semantic analysis to determine the role of software entities. Researchers proposed a number of approaches and tools to facilitate renamings. We present a survey on existing approaches and classify them into identification of renaming opportunities, execution of renamings, and detection of renamings. We find that there is an imbalance between the three type of approaches, and most of implementation of approaches and evaluation dataset are not publicly available. We also discuss the challenges and present potential research directions. To the best of our knowledge, this survey is the first comprehensive study on renamings of software entities.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {41},
numpages = {38},
keywords = {Rename refactoring, identifier, software quality}
}

@inproceedings{10.1145/3173574.3173851,
author = {Huber, Bernd and McDuff, Daniel and Brockett, Chris and Galley, Michel and Dolan, Bill},
title = {Emotional Dialogue Generation using Image-Grounded Language Models},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173851},
doi = {10.1145/3173574.3173851},
abstract = {Computer-based conversational agents are becoming ubiquitous. However, for these systems to be engaging and valuable to the user, they must be able to express emotion, in addition to providing informative responses. Humans rely on much more than language during conversations; visual information is key to providing context. We present the first example of an image-grounded conversational agent using visual sentiment, facial expression and scene features. We show that key qualities of the generated dialogue can be manipulated by the features used for training the agent. We evaluate our model on a large and very challenging real-world dataset of conversations from social media (Twitter). The image-grounding leads to significantly more informative, emotional and specific responses, and the exact qualities can be tuned depending on the image features used. Furthermore, our model improves the objective quality of dialogue responses when evaluated on standard natural language metrics.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {computer vision, conversation, conversational agents, dialogue, emotion},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/3331453.3361319,
author = {Deng, Chunhui and Deng, Huifang and Liu, Yuxin},
title = {Online Hot Topic Discovery and Hotness Evaluation},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3361319},
doi = {10.1145/3331453.3361319},
abstract = {In this paper, by analyzing the inadequacies of traditional TF-IDF(Term Frequency-Inverse Document Frequency) method and taking into account the factors of the location information, named entity and feature term burstiness, we put forward an improved weight calculation formula i.e., a new TF-IDF to update the feature term weight in real time. In this way, the accuracy of news representation model can be improved to some extent. Incremental k-means clustering based on time window and multi-center topic model is proposed to tackle topic center drift problem, reduce the error caused by inadequate topic model, and therefore, improve the clustering accuracy. At last, we defined an improved energy accumulation formula. And based on media attention, topic competition, topic burstiness magnitude and topic cohesiveness, we constructed a topic hotness evaluation model to quantify the topic hotness and therefore to better distinguish the hot topics from the cold topics. The experimental results demonstrated the effectiveness of our approaches and models.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {43},
numpages = {8},
keywords = {Hotness evaluation, Improved TF-IDF, Incremental k-means clustering, Multi-center topic model, Online hot topic discovery, Single pass, Vector space model},
location = {Sanya, China},
series = {CSAE '19}
}

@inproceedings{10.1145/3278681.3278704,
author = {Ramunyisi, Ndivhuwo Stan and Badenhorst, Jaco and Moors, Carmen and Gumede, Tebogo},
title = {Rapid development of a command and control interface for smart office environments},
year = {2018},
isbn = {9781450366472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278681.3278704},
doi = {10.1145/3278681.3278704},
abstract = {Speech-driven command and control is increasingly gaining momentum in the world around us. Using human speech to control appliances in trendy living spaces allows a user to control many appliances remotely. This paper discusses the rapid development of a Speech Node device as a solution to an internet-of-things requirement, creating a smart office environment. An intuitive user interface design, and the development and evaluation of an accented command and control model is described. User evaluation indicates good acceptance of the UI and highlights specific future improvement strategies.},
booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
pages = {188–194},
numpages = {7},
keywords = {accented speech corpus, command and control, pocketsphinx, speech recognition},
location = {Port Elizabeth, South Africa},
series = {SAICSIT '18}
}

@inproceedings{10.1145/3209978.3209983,
author = {Ahmad, Wasi Uddin and Chang, Kai-Wei and Wang, Hongning},
title = {Intent-aware Query Obfuscation for Privacy Protection in Personalized Web Search},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3209983},
doi = {10.1145/3209978.3209983},
abstract = {Modern web search engines exploit users' search history to personalize search results, with a goal of improving their service utility on a per-user basis. But it is this very dimension that leads to the risk of privacy infringement and raises serious public concerns. In this work, we propose a client-centered intent-aware query obfuscation solution for protecting user privacy in a personalized web search scenario. In our solution, each user query is submitted with l additional cover queries and corresponding clicks, which act as decoys to mask users' genuine search intent from a search engine. The cover queries are sequentially sampled from a set of hierarchically organized language models to ensure the coherency of fake search intents in a cover search task. Our approach emphasizes the plausibility of generated cover queries, not only to the current genuine query but also to previous queries in the same task, to increase the complexity for a search engine to identify a user's true intent. We also develop two new metrics from an information theoretic perspective to evaluate the effectiveness of provided privacy protection. Comprehensive experiment comparisons with state-of-the-art query obfuscation techniques are performed on the public AOL search log, and the propitious results substantiate the effectiveness of our solution.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {285–294},
numpages = {10},
keywords = {query obfuscation, search privacy, search tasks},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@article{10.1145/3418062,
author = {Guangce, Ruan and Lei, Xia},
title = {Knowledge Discovery of News Text Based on Artificial Intelligence},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3418062},
doi = {10.1145/3418062},
abstract = {The explosion of news text and the development of artificial intelligence provide a new opportunity and challenge to provide high-quality media monitoring service. In this article, we propose a semantic analysis approach based on the Latent Dirichlet Allocation (LDA) and Apriori algorithm, and we realize application to improve media monitoring reports by mining large-scale news text. First, we propose to use LDA model to mine news text topic words and reducing news dimensionality. Then, we propose to use Apriori algorithm to discovering the relationship of topic words. Finally, we discovery the relevance of news text topic words and show the intensity and dependency among topic words through drawing. This application can realize to extract the news topics and discover the correlation and dependency among news topics in mass news text. The results show that the method based on LDA and Apriori can help the media monitoring staff to better understand the hidden knowledge in the news text and improve the media analysis report.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {6},
numpages = {18},
keywords = {LDA, association rules, knowledge discovery, news text}
}

@proceedings{10.5555/3041021,
title = {WWW '17 Companion: Proceedings of the 26th International Conference on World Wide Web Companion},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
abstract = {We welcome you to the WWW2017 conference, the 26th of the series, and only the second one to be held in Australia.The annual World Wide Web Conference is the premier international forum to present and discuss progress in research, development, standards, and applications related to the Web. This conference is organised under the aegis of the International World Wide Web Conference Committee (IW3C2) in collaboration with local conference organisers in the host city, in this case, the four public universities in Western Australia: Curtin University, Murdoch University, the University of Western Australia and Edith Cowan University. This year, WWW 2017 is offered as the centerpiece of the inaugural Festival of the Web in Perth, a week-long celebration.WWW 2017 provides an opportunity to hear from the leaders of the web including three distinguished keynote speeches by world-class experts: Mark Pesce, Yoelle Maarek, and Melanie Johnston-Hollitt. There is a rich environment of technical activities, including 164 high quality papers in the Research Tracks, 54 papers in the four alternate tracks, over 100 papers in 15 workshops, 13 tutorial sessions, a Ph.D. Symposium track comprising presentations by seven doctoral students, an Industry track consisting of 20 papers focused on applied research, 20 demonstrations, a W3C track examining the latest Web standards and emerging technologies and 64 posters with, for the first time, a number of these offered as e-posters to augment the static poster panels. Overall, WWW2017 provides more than 400 high quality presentations on the key research and development issues of the World Wide Web.Co-located events in the Festival of the Web 2017 include the 4th Big Data Innovators Gathering (BIG 2017), the Web for All conference (W4A2017), and the 5th Serious Games and Applications for Health conference (SeGaH'17). In addition, several new events include Collaboration-Innovation, a one day conference focusing on building smart business innovation through collaboration; the Trust Factory, a curated forum exploring issues of trust and privacy on the web; and Bytes and Rights, a conference focused on issues of web governance, copyright, digital rights, privacy and security on the web. Finally, the Big Day In is a one-day IT careers conference designed by students for students, including tips and advice for secondary school students interested in IT and the web.Given Perth's location in one of the world's richest areas of natural resources, DeepSensor, a world class gathering of industry professionals, is being conducted as part of the Festival as an opportunity for professionals to share their real-world insights into the continuing development of the internet of things in the mining, oil and gas industries.},
location = {Perth, Australia}
}

@inproceedings{10.1145/3411564.3411628,
author = {Mane, Babacar and Rocha, Wita dos Santos and Ribeiro, Elivaldo Lozer Fracalossi and Jesus, Luis Emanuel Neves de and Motta, Israel Cerqueira and Lima, Edmilson and Claro, Daniela Barreiro},
title = {Enhancing Semantic Interoperability on MIDAS with Similar DaaS Parameters},
year = {2020},
isbn = {9781450388733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411564.3411628},
doi = {10.1145/3411564.3411628},
abstract = {The vast amount of social data and the extensive use of smart devices have enabled different formats of DaaS (Data as a Service). Cloud consumers are encouraged to access such DaaS from a SaaS (Software as a Service) directly. However, DaaS can evolve and modify some of its parameters, thus avoiding a SaaS to catch all such adjustments. Thus, it is important to have a middleware to bind the request from SaaS applications to data from DaaS. One such middleware is MIDAS (Middleware for DaaS and SaaS), which provides a way from SaaS to DaaS seamlessly. Although there are many advantages to publishing data as a DaaS format, the major challenge is the dynamicity of DaaS parameters, which can change and evolve. Such changes can affect SaaS applications, even providing runtime errors. Thus, our approach overcomes this problem by ensuring a transparent mechanism to DaaS parameters when modifications have occurred. Our work aims to recognize each similar attribute seamlessly, increasing availability to interoperate SaaS and DaaS through MIDAS. We evaluate the distance measures thought 22 parameters from 11 DaaS providers into five scenarios on parameters change. Two of them were the most outstanding measures (Cosine and Jaccard), and we included them in our approach to better obtain the similarity of two parameters. Each parameter was carried out toward WordNet thesaurus, and a second evaluation was performed analyzing our approach into MIDAS through three criteria: overhead, execution time, and correctness. Our experiments have shown that we are in a good direction to provide interoperability into SaaS and DaaS.},
booktitle = {Proceedings of the XVI Brazilian Symposium on Information Systems},
articleno = {19},
numpages = {8},
keywords = {DaaS, Interoperability, Middleware, SaaS, Semantic similarity},
location = {S\~{a}o Bernardo do Campo, Brazil},
series = {SBSI '20}
}

@inproceedings{10.1145/3127526.3127527,
author = {Singh, Mayank and Dan, Soham and Agarwal, Sanyam and Goyal, Pawan and Mukherjee, Animesh},
title = {AppTechMiner: Mining Applications and Techniques from Scientific Articles},
year = {2017},
isbn = {9781450353885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127526.3127527},
doi = {10.1145/3127526.3127527},
abstract = {This paper presents AppTechMiner, a rule-based information extraction framework that automatically constructs a knowledge base of all application areas and problem solving techniques. Techniques include tools, methods, datasets or evaluation metrics. We also categorize individual research articles based on their application areas and the techniques proposed/improved in the article. Our system achieves high average precision (~82%) and recall (~84%) in knowledge base creation. It also performs well in application and technique assignment to an individual article (average accuracy ~66%). In the end, we further present two use cases presenting a trivial information retrieval system and an extensive temporal analysis of the usage of techniques and application areas. At present, we demonstrate the framework for the domain of computational linguistics but this can be easily generalized to any other field of research. We plan to make the codes publicly available.},
booktitle = {Proceedings of the 6th International Workshop on Mining Scientific Publications},
pages = {1–8},
numpages = {8},
keywords = {Information extraction, application area, computational linguistic, techniques},
location = {Toronto, ON, Canada},
series = {WOSP 2017}
}

@inproceedings{10.1145/3022227.3022299,
author = {Ryu, Seonghan and Yu, Hwanjo and Lee, Gary Geunbae},
title = {Two-stage approach to named entity recognition using Wikipedia and DBpedia},
year = {2017},
isbn = {9781450348881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022227.3022299},
doi = {10.1145/3022227.3022299},
abstract = {In natural language understanding, extraction of named entity (NE) mentions in given text and classification of the mentions into pre-defined NE types are important processes. Most NE recognition (NER) relies on resources such as a training corpus or NE dictionary, but collecting them manually is laborious and time-consuming. This paper proposes a two-stage approach based on nothing but Wikipedia and DBpedia to implement NER. This paper also addresses technical problems in developing Korean NER. In experiments, the proposed method can recognize NEs in short question sentences with 14.2% errors.},
booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
articleno = {73},
numpages = {4},
keywords = {DBpedia, Wikipedia, information extraction, named entity recognition, question answering},
location = {Beppu, Japan},
series = {IMCOM '17}
}

@article{10.1109/TCBB.2019.2897769,
author = {Yao, Heng and Shi, Yunjia and Guan, Jihong and Zhou, Shuigeng},
title = {Accurately Detecting Protein Complexes by Graph Embedding and Combining Functions with Interactions},
year = {2020},
issue_date = {May-June 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2897769},
doi = {10.1109/TCBB.2019.2897769},
abstract = {Identifying protein complexes is helpful for understanding cellular functions and designing drugs. In the last decades, many computational methods have been proposed based on detecting dense subgraphs or subnetworks in Protein-Protein Interaction Networks (PINs). However, the high rate of false positive/negative interactions in PINs prevents from the achievement of satisfactory detection results directly from PINs, because most of such existing methods exploit mainly topological information to do network partitioning. In this paper, we propose a new approach for protein complex detection by merging topological information of PINs and functional information of proteins. We first split proteins to a number of protein groups from the perspective of protein functions by using FunCat data. Then, for each of the resulting protein groups, we calculate two protein-protein similarity matrices: one is computed by using graph embedding over a PIN, the other is by using GO terms, and combine these two matrices to get an integrated similarity matrix. Following that, we cluster the proteins in each group based on the corresponding integrated similarity matrix, and obtain a number of small protein clusters. We map these clusters of proteins onto the PIN, and get a number of connected subgraphs. After a round of merging of overlapping subgraphs, finally we get the detected complexes. We conduct empirical evaluation on four PPI datasets (Collins, Gavin, Krogan, and Wiphi) with two complex benchmarks (CYC2008 and MIPS). Experimental results show that our method performs better than the state-of-the-art methods.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {777–787},
numpages = {11}
}

@article{10.1145/3178315.3178331,
author = {Dwivedi, Ashish Kumar and Rath, Santanu Kumar},
title = {Transformation of Alloy Notation into a Semantic Notation},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3178315.3178331},
doi = {10.1145/3178315.3178331},
abstract = {Transformation of a model based on first-order logic to a model that provides semantic notations is helpful necessary during the analysis phase of any proposed software. The semantic notations often guide the designer to develop pseudocode correctly. This study focuses on facilitation of transformation of one formal model, i.e., Alloy into another, i.e., OWL. The proposed approach extends the concept of existing techniques i.e., UML2Alloy and TwoUse to transform Alloy model into OWL. UML2Alloy transforms UML model into Alloy model, whereas TwoUse approach bridges the gap between UML model and OWL model. Alloy2OWL is based on metamodel-based transformation techniques, which help to map source model, i.e., Alloy into target model, i.e., OWL. For the proper explanation of this study, a model transformation framework is presented, which can be applied to other transformation languages. The proposed approach utilizes the Model-Driven Development techniques to deal with the analysis of Alloy model and determines design problems within a specification. In this paper, various challenges are also presented which occur during the transformation of Alloy to OWL.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6},
keywords = {alloy, formal methods, model-driven development, owl, uml}
}

@inbook{10.1145/3233795.3233811,
author = {Waibel, Alexander},
title = {Multimodal dialogue processing for machine translation},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233811},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {577–620},
numpages = {44}
}

@inproceedings{10.1145/3176653.3176665,
author = {Basha, Syed Muzamil and Rajput, Dharmendra Singh},
title = {Evaluating the Impact of Feature Selection on Overall Performance of Sentiment Analysis},
year = {2017},
isbn = {9781450363518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176653.3176665},
doi = {10.1145/3176653.3176665},
abstract = {Now a days the importance of analyzing the hidden sentiments from user reviews playing a prominent role towards increasing profitability in any organization. To address the challenges being faced in analyzing the text information and transforming the same in to polarities values with an objective of saving time in understanding the public opinion on particular product or service. Traditionally, there are different approaches carried out in transforming text data in to values based on different features of Text. In our research we make use of Stanford CoreNLP, Alias-i's Lingpipe (uses Logistic regression for document classification), Senti WordNet and synthesize libraries from different sources to include several other techniques that are used for text mining to evaluate the impact of feature selection on overall sentiment analysis by scoring a sentences in a review using different scoring Techniques. we also included NTU Lib Linear to make use of linear SVM for document classification. The Features considered on our experiments are Term Frequency and N-Gram (1Gram &amp; 2Gram) with Decision Tree as Prediction model to evaluate the Accuracy, Area under ROC Curve and Kappa value. Finally, Compared the polarities of the reviews obtained using three different sentiment scoring approaches. The findings in our research is, Term Frequency have good impact of (0.932) on classifying the sentiment, In contrast, 2Gram have an impact of (0.8505).},
booktitle = {Proceedings of the 2017 International Conference on Information Technology},
pages = {96–102},
numpages = {7},
keywords = {Alias-i's Lingpipe, SentiWordNet, Stanford CoreNLP, YTextMiner, sentiment analysis},
location = {Singapore, Singapore},
series = {ICIT '17}
}

@article{10.1145/3212695,
author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
title = {A Survey of Machine Learning for Big Code and Naturalness},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3212695},
doi = {10.1145/3212695},
abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {81},
numpages = {37},
keywords = {Big code, code naturalness, machine learning, software engineering tools}
}

@article{10.1145/3094786,
author = {Gao, Yang and Li, Yuefeng and Lau, Raymond Y. K. and Xu, Yue and Bashar, Md Abul},
title = {Finding Semantically Valid and Relevant Topics by Association-Based Topic Selection Model},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3094786},
doi = {10.1145/3094786},
abstract = {Topic modelling methods such as Latent Dirichlet Allocation (LDA) have been successfully applied to various fields, since these methods can effectively characterize document collections by using a mixture of semantically rich topics. So far, many models have been proposed. However, the existing models typically outperform on full analysis on the whole collection to find all topics but difficult to capture coherent and specifically meaningful topic representations. Furthermore, it is very challenging to incorporate user preferences into existing topic modelling methods to extract relevant topics. To address these problems, we develop a novel personalized Association-based Topic Selection (ATS) model, which can identify semantically valid and relevant topics from a set of raw topics based on the semantical relatedness between users’ preferences and the structured patterns captured in topics. The advantage of the proposed ATS model is that it enables an interactive topic modelling process driven by users’ specific interests. Based on three benchmark datasets, namely, RCV1, R8, and WT10G under the context of information filtering (IF) and information retrieval (IR), our rigorous experiments show that the proposed ATS model can effectively identify relevant topics with respect to users’ specific interests, and hence to improve the performance of IF and IR.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = aug,
articleno = {3},
numpages = {22},
keywords = {Topic selection, information filtering, topic components, topic evaluation}
}

@inproceedings{10.1145/3377713.3377803,
author = {Wei, Chao and Zhu, Lijun and Wang, Juncheng and Shi, Jiaoxiang and Wang, Zheng and Chen, Liang},
title = {Extracting Word Embeddings via Joint Learning of Syntagmatic and Paradigmatic Structure},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377803},
doi = {10.1145/3377713.3377803},
abstract = {In this work, we explore the latent distributional semantic between each given word and its context in a way of reconstruction, and propose an Auto-encoder architecture to encourage the model pay attention to the interactions of Syntagmatic and Paradigmatic structure of context (SPC). In particular, SPC represent the syntagmatic words of the context through a vocabulary co-occurrence matrix, and extract a likely embedding for a better joint predicting of all paradigmatic words to fill in the input context. During the error back propagation, our model focus on the salient statistical structure by allowing a sub-network to approximate the nonzero sub-matrix. Finally, we trained our model on a public Wikipedia corpus and evaluated on word analogy, word similarity tasks and documents clustering and classification. The results show that our method yields almost 4% improvement on word analogy and nearly 3% improvement on word similarity tasks compared to state-of-the-art methods. The evidences demonstrate that SPC outperform baseline and the state-of-the-art methods on all tasks.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {526–532},
numpages = {7},
keywords = {Auto-encoder, Syntagmatic and Paradigmatic structure, Word Analogy and Similarity, Word embeddings},
location = {Sanya, China},
series = {ACAI '19}
}

@inproceedings{10.1145/3340531.3411974,
author = {Xu, Silei and Campagna, Giovanni and Li, Jian and Lam, Monica S.},
title = {Schema2QA: High-Quality and Low-Cost Q&amp;A Agents for the Structured Web},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411974},
doi = {10.1145/3340531.3411974},
abstract = {Building a question-answering agent currently requires large annotated datasets, which are prohibitively expensive. This paper proposes Schema2QA, an open-source toolkit that can generate a Q&amp;A system from a database schema augmented with a few annotations for each field. The key concept is to cover the space of possible compound queries on the database with a large number of in-domain questions synthesized with the help of a corpus of generic query templates. The synthesized data and a small paraphrase set are used to train a novel neural network based on the BERT pretrained model. We use Schema2QA to generate Q&amp;A systems for five Schema.org domains, restaurants, people, movies, books and music, and obtain an overall accuracy between 64% and 75% on crowdsourced questions for these domains. Once annotations and paraphrases are obtained for a Schema.org schema, no additional manual effort is needed to create a Q&amp;A agent for any website that uses the same schema. Furthermore, we demonstrate that learning can be transferred from the restaurant to the hotel domain, obtaining a 64% accuracy on crowdsourced questions with no manual effort. Schema2QA achieves an accuracy of 60% on popular restaurant questions that can be answered using Schema.org. Its performance is comparable to Google Assistant, 7% lower than Siri, and 15% higher than Alexa. It outperforms all these assistants by at least 18% on more complex, long-tail questions.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {1685–1694},
numpages = {10},
keywords = {KBQA, NLIDB, data augmentation, data synthesis, linked data, question answering, schema.org, semantic parsing, semantic web},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1109/TCBB.2020.3010975,
author = {Nguyen, Trinh-Trung-Duong and Ho, Quang-Thai and Le, Nguyen-Quoc-Khanh and Phan, Van-Dinh and Ou, Yu-Yen},
title = {Use Chou's 5-Steps Rule With Different Word Embedding Types to Boost Performance of Electron Transport Protein Prediction Model},
year = {2020},
issue_date = {March-April 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3010975},
doi = {10.1109/TCBB.2020.3010975},
abstract = {Living organisms receive necessary energy substances directly from cellular respiration. The completion of electron storage and transportation requires the process of cellular respiration with the aid of electron transport chains. Therefore, the work of deciphering electron transport proteins is inevitably needed. The identification of these proteins with high performance has a prompt dependence on the choice of methods for feature extraction and machine learning algorithm. In this study, protein sequences served as natural language sentences comprising words. The nominated word embedding-based feature sets, hinged on the word embedding modulation and protein motif frequencies, were useful for feature choosing. Five word embedding types and a variety of conjoint features were examined for such feature selection. The support vector machine algorithm consequentially was employed to perform classification. The performance statistics within the 5-fold cross-validation including average accuracy, specificity, sensitivity, as well as MCC rates surpass 0.95. Such metrics in the independent test are 96.82, 97.16, 95.76 percent, and 0.9, respectively. Compared to state-of-the-art predictors, the proposed method can generate more preferable performance above all metrics indicating the effectiveness of the proposed method in determining electron transport proteins. Furthermore, this study reveals insights about the applicability of various word embeddings for understanding surveyed sequences.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {1235–1244},
numpages = {10}
}

@inproceedings{10.1145/3077136.3080740,
author = {Cohan, Arman and Goharian, Nazli},
title = {Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080740},
doi = {10.1145/3077136.3080740},
abstract = {Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to reflect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the effectiveness of our model by significantly outperforming the state-of-the-art. We furthermore demonstrate how an effective contextualization method results in improving citation-based summarization of the scientific articles.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1133–1136},
numpages = {4},
keywords = {information retrieval, scientific text, text summarization},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/3041021.3054163,
author = {Huang, Chieh-Yang and Chen, Mei-Hua and Ku, Lun-Wei},
title = {Towards a Better Learning of Near-Synonyms: Automatically Suggesting Example Sentences via Fill in the Blank},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054163},
doi = {10.1145/3041021.3054163},
abstract = {Language learners are confused by near-synonyms and often look for answers from the Web. However, there is little to aid them in sorting through the overwhelming load of information that is offered. In this paper, we propose a new research problem: suggesting example sentences for learning word distinctions. We focus on near-synonyms as the first step. Two kinds of one-class classifiers, the GMM and BiLSTM models, are used to solve fill-in-the-blank (FITB) questions and further to select example sentences which best differentiate groups of near-synonyms. Experiments are conducted on both an open benchmark and a private dataset for the FITB task. Experiments show that the proposed approach yields an accuracy of 73.05% and 83.59% respectively, comparable to state-of-the-art multi-class classifiers. Learner study further shows the results of the example sentence suggestion by the learning effectiveness and demonstrates the proposed model indeed is more effective in learning near-synonyms compared to the resource-based models.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {293–302},
numpages = {10},
keywords = {bilstm, computer-assisted language learning, data-driven language learning, gmm, natural language processing},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3080546.3080547,
author = {Kapoor, Rahul and Kejriwal, Mayank and Szekely, Pedro},
title = {Using contexts and constraints for improved geotagging of human trafficking webpages},
year = {2017},
isbn = {9781450350471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3080546.3080547},
doi = {10.1145/3080546.3080547},
abstract = {Extracting geographical tags from webpages is a well-motiva-ted application in many domains. In illicit domains with unusual language models, like human trafficking, extracting geotags with both high precision and recall is a challenging problem. In this paper, we describe a geotag extraction framework in which context, constraints and the openly available Geonames knowledge base work in tandem in an Integer Linear Programming (ILP) model to achieve good performance. In preliminary empirical investigations, the framework improves precision by 28.57% and F-measure by 36.9% on a difficult human trafficking geotagging task compared to a machine learning-based baseline. The method is already being integrated into an existing knowledge base construction system widely used by US law enforcement agencies to combat human trafficking.},
booktitle = {Proceedings of the Fourth International ACM Workshop on Managing and Mining Enriched Geo-Spatial Data},
articleno = {3},
numpages = {6},
keywords = {distributional semantics, feature-agnostic, human trafficking, information extraction, integer linear programming, named entity recognition},
location = {Chicago, Illinois},
series = {GeoRich '17}
}

@article{10.1145/3295822,
author = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Wang, Yinglong and Ma, Jun and Kankanhalli, Mohan},
title = {Attentive Long Short-Term Preference Modeling for Personalized Product Search},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3295822},
doi = {10.1145/3295822},
abstract = {E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well known that there are two types of preferences: long-term ones and short-term ones. The former refers to users’ inherent purchasing bias and evolves slowly. By contrast, the latter reflects users’ purchasing inclination in a relatively short period. They both affect users’ current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long- and short-term user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users’ current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long- and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {27},
keywords = {Personalized product search, attention mechanism, long short-term preference}
}

@inproceedings{10.1145/3382507.3418813,
author = {Yin, Yufeng and Huang, Baiyu and Wu, Yizhen and Soleymani, Mohammad},
title = {Speaker-Invariant Adversarial Domain Adaptation for Emotion Recognition},
year = {2020},
isbn = {9781450375818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382507.3418813},
doi = {10.1145/3382507.3418813},
abstract = {Automatic emotion recognition methods are sensitive to the variations across different datasets and their performance drops when evaluated across corpora. We can apply domain adaptation techniques e.g., Domain-Adversarial Neural Network (DANN) to mitigate this problem. Though the DANN can detect and remove the bias between corpora, the bias between speakers still remains which results in reduced performance. In this paper, we propose Speaker-Invariant Domain-Adversarial Neural Network (SIDANN) to reduce both the domain bias and the speaker bias. Specifically, based on the DANN, we add a speaker discriminator to unlearn information representing speakers' individual characteristics with a gradient reversal layer (GRL). Our experiments with multimodal data (speech, vision, and text) and the cross-domain evaluation indicate that the proposed SIDANN outperforms (+5.6% and +2.8% on average for detecting arousal and valence) the DANN model, suggesting that the SIDANN has a better domain adaptation ability than the DANN. Besides, the modality contribution analysis shows that the acoustic features are the most informative for arousal detection while the lexical features perform the best for valence detection.},
booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
pages = {481–490},
numpages = {10},
keywords = {domain adaptation, emotion recognition, multimodal learning, neural networks},
location = {Virtual Event, Netherlands},
series = {ICMI '20}
}

@inproceedings{10.1145/3243082.3243096,
author = {Soares, Eduardo R. and Barr\'{e}re, Eduardo},
title = {Automatic Topic Segmentation for Video Lectures Using Low and High-Level Audio Features},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3243096},
doi = {10.1145/3243082.3243096},
abstract = {Nowadays, video lectures are a very popular way to transmit knowledge, and because of that, there are many repositories with a large catalog of those videos on web. Despite all benefits that this high availability of video lectures brings, some problems also emerge from this scenario. One of these problems is that, it is very difficult find relevant content associate with those videos. Many times, students must to watch the entire video lecture to find the point of interest and, sometimes, these points are not found. For that reason, in this work we propose a novel method based on early fusion of low and high-level audio features for automatic topic segmentation in video lectures. We have performed experiments in two sets of video lectures where we obtained very satisfactory results that evidence the applicability of our method on improving content search in those videos.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {189–196},
numpages = {8},
keywords = {Audio processing, Automatic Speech Recognition, Knowledge base, Semantic annotation, Topic segmentation, Video lectures},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@article{10.5555/3207692.3207704,
author = {Paetzold, Gustavo H. and Specia, Lucia},
title = {A survey on lexical simplification},
year = {2017},
issue_date = {September 2017},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {60},
number = {1},
issn = {1076-9757},
abstract = {Lexical Simplification is the process of replacing complex words in a given sentence with simpler alternatives of equivalent meaning. This task has wide applicability both as an assistive technology for readers with cognitive impairments or disabilities, such as Dyslexia and Aphasia, and as a pre-processing tool for other Natural Language Processing tasks, such as machine translation and summarisation. The problem is commonly framed as a pipeline of four steps: the identification of complex words, the generation of substitution candidates, the selection of those candidates that fit the context, and the ranking of the selected substitutes according to their simplicity. In this survey we review the literature for each step in this typical Lexical Simplification pipeline and provide a benchmarking of existing approaches for these steps on publicly available datasets. We also provide pointers for datasets and resources available for the task.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {549–593},
numpages = {45}
}

@inproceedings{10.1145/3341161.3343510,
author = {M\"{u}ngen, Ahmet An\i{}l and Do\u{G}an, Emre and Kaya, Mehmet},
title = {Text generation with diversified source literature review},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343510},
doi = {10.1145/3341161.3343510},
abstract = {Almost all academic studies include a literature review section. This section is of significance in terms of presenting the value of the suggested method of the researcher and making comparisons. Due to the increasing number of academic papers and the emergence of various directories and indices, the time spent for finding the related previous studies is an important period for the researcher, which consumes a significant amount of time. By means of the suggested method, researchers can access various types of featured publications related to the keyword from different years from a single address. The system also helps to reveal an exemplary and guiding literature review among the found publications by conducting a text generation. The system uses the TF-IDF method for keyword-based publication search and "Template-Based Text Generation" method for the text generation algorithm. In the study, the largest open-access journal platform, T\"{U}BundefinedTAK Dergipark and SOBIAD Citation Index were used as the data set. As a result of the conducted tests, a method that supports the literature review process, even helping to the writing of literature review, was suggested. Along with the fact that there has not been an equivalent of the suggested study, the comparisons for success, "Text Generation" and "Literature Review" were independently calculated and presented.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {765–770},
numpages = {6},
keywords = {TF-IDF, academic data, literature generation, text generation, text mining},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.1145/3358501.3361235,
author = {Roy Chaudhuri, Subhrojyoti and Natarajan, Swaminathan and Banerjee, Amar and Choppella, Venkatesh},
title = {Methodology to develop domain specific modeling languages},
year = {2019},
isbn = {9781450369848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358501.3361235},
doi = {10.1145/3358501.3361235},
abstract = {Domain Specific Modeling Languages (DSML) significantly improve productivity in designing Computer Based System (CBS), by enabling them to be modeled at higher levels of abstraction. It is common for large and complex systems with distributed teams, to use DSMLs, to express and communicate designs of such systems uniformly, using a common language. DSMLs enable domain experts, with no or minimal software development background, to model solutions, using the language and terminologies used in their respective domains. Although, there are already a number of DSMLs available for modeling CBSs, their need is felt strongly across multiple domains, which still are not well supported with DSMLs. Developing a new DSML, however, is non trivial, as it requires (a) significant knowledge about the domain for which the DSML needs to be developed, as well as (b) skills to create new languages. In the current practice, DSMLs are developed by experts, who have substantial understanding of the domain of interest and strong background in computer science. One of the many challenges in the development of DSMLs, is the collection of domain knowledge and its utilization, based on which the abstract syntax, the backbone of the DSML is defined. There is a clear gap in the current state of art and practice, with respect to overcoming this challenge. We propose a methodology, which makes it easier for people with different backgrounds such as domain experts, solution architects, to contribute towards defining the abstract syntax of the DSML. The methodology outlines a set of steps to systematically capture knowledge about the domain of interest, and use that to arrive at the abstract syntax of the DSML. The key contribution of our work is in abstracting a CBS from a domain into a Domain Specific Machine, embodied in domain specific concepts. The methodology outlines, how the Domain Specific Machine, when coupled with guidelines from current practices of developing DSMLs, results in the definition of the abstract syntax of the intended DSML. We discuss our methodology in detail, in this paper.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Workshop on Domain-Specific Modeling},
pages = {1–10},
numpages = {10},
keywords = {domain specific language, domain specific modeling language, language engineering, modeling},
location = {Athens, Greece},
series = {DSM 2019}
}

@inproceedings{10.1145/3183713.3193562,
author = {Basik, Fuat and H\"{a}ttasch, Benjamin and Ilkhechi, Amir and Usta, Arif and Ramaswamy, Shekar and Utama, Prasetya and Weir, Nathaniel and Binnig, Carsten and Cetintemel, Ugur},
title = {DBPal: A Learned NL-Interface for Databases},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3193562},
doi = {10.1145/3183713.3193562},
abstract = {In this demo, we present DBPal, a novel data exploration tool with a natural language interface. DBPal leverages recent advances in deep models to make query understanding more robust in the following ways: First, DBPal uses novel machine translation models to translate natural language statements to SQL, making the translation process more robust to paraphrasing and linguistic variations. Second, to support the users in phrasing questions without knowing the database schema and the query features, DBPal provides a learned auto-completion model that suggests to users partial query extensions during query formulation and thus helps to write complex queries.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1765–1768},
numpages = {4},
keywords = {natural language to sql, nlidb, relational database, robust natural language interface},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3323771.3323824,
author = {Maroengsit, Wari and Piyakulpinyo, Thanarath and Phonyiam, Korawat and Pongnumkul, Suporn and Chaovalit, Pimwadee and Theeramunkong, Thanaruk},
title = {A Survey on Evaluation Methods for Chatbots},
year = {2019},
isbn = {9781450366397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323771.3323824},
doi = {10.1145/3323771.3323824},
abstract = {Nowadays chatbots have been widely adopted in many industries to automatically answer users' questions and requests via chat interfaces. While it has become much easier to develop a chatbot system, the system itself is a complex system in nature. It is a challenge to evaluate and compare various chatbot systems in terms of effectiveness, efficiency, goal achievability, and the ability to satisfy users. This paper presents a survey, starting from literature review, chatbot architecture, evaluation methods/criteria, and comparison of evaluation methods. Focused on the three subprocesses in the chatbot architecture: text processing, semantic understanding, and response generation. Moreover, the survey is conducted with classification of chatbot evaluation methods and their analysis according to chatbot types and three main evaluation schemes; content evaluation, user satisfaction, and chat function.},
booktitle = {Proceedings of the 2019 7th International Conference on Information and Education Technology},
pages = {111–119},
numpages = {9},
keywords = {Chatbot Evaluation, Information Systems, Natural Language Processing, Natural Language Understanding},
location = {Aizu-Wakamatsu, Japan},
series = {ICIET 2019}
}

@inproceedings{10.1145/3209978.3210046,
author = {Yang, Xiao and Awadallah, Ahmed Hassan and Khabsa, Madian and Wang, Wei and Wang, Miaosen},
title = {Characterizing and Supporting Question Answering in Human-to-Human Communication},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210046},
doi = {10.1145/3209978.3210046},
abstract = {Email continues to be one of the most important means of online communication. People spend a significant amount of time sending, reading, searching and responding to email in order to manage tasks, exchange information, etc. In this paper, we focus on information exchange over enterprise email in the form of questions and answers. We study a large scale publicly available email dataset to characterize information exchange via questions and answers in enterprise email. We augment our analysis with a survey to gain insights on the types of questions exchanged, when and how do people get back to them and whether this behavior is adequately supported by existing email management and search functionality. We leverage this understanding to define the task of extracting question/answer pairs from threaded email conversations. We propose a neural network based approach that matches the question to the answer considering comparisons at different levels of granularity. We also show that we can improve the performance by leveraging external data of question and answer pairs. We test our approach using a manually labeled email data collected using a crowd-sourcing annotation study. Our findings have implications for designing email clients and intelligent agents that support question answering and information lookup in email.},
booktitle = {The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {345–354},
numpages = {10},
keywords = {email, information retrieval, question answering},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{10.1145/3278721.3278778,
author = {Zhao, Jianxin and Mortier, Richard and Crowcroft, Jon and Wang, Liang},
title = {Privacy-Preserving Machine Learning Based Data Analytics on Edge Devices},
year = {2018},
isbn = {9781450360128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278721.3278778},
doi = {10.1145/3278721.3278778},
abstract = {Emerging Machine Learning (ML) techniques, such as Deep Neural Network, are widely used in today's applications and services. However, with social awareness of privacy and personal data rapidly rising, it becomes a pressing and challenging societal issue to both keep personal data private and benefit from the data analytics power of ML techniques at the same time. In this paper, we argue that to avoid those costs, reduce latency in data processing, and minimise the raw data revealed to service providers, many future AI and ML services could be deployed on users' devices at the Internet edge rather than putting everything on the cloud. Moving ML-based data analytics from cloud to edge devices brings a series of challenges. We make three contributions in this paper. First, besides the widely discussed resource limitation on edge devices, we further identify two other challenges that are not yet recognised in existing literature: lack of suitable models for users, and difficulties in deploying services for users. Second, we present preliminary work of the first systematic solution, i.e. Zoo, to fully support the construction, composing, and deployment of ML models on edge and local devices. Third, in the deployment example, ML service are proved to be easy to compose and deploy with Zoo. Evaluation shows its superior performance compared with state-of-art deep learning platforms and Google ML services.},
booktitle = {Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {341–346},
numpages = {6},
keywords = {edge computing, machine learning, privacy},
location = {New Orleans, LA, USA},
series = {AIES '18}
}

@inproceedings{10.1145/3216122.3216173,
author = {Vaira, Lucia and Bochicchio, Mario A. and Conte, Matteo and Casaluci, Francesco Margiotta and Melpignano, Antonio},
title = {MamaBot: a System based on ML and NLP for supporting Women and Families during Pregnancy},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216173},
doi = {10.1145/3216122.3216173},
abstract = {Artificial intelligence is transforming healthcare with a profound paradigm shift impacting diagnostic techniques, drug discovery, health analytics, interventions and much more. In this paper we focus on exploiting AI-based chatbot systems, mainly based on machine learning algorithms and Natural Language Processing, to understand and respond to needs of patients and their families. In particular, we describe an application scenario for an AI-chatbot delivering support to pregnant women, mothers, and families with young children, by giving them help and instructions in relevant situations.},
booktitle = {Proceedings of the 22nd International Database Engineering &amp; Applications Symposium},
pages = {273–277},
numpages = {5},
keywords = {Artificial Intelligence, Chatbot, Machine Learning, Natural Language Processing, eHealth, mHealth},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@inproceedings{10.1145/3366423.3380226,
author = {Chen, Jiaoyan and Chen, Xi and Horrocks, Ian and B. Myklebust, Erik and Jimenez-Ruiz, Ernesto},
title = {Correcting Knowledge Base Assertions},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380226},
doi = {10.1145/3366423.3380226},
abstract = {The usefulness and usability of knowledge bases (KBs) is often limited by quality issues. One common issue is the presence of erroneous assertions, often caused by lexical or semantic confusion. We study the problem of correcting such assertions, and present a general correction framework which combines lexical matching, semantic embedding, soft constraint mining and semantic consistency checking. The framework is evaluated using DBpedia and an enterprise medical KB.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1537–1547},
numpages = {11},
keywords = {Assertion Correction, Consistency Checking, Constraint Mining, Knowledge Base Quality, Semantic Embedding},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3106426.3109420,
author = {Franzoni, Valentina and Li, Yuanxi and Mengoni, Paolo},
title = {A path-based model for emotion abstraction on facebook using sentiment analysis and taxonomy knowledge},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109420},
doi = {10.1145/3106426.3109420},
abstract = {Each term in a short text can potentially convey emotional meaning. Facebook comments and shared posts often convey human biases, which play a pivotal role in information spreading and content consumption. Such bias is at the basis of human-generated content, and capable of conveying contexts which shape the opinion of users through the social media flow of information. Starting from the observation that a separation in topic clusters, i.e. sub-contexts, spontaneously occur if evaluated by human common sense, this work introduces a process for automated extraction of sub-context in Facebook. Basing on emotional abstraction and valence, the automated extraction is exploited through a class of path-based semantic similarity measures and sentiment analysis. Experimental results are obtained using validated clustering techniques on such features, on the domain of information security, over a sample of over 9 million page users. An additional expert evaluation of clusters in tag clouds confirms that the proposed automated algorithm for emotional abstraction clusters Facebook comments compatibly with human common sense. The baseline methods rely on the robust notion of collective concept similarity.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {947–952},
numpages = {6},
keywords = {artificial intelligence, collective knowledge, data mining, emotional abstraction, knowledge discovery, semantic distance, sentiment analysis, word similarity},
location = {Leipzig, Germany},
series = {WI '17}
}

@article{10.1145/3310254,
author = {Jha, Kishlay and Xun, Guangxu and Gopalakrishnan, Vishrawas and Zhang, Aidong},
title = {DWE-Med: Dynamic Word Embeddings for Medical Domain},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3310254},
doi = {10.1145/3310254},
abstract = {Recent advances in unsupervised language processing methods have created an opportunity to exploit massive text corpora for developing high-quality vector space representation (also known as word embeddings) of words. Towards this direction, practitioners have developed and applied several data driven embedding models with quite good rate of success. However, a drawback of these models lies in their premise of static context; wherein, the meaning of a word is assumed to remain the same over the period of time. This is limiting because it is known that the semantic meaning of a concept evolves over time. While such semantic drifts are routinely observed in almost all the domains; their effect is acute in domain such as biomedicine, where the semantic meaning of a concept changes relatively fast. To address this, in this study, we aim to learn temporally aware vector representation of medical concepts from the timestamped text data, and in doing so provide a systematic approach to formalize the problem. More specifically, a dynamic word embedding based model that jointly learns the temporal characteristics of medical concepts and performs across time-alignment is proposed. Apart from capturing the evolutionary characteristics in an optimal manner, the model also factors in the implicit medical properties useful for a variety of bio-medical applications. Empirical studies conducted on two important bio-medical use cases validates the effectiveness of the proposed approach and suggests that the model not only learns quality embeddings but also facilitates intuitive trajectory visualizations.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {19},
numpages = {21},
keywords = {Biomedical domain, temporal dynamics, word embeddings}
}

@inproceedings{10.1145/3340555.3353737,
author = {Soleymani, Mohammad and Stefanov, Kalin and Kang, Sin-Hwa and Ondras, Jan and Gratch, Jonathan},
title = {Multimodal Analysis and Estimation of Intimate Self-Disclosure},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3353737},
doi = {10.1145/3340555.3353737},
abstract = {Self-disclosure to others has a proven benefit for one’s mental health. It is shown that disclosure to computers can be similarly beneficial for emotional and psychological well-being. In this paper, we analyzed verbal and nonverbal behavior associated with self-disclosure in two datasets containing structured human-human and human-agent interviews from more than 200 participants. Correlation analysis of verbal and nonverbal behavior revealed that linguistic features such as affective and cognitive content in verbal behavior, and nonverbal behavior such as head gestures are associated with intimate self-disclosure. A multimodal deep neural network was developed to automatically estimate the level of intimate self-disclosure from verbal and nonverbal behavior. Between modalities, verbal behavior was the best modality for estimating self-disclosure within-corpora achieving r = 0.66. However, the cross-corpus evaluation demonstrated that nonverbal behavior can outperform language modality in cross-corpus evaluation. Such automatic models can be deployed in interactive virtual agents or social robots to evaluate rapport and guide their conversational strategy.},
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {59–68},
numpages = {10},
keywords = {natural language understanding, neural networks, nonverbal behavior, self-disclosure},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3018896.3036392,
author = {Xiao, Bin and Rahmani, Rahim},
title = {A deep relation learning method for IoT interoperability enhancement within semantic formalization framework},
year = {2017},
isbn = {9781450347747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018896.3036392},
doi = {10.1145/3018896.3036392},
abstract = {Internet of Things (IoT) is facing with the interoperability issue due to the massive amount of heterogeneous entities (both physical and virtual entities) constantly generating heterogeneous data objects; semantic formalization has been widely recognized as a basis for the IoT interoperability, by which IoT can acquire the ability to comprehend data and further recognize the logic relations among heterogeneous IoT entities and heterogeneous data objects, thus to establish mutual understanding between each other to support with interoperability. Even semantic-driven track has emphasizes a lot on the logic relations in connection to the service rules and policies for interoperability, it is important that the quantity-driven relations should be also explored with adhering to the framework of semantic formalization. This paper explores a Deep Recursive Auto-encoders formed data relation learner in line with the semantic framework, which supports the data interoperability enhancement in a quantity-driven way based on the logic-driven framework. The learner starts with representing the virtual IoT entities via feature extraction; based on that, learner is trained in a manner of considering the surrounding relations of the targeted entity. As a baseline, a contrast learner with "regular" structure has been proposed which cannot functionally support semantic framework, even though the semantic formalization is indispensable; regardless the limitations in lab environment, the conducted experiments show that the proposed learner performs a bit better than the contrast learner under the same conditions. So that, the proposed method can synergistically enhances the interoperability within a semantic formalization framework.},
booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
articleno = {149},
numpages = {8},
keywords = {big data, deep recursive neural network, internet of things, interoperability, semantic formalization},
location = {Cambridge, United Kingdom},
series = {ICC '17}
}

@inproceedings{10.1145/3308560.3316584,
author = {Wang, Dongsheng and Li, Qiuchi and Chaves Lima, Lucas and Grue Simonsen, Jakob and Lioma, Christina},
title = {Contextual Compositionality Detection with External Knowledge Bases and Word Embeddings},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316584},
doi = {10.1145/3308560.3316584},
abstract = {When the meaning of a phrase cannot be inferred from the individual meanings of its words (e.g., hot dog), that phrase is said to be non-compositional. Automatic compositionality detection in multi-word phrases is critical in any application of semantic processing, such as search engines [9]; failing to detect non-compositional phrases can hurt system effectiveness notably. Existing research treats phrases as either compositional or non-compositional in a deterministic manner. In this paper, we operationalize the viewpoint that compositionality is contextual rather than deterministic, i.e., that whether a phrase is compositional or non-compositional depends on its context. For example, the phrase “green card” is compositional when referring to a green colored card, whereas it is non-compositional when meaning permanent residence authorization. We address the challenge of detecting this type of contextual compositionality as follows: given a multi-word phrase, we enrich the word embedding representing its semantics with evidence about its global context (terms it often collocates with) as well as its local context (narratives where that phrase is used, which we call usage scenarios). We further extend this representation with information extracted from external knowledge bases. The resulting representation incorporates both localized context and more general usage of the phrase and allows to detect its compositionality in a non-deterministic and contextual way. Empirical evaluation of our model on a dataset of phrase compositionality1, manually collected by crowdsourcing contextual compositionality assessments, shows that our model outperforms state-of-the-art baselines notably on detecting phrase compositionality.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {317–323},
numpages = {7},
keywords = {Compositionality detection, Knowledge base, Word embedding},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3090354.3090380,
author = {Mifrah, Sara and Ben Lahmar, El Habib},
title = {Semantico-automatic Evaluation of Scientific Papers: State of the Art},
year = {2017},
isbn = {9781450348522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3090354.3090380},
doi = {10.1145/3090354.3090380},
abstract = {Natural Language Processing (NLP) and Automatic Language Processing (ALP) are fields that becomes more attractive for researchers in the study of potential and emerging applications. This article presents a state of the art, which aims to classify scientific documents automatically; Based on the semantic aspect defining the semantic links between the document and its references, determining its relevant subjects, and then comparing them with other cited documents. Citations considered as a bridge between a scientific paper and its references. The realization of this work requires the analysis of citations and the detection of all subjects "Topics" covered in the document.},
booktitle = {Proceedings of the 2nd International Conference on Big Data, Cloud and Applications},
articleno = {25},
numpages = {6},
keywords = {Semantic evaluation, automatic analysis, citation analysis, detection of subjects, scientific paper},
location = {Tetouan, Morocco},
series = {BDCA'17}
}

@inproceedings{10.1145/3343031.3350972,
author = {Sheng, Shurong and Moens, Marie-Francine},
title = {Generating Captions for Images of Ancient Artworks},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3350972},
doi = {10.1145/3343031.3350972},
abstract = {The neural encoder-decoder framework is widely adopted for image captioning of natural images. However, few works have contributed to generating captions for cultural images using this scheme. In this paper, we propose an artwork type enriched image captioning model where the encoder represents an input artwork image as a 512-dimensional vector and the decoder generates a corresponding caption based on the input image vector. The artwork type is first predicted by a convolutional neural network classifier and then merged into the decoder. We investigate multiple approaches to integrate the artwork type into the captioning model among which is one that applies a step-wise weighted sum of the artwork type vector and the hidden representation vector of the decoder. This model outperforms three baseline image captioning models for a Chinese art image captioning dataset on all evaluation metrics. One of the baselines is a state-of-the-art approach fusing textual image attributes into the captioning model for natural images. The proposed model also obtains promising results for another Egyptian art image captioning dataset.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {2478–2486},
numpages = {9},
keywords = {artwork type, image captioning, neural encoder-decoder},
location = {Nice, France},
series = {MM '19}
}

@article{10.1145/3380954,
author = {Nie, Liqiang and Li, Yongqi and Feng, Fuli and Song, Xuemeng and Wang, Meng and Wang, Yinglong},
title = {Large-Scale Question Tagging via Joint Question-Topic Embedding Learning},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3380954},
doi = {10.1145/3380954},
abstract = {Recent years have witnessed a flourishing of community-driven question answering (cQA), like Yahoo! Answers and AnswerBag, where people can seek precise information. After 2010, some novel cQA systems, including Quora and Zhihu, gained momentum. Besides interactions, the latter enables users to label the questions with topic tags that highlight the key points conveyed in the questions. In this article, we shed light on automatically annotating a newly posted question with topic tags that are predefined and preorganized into a directed acyclic graph. To accomplish this task, we present an end-to-end deep interactive embedding model to jointly learn the embeddings of questions and topics by projecting them into the same space for a similarity measure. In particular, we first learn the embeddings of questions and topic tags by two deep parallel models. Thereinto, we regularize the embeddings of topic tags via fully exploring their hierarchical structures, which is able to alleviate the problem of imbalanced topic distribution. Thereafter, we interact each question embedding with the topic tag matrix, i.e., all the topic tag embeddings. Following that, a sigmoid cross-entropy loss is appended to reward the positive question-topic pairs and penalize the negative ones. To justify our model, we have conducted extensive experiments on an unprecedented large-scale social QA dataset obtained from Zhihu.com, and the experimental results demonstrate that our model achieves superior performance to several state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {20},
numpages = {23},
keywords = {CQA, Question tagging, embedding learning, topic hierarchy}
}

@inbook{10.1145/3233795.3233798,
author = {Johnston, Michael},
title = {Multimodal integration for interactive conversational systems},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233798},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {21–76},
numpages = {56}
}

@inproceedings{10.1145/3077136.3080825,
author = {Glater, Rafael and Santos, Rodrygo L.T. and Ziviani, Nivio},
title = {Intent-Aware Semantic Query Annotation},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080825},
doi = {10.1145/3077136.3080825},
abstract = {Query understanding is a challenging task primarily due to the inherent ambiguity of natural language. A common strategy for improving the understanding of natural language queries is to annotate them with semantic information mined from a knowledge base. Nevertheless, queries with different intents may arguably benefit from specialized annotation strategies. For instance, some queries could be effectively annotated with a single entity or an entity attribute, others could be better represented by a list of entities of a single type or by entities of multiple distinct types, and others may be simply ambiguous. In this paper, we propose a framework for learning semantic query annotations suitable to the target intent of each individual query. Thorough experiments on a publicly available benchmark show that our proposed approach can significantly improve state-of-the-art intent-agnostic approaches based on Markov random fields and learning to rank. Our results further demonstrate the consistent effectiveness of our approach for queries of various target intents, lengths, and difficulty levels, as well as its robustness to noise in intent detection.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {485–494},
numpages = {10},
keywords = {intent-aware, learning to rank, semantic query annotation},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1109/ICSE.2017.9,
author = {Guo, Jin and Cheng, Jinghui and Cleland-Huang, Jane},
title = {Semantically enhanced software traceability using deep learning techniques},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.9},
doi = {10.1109/ICSE.2017.9},
abstract = {In most safety-critical domains the need for trace-ability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts; however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links; however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {3–14},
numpages = {12},
keywords = {deep learning, recurrent neural network, semantic representation, traceability},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/3349527,
author = {Tamine, Lynda and Soulier, Laure and Nguyen, Gia-Hung and Souf, Nathalie},
title = {Offline versus Online Representation Learning of Documents Using External Knowledge},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3349527},
doi = {10.1145/3349527},
abstract = {An intensive recent research work investigated the combined use of hand-curated knowledge resources and corpus-driven resources to learn effective text representations. The overall learning process could be run by online revising the learning objective or by offline refining an original learned representation. The differentiated impact of each of the learning approaches on the quality of the learned representations has not been studied so far in the literature. This article focuses on the design of comparable offline vs. online knowledge-enhanced document representation learning models and the comparison of their effectiveness using a set of standard IR and NLP downstream tasks. The results of quantitative and qualitative analyses show that (1) offline vs. online learning approaches have dissimilar result trends regarding the task as well as the dataset distribution counts with regard to domain application; (2) while considering external knowledge resources is undoubtedly beneficial, the way used to express relational constraints could affect semantic inference effectiveness. The findings of this work present opportunities for the design of future representation learning models, but also for providing insights about the evaluation of such models.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {42},
numpages = {34},
keywords = {Representation learning, information retrieval, knowledge resources, natural language processing}
}

@inbook{10.1145/3122865.3122867,
author = {Wu, Zuxuan and Yao, Ting and Fu, Yanwei and Jiang, Yu-Gang},
title = {Deep learning for video classification and captioning},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122867},
booktitle = {Frontiers of Multimedia Research},
pages = {3–29},
numpages = {27}
}

@article{10.1145/3290768.3290775,
author = {Zhao, Pengfei and Ma, Jian and Hua, Zhongsheng and Fang, Shijian},
title = {Academic Social Network-Based Recommendation Approach for Knowledge Sharing},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/3290768.3290775},
doi = {10.1145/3290768.3290775},
abstract = {Academic information overload has brought researchers great difficulty due to the rapid growth of scientific articles. Methods have been proposed to help professional readers find relevant articles on the basis of their publications. Although effectively sharing publications is essential to spreading knowledge and ideas, few studies have focused on knowledge sharing from an author perspective. This study leverages the online academic social network to propose a recommendation approach for knowledge sharing. In our approach, we integrate researcher-level and document-level analyses in the same model. Our model works in two stages: 1) researcher-level analysis and 2) document-level analysis. The former combines research topic relevance, social relations, and research quality dimension, and the latter uses the machine learning method to learn the vector representation for each word. Online social behavior information is also leveraged to enhance readers' short-term interests. Our approach is deployed in ScholarMate, a prevalent academic social network. Compared with other baseline methods (CB, LDA, and part of the proposed approach), our approach significantly improves the accuracy of recommendations. Moreover, our method can disseminate papers efficiently to readers who have no publications.},
journal = {SIGMIS Database},
month = nov,
pages = {78–91},
numpages = {14},
keywords = {academic social network, knowledge sharing, recommender systems}
}

@inproceedings{10.1145/3357384.3357967,
author = {Zou, Jie and Kanoulas, Evangelos},
title = {Learning to Ask: Question-based Sequential Bayesian Product Search},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357967},
doi = {10.1145/3357384.3357967},
abstract = {Product search is generally recognized as the first and foremost stage of online shopping and thus significant for users and retailers of e-commerce. Most of the traditional retrieval methods use some similarity functions to match the user's query and the document that describes a product, either directly or in a latent vector space. However, user queries are often too general to capture the minute details of the specific product that a user is looking for. In this paper, we propose a novel interactive method to effectively locate the best matching product. The method is based on the assumption that there is a set of candidate questions for each product to be asked. In this work, we instantiate this candidate set by making the hypothesis that products can be discriminated by the entities that appear in the documents associated with them. We propose a Question-based Sequential Bayesian Product Search method, QSBPS, which directly queries users on the expected presence of entities in the relevant product documents. The method learns the product relevance as well as the reward of the potential questions to be asked to the user by being trained on the search history and purchase behavior of a specific user together with that of other users. The experimental results show that the proposed method can greatly improve the performance of product search compared to the state-of-the-art baselines.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {369–378},
numpages = {10},
keywords = {bayesian search, learning to asking, product search, question-based search},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3106426.3109416,
author = {Ohbe, Tatsuya and Ozono, Tadachika and Shintani, Toramatsu},
title = {A sentiment polarity classifier for regional event reputation analysis},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3109416},
doi = {10.1145/3106426.3109416},
abstract = {It is important to analyze the reputation or demands for a regional event, such as a school festival. In our work, we use sentiment polarity classification in order to coordinate regional event reputation. We proposed sentiment polarity classification based on bag-of-words models in the previous works. To get over the traditional models, we proposed several classifier models based on deep learning models. As the application, we also described the overview of a system supports to analyze regional event reputation and an example of regional event analysis using our system. In this paper, we described how to improve the performance of the sentiment polarity classification using deep learning models. We compared the performance of four models in terms of the classification accuracy and the training speed. We found the Convolutional Neural Networks based model, three words convolutions, was the best model among the four models.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {1207–1213},
numpages = {7},
keywords = {convolutional neural networks, recurrent neural networks, sentiment polarity classification, sentiment visualization},
location = {Leipzig, Germany},
series = {WI '17}
}

@article{10.1109/TASLP.2017.2788182,
author = {Yu, Liang-Chih and Wang, Jin and Lai, K. Robert and Zhang, Xuejie},
title = {Refining Word Embeddings Using Intensity Scores for Sentiment Analysis},
year = {2018},
issue_date = {March 2018},
publisher = {IEEE Press},
volume = {26},
number = {3},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2788182},
doi = {10.1109/TASLP.2017.2788182},
abstract = {Word embeddings that provide continuous low-dimensional vector representations of words have been extensively used for various natural language processing tasks. However, existing context-based word embeddings such as Word2vec and GloVe typically fail to capture sufficient sentiment information, which may result in words with similar vector representations having an opposite sentiment polarity e.g., good and bad, thus degrading sentiment analysis performance. To tackle this problem, recent studies have suggested learning sentiment embeddings to incorporate the sentiment polarity positive and negative information from labeled corpora. This study adopts another strategy to learn sentiment embeddings. Instead of creating a new word embedding from labeled corpora, we propose a word vector refinement model to refine existing pretrained word vectors using real-valued sentiment intensity scores provided by sentiment lexicons. The idea of the refinement model is to improve each word vector such that it can be closer in the lexicon to both semantically and sentimentally similar words i.e., those with similar intensity scores and further away from sentimentally dissimilar words i.e., those with dissimilar intensity scores. An obvious advantage of the proposed method is that it can be applied to any pretrained word embeddings. In addition, the intensity scores can provide more fine-grained real-valued sentiment information than binary polarity labels to guide the refinement process. Experimental results show that the proposed refinement model can improve both conventional word embeddings and previously proposed sentiment embeddings for binary, ternary, and fine-grained sentiment classification on the SemEval and Stanford Sentiment Treebank datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {671–681},
numpages = {11}
}

@inproceedings{10.1145/3132847.3132947,
author = {Huang, Zhengjie and Ye, Zi and Li, Shuangyin and Pan, Rong},
title = {Length Adaptive Recurrent Model for Text Classification},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132947},
doi = {10.1145/3132847.3132947},
abstract = {In recent years, recurrent neural networks have been widely used for various text classification tasks. However, most of the recurrent architectures will not assign a class label to a text until they read the last word, while human beings are able to determine the text class before reading the whole text. In this paper, we propose a Length Adaptive Recurrent Model (LARM) which can automatically determine the minimum text length that is necessary to perform the classification. With three parts includingReader, Predictor andAgent, our model is designed to read a text word by word, and terminate the process when the adequate information has been caught for the text classification task. The experimental results show that our model has comparable or even better performance compared to the vanilla LSTM when both are fed with partial text input. Besides, we can speed up text classification by truncating the text when sufficient evidence is found for classification. Furthermore, we also visualize our model and show that our model works like human beings, who can gradually come up with the general idea of a text while reading texts sequentially.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1019–1027},
numpages = {9},
keywords = {recurrent neural network, text classification},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.5555/3241691.3241693,
author = {Gatt, Albert and Krahmer, Emiel},
title = {Survey of the state of the art in natural language generation: core tasks, applications and evaluation},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past two decades, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artifical intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of nlp, with an emphasis on different evaluation methods and the relationships between them.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {65–170},
numpages = {106}
}

@inbook{10.1145/3122865.3122878,
title = {Bibliography},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122878},
booktitle = {Frontiers of Multimedia Research},
pages = {315–377},
numpages = {63}
}

@inproceedings{10.1145/3127526.3127529,
author = {Accuosto, Pablo and Ronzano, Francesco and Ferr\'{e}s, Daniel and Saggion, Horacio},
title = {Multi-level mining and visualization of scientific text collections: Exploring a bi-lingual scientific repository},
year = {2017},
isbn = {9781450353885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127526.3127529},
doi = {10.1145/3127526.3127529},
abstract = {We present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles. The text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper's contents. In addition to the extraction and enrichment of documents with metadata (titles, authors, affiliations, etc), the deep analysis performed comprises semantic interpretation, rhetorical analysis of sentences, triple-based information extraction, and text summarization. The visualization components allow geographical-based exploration of collections, topic-evolution interpretation, and collaborative network analysis among others. The paper presents a case study of a bi-lingual collection in the field of Natural Language Processing (NLP).},
booktitle = {Proceedings of the 6th International Workshop on Mining Scientific Publications},
pages = {9–16},
numpages = {8},
keywords = {Big Scientific Data, Data Visualization, Information Extraction, Language Resources, PDF Conversion, Scientific Text Mining},
location = {Toronto, ON, Canada},
series = {WOSP 2017}
}

@article{10.1145/3237189,
author = {Marge, Matthew and Rudnicky, Alexander I.},
title = {Miscommunication Detection and Recovery in Situated Human–Robot Dialogue},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3237189},
doi = {10.1145/3237189},
abstract = {Even without speech recognition errors, robots may face difficulties interpreting natural-language instructions. We present a method for robustly handling miscommunication between people and robots in task-oriented spoken dialogue. This capability is implemented in TeamTalk, a conversational interface to robots that supports detection and recovery from the situated grounding problems of referential ambiguity and impossible actions. We introduce a representation that detects these problems and a nearest-neighbor learning algorithm that selects recovery strategies for a virtual robot. When the robot encounters a grounding problem, it looks back on its interaction history to consider how it resolved similar situations. The learning method is trained initially on crowdsourced data but is then supplemented by interactions from a longitudinal user study in which six participants performed navigation tasks with the robot. We compare results collected using a general model to user-specific models and find that user-specific models perform best on measures of dialogue efficiency, while the general model yields the highest agreement with human judges. Our overall contribution is a novel approach to detecting and recovering from miscommunication in dialogue by including situated context, namely, information from a robot’s path planner and surroundings.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = feb,
articleno = {3},
numpages = {40},
keywords = {Human–robot communication, human–robot interaction, language grounding, physically situated dialogue, spoken-dialogue systems}
}

@inbook{10.1145/3233795.3233801,
author = {Feld, Michael and Neβelrath, Robert and Schwartz, Tim},
title = {Software platforms and toolkits for building multimodal systems and applications},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233801},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {145–190},
numpages = {46}
}

@inbook{10.1145/3233795.3233814,
title = {Index},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233814},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {705–745},
numpages = {41}
}

@article{10.1145/3078841,
author = {Mills, Chris and Bavota, Gabriele and Haiduc, Sonia and Oliveto, Rocco and Marcus, Andrian and Lucia, Andrea De},
title = {Predicting Query Quality for Applications of Text Retrieval to Software Engineering Tasks},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3078841},
doi = {10.1145/3078841},
abstract = {Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts.Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself.Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery.Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average.Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {3},
numpages = {45},
keywords = {Text retrieval, artifact traceability, concept location}
}

@inbook{10.1145/3233795.3233797,
title = {Introduction: toward the design, construction, and deployment of multimodal-multisensor interfaces},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233797},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {1–20},
numpages = {20}
}

@inbook{10.1145/3233795.3233796,
title = {Preface},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233796},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {xvii–xix}
}

@inbook{10.1145/3233795.3233808,
author = {Sonntag, Daniel},
title = {Medical and health systems},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233808},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {423–476},
numpages = {54}
}

@article{10.1145/3092698,
author = {Kaur, Kiranbir and Sharma, DR. Sandeep and Kahlon, DR. Karanjeet Singh},
title = {Interoperability and Portability Approaches in Inter-Connected Clouds: A Review},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092698},
doi = {10.1145/3092698},
abstract = {Inter-connected cloud computing is an inherent evolution of Cloud Computing. Numerous benefits provided by connecting clouds have garnered attraction from the academic as well as the industry sector. Just as every new evolution faces challenges, inter-connected clouds have their own set of challenges such as security, monitoring, authorization and identity management, vendor lock-in, and so forth. This article considers the vendor lock-in problem, which is a direct consequence of the lack of interoperability and portability. An extensive literature review by surveying more than 120 papers has been done to analyze and categorize various solutions suggested in literature for solving the interoperability and portability issues of inter-connected clouds. After categorizing the solutions, the literature has been mapped to a specific solution and a comparative analysis of the papers under the same solution has been done. The term “inter-connected clouds” has been used generically in this article to refer to any collaboration of clouds which may be from the user side (Multi-clouds or Aggregated service by Broker) or the provider side (Federated clouds or Hybrid clouds). Lastly, two closely related issues (Brokers and Meta-scheduling) and the remaining challenges of inter-connected clouds are discussed.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {49},
numpages = {40},
keywords = {Cloud computing, inter-connected clouds, model-based techniques, open solutions, semantic-based techniques, standards}
}

@article{10.1145/3105906,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
abstract = {This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {17},
numpages = {24},
keywords = {Program repair, self-healing software}
}

@article{10.1162/evco_a_00266,
author = {Wever, Marcel and van Rooijen, Lorijn and Hamann, Heiko},
title = {Multioracle Coevolutionary Learning of Requirements Specifications from Examples in On-The-Fly Markets},
year = {2020},
issue_date = {Summer 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {28},
number = {2},
issn = {1063-6560},
url = {https://doi.org/10.1162/evco_a_00266},
doi = {10.1162/evco_a_00266},
abstract = {In software engineering, the imprecise requirements of a user are transformed to a formal requirements specification during the requirements elicitation process. This process is usually guided by requirements engineers interviewing the user. We want to partially automate this first step of the software engineering process in order to enable users to specify a desired software system on their own. With our approach, users are only asked to provide exemplary behavioral descriptions. The problem of synthesizing a requirements specification from examples can partially be reduced to the problem of grammatical inference, to which we apply an active coevolutionary learning approach. However, this approach would usually require many feedback queries to be sent to the user. In this work, we extend and generalize our active learning approach to receive knowledge from multiple oracles, also known as proactive learning. The ``user oracle'' represents input received from the user and the “knowledge oracle” represents available, formalized domain knowledge. We call our two-oracle approach the “first apply knowledge then query” (FAKT/Q) algorithm. We compare FAKT/Q to the active learning approach and provide an extensive benchmark evaluation. As result we find that the number of required user queries is reduced and the inference process is sped up significantly. Finally, with so-called On-The-Fly Markets, we present a motivation and an application of our approach where such knowledge is available.},
journal = {Evol. Comput.},
month = jun,
pages = {165–193},
numpages = {29},
keywords = {Multiobjective optimization, proactive learning, multioracle, coevolution, search-based software engineering, requirements specification, grammatical inference.}
}

@inbook{10.1145/3233795.3233802,
author = {Allen, James and Andr\'{e}, Elisabeth and Cohen, Philip R. and Hakkani-T\"{u}r, Dilek and Kaplan, Ronald and Lemon, Oliver and Traum, David},
title = {Challenge discussion: advancing multimodal dialogue},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233802},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {191–217},
numpages = {27}
}

@article{10.1145/3242177,
author = {Jarrar, Mustafa and Zaraket, Fadi and Asia, Rami and Amayreh, Hamzeh},
title = {Diacritic-Based Matching of Arabic Words},
year = {2018},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3242177},
doi = {10.1145/3242177},
abstract = {Words in Arabic consist of letters and short vowel symbols called diacritics inscribed atop regular letters. Changing diacritics may change the syntax and semantics of a word; turning it into another. This results in difficulties when comparing words based solely on string matching. Typically, Arabic NLP applications resort to morphological analysis to battle ambiguity originating from this and other challenges. In this article, we introduce three alternative algorithms to compare two words with possibly different diacritics. We propose the Subsume knowledge-based algorithm, the Imply rule-based algorithm, and the Alike machine-learning-based algorithm. We evaluated the soundness, completeness, and accuracy of the algorithms against a large dataset of 86,886 word pairs. Our evaluation shows that the accuracy of Subsume (100%), Imply (99.32%), and Alike (99.53%). Although accurate, Subsume was able to judge only 75% of the data. Both Subsume and Imply are sound, while Alike is not. We demonstrate the utility of the algorithms using a real-life use case -- in lemma disambiguation and in linking hundreds of Arabic dictionaries.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {10},
numpages = {21},
keywords = {Arabic, diacritics, disambiguation}
}

@inbook{10.1145/3233795.3233807,
author = {Valstar, Michel},
title = {Multimodal databases},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233807},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {393–421},
numpages = {29}
}

@inbook{10.1145/3233795.3233809,
author = {Schnelle-Walka, Dirk and Radomski, Stefan},
title = {Automotive multimodal human-machine interface},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233809},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {477–522},
numpages = {46}
}

@inbook{10.1145/3233795.3233804,
author = {Heloir, Alexis and Nunnari, Fabrizio and Bachynskyi, Myroslav},
title = {Ergonomics for the design of multimodal interfaces},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233804},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {263–304},
numpages = {42}
}

@inbook{10.1145/3233795.3233812,
author = {Cohen, Philip R. and Tumuluri, Raj},
title = {Commercialization of multimodal systems},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233812},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {621–658},
numpages = {38}
}

@inbook{10.1145/3233795.3233806,
author = {Tumuluri, Raj and Dahl, Deborah and Patern\`{o}, Fabio and Zancanaro, Massimo},
title = {Standardized representations and markup languages for multimodal interaction},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233806},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {347–392},
numpages = {46}
}

@inbook{10.1145/3233795.3233799,
author = {Skantze, Gabriel and Gustafson, Joakim and Beskow, Jonas},
title = {Multimodal conversational interaction with robots},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233799},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {77–104},
numpages = {28}
}

@article{10.1145/3363560,
author = {Zhao, Sicheng and Wang, Shangfei and Soleymani, Mohammad and Joshi, Dhiraj and Ji, Qiang},
title = {Affective Computing for Large-scale Heterogeneous Multimedia Data: A Survey},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3363560},
doi = {10.1145/3363560},
abstract = {The wide popularity of digital photography and social networks has generated a rapidly growing volume of multimedia data (i.e., images, music, and videos), resulting in a great demand for managing, retrieving, and understanding these data. Affective computing (AC) of these data can help to understand human behaviors and enable wide applications. In this article, we survey the state-of-the-art AC technologies comprehensively for large-scale heterogeneous multimedia data. We begin this survey by introducing the typical emotion representation models from psychology that are widely employed in AC. We briefly describe the available datasets for evaluating AC algorithms. We then summarize and compare the representative methods on AC of different multimedia types, i.e., images, music, videos, and multimodal data, with the focus on both handcrafted features-based methods and deep learning methods. Finally, we discuss some challenges and future directions for multimedia affective computing.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = dec,
articleno = {93},
numpages = {32},
keywords = {Affective computing, emotion recognition, large-scale multimedia, sentiment analysis}
}

@book{10.1145/3233795,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.}
}

@article{10.1162/coli_a_00342,
author = {Cheng, Jianpeng and Reddy, Siva and Saraswat, Vijay and Lapata, Mirella},
title = {Learning an executable neural semantic parser},
year = {2019},
issue_date = {March 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {45},
number = {1},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00342},
doi = {10.1162/coli_a_00342},
abstract = {This article describes a neural semantic parser that maps natural language utterances onto logical forms that can be executed against a task-specific environment, such as a knowledge base or a database, to produce a response. The parser generates tree-structured logical forms with a transition-based approach, combining a generic tree-generation algorithm with domain-general grammar defined by the logical language. The generation process is modeled by structured recurrent neural networks, which provide a rich encoding of the sentential context and generation history for making predictions. To tackle mismatches between natural language and logical form tokens, various attention mechanisms are explored. Finally, we consider different training settings for the neural semantic parser, including fully supervised training where annotated logical forms are given, weakly supervised training where denotations are provided, and distant supervision where only unlabeled sentences and a knowledge base are available. Experiments across a wide range of data sets demonstrate the effectiveness of our parser.},
journal = {Comput. Linguist.},
month = mar,
pages = {59–94},
numpages = {36}
}

@article{10.1145/3291124,
author = {Zhang, Jing and Li, Wanqing and Ogunbona, Philip and Xu, Dong},
title = {Recent Advances in Transfer Learning for Cross-Dataset Visual Recognition: A Problem-Oriented Perspective},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291124},
doi = {10.1145/3291124},
abstract = {This article takes a problem-oriented perspective and presents a comprehensive review of transfer-learning methods, both shallow and deep, for cross-dataset visual recognition. Specifically, it categorises the cross-dataset recognition into 17 problems based on a set of carefully chosen data and label attributes. Such a problem-oriented taxonomy has allowed us to examine how different transfer-learning approaches tackle each problem and how well each problem has been researched to date. The comprehensive problem-oriented review of the advances in transfer learning with respect to the problem has not only revealed the challenges in transfer learning for visual recognition but also the problems (e.g., 8 of the 17 problems) that have been scarcely studied. This survey not only presents an up-to-date technical review for researchers but also a systematic approach and a reference for a machine-learning practitioner to categorise a real problem and to look up for a possible solution accordingly.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {7},
numpages = {38},
keywords = {Cross-dataset recognition, domain adaptation}
}

@inbook{10.1145/3233795.3233803,
author = {Cafaro, Angelo and Pelachaud, Catherine and Marsella, Stacy C.},
title = {Nonverbal behavior in multimodal performances},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233803},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {219–262},
numpages = {44}
}

@inbook{10.1145/3233795.3233813,
author = {Friedland, Gerald and Tschantz, Michael Carl},
title = {Privacy concerns of multimodal sensor systems},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233813},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {659–704},
numpages = {46}
}

@inbook{10.1145/3233795.3233805,
author = {Hornung, Rachel and Chen, Nutan and van der Smagt, Patrick},
title = {Early integration for movement modeling in latent spaces},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233805},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {305–345},
numpages = {41}
}

@inbook{10.1145/3233795.3233800,
author = {Bohus, Dan and Horvitz, Eric},
title = {Situated interaction},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233800},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {105–143},
numpages = {39}
}

@inbook{10.1145/3233795.3233810,
author = {Kirchner, Elsa A. and Fairclough, Stephen H. and Kirchner, Frank},
title = {Embedded multimodal interfaces in robotics: applications, future trends, and societal implications},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233810},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {523–576},
numpages = {54}
}

@article{10.1145/3313801,
author = {Gon\c{c}ales, Lucian Jos\'{e} and Farias, Kleinner and Oliveira, Toacy Cavalcante De and Scholl, Murilo},
title = {Comparison of Software Design Models: An Extended Systematic Mapping Study},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3313801},
doi = {10.1145/3313801},
abstract = {Model comparison has been widely used to support many tasks in model-driven software development. For this reason, many techniques of comparing them have been proposed in the last few decades. However, academia and industry have overlooked a classification of currently available approaches to the comparison of design models. Hence, a thorough understanding of state-of-the-art techniques remains limited and inconclusive. This article, therefore, focuses on providing a classification and a thematic analysis of studies on the comparison of software design models. We carried out a systematic mapping study following well-established guidelines to answer nine research questions. In total, 56 primary studies (out of 4,132) were selected from 10 widely recognized electronic databases after a careful filtering process. The main results are that a majority of the primary studies (1) provide coarse-grained techniques of the comparison of general-purpose diagrams, (2) adopt graphs as principal data structure and compare software design models considering structural properties only, (3) pinpoint commonalities and differences between software design models rather than assess their similarity, and (4) propose new techniques while neglecting the production of empirical knowledge from experimental studies. Finally, this article highlights some challenges and directions that can be explored in upcoming studies.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {48},
numpages = {41},
keywords = {UML, model comparison, model similarity, software design models}
}

@article{10.1145/3037755,
author = {Muram, Faiz ul and Tran, Huy and Zdun, Uwe},
title = {Systematic Review of Software Behavioral Model Consistency Checking},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3037755},
doi = {10.1145/3037755},
abstract = {In software development, models are often used to represent multiple views of the same system. Such models need to be properly related to each other in order to provide a consistent description of the developed system. Models may contain contradictory system specifications, for instance, when they evolve independently. Therefore, it is very crucial to ensure that models conform to each other. In this context, we focus on consistency checking of behavior models. Several techniques and approaches have been proposed in the existing literature to support behavioral model consistency checking. This article presents a Systematic Literature Review (SLR) that was carried out to obtain an overview of the various consistency concepts, problems, and solutions proposed regarding behavior models. In our study, the identification and selection of the primary studies was based on a well-planned search strategy. The search process identified a total of 1770 studies, out of which 96 have been thoroughly analyzed according to our predefined SLR protocol. The SLR aims to highlight the state-of-the-art of software behavior model consistency checking and identify potential gaps for future research. Based on research topics in selected studies, we have identified seven main categories: targeted software models, types of consistency checking, consistency checking techniques, inconsistency handling, type of study and evaluation, automation support, and practical impact. The findings of the systematic review also reveal suggestions for future research, such as improving the quality of study design and conducting evaluations, and application of research outcomes in industrial settings. For this purpose, appropriate strategy for inconsistency handling, better tool support for consistency checking and/or development tool integration should be considered in future studies.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {17},
numpages = {39},
keywords = {Software behavioral model, consistency checking, consistency types, systematic literature review}
}

@inbook{10.1145/3015783.3015795,
author = {Cohen, Philip R. and Oviatt, Sharon},
title = {Multimodal speech and pen interfaces},
year = {2017},
isbn = {9781970001679},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3015783.3015795},
abstract = {This chapter describes interfaces that enable users to combine digital pen and speech input for interacting with computing systems. Such interfaces promise natural and efficient interaction, taking advantage of skills that users have developed over many years. Many applications for such systems have been explored, such as speech and pen systems for computer-aided design (CAD), with which an architect can sketch to create and position entities while speaking information about them. For instance, a user could draw a hardwood floor outline while saying "threefourths inch thick heart pine." In response, the CAD system would create a floor of the correct shape, thickness, and materials, while also updating the list of materials to purchase for the job. Then the user could touch the floor and say "finish with polyurethane." The user of such a system could concentrate on creating the planned building, without interrupting their concentration to navigate a complex interface menu system. In fact, multimodal CAD systems like Think3 are preferred by users, and have been documented to significantly increase their productivity by speeding up interaction 23% [Engineer Live 2013, Price 2004].This chapter will discuss how speech and pen multimodal systems have been built, and also how well they have performed. By pen input we include such devices as light pens, styluses, wireless digital pens, and digital pens that can write on paper while either storing digital data, or streaming it to a receiver [Anoto 2016]. We will also occasionally refer to other devices that can, like digital pens, provide a continuous stream of &lt; x, y &gt;coordinates---such as tracked laser pointers, finger input on touch-screens, and the ubiquitous mouse. Pen input devices can be used for a number of communicative functions, such as handwriting letters and numbers, drawing symbols, sketching diagrams or shapes, pointing, or gesturing (e.g., drawing an arrow to scroll a map). See the Glossary for defined terms.This chapter begins by discussing users' multimodal speech and pen interaction patterns, and the documented advantages of this type of multimodal system (Section 10.2). Section 10.3 describes the simulation infrastructure that's ideally required for prototyping new systems, and the process of collecting multimodal data resources. In terms of system development, Sections 10.4 and 10.5 outline general signal processing and information flow, and major architectural components. Section 10.6 describes implemented approaches to multimodal fusion and semantic integration. Section 10.7 presents examples of multimodal speech and pen systems, some of which are commercial applications [Tumuluri 2017], with the Sketch-Thru-Plan system provided as a walk-through case study. The chapter concludes with Section 10.8 by discussing future directions for research and development. As an aid to comprehension, readers are referred to the Glossary for newly introduced terms throughout the chapter, and also the Focus Questions at the end of the chapter.},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1},
pages = {403–447},
numpages = {45}
}

@inproceedings{10.1145/3339252.3341497,
author = {Chora\'{s}, Micha\l{} and Pawlicki, Marek and Kozik, Rafa\l{} and Demestichas, Konstantinos and Kosmides, Pavlos and Gupta, Manik},
title = {SocialTruth Project Approach to Online Disinformation (Fake News) Detection and Mitigation},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3341497},
doi = {10.1145/3339252.3341497},
abstract = {The extreme growth and adoption of Social Media, in combination with their poor governance and the lack of quality control over the digital content being published and shared, has led information veracity to a continuous deterioration. Current approaches entrust content verification to a single centralised authority, lack resilience towards attempts to successfully "game" verification checks, and make content verification difficult to access and use. In response, our ambition is to create an open, democratic, pluralistic and distributed ecosystem that allows easy access to various verification services (both internal and third-party), ensuring scalability and establishing trust in a completely decentralized environment. In fact, this is the ambition of the EU H2020 SocialTruth project. In this paper, we present the innovative project approach and the vision of effective online disinformation detection for various practical use-cases.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {68},
numpages = {10},
keywords = {detection, fake news, networks, pattern recognition, safety, security},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inbook{10.1145/3122865.3122866,
title = {Preface},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122866},
abstract = {The field of multimedia is dedicated to research and studies that leverage multiple modalities of signals and data in developing intelligent systems and technologies. Be it search engine, recommendation system, streaming service, interactive agent, or collaborative system, multimedia plays a critical role in ensuring full understanding of multimodal sensory signals, robust modeling of user-content interaction, natural and rich communication experience, and scalable system deployment. The goal is to utilize unique contributions from each modality, integrate complementary synergies, and achieve the best performance and novel functions beyond what's separately available in each individual medium. In this community, most contributors also maintain strong activities in other disciplines such as networking, computer vision, human-computer interaction, and machine learning. But the field of multimedia is unique in offering a rich and dynamic forum for researchers from "traditional" fields to collaborate and develop new solutions and knowledge that transcend the boundaries of individual disciplines.The field enjoys a long history of vibrant research. For example, the flagship ACM SIGMM Multimedia Conference was established in 1993, celebrating its 25th anniversary this year. The community also has several well-known conferences and journals organized by ACM, IEEE, and other groups, attracting a large number of researchers and practitioners from around the world. However, despite the prolific research activities and outcomes, there has been less effort toward developing books that serve as an introduction to the rich spectrum of topics in this broad field. Most of the few books available today either focus on specific subfields or basic background. There is a lack of tutorial-style materials covering the active topics being pursued by the leading researchers at frontiers of the field.SIGMM launched a new initiative to address this need in 2015, by selecting and inviting 12 rising-star speakers from different subfields of multimedia to deliver plenary tutorial style talks at ACM Multimedia 2015. Each speaker discussed challenges and the state of the art within their prospective research areas in a general manner to the broad community. Topics covered were comprehensive, including multimedia content understanding, multimodal human-human and human-computer interaction, multimedia social media, and multimedia system architecture and deployment. Following the very positive responses to the talks, these rising-star speakers were invited to expand the content covered in their talks to chapters that can be used as reference materials for researchers, students, and practitioners. Each resulting chapter discusses problems, technical challenges, state-of-the-art approaches and performances, open issues, and promising directions for future work. Collectively, the chapters provide an excellent sampling of major topics addressed by the community as a whole. This book, capturing outcomes of such efforts, is well positioned to fill the aforementioned needs by providing tutorial-style reference materials for frontier topics of multimedia.Section 1 of the book includes five chapters that are focused on analysis and understanding of multimedia content. Topics covered range from analysis of video content, audio content, multimodal content about interaction of freestanding conversational groups, and analysis of multimedia data in the encrypted format for preserving privacy on cloud servers, to efficient approximate similarity search techniques for searching over large-scale databases.First, Zuxuan Wu et al. review current research on understanding video content by detecting the classes of actions or events contained in a given video clip and generation of full-sentence captions describing the content in each such video. Unlike previous surveys, this review focuses on solutions based on deep learning, reflecting the recent trend of research in this area. The chapter also gives extensive reviews of the datasets used in state-of-the-art research and benchmarking efforts.Extending the modality from video to audio, in Chapter 2, Gerald Friedland et al. introduce the field of computer audition, aiming to develop the theory behind artificial systems that can extract information from sound. This chapter reviews the research datasets available, appropriate representations needed for audio, and a few challenging problems such as automatic extraction of hierarchical semantic structures from audio content and automatic discovery of high-level semantic concepts from massive audio data and associated metadata.The holy grail of research for the multimedia community is to be able to integrate and fuse information extracted from multiple modalities of data. In Chapter 3, Xavier Alameda-Pineda et al. present an excellent example and emergent research challenges in the application of detecting social interaction among freestanding conversational groups. The chapter includes overviews of research issues, approaches, evaluation of joint estimation of head and body poses using multiPreface modality data (such as wearable sensors and distributed camera networks), and results of detecting dynamic group formation of interacting people.Chapter 4 addresses a novel emerging topic prompted by the popular approach to multimedia analysis using cloud computing servers. When multimedia data is sent to the cloud for storage or processing, there is a risk of privacy breach via unauthorized access by third parties to the content in the cloud. Pradeep Atrey et al. review state-of-the-art methods and open issues for processing multimedia content in the encrypted domain without needing to convert data to the original format. This allows content to stay in its protected form while useful analysis is performed on it.In Chapter 5, Herv\'{e}e Jeundefinedou surveys efficient techniques for finding approximate solutions for similarity search, which is of particular interest when searching massive multimedia data like images, videos, and audio recordings. Jeundefinedou considers various performance factors like query speed, memory requirement, and search accuracy. Multiple frameworks based on locality sensitive hashing (LSH), quantization/ compression, and hybrid combinations are also reviewed in a coherent manner.In Section 2 of the book, the emphasis shifts from content analysis to humancentered aspects of multimedia computing. This new focus goes beyond extraction of semantic information from multimedia data. Instead, the broad research scope incorporates understanding of users and user-content interaction so as to improve effectiveness of multimedia systems in many applications, such as search and recommendation.Under the human-centric theme, Chapter 6, authored by Peng Cui, discusses the evolution of multimedia computing paradigms from the data-centric, to the content-centric, and recently to the human-centric. Cui presents a new framework, called social-sensed multimedia computing, to capture many key issues involved and advances achieved, including understanding of user-content interaction behavior, understanding of user intent, multimedia representation considering user intention, and integration of heterogeneous data sensed on multimedia social networks.Chapter 7 follows the human-centric theme and further moves the focus from processing individual multimedia data streams to processing a large number of heterogeneous streams in different modalities involving a large number of people. Analysis of such massive streams offers the possibility of detecting important situations of society, such as socio-economic affairs, as well as the living environment. Vivek Singh provides an overview of the problem definition, research framework, and the EventShop toolkit he developed for application development in this emerging area.The extension to the human-centric computing paradigm also calls for formal mathematical theories and tools for explaining the phenomena observed, such as the information propagation behaviors and the occurrences of information cascades on social networks. In Chapter 8, Marian-Andrei Rizoiu et al. review stochastic processes such as the Hawkes point process for modeling discrete, interdependent events over continuous time. These are strongly related to patterns corresponding to retweet cascade events on social media. Successful models like these can help researchers understand information dissemination patterns and predict popularity on social media.Interaction between users and content reveals not only the intent of the user (covered in Chapter 6), but also attributes of the content as well as of the user him/herself. Such interaction can be manifested in multiple forms including explicit cues such as visual and verbal expressions, and implicit cues such as eye movement and physiological signals like brain activity and heart rate. Chapter 9 includes a survey by Subramanian Ramanathan et al. on how such implicit user interaction cues can be explored to improve analysis of content (scene understanding) and user (user emotion recognition).To support research and development of emerging multimedia topics discussed above, there is a critical need for new generations of communication and computing systems that take into account the unique requirements of multimedia, such as real-time, high bandwidth, distributiveness, major power consumption, and resource uncertainty. The popular cloud-based computing systems, though prevalent formanyapplications, are not suitable for large-scale multimedia applications such as cloud-based gaming service and animation rendering service.The last section of the book focuses on the systems aspect, covering distinct topics of multimedia fog computing (Chapter 10) and cloud gaming (Chapter 11). Cheng-Hsin Hsu et al. survey the emerging paradigm focused on fog computing, in which computing services are crowdsourced to the edge nodes or even to the client devices on the user end. This offers major potential benefits in terms of low latency, location awareness, scalability, and heterogeneity. However, it also poses many significant challenges in areas such as resource discovery, resource allocation and management, quality of service, and security. Discussion of these challenges, along with recent advances in this area, are presented in Chapter 10.Finally, as a concrete example of large-scale distributed multimedia computing systems, Chapter 11 (by Kuan-Ta Chen et al.) presents a comprehensive survey of cloud gaming, with emphasis on the development of platform and testbed, test scenarios, and evaluation of performance, in order to enhance optimal design of various components of the complex cloud gaming systems. In particular, the chapter overviews extensive research in areas such as open-source platforms, cloud deployment, client design, and communication between gaming servers and clients.The scope of this book is by no means exhaustive or complete. For example, it can be expanded to include other important topics such as (but not limited to) multimedia content generation, multimodal knowledge discovery and representation, multimedia immersive networked environments, and applications in areas like healthcare, learning, and infrastructure. Nonetheless, the comprehensive survey materials already covered in the book provide an excellent foundation for exploring additional topics mentioned above, and many other relevant fields.We would like to give sincere acknowledgment to Dr. Svebor Karaman, who has provided tremendous assistance in communicating with contributors and organizing the content of this book. In addition, Diane Cerra and her team at Morgan &amp; Claypool Publishers have provided valuable guidance and editorial help},
booktitle = {Frontiers of Multimedia Research},
pages = {xi–xv}
}

@article{10.1162/coli_a_00378,
author = {Ranta, Aarne and Angelov, Krasimir and Gruzitis, Normunds and Kolachina, Prasanth},
title = {Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework
                    from Controlled Languages to Robust Pipelines},
year = {2020},
issue_date = {June 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00378},
doi = {10.1162/coli_a_00378},
abstract = {Abstract syntax is an interlingual representation used in compilers. Grammatical
                    Framework (GF) applies the abstract syntax idea to natural languages. The
                    development of GF started in 1998, first as a tool for controlled language
                    implementations, where it has gained an established position in both academic
                    and commercial projects. GF provides grammar resources for over 40 languages,
                    enabling accurate generation and translation, as well as grammar engineering
                    tools and components for mobile and Web applications. On the research side, the
                    focus in the last ten years has been on scaling up GF to wide-coverage language
                    processing. The concept of abstract syntax offers a unified view on many other
                    approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars,
                    and Abstract Meaning Representations. This makes it possible for GF to utilize
                    data from the other approaches and to build robust pipelines. In return, GF can
                    contribute to data-driven approaches by methods to transfer resources from one
                    language to others, to augment data by rule-based generation, to check the
                    consistency of hand-annotated corpora, and to pipe analyses into high-precision
                    semantic back ends. This article gives an overview of the use of abstract syntax
                    as interlingua through both established and emerging NLP applications involving
                    GF.},
journal = {Comput. Linguist.},
month = jun,
pages = {425–486},
numpages = {62}
}

@inbook{10.1145/3122865.3122872,
author = {Cui, Peng},
title = {Social-sensed multimedia computing},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122872},
booktitle = {Frontiers of Multimedia Research},
pages = {137–157},
numpages = {21}
}

@inbook{10.1145/3122865.3122868,
author = {Friedland, Gerald and Smaragdis, Paris and McDermott, Josh and Raj, Bhiksha},
title = {Audition for multimedia computing},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122868},
abstract = {What do the fields of robotics, human-computer interaction, AI, video retrieval, privacy, cybersecurity, Internet of Things, and big data all have in common? They all work with various sources of data: visual, textual, time stamps, links, records. But there is one source of data that has been almost completely ignored by the academic community---sound.Our comprehension of the world relies critically on audition---the ability to perceive and interpret the sounds we hear. Sound is ubiquitous, and is a unique source of information about our environment and the events occurring in it. Just by listening, we can determine whether our child's laughter originated inside or outside our house, how far away they were when they laughed, and whether the window through which the sound passed was open or shut. The ability to derive information about the world from sound is a core aspect of perceptual intelligence.Auditory inferences are often complex and sophisticated despite their routine occurrence. The number of possible inferences is typically not enumerable, and the final interpretation is not merely one of selection from a fixed set. And yet humans perform such inferences effortlessly, based only on sounds captured using two sensors, our ears.Electronic devices can also "perceive" sound. Every phone and tablet has at least one microphone, as do most cameras. Any device or space can be equipped with microphones at minimal expense. Indeed, machines can not only "listen"; they have potential advantages over humans as listening devices, in that they can communicate and coordinate their experiences in ways that biological systems simply cannot. Collections of devices that can sense sound and communicate with each other could instantiate a single electronic entity that far surpasses humans in its ability to record and process information from sound.And yet machines at present cannot truly hear. Apart from well-developed efforts to recover structure in speech and music, the state of the art in machine hearing is limited to relatively impoverished descriptions of recorded sounds: detecting occurrences of a limited pre-specified set of sound types, and their locations. Although researchers typically envision artificially intelligent agents such as robots to have human-like hearing abilities, at present the rich descriptions and inferences humans can make about sound are entirely beyond the capability of machine systems.In this chapter, we suggest establishing the field of Computer Audition to develop the theory behind artificial systems that extract information from sound. Our objective is to enable computer systems to replicate and exceed human abilities. This chapter describes the challenges of this field.},
booktitle = {Frontiers of Multimedia Research},
pages = {31–50},
numpages = {20}
}

@inbook{10.1145/3122865.3122869,
author = {Alameda-Pineda, Xavier and Ricci, Elisa and Sebe, Nicu},
title = {Multimodal analysis of free-standing conversational groups},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122869},
abstract = {"Free-standing conversational groups" are what we call the elementary building blocks of social interactions formed in settings when people are standing and congregate in groups. The automatic detection, analysis, and tracking of such structural conversational units captured on camera poses many interesting challenges for the research community. First, although delineating these formations is strongly linked to other behavioral cues such as head and body poses, finding methods that successfully describe and exploit these links is not obvious. Second, the use of visual data is crucial, but when analyzing crowded scenes, one must account for occlusions and low-resolution images. In this regard, the use of other sensing technologies such as wearable devices can facilitate the analysis of social interactions by complementing the visual information. Yet the exploitation of multiple modalities poses other challenges in terms of data synchronization, calibration, and fusion. In this chapter, we discuss recent advances in multimodal social scene analysis, in particular for the detection of conversational groups or F-formations [Kendon 1990]. More precisely, a multimodal joint head and body pose estimator is described and compared to other recent approaches for head and body pose estimation and F-formation detection. Experimental results on the recently published SALSA dataset are reported, they evidence the long road toward a fully automated high-precision social scene analysis framework.},
booktitle = {Frontiers of Multimedia Research},
pages = {51–74},
numpages = {24}
}

@inbook{10.1145/3122865.3122876,
author = {Hsu, Cheng-Hsin and Hong, Hua-Jun and Elgamal, Tarek and Nahrstedt, Klara and Venkatasubramanian, Nalini},
title = {Multimedia fog computing: minions in the cloud and crowd},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122876},
abstract = {In cloud computing, minions refer to virtual or physical machines that carry out the actual workload. Minions in the cloud hide in faraway data centers and thus cloud computing is less friendly to multimedia applications. The fog computing paradigm pushes minions toward edge networks. We adopt a generalized definition, where minions get into end devices owned by the crowd. The serious uncertainty, such as dynamic network conditions, limited battery levels, and unpredictable minion availability in multimedia fog platforms makes them harder to be managed than cloud platforms. In this chapter, we share our experience on utilizing resources from the crowd to optimize multimedia applications. The learned lessons shed some light on the optimal design of a unified multimedia fog platform for distributed multimedia applications.},
booktitle = {Frontiers of Multimedia Research},
pages = {255–286},
numpages = {32}
}

@inbook{10.1145/3122865.3122877,
author = {Chen, Kuan-Ta and Cai, Wei and Shea, Ryan and Huang, Chun-Ying and Liu, Jiangchuan and Leung, Victor C. M. and Hsu, Cheng-Hsin},
title = {Cloud gaming},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122877},
booktitle = {Frontiers of Multimedia Research},
pages = {287–314},
numpages = {28}
}

@inbook{10.1145/3122865.3122873,
author = {Singh, Vivek},
title = {Situation recognition using multimodal data},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122873},
booktitle = {Frontiers of Multimedia Research},
pages = {159–189},
numpages = {31}
}

@inbook{10.1145/3122865.3122875,
author = {Ramanathan, Subramanian and Gilani, Syed Omer and Sebe, Nicu},
title = {Utilizing implicit user cues for multimedia analytics},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122875},
booktitle = {Frontiers of Multimedia Research},
pages = {219–251},
numpages = {33}
}

@inbook{10.1145/3122865.3122874,
author = {Rizoiu, Marian-Andrei and Lee, Young and Mishra, Swapnil and Xie, Lexing},
title = {Hawkes processes for events in social media},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122874},
abstract = {This chapter provides an accessible introduction for point processes, and especially Hawkes processes, for modeling discrete, inter-dependent events over continuous time. We start by reviewing the definitions and key concepts in point processes. We then introduce the Hawkes process and its event intensity function, as well as schemes for event simulation and parameter estimation. We also describe a practical example drawn from social media data---we show how to model retweet cascades using a Hawkes self-exciting process.We present a design of the memory kernel, and results on estimating parameters and predicting popularity. The code and sample event data are available in an online repository.},
booktitle = {Frontiers of Multimedia Research},
pages = {191–218},
numpages = {28}
}

@inbook{10.1145/3122865.3122871,
author = {Jeundefinedou, Herv\'{e}},
title = {Efficient similarity search},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122871},
abstract = {This chapter addresses one of the fundamental problems involved in multimedia systems, namely efficient similarity search for large collections of multimedia content. This problem has received a lot of attention from various research communities. In particular, it is a historical line of research in computational geometry and databases. The computer vision and multimedia communities have adopted pragmatic approaches guided by practical requirements: the large sets of features required to describe image collections make visual search a highly demanding task. As a result, early works [Flickner et al. 1995, Fagin 1998, Beis and Lowe 1997] in image indexing have foreseen the interest in approximate algorithms, especially after the dissemination of methods based on local description in the 90s, as any improvement obtained on this indexing part improves the whole visual search system.Among the existing approximate nearest neighbors (ANN) strategies, the popular framework of Locality-Sensitive Hashing (LSH) [Indyk and Motwani 1998, Gionis et al. 1999] provides theoretical guarantees on the search quality with limited assumptions on the underlying data distribution. It was first proposed [Indyk and Motwani 1998] for the Hamming and l1 spaces, and was later extended to the Euclidean/ cosine cases [Charikar 2002, Datar et al. 2004] or the earth mover's distance [Charikar 2002, Andoni and Indyk 2006]. LSH has been successfully used for local descriptors [Ke et al. 2004], 3D object indexing [Matei et al. 2006, Shakhnarovich et al. 2006], and other fields such as audio retrieval [Casey and Slaney 2007, Ryynanen and Klapuri 2008]. It has also received some attention in a context of private information retrieval [Pathak and Raj 2012, Aghasaryan et al. 2013, Furon et al. 2013].A few years ago, approaches inspired by compression and more specifically quantization-based approaches [Jundefinedou et al. 2011] were shown to be a viable alternative to hashing methods, and shown successful for efficiently searching in a billion-sized dataset.This chapter discusses these different trends. It is organized as follows. Section 5.1 gives some background references and concepts, including evaluation issues. Most of the methods and variants are exposed within the LSH framework. It is worth mentioning that LSH is more of a concept than a particular algorithm. The search algorithms associated with LSH follow two distinct search mechanisms, the probe-cell model and sketches, which are discussed in Sections 5.2 and 5.3, respectively. Section 5.4 describes methods inspired by compression algorithms, while Section 5.5 discusses hybrid approaches combining the non-exhaustiveness of the cell-probe model with the advantages of sketches or compression-based algorithms. Other metrics than Euclidean and cosine are briefly discussed in Section 5.6.},
booktitle = {Frontiers of Multimedia Research},
pages = {105–134},
numpages = {30}
}

@inbook{10.1145/3122865.3122870,
author = {Atrey, Pradeep K. and Lathey, Ankita and Yakubu, Abukari M.},
title = {Encrypted domain multimedia content analysis},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122870},
booktitle = {Frontiers of Multimedia Research},
pages = {75–104},
numpages = {30}
}

@article{10.1145/3229329.3229333,
author = {Zhang, Jiawei and Yu, Philip S.},
title = {Broad Learning: An Emerging Area in Social Network Analysis},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3229329.3229333},
doi = {10.1145/3229329.3229333},
abstract = {Looking from a global perspective, the landscape of online social networks is highly fragmented. A large number of online social networks have appeared, which can provide users with various types of services. Generally, information available in these online social networks is of diverse categories, which can be represented as heterogeneous social networks (HSNs) formally. Meanwhile, in such an age of online social media, users usually participate in multiple online social networks simultaneously, who can act as the anchors aligning different social networks together. So multiple HSNs not only represent information in each social network, but also fuse information from multiple networks.Formally, the online social networks sharing common users are named as the aligned social networks, and these shared users are called the anchor users. The heterogeneous information generated by users' social activities in the multiple aligned social networks provides social network practitioners and researchers with the opportunities to study individual user's social behaviors across multiple social platforms simultaneously. This paper presents a comprehensive survey about the latest research works on multiple aligned HSNs studies based on the broad learning setting, which covers 5 major research tasks, including network alignment, link prediction, community detection, information diffusion and network embedding respectively.},
journal = {SIGKDD Explor. Newsl.},
month = may,
pages = {24–50},
numpages = {27},
keywords = {Network Embedding, Network Alignment, Link Prediction, Information Diffusion, Heterogeneous Social Networks, Community Detection}
}

@book{10.1145/3122865,
editor = {Chang, Shih-Fu},
title = {Frontiers of Multimedia Research},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {17},
abstract = {The field of multimedia is unique in offering a rich and dynamic forum for researchers from "traditional" fields to collaborate and develop new solutions and knowledge that transcend the boundaries of individual disciplines. Despite the prolific research activities and outcomes, however, few efforts have been made to develop books that serve as an introduction to the rich spectrum of topics covered by this broad field. A few books are available that either focus on specific subfields or basic background in multimedia. Tutorial-style materials covering the active topics being pursued by the leading researchers at frontiers of the field are currently lacking. In 2015, ACM SIGMM, the special interest group on multimedia, launched a new initiative to address this void by selecting and inviting 12 rising-star speakers from different subfields of multimedia research to deliver plenary tutorial-style talks at the ACM Multimedia conference for 2015. Each speaker discussed the challenges and state-of-the-art developments of their prospective research areas in a general manner to the broad community. The covered topics were comprehensive, including multimedia content understanding, multimodal human-human and human-computer interaction, multimedia social media, and multimedia system architecture and deployment.  Following the very positive responses to these talks, the speakers were invited to expand the content covered in their talks into chapters that can be used as reference material for researchers, students, and practitioners. Each chapter discusses the problems, technical challenges, state-of-the-art approaches and performances, open issues, and promising direction for future work. Collectively, the chapters provide an excellent sampling of major topics addressed by the community as a whole. This book, capturing some of the outcomes of such efforts, is well positioned to fill the aforementioned needs in providing tutorial-style reference materials for frontier topics in multimedia.}
}

@book{10.1145/3015783,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Foundations, User Modeling, and Common Modality Combinations - Volume 1},
year = {2017},
isbn = {9781970001679},
publisher = {Association for Computing Machinery and Morgan &amp; Claypool},
volume = {14},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces-user input involving new media (speech, multi-touch, gestures, writing) embedded in multimodal-multisensor interfaces. These interfaces support smartphones, wearables, in-vehicle, robotic, and many other applications that are now highly competitive commercially.   This edited collection is written by international experts and pioneers in the field. It provides a textbook for students, and a reference and technology roadmap for professionals working in this rapidly emerging area.    Volume 1 of the handbook presents relevant theory and neuroscience foundations for guiding the development of high-performance systems. Additional chapters discuss approaches to user modeling, interface design that supports user choice, synergistic combination of modalities with sensors, and blending of multimodal input and output. They also highlight an in-depth look at the most common multimodal-multisensor combinations- for example, touch and pen input, haptic and non-speech audio output, and speech co-processed with visible lip movements, gaze, gestures, or pen input. A common theme throughout is support for mobility and individual differences among users-including the world's rapidly growing population of seniors.    These handbook chapters provide walk-through examples and video illustrations of different system designs and their interactive use. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal-multisensor systems. In the final chapter, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces should be designed in the future to most effectively advance human performance.}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}


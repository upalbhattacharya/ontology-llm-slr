@inproceedings{10.1145/3544548.3581441,
author = {Ashby, Trevor and Webb, Braden K and Knapp, Gregory and Searle, Jackson and Fulda, Nancy},
title = {Personalized Quest and Dialogue Generation in Role-Playing Games: A Knowledge Graph- and Language Model-based Approach},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581441},
doi = {10.1145/3544548.3581441},
abstract = {Procedural content generation (PCG) in video games offers unprecedented opportunities for customization and user engagement. Working within the specialized context of role-playing games (RPGs), we introduce a novel framework for quest and dialogue generation that places the player at the core of the generative process. Drawing on a hand-crafted knowledge base, our method grounds generated content with in-game context while simultaneously employing a large-scale language model to create fluent, unique, accompanying dialogue. Through human evaluation, we confirm that quests generated using this method can approach the performance of hand-crafted quests in terms of fluency, coherence, novelty, and creativity; demonstrate the enhancement to the player experience provided by greater dynamism; and provide a novel, automated metric for the relevance between quest and dialogue. We view our contribution as a critical step toward dynamic, co-creative narrative frameworks in which humans and AI systems jointly collaborate to create unique and user-specific playable experiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {290},
numpages = {20},
keywords = {English, GPT-2, MMORPG, NPC dialogue, RPG, World of Warcraft, computational creativity, dynamic quest generation, human-AI co-creativity, human-computer interaction, knowledge graph, knowledge-grounded text generation, language model, large-scale language models, narrative, natural language processing, procedural content generation, quest, quests, text generation, transformers, video games},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3618295,
author = {Zhong, Lingfeng and Wu, Jia and Li, Qian and Peng, Hao and Wu, Xindong},
title = {A Comprehensive Survey on Automatic Knowledge Graph Construction},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3618295},
doi = {10.1145/3618295},
abstract = {Automatic knowledge graph construction aims at manufacturing structured human knowledge. To this end, much effort has historically been spent extracting informative fact patterns from different data sources. However, more recently, research interest has shifted to acquiring conceptualized structured knowledge beyond informative data. In addition, researchers have also been exploring new ways of handling sophisticated construction tasks in diversified scenarios. Thus, there is a demand for a systematic review of paradigms to organize knowledge structures beyond data-level mentions. To meet this demand, we comprehensively survey more than 300 methods to summarize the latest developments in knowledge graph construction. A knowledge graph is built in three steps: knowledge acquisition, knowledge refinement, and knowledge evolution. The processes of knowledge acquisition are reviewed in detail, including obtaining entities with fine-grained types and their conceptual linkages to knowledge graphs; resolving coreferences; and extracting entity relationships in complex scenarios. The survey covers models for knowledge refinement, including knowledge graph completion, and knowledge fusion. Methods to handle knowledge evolution are also systematically presented, including condition knowledge acquisition, condition knowledge graph completion, and knowledge dynamic. We present the paradigms to compare the distinction among these methods along the axis of the data environment, motivation, and architecture. Additionally, we also provide briefs on accessible resources that can help readers to develop practical knowledge graph systems. The survey concludes with discussions on the challenges and possible directions for future exploration.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {94},
numpages = {62},
keywords = {logic reasoning, knowledge fusion, knowledge graph completion, information extraction, deep learning, Knowledge graph}
}

@inproceedings{10.1145/3611643.3616317,
author = {Du, Xueying and Lou, Yiling and Liu, Mingwei and Peng, Xin and Yang, Tianyong},
title = {KG4CraSolver: Recommending Crash Solutions via Knowledge Graph},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616317},
doi = {10.1145/3611643.3616317},
abstract = {Fixing crashes is challenging, and developers often discuss their encountered crashes and refer to similar crashes and solutions on online Q&amp;A forums (e.g., Stack Overflow). However, a crash often involves very complex contexts, which includes different contextual elements, e.g., purposes, environments, code, and crash traces. Existing crash solution recommendation or general solution 
recommendation techniques only use an incomplete context or treat the entire context as pure texts to search relevant solutions for a given crash, resulting in inaccurate recommendation results. In this work, we propose a novel crash solution knowledge graph (KG) to summarize the complete crash context and its solution with a graph-structured representation. To construct the crash solution KG automatically, we propose to leverage prompt learning to construct the KG from SO threads with a small set of labeled data. Based on the constructed KG, we further propose a novel KG-based crash solution recommendation technique KG4CraSolver, which precisely finds the relevant SO thread for an encountered crash by finely analyzing and matching the complete crash context based on the crash 
solution KG. The evaluation results show that the constructed KG is of high quality and KG4CraSolver outperforms baselines in terms of all metrics (e.g., 13.4%-113.4% MRR improvements). Moreover, we perform a user study and find that KG4CraSolver helps participants find crash solutions 34.4% faster and 63.3% more accurately.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1242–1254},
numpages = {13},
keywords = {Crash Solution Recommendation, Knowledge Graph, Stack Overflow},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3581783.3612863,
author = {Liu, Runze and Fang, Yaqun and Yu, Fan and Tian, Ruiqi and Ren, Tongwei and Wu, Gangshan},
title = {Deep Video Understanding with Video-Language Model},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612863},
doi = {10.1145/3581783.3612863},
abstract = {Pre-trained video-language models (VLMs) have shown superior performance in high-level video understanding tasks, analyzing multi-modal information, aligning with Deep Video Understanding Challenge (DVUC) requirements.In this paper, we explore pre-trained VLMs' potential in multimodal question answering for long-form videos. We propose a solution called Dual Branches Video Modeling (DBVM), which combines knowledge graph (KG) and VLMs, leveraging their strengths and addressing shortcomings.The KG branch recognizes and localizes entities, fuses multimodal features at different levels, and constructs KGs with entities as nodes and relationships as edges.The VLM branch applies a selection strategy to adapt input movies into acceptable length and a cross-matching strategy to post-process results providing accurate scene descriptions.Experiments conducted on the DVUC dataset validate the effectiveness of our DBVM.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9551–9555},
numpages = {5},
keywords = {cross-matching strategy, deep video understanding, knowledge graph, pre-trained video-language model},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3617184.3630130,
author = {Ye, Xin and Wang, Shimin and Wang, Han and Wei, Qinwei and Yang, Ting and Tao, Yu},
title = {Application of Knowledge Graph in Financial Information Security Strategy},
year = {2023},
isbn = {9798400708800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617184.3630130},
doi = {10.1145/3617184.3630130},
abstract = {It is very important to utilize various financial security data resources for intelligent data processing and analysis, which can improve risk assessment, analysis, and early warning capabilities. This paper takes the knowledge graph as the core to study the application of the enhanced representation of the knowledge graph in financial information security risk control, which can give play to its advantages of knowledge integration, organizing the scattered and distributed multi-source heterogeneous security data, providing data analysis and knowledge reasoning support for threat modeling, risk analysis, attack reasoning, etc. in the network security, and thus providing a basis knowledge for screening potential information security risks. The establishment of knowledge graph enhanced by knowledge representation and reasoning has significantly improved the Identification efficiency of financial risk information, as well as the accuracy and integrity of the results of the strategy scheme, thus reducing the workload of manual screening and integration of information, and verifying the effectiveness of the financial information risk knowledge graph in this paper.},
booktitle = {Proceedings of the 8th International Conference on Cyber Security and Information Engineering},
pages = {188–192},
numpages = {5},
keywords = {financial information, information security, knowledge graph, security strategy, text knowledge enhancement},
location = {Putrajaya, Malaysia},
series = {ICCSIE '23}
}

@inproceedings{10.1145/3625469.3625470,
author = {Wu, Wenjing and Yuan, Qi and Chen, Qiulan and Cao, Yunzhong},
title = {Construction Safety Knowledge Graph Integrating Text and Image Information},
year = {2023},
isbn = {9798400707681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625469.3625470},
doi = {10.1145/3625469.3625470},
abstract = {To improve the extraction efficiency and visualization of construction safety knowledge, this paper combines knowledge graph technology with construction safety domain, and proposes a basic framework of construction safety knowledge ontology based on the evolution logic of safety events according to the characteristics of knowledge. Considering two types of safety knowledge carriers and data sources, text and image, a knowledge graph is designed to contain text semantic features and image features, and the knowledge services based on different dimensional knowledge queries are validated in the experiments. The results show that the BERT-BiLSTM-CRF algorithm can be used to extract entities in text, and YOLOv5-FastPose can extract excavator poses from images. This paper verifies the applicability of knowledge graphs for safety knowledge mining, visualization and services.},
booktitle = {Proceedings of the 2023 6th International Conference on Information Management and Management Science},
pages = {26–32},
numpages = {7},
keywords = {Pose estimation, Knowledge graph, Entity recognition, Construction safety management},
location = {Chengdu, China},
series = {IMMS '23}
}

@article{10.1145/3638065,
author = {Liu, Shenghao and Lu, Lingyun and Wang, Bang},
title = {On Mining User-Item Interactions via Knowledge Graph for Recommendation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638065},
doi = {10.1145/3638065},
abstract = {Introducing Knowledge Graph (KG) to facilitate recommender system has become a tendency in recent years. Many existing methods leverage KG to obtain side information of items to promote item representation learning for enhancing recommendation performance. However, they ignore that KG also may contribute to better user representation learning. To solve the above issue, we propose a novel algorithm, KIGR (Knowledge-aware Interaction Graph for Recommendation), to mine user-item interactions via Knowledge Graph for assisting user representation learning. Specifically, a user-item interaction is encoded by attentively summing up the relation embedding about the item in KG. Then an unsupervised learning method is used to group the user-item interactions into different latent types. Further, a user-item interaction graph is divided into several subgraphs, which is referred to as Knowledge-aware Interaction Graph, making each subgraph only contains one latent type of interactions. Finally, user representation is the fusion of user interest embedding, which is learned on knowledge-aware interaction graph; While item representation is learned on KG. Experimental results on MovieLens, LastFM and Amazon-Book validate that the proposed KIGR has a superior performance compared with the SOTA algorithms.},
note = {Just Accepted},
journal = {ACM Trans. Recomm. Syst.},
month = dec,
keywords = {Recommendation System; Knowledge Graph; Graph Neural Network}
}

@inproceedings{10.1145/3583780.3615105,
author = {Wang, Yu and Ye, Feng and Li, Binquan and Jin, Gaoyang and Xu, Dong and Li, Fengsheng},
title = {UrbanFloodKG: An Urban Flood Knowledge Graph System for Risk Assessment},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615105},
doi = {10.1145/3583780.3615105},
abstract = {Increasing numbers of people live in flood-prone areas worldwide. With continued development, urban flood will become more frequent, which has caused casualties and property damage. Researchers have been dedicating to urban flood risk assessments in recent years. However, current research is still facing the challenges of multi-modal data fusion and knowledge representation of urban flood events. Therefore, in this paper, we propose an Urban Flood Knowledge Graph (UrbanFloodKG) system that enables KG to support urban flood risk assessment. The system consists of data layer, graph layer, algorithm layer, and application layer, which implements knowledge extraction and storage functions, integrates knowledge representation learning models and graph neural network models to support link prediction and node classification tasks. We conduct model comparison experiments on link prediction and node classification tasks based on urban flood event data from Guangzhou, and demonstrate the effectiveness of the models used. Our experiments prove that the accuracy of risk assessment can reach 91% when using GEN, which provides a a promising research direction for urban flood risk assessment.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2574–2584},
numpages = {11},
keywords = {urban flood, link prediction, knowledge graph, graph neural network},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3576842.3589161,
author = {Zhang, Ruipeng and Xie, Mengjun},
title = {A Knowledge Graph Question Answering Approach to IoT Forensics},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3589161},
doi = {10.1145/3576842.3589161},
abstract = {Internet of Things (IoT) forensics has been a particularly challenging task for forensic practitioners due to the heterogeneity of IoT environments as well as the complexity and volume of IoT data. With the advent of artificial intelligence, question-answering (QA) systems have emerged as a potential solution for users to access sophisticated forensic knowledge and data. In this light, we present a novel IoT forensics framework that employs knowledge graph question answering (KGQA). Our framework enables investigators to access forensic artifacts and cybersecurity knowledge using natural language questions facilitated by a deep-learning-powered KGQA model. The proposed framework demonstrates high efficacy in answering natural language questions over the experimental IoT forensic knowledge graph.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {446–447},
numpages = {2},
keywords = {Digital Forensics, Internet of Things, Knowledge Graph, Ontology Design, Question Answering},
location = {San Antonio, TX, USA},
series = {IoTDI '23}
}

@article{10.1145/3597022,
author = {Yang, Yang and Zhang, Chubing and Song, Xin and Dong, Zheng and Zhu, Hengshu and Li, Wenjie},
title = {Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3597022},
doi = {10.1145/3597022},
abstract = {Learning and development, or L&amp;D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {33},
numpages = {27},
keywords = {Transformer, knowledge graph, Course recommendation}
}

@inproceedings{10.1145/3583780.3614769,
author = {Pahuja, Vardaan and Wang, Boshi and Latapie, Hugo and Srinivasa, Jayanth and Su, Yu},
title = {A Retrieve-and-Read Framework for Knowledge Graph Link Prediction},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614769},
doi = {10.1145/3583780.3614769},
abstract = {Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to superfluous computation, over-smoothing of node representations, and also limits their expressive power. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the reader, which incorporates graph-based attention structure and cross-attention between query and context for deep fusion. This simple yet effective design enables the model to focus on salient context information relevant to the query. Empirical results on two standard KG link prediction datasets demonstrate the competitive performance of the proposed method. Furthermore, our analysis yields valuable insights for designing improved retrievers within the framework.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {1992–2002},
numpages = {11},
keywords = {transformers, knowledge graph link prediction, knowledge graph completion, graph neural networks},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3627704,
author = {Li, Da and Zhu, Boqing and Yang, Sen and Xu, Kele and Yi, Ming and He, Yukai and Wang, Huaimin},
title = {Multi-task Pre-training Language Model for Semantic Network Completion},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {11},
issn = {2375-4699},
url = {https://doi.org/10.1145/3627704},
doi = {10.1145/3627704},
abstract = {Semantic networks, exemplified by the knowledge graph, serve as a means to represent knowledge by leveraging the structure of a graph. While the knowledge graph exhibits promising potential in the field of natural language processing, it suffers from incompleteness. This article focuses on the task of completing knowledge graphs by predicting linkages between entities, which is fundamental yet critical. Traditional methods based on translational distance struggle when dealing with unseen entities. In contrast, semantic matching presents itself as a potential solution due to its ability to handle such cases. However, semantic matching-based approaches necessitate large-scale datasets for effective training, which are typically unavailable in practical scenarios, hindering their competitive performance. To address this challenge, we propose a novel architecture for knowledge graphs known as LP-BERT, which incorporates a language model. LP-BERT consists of two primary stages: multi-task pre-training and knowledge graph fine-tuning. During the pre-training phase, the model acquires relationship information from triples by predicting either entities or relations through three distinct tasks. In the fine-tuning phase, we introduce a batch-based triple-style negative sampling technique inspired by contrastive learning. This method significantly increases the proportion of negative sampling while maintaining a nearly unchanged training time. Furthermore, we propose a novel data augmentation approach that leverages the inverse relationship of triples to enhance both the performance and robustness of the model. To demonstrate the effectiveness of our proposed framework, we conduct extensive experiments on three widely used knowledge graph datasets: WN18RR, FB15k-237, and UMLS. The experimental results showcase the superiority of our methods, with LP-BERT achieving state-of-the-art performance on the WN18RR and FB15k-237 datasets.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {250},
numpages = {20},
keywords = {multi-task learning, translational distance, semantic matching, link prediction, Knowledge graph}
}

@inproceedings{10.1145/3580305.3599791,
author = {Zang, Xiaoling and Hu, Binbin and Chu, Jun and Zhang, Zhiqiang and Zhang, Guannan and Zhou, Jun and Zhong, Wenliang},
title = {Commonsense Knowledge Graph towards Super APP and Its Applications in Alipay},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599791},
doi = {10.1145/3580305.3599791},
abstract = {The recently explosive growth of Super Apps brings great convenience to people's daily life by providing a wide variety of services through mini-programs, including online shopping, travel, finance, and so on. Due to the considerable gap between various scenarios, the restriction of effective information transfer and sharing severely blocks the efficient delivery of online services, potentially affecting the user's app experience. To deeply understand users' needs, we propose SupKG, a commonsense knowledge graph towards Super APP to help comprehensively characterize user behaviors across different business scenarios. In particular, our SupKG is carefully established from multiplex and heterogeneous data source in Alipay (a well-known Super App in China), which also emphasize abundant spatiotemporal relations and intent-related entities to answer the fundamental question in life service ''which service do users need at what time and where''.On the hand, the successful application of SupKG hinges on the effective form of network representation ie Knowledge Graph Embedding (KGE). However, a series of unsatisfying issues still need to be carefully considered in the industrial environment: i) bridging language representations with knowledge structure in a unified manner, ii) alleviating the skewed data distribution in SupKG, and iii) effectively characterizing hierarchical structures in SupKG. With these motivations, we develop a novel knowledge graph representation learning framework for SupKG, enabling various downstream applications to benefit from learned representations of entities and relations. Extensive experiments on the standard knowledge graph completion task demonstrate the consistent and significant performance improvement of our representation learning framework, which also greatly benefits the supplementation of potential knowledge of SupKG. Towards real-world applications in Alipay, our SupKG and learned representations show the potential superiority of integrating global behaviors in cold-start scenarios and providing high-quality knowledge for warming up the graph-based ranking.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5509–5519},
numpages = {11},
keywords = {super apps, representation learning, knowledge graph},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3583780.3615252,
author = {Fang, Jinyuan and Meng, Zaiqiao and Macdonald, Craig},
title = {KGPR: Knowledge Graph Enhanced Passage Ranking},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615252},
doi = {10.1145/3583780.3615252},
abstract = {Passage ranking aims to rank a set of passages based on their relevance to a query. Current state-of-the-art models for this task typically employ a cross-encoder structure. However, these models lack access to background knowledge, i.e., information related to the query that can be helpful in retrieving relevant passages. Knowledge Graphs (KGs) provide a structured way of storing information about entities and their relationships, offering valuable background knowledge about entities. While KGs have been used to augment pretrained language models (LMs) to perform several reasoning tasks such as question answering, it remains an open question of how to utilise the information from KGs to enhance the performance of cross-encoders on the passage ranking task. Therefore, we propose KGPR, a KG-enhanced cross-encoder for the Passage Retrieval task. KGPR is built upon LUKE, an entity-aware pretrained LM, with an additional module that fuses information from KGs into LUKE. By leveraging the background knowledge from KGs, KGPR enhances the model's comprehension of queries and passages, resulting in improved ranking performance. Experimental results demonstrate that using KGs can enhance the performance of LUKE in the passage retrieval task, and KGPR can outperform state-of-the-art monoT5 cross-encoder by 3.32% and 10.77% on the MS MARCO development set and TREC DL-HARD query set respectively, using a model with a similar number of parameters.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {3880–3885},
numpages = {6},
keywords = {passage ranking, knowledge graphs, cross-encoder},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3583780.3615294,
author = {Xiong, Bo and Nayyeri, Mojtaba and Daza, Daniel and Cochez, Michael},
title = {Reasoning beyond Triples: Recent Advances in Knowledge Graph Embeddings},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615294},
doi = {10.1145/3583780.3615294},
abstract = {Knowledge Graphs (KGs) are a collection of facts describing entities connected by relationships. KG embeddings map entities and relations into a vector space while preserving their relational semantics. This enables effective inference of missing knowledge from the embedding space. Most KG embedding approaches focused on triple-shaped KGs. A great amount of real-world knowledge, however, cannot simply be represented by triples. In this tutorial, we give a systematic introduction to KG embeddings that go beyond the triple representation. In particular, the tutorial will focus on temporal facts where the triples are enriched with temporal information, hyper-relational facts where the triples are enriched with qualifiers, n-ary facts describing relationships between multiple entities, and also facts that are augmented with literal and text descriptions. During the tutorial, we will introduce both fundamental knowledge and advanced topics for understanding recent embedding approaches for beyond-triple representations.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5228–5231},
numpages = {4},
keywords = {link prediction, knowledge representation and reasoning, knowledge graph embeddings, complex question answering},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3617379,
author = {Wang, Yashen and Ouyang, Xiaoye and Guo, Dayu and Zhu, Xiaoling},
title = {MEGA: Meta-Graph Augmented Pre-Training Model for Knowledge Graph Completion},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/3617379},
doi = {10.1145/3617379},
abstract = {Nowadays, a large number of Knowledge Graph Completion (KGC) methods have been proposed by using embedding based manners, to overcome the incompleteness problem faced with knowledge graph (KG). One important recent innovation in Natural Language Processing (NLP) domain is the employ of deep neural models that make the most of pre-training, culminating in BERT, the most popular example of this line of approaches today. Recently, a series of new KGC methods introducing a pre-trained language model, such as KG-BERT, have been developed and released compelling performance. However, previous pre-training based KGC methods usually train the model by using simple training task and only utilize one-hop relational signals in KG, which leads that they cannot model high-order semantic contexts and multi-hop complex relatedness. To overcome this problem, this article presents a novel pre-training framework for KGC task, which especially consists of both one-hop relation level task (low-order) and multi-hop meta-graph level task (high-order). Hence, the proposed method can capture not only the elaborate sub-graph structure but also the subtle semantic information on the given KG. The empirical results show the efficiency of the proposed method on the widely used real-world datasets.},
journal = {ACM Trans. Knowl. Discov. Data},
month = oct,
articleno = {30},
numpages = {24},
keywords = {semantic enhancement, multi-task learning, pre-training model, meta-graph, Knowledge graph completion}
}

@inproceedings{10.1145/3477495.3531757,
author = {Chen, Mingyang and Zhang, Wen and Zhu, Yushan and Zhou, Hongting and Yuan, Zonggang and Xu, Changliang and Chen, Huajun},
title = {Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531757},
doi = {10.1145/3477495.3531757},
abstract = {Knowledge graphs (KGs) consisting of a large number of triples have become widespread recently, and many knowledge graph embedding (KGE) methods are proposed to embed entities and relations of a KG into continuous vector spaces. Such embedding methods simplify the operations of conducting various in-KG tasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering). They can be viewed as general solutions for representing KGs. However, existing KGE methods are not applicable to inductive settings, where a model trained on source KGs will be tested on target KGs with entities unseen during model training. Existing works focusing on KGs in inductive settings can only solve the inductive relation prediction task. They can not handle other out-of-KG tasks as general as KGE methods since they don't produce embeddings for entities. In this paper, to achieve inductive knowledge graph embedding, we propose a model MorsE, which does not learn embeddings for entities but learns transferable meta-knowledge that can be used to produce entity embeddings. Such meta-knowledge is modeled by entity-independent modules and learned by meta-learning. Experimental results show that our model significantly outperforms corresponding baselines for in-KG and out-of-KG tasks in inductive settings.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {927–937},
numpages = {11},
keywords = {meta-learning, meta-knowledge transfer, knowledge graph, inductive knowledge graph embedding},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3539618.3592052,
author = {Yu, Donghan and Yang, Yiming},
title = {Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592052},
doi = {10.1145/3539618.3592052},
abstract = {The task of knowledge graph completion (KGC) is of great importance. To achieve scalability when dealing with large-scale knowledge graphs, recent works formulate KGC as a sequence-to-sequence process, where the incomplete triplet (input) and the missing entity (output) are both verbalized as text sequences. However, inference with these methods relies solely on the model parameters for implicit reasoning and neglects the use of KG itself, which limits the performance since the model lacks the capacity to memorize a vast number of triplets. To tackle this issue, we introduce ReSKGC, a Retrieval-enhanced Seq2seq KGC model, which selects semantically relevant triplets from the KG and uses them as evidence to guide output generation with explicit reasoning. Our method has demonstrated state-of-the-art performance on benchmark datasets Wikidata5M and WikiKG90Mv2, which contain about 5M and 90M entities, respectively.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2334–2338},
numpages = {5},
keywords = {information retrieval, knowledge graph completion, neural network},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3539618.3591846,
author = {Er-Rahmadi, Btissam and Oncevay, Arturo and Ji, Yuanyi and Pan, Jeff Z.},
title = {KATIE: A System for Key Attributes Identification in Product Knowledge Graph Construction},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591846},
doi = {10.1145/3539618.3591846},
abstract = {We present part of Huawei's efforts in building a Product Knowledge Graph (PKG). We want to identify which product attributes (i.e. properties) are relevant and important in terms of shopping decisions to product categories (i.e. classes). This is particularly challenging when the attributes and their values are mined from online product catalogues, i.e. HTML pages. These web pages contain semi-structured data, which do not follow a concerted format and use diverse vocabulary to designate the same features. We propose a system for key attribute identification (KATIE) based on fine-tuning pre-trained models (e.g., DistilBERT) to predict the applicability and importance of an attribute to a category. We also propose an attribute synonyms identification module that allows us to discover synonymous attributes by considering not only their labels' similarities but also the similarity of their values sets. We have evaluated our approach to Huawei categories taxonomy and a set of internally mined attributes from web pages. KATIE guarantees promising performance results compared to the most recent baselines.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3320–3324},
numpages = {5},
keywords = {entity resolution, fine-tuning, pre-trained language model, product knowledge graph, relation discovery},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3404835.3463072,
author = {Chung, Chanyoung and Whang, Joyce Jiyoung},
title = {Knowledge Graph Embedding via Metagraph Learning},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463072},
doi = {10.1145/3404835.3463072},
abstract = {Knowledge graph embedding aims to represent entities and relations in a continuous feature space while preserving the structure of a knowledge graph. Most existing knowledge graph embedding methods either focus only on a flat structure of the given knowledge graph or exploit the predefined types of entities to explore an enriched structure. In this paper, we define the metagraph of a knowledge graph by proposing a new affinity metric that measures the structural similarity between entities, and then grouping close entities by hypergraph clustering. Without any prior information about entity types, a set of semantically close entities is successfully merged into one super-entity in our metagraph representation. We propose the metagraph-based pre-training model of knowledge graph embedding where we first learn representations in the metagraph and initialize the entities and relations in the original knowledge graph with the learned representations. Experimental results show that our method is effective in improving the accuracy of state-of-the-art knowledge graph embedding methods.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2212–2216},
numpages = {5},
keywords = {clustering, embedding, knowledge graph, metagraph, pre-train},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3632314.3632332,
author = {Guo, Dongdong and Ma, Haitao and Zhao, Can and Peng, Hao and Du, Wenbo and Jiang, Zongrui and Zhang, Yan},
title = {Construction and Application of the Knowledge Graph Method in Maintenance of Robot in Automotive Manufacturing Industry},
year = {2023},
isbn = {9798400709401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632314.3632332},
doi = {10.1145/3632314.3632332},
abstract = {Based on the spare parts and structure data of industrial robots, the entity list of robot parts is established to form a query dictionary, and entity annotation is performed on the robot corpus by means of dictionary query, which reduces the cost of manual annotation and ensures the quality of annotation data. In the process of entity recognition training, Bert+Bilstm+CRF model structure is used to initially use 70% of the dictionary data for annotation, and the model is trained by iteratively increasing the annotation data in a continuous cycle, so that the model can extract all the entities in the robot corpus as much as possible. In addition, the material number/model information and PM maintenance content/strategy of the entity have been used as attributes of the entity. Meanwhile, the experience summarized by the failure model and effect analysis of industrial robots is fully utilized to connect the phenomena, causes and measures through the entities in order to build the industrial robot knowledge graph relationships. The constructed knowledge graph relationship is stored in a Neo4j graphical database, making it convenient for content retrieval and inquiry of application systems.In the industrial robot knowledge graph application side, the field maintenance personnel requirements are collected through a questionnaire survey and the requirements are classified into intent. A Bert+TextCNN structure model is built to realize the intention recognition of user inquiries. By combining entity recognition models and intent classification models, the system is able to better understand user inquiry needs, leading to the implementation of an intelligent maintenance system for industrial robots.},
booktitle = {Proceedings of the 2023 International Conference on Intelligent Sensing and Industrial Automation},
articleno = {15},
numpages = {9},
keywords = {Knowledge Graph, Robot Maintenance},
location = {Virtual Event, China},
series = {ISIA '23}
}

@inproceedings{10.1145/3503161.3548257,
author = {Ma, Yue and Wang, Yali and Wu, Yue and Lyu, Ziyu and Chen, Siran and Li, Xiu and Qiao, Yu},
title = {Visual Knowledge Graph for Human Action Reasoning in Videos},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548257},
doi = {10.1145/3503161.3548257},
abstract = {Action recognition has been traditionally treated as a high-level video classification problem. However, such a manner lacks the detailed and semantic understanding of body movement, which is the critical knowledge to explain and infer complex human actions. To fill this gap, we propose to summarize a novel visual knowledge graph from over 15M detailed human annotations, for describing action as the distinct composition of body parts, part movements and interactive objects in videos. Based on it, we design a generic multi-modal Action Knowledge Understanding (AKU) framework, which can progressively infer human actions from body part movements in the videos, with assistance of visual-driven semantic knowledge mining. Finally, we validate AKU on the recent Kinetics-TPS benchmark, which contains body part parsing annotations for detailed understanding of human action in videos. The results show that, our AKU significantly boosts various video backbones with explainable action knowledge in both supervised and few shot settings, and outperforms the recent knowledge-based action recognition framework, e.g., our AKU achieves 83.9% accuracy on Kinetics-TPS while PaStaNet achieves 63.8% accuracy under the same backbone. The codes and models will be released at https://github.com/mayuelala/AKU.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4132–4141},
numpages = {10},
keywords = {action recognition, knowledge graph, video understanding},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3477314.3507031,
author = {Liu, Xinglan and Hussain, Hussain and Razouk, Houssam and Kern, Roman},
title = {Effective use of BERT in graph embeddings for sparse knowledge graph completion},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507031},
doi = {10.1145/3477314.3507031},
abstract = {Graph embedding methods have emerged as effective solutions for knowledge graph completion. However, such methods are typically tested on benchmark datasets such as Freebase, but show limited performance when applied on sparse knowledge graphs with orders of magnitude lower density. To compensate for the lack of structure in a sparse graph, low dimensional representations of textual information such as word2vec or BERT embeddings have been used. This paper proposes a BERT-based method (BERT-ConvE), to exploit transfer learning of BERT in combination with a convolutional network model ConvE. Comparing to existing text-aware approaches, we effectively make use of the context dependency of BERT embeddings through optimizing the features extraction strategies. Experiments on ConceptNet show that the proposed method outperforms strong baselines by 50% on knowledge graph completion tasks. The proposed method is suitable for sparse graphs as also demonstrated by empirical studies on ATOMIC and sparsified-FB15k-237 datasets. Its effectiveness and simplicity make it appealing for industrial applications.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {799–802},
numpages = {4},
keywords = {BERT, context aware embedding, knowledge graph embedding, language model, sparse knowledge graph},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3487553.3524238,
author = {Xie, Xin and Zhang, Ningyu and Li, Zhoubo and Deng, Shumin and Chen, Hui and Xiong, Feiyu and Chen, Mosha and Chen, Huajun},
title = {From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524238},
doi = {10.1145/3487553.3524238},
abstract = {Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset OpenBG500 for research purpose1.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {162–165},
numpages = {4},
keywords = {Generation, Knowledge Graph Completion, Transformer},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3583780.3615110,
author = {Fan, Ziwei and Liu, Zhiwei and Heinecke, Shelby and Zhang, Jianguo and Wang, Huan and Xiong, Caiming and Yu, Philip S.},
title = {Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615110},
doi = {10.1145/3583780.3615110},
abstract = {Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset are conducted to verify the effectiveness of the proposed MPKG model.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {483–493},
numpages = {11},
keywords = {multi-task pre-training, product knowledge graph, zero-shot recommendation},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3591106.3592258,
author = {Deng, Jiaxin and Shen, Dong and Pan, Haojie and Wu, Xiangyu and Liu, Ximan and Meng, Gaofeng and Yang, Fan and Gao, Tingting and Fu, Ruiji and Wang, Zhongyuan},
title = {A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592258},
doi = {10.1145/3591106.3592258},
abstract = {Video understanding is an important task in short video business platforms and it has a wide application in video recommendation and classification. Most of the existing video understanding works only focus on the information that appeared within the video content, including the video frames, audio and text. However, introducing common sense knowledge from the external Knowledge Graph (KG) dataset is essential for video understanding when referring to the content which is less relevant to the video. Owing to the lack of video knowledge graph dataset, the work which integrates video understanding and KG is rare. In this paper, we propose a heterogeneous dataset that contains the multi-modal video entity and fruitful common sense relations. This dataset also provides multiple novel video inference tasks like the Video-Relation-Tag (VRT) and Video-Relation-Video (VRV) tasks. Furthermore, based on this dataset, we propose an end-to-end model that jointly optimizes the video understanding objective with knowledge graph embedding, which can not only better inject factual knowledge into video understanding but also generate effective multi-modal entity embedding for KG. Comprehensive experiments indicate that combining video understanding embedding with factual knowledge benefits the content-based video retrieval performance. Moreover, it also helps the model generate better knowledge graph embedding which outperforms traditional KGE-based methods on VRT and VRV tasks with at least 42.36% and 17.73% improvement in HITS@10.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {95–104},
numpages = {10},
keywords = {knowledge graph, multi-modal learning, video inference, video understanding},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3480001.3480022,
author = {Su, Mengmeng and Su, Hongyi and Zheng, Hong and Yan, Bo},
title = {Deep Learning For Knowledge Graph Completion With XLNET},
year = {2021},
isbn = {9781450390163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3480001.3480022},
doi = {10.1145/3480001.3480022},
abstract = {Knowledge Graph is a graph knowledge base composed of fact entities and relations. Recently, the adoption of Knowledge Graph in Natural Language Processing tasks has proved the efficiency and convenience of KG. Therefore, the plausibility of Knowledge Graph become an import subject, which is also named as KG Completion or Link Prediction. The plausibility of Knowledge Graph reflects in the validness of triples which is structured representation of the entities and relations of Knowledge Graph. Some research work has devoted to KG Completion tasks. The typical methods include semantic matching models like TransE or TransH and Pre-trained models like KG-BERT. In this article, we propose a novel method based on the pre-trained model XLNET and the classification model to verify whether the triples of Knowledge Graph are valid or not. This method takes description of entities or relations as the input sentence text for fine-tuning. Meanwhile contextualized representations with rich semantic information can be obtained by XLNET, avoiding limitations and shortcomings of other typical neural network models. Then these representations are fed into a classifier for classification. Experimental results show that there is an improvement in KG Completion Tasks that the proposed method has achieved.},
booktitle = {Proceedings of the 2021 5th International Conference on Deep Learning Technologies},
pages = {13–19},
numpages = {7},
keywords = {GRU, KG Completion, Knowledge Graph, LSTM, XLNet},
location = {Qingdao, China},
series = {ICDLT '21}
}

@inproceedings{10.1145/3543873.3587585,
author = {Timmer, Roelien C. and Mark, Megan and Khoo, Fech Scen and Ribeiro Martins, Marcella Scoczynski and Berea, Anamaria and Renard, Gregory and Bugbee, Kaylin},
title = {NASA Science Mission Directorate Knowledge Graph Discovery},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587585},
doi = {10.1145/3543873.3587585},
abstract = {The size of the National Aeronautics and Space Administration (NASA) Science Mission Directorate (SMD) data catalog is growing exponentially, allowing researchers to make discoveries. However, making discoveries is challenging and time-consuming due to the size of the data catalogs, and as many concepts and data are indirectly connected. This paper proposes a pipeline to generate knowledge graphs (KGs) representing different NASA SMD domains. These KGs can be used as the basis for dataset search engines, saving researchers time and supporting them in finding new connections. We collected textual data and used several modern natural language processing (NLP) methods to create the nodes and the edges of the KGs. We explore the cross-domain connections, discuss our challenges, and provide future directions to inspire researchers working on similar challenges.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {795–799},
numpages = {5},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3562007.3562030,
author = {Xu, Yong and Chen, Bao and Zhen, Jingru and Ma, Guoqing and Chen, Gongbin and Liu, Yan and Fang, Qun},
title = {NRKM: News Recommendation Based on Knowledge Graph with Multi-View Learning},
year = {2022},
isbn = {9781450396851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562007.3562030},
doi = {10.1145/3562007.3562030},
abstract = {News recommendation is necessary to help users find interesting news, improve their experience, and alleviate information overload. Accurately learning news and user representations is a key task in news recommendation systems. News texts usually contain rich entities, however existing recommender systems ignore the importance of news entities. In order to effectively alleviate the above problems, we design a multi-view news recommendation system based on knowledge graph. First, with news headlines, summaries, categories, and knowledge graph features, we learn news representations using a graph interactive attention network and a multi-head attention mechanism. Second, we combine a recurrent neural network and an interactive attention network to learn user representations from user historical click news records. Finally, predict the probability that the user will click on the candidate news. This method effectively alleviates the problem that the current news recommendation model has a weak ability to capture news representations and user interest representations. Experiments on real datasets show that this method can effectively improve the performance of news recommendation.},
booktitle = {Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System},
pages = {123–127},
numpages = {5},
keywords = {Attention Mechanism, Knowledge Graph, News Recommendation},
location = {Virtual Event, China},
series = {CCRIS '22}
}

@inproceedings{10.1145/3503161.3548273,
author = {Cao, Xianshuai and Shi, Yuliang and Wang, Jihu and Yu, Han and Wang, Xinjun and Yan, Zhongmin},
title = {Cross-modal Knowledge Graph Contrastive Learning for Machine Learning Method Recommendation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548273},
doi = {10.1145/3503161.3548273},
abstract = {The explosive growth of machine learning (ML) methods is overloading users with choices for learning tasks. Method recommendation aims to alleviate this problem by selecting the most appropriate ML methods for given learning tasks. Recent research shows that the descriptive and structural information of the knowledge graphs (KGs) can significantly enhance the performance of ML method recommendation. However, existing studies have not fully explored the descriptive information in KGs, nor have they effectively exploited the descriptive and structural information to provide the necessary supervision. To address these limitations, we distinguish descriptive attributes from the traditional relationships in KGs with the rest as structural connections to expand the scope of KG descriptive information. Based on this insight, we propose the Cross-modal Knowledge Graph Contrastive learning (CKGC) approach, which regards information from descriptive attributes and structural connections as two modalities, learning informative node representations by maximizing the agreement between the descriptive view and the structural view. Through extensive experiments, we demonstrate that CKGC significantly outperforms the state-of-the-art baselines, achieving around 2% higher accurate click-through-rate (CTR) prediction, over 30% more accurate top-10 recommendation, and over 50% more accurate top-20 recommendation compared to the best performing existing approach.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3694–3702},
numpages = {9},
keywords = {contrastive learning, cross modalities, knowledge graph, recommender system},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3615887.3627754,
author = {Rawsthorne, Helen Mair and Abadie, Nathalie and Kergosien, Eric and Duch\^{e}ne, C\'{e}cile and Saux, \'{E}ric},
title = {Automatic Nested Spatial Entity and Spatial Relation Extraction From Text for Knowledge Graph Creation: A Baseline Approach and a Benchmark Dataset},
year = {2023},
isbn = {9798400703492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615887.3627754},
doi = {10.1145/3615887.3627754},
abstract = {Automatically extracting geographic information from text is the key to harnessing the vast amount of spatial knowledge that only exists in this unstructured form. The fundamental elements of spatial knowledge include spatial entities, their types and the spatial relations between them. Structuring the spatial knowledge contained within text as a geospatial knowledge graph, and disambiguating the spatial entities, significantly facilitates its reuse. The automatic extraction of geographic information from text also allows the creation or enrichment of gazetteers. We propose a baseline approach for nested spatial entity and binary spatial relation extraction from text, a new annotated French-language benchmark dataset on the maritime domain that can be used to train algorithms for both extraction tasks, and benchmark results for the two tasks carried out individually and end-to-end. Our approach involves applying the Princeton University Relation Extraction system (PURE), made for flat, generic entity extraction and generic binary relation extraction, to the extraction of nested, spatial entities and spatial binary relations. By extracting nested spatial entities and the spatial relations between them, we have more information to aid entity disambiguation. In our experiments we compare the performance of a pretrained monolingual French BERT language model with that of a pretrained multilingual BERT language model, and study the effect of including cross-sentence context. Our results reveal very similar results for both models, although the multilingual model performs slightly better in entity extraction, and the monolingual model has slightly better relation extraction and end-to-end performances. We observe that increasing the amount of cross-sentence context improves the results for entity extraction whereas it has the opposite effect on relation extraction.},
booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
pages = {21–30},
numpages = {10},
keywords = {binary spatial relation, deep learning, geographic information, language model, maritime data, nested spatial entity, neural network, spatial knowledge},
location = {Hamburg, Germany},
series = {GeoHumanities '23}
}

@inproceedings{10.1145/3580305.3599801,
author = {Zhang, Shiyuan and Li, Tong and Hui, Shuodi and Li, Guangyu and Liang, Yanping and Yu, Li and Jin, Depeng and Li, Yong},
title = {Deep Transfer Learning for City-scale Cellular Traffic Generation through Urban Knowledge Graph},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599801},
doi = {10.1145/3580305.3599801},
abstract = {The problem of cellular traffic generation in cities without historical traffic data is critical and urgently needs to be solved to assist 5G base station deployments in mobile networks. In this paper, we propose ADAPTIVE, a deep transfer learning framework for city-scale cellular traffic generation through the urban knowledge graph. ADAPTIVE leverages historical data from other cities that have deployed 5G networks to assist cities that are newly deploying 5G networks through deep transfer learning. Specifically, ADAPTIVE can align the representations of base stations in the target city and source city while considering the environmental factors of cities, spatial and environmental contextual relations between base stations, and traffic temporal patterns at base stations. We next design a feature-enhanced generative adversarial network, which is trained based on the historical traffic data and representations of base stations in the source city. By feeding the aligned target city's base station representations into the trained model, we can then obtain the generated traffic data for the target city. Extensive experiments on real-world cellular traffic datasets show that ADAPTIVE generally outperforms state-of-the-art baselines by more than 40% in terms of Jensen-Shannon divergence and root-mean-square error. Also, ADAPTIVE has strong robustness based on the results of various cross-city experiments. ADAPTIVE has been successfully deployed on the 'Jiutian' Artificial Intelligence Platform of China Mobile to support cellular traffic generation and assist in the construction and operation of mobile networks.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4842–4851},
numpages = {10},
keywords = {gan}, keywords{cellular traffic, transfer learning, urban knowledge graph},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3534678.3539210,
author = {Liu, Xiao and Yin, Da and Zheng, Jingnan and Zhang, Xingjian and Zhang, Peng and Yang, Hongxia and Dong, Yuxiao and Tang, Jie},
title = {OAG-BERT: Towards a Unified Backbone Language Model for Academic Knowledge Services},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539210},
doi = {10.1145/3534678.3539210},
abstract = {Academic Knowledge Services have substantially facilitated the development of human science and technology, providing a plenitude of useful research tools. However, many applications highly depend on ad-hoc models and expensive human labeling to understand professional contents, hindering deployments in real world. To create a unified backbone language model for various knowledge-intensive academic knowledge mining challenges, based on the world's largest public academic graph Open Academic Graph (OAG), we pre-train an academic language model, namely OAG-BERT, to integrate massive heterogeneous entity knowledge beyond scientific corpora. We develop novel pre-training strategies along with zero-shot inference techniques. OAG-BERT's superior performance on 9 knowledge-intensive academic tasks (including 2 demo applications) demonstrates its qualification to serve as a foundation for academic knowledge services. Its zero-shot capability also offers great potential to mitigate the need of costly annotations. OAG-BERT has been deployed to multiple real-world applications, such as reviewer recommendations for NSFC (National Nature Science Foundation of China) and paper tagging in the AMiner system. All codes and pre-trained models are available via the CogDL.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3418–3428},
numpages = {11},
keywords = {heterogeneous knowledge graph, language model, pre-training},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3627915.3628091,
author = {Ding, Kai and Liu, Jiamei and Gu, Caiyuan and Wang, Ning and Fan, Xiaoyan},
title = {Research on Construction of Chinese Technology Literature Question Answering System Based on Knowledge Graph},
year = {2023},
isbn = {9798400700590},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627915.3628091},
doi = {10.1145/3627915.3628091},
abstract = {In order to improve the knowledge service ability of the library and meet the user's demand for querying Chinese technology literature in natural language. This article proposes a two-stage method to implement the question answering system for Chinese technology literature. In the first stage, the Chinese question classification system based on sentence pattern is designed, and the stacking framework of ensemble learning algorithm is used to classify Chinese question. In the second stage, the pipeline is employed to parse the natural language question sentences and transfer them into Cypher statement. The field of "competitive intelligence" is selected for experiment. The experimental results show that the proposed stacking classification model improves the metric F1 by 10.81% compared with the single model average. Overall, the accuracy of translating Chinese questions into Cypher statements reached 83.96%. The proposed method can effectively convert Chinese questions into Cypher statements and directly return the answer to users. It provides a reference scheme for the application of the question answering based on knowledge map, which can improve the knowledge service ability of the library.},
booktitle = {Proceedings of the 7th International Conference on Computer Science and Application Engineering},
articleno = {34},
numpages = {5},
keywords = {Cypher Statement, Ensemble Learning, Graph Database, Knowledge Graph, Q&amp;A System},
location = {Virtual Event, China},
series = {CSAE '23}
}

@inproceedings{10.1145/3539618.3591763,
author = {Yao, Yunzhi and Mao, Shengyu and Zhang, Ningyu and Chen, Xiang and Deng, Shumin and Chen, Xi and Chen, Huajun},
title = {Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591763},
doi = {10.1145/3539618.3591763},
abstract = {With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supervised data as a prompt for each sample, which is model-agnostic and can be plugged into widespread existing approaches. Experimental results demonstrate that previous methods integrated with RAP can achieve impressive performance gains in low-resource settings on five datasets of relational triple extraction and event extraction for knowledge graph construction Code is available in https://github.com/zjunlp/RAP.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {911–921},
numpages = {11},
keywords = {event extraction, prompt-based learning, triple extraction},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3477495.3531992,
author = {Chen, Xiang and Zhang, Ningyu and Li, Lei and Deng, Shumin and Tan, Chuanqi and Xu, Changliang and Huang, Fei and Si, Luo and Chen, Huajun},
title = {Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531992},
doi = {10.1145/3477495.3531992},
abstract = {Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware fusion modules. We conduct extensive experiments to validate that our MKGformer can obtain SOTA performance on four datasets of multimodal link prediction, multimodal RE, and multimodal NER1. https://github.com/zjunlp/MKGformer.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {904–915},
numpages = {12},
keywords = {knowledge graph completion, multimodal, named entity recognition, relation extraction},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3603765.3603771,
author = {Diaz Gonzalez, Armando D. and Hughes, Kevin S. and Yue, Songhui and Hayes, Sean T.},
title = {Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature},
year = {2023},
isbn = {9798400700637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603765.3603771},
doi = {10.1145/3603765.3603771},
abstract = {Published biomedical information has and continues to rapidly increase. The recent advancements in Natural Language Processing (NLP), have generated considerable interest in automating the extraction, normalization, and representation of biomedical knowledge about entities such as genes and diseases. Our study analyzes germline abstracts in the construction of knowledge graphs of the immense work that has been done in this area for genes and diseases. This paper presents SimpleGermKG, an automatic knowledge graph construction approach that connects germline genes and diseases. For the extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model on biomedical corpora. We propose an ontology-based and rule-based algorithm to standardize and disambiguate medical terms. For semantic relationships between articles, genes, and diseases, we implemented a part-whole relation approach to connect each entity with its data source and visualize them in a graph-based knowledge representation. Lastly, we discuss the knowledge graph applications, limitations, and challenges to inspire the future research of germline corpora. Our knowledge graph contains 297 genes, 130 diseases, and 46,747 triples. Graph-based visualizations are used to show the results.},
booktitle = {Proceedings of the 2023 7th International Conference on Information System and Data Mining},
pages = {37–42},
numpages = {6},
keywords = {BioBERT, entity recognition, germline mutations, knowledge graph, semantic relation},
location = {Atlanta, USA},
series = {ICISDM '23}
}

@inproceedings{10.1145/3459637.3482003,
author = {Deng, Cheng and Jia, Yuting and Xu, Hui and Zhang, Chong and Tang, Jingyao and Fu, Luoyi and Zhang, Weinan and Zhang, Haisong and Wang, Xinbing and Zhou, Chenghu},
title = {GAKG: A Multimodal Geoscience Academic Knowledge Graph},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482003},
doi = {10.1145/3459637.3482003},
abstract = {The research of geoscience plays a strong role in helping people gain a better understanding of the Earth. To effectively represent the knowledge (KG) from enormous geoscience research papers, knowledge graphs can be a powerful means. In the face of enormous geoscience research papers, knowledge graphs can be a powerful means to manage the relationships of data and integrate knowledge extracted from them. However, the existing geoscience KGs mainly focus on the external connection between concepts, whereas the potential abundant information contained in the internal multimodal data of the paper is largely overlooked for more fine-grained knowledge mining. To this end, we propose GAKG, a large-scale multimodal academic KG based on 1.12 million papers published in various geoscience-related journals. In addition to the bibliometrics elements, we also extracted the internal illustrations, tables, and text information of the articles, and dig out the knowledge entities of the papers and the era and spatial attributes of the articles, coupling multimodal academic data and features. Specifically, GAKG realizes knowledge entity extraction under our proposed Human-In-the-Loop framework, the novelty of which is to combine the techniques of machine reading and information retrieval with manual annotation of geoscientists in the loop. Considering the fact that literature of geoscience often contains more abundant illustrations and time scale information compared with that of other disciplines, we extract all the geographical information and era from the geoscience papers' text and illustrations, mapping papers to the atlas and chronology. Based on GAKG, we build several knowledge discovery benchmarks for finding geoscience communities and predicting potential links. GAKG and its services have been made publicly available and user-friendly.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4445–4454},
numpages = {10},
keywords = {data management, geoscience academic knowledge graph, information extraction, knowledge base},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3502223.3502244,
author = {Ye, Ganqiang and Zhang, Wen and Bi, Zhen and Wong, Chi Man and Hui, Chen and Chen, Huajun},
title = {Improving Knowledge Graph Representation Learning by Structure Contextual Pre-training},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502244},
doi = {10.1145/3502223.3502244},
abstract = {Representation learning models for Knowledge Graphs (KG) have proven to be effective in encoding structural information and performing reasoning over KGs. In this paper, we propose a novel pre-training-then-fine-tuning framework for knowledge graph representation learning, in which a KG model is firstly pre-trained with triple classification task, followed by discriminative fine-tuning on specific downstream tasks such as entity type prediction and entity alignment. Drawing on the general ideas of learning deep contextualized word representations in typical pre-trained language models, we propose SCoP to learn pre-trained KG representations with structural and contextual triples of the target triple encoded. Experimental results demonstrate that fine-tuning SCoP not only outperforms results of baselines on a portfolio of downstream tasks but also avoids tedious task-specific model design and parameter training.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {151–155},
numpages = {5},
keywords = {Contextual Triple, Embedding, Knowledge Graph, Pre-training},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@inproceedings{10.1145/3488560.3502193,
author = {Su, Juntao and Dougherty, Edward T. and Jiang, Shuang and Jin, Fang},
title = {An Interactive Knowledge Graph Based Platform for COVID-19 Clinical Research},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3502193},
doi = {10.1145/3488560.3502193},
abstract = {Since the first identified case of COVID-19 in December 2019, a plethora of pharmaceuticals and therapeutics have been tested for COVID-19 treatment. While medical advancements and breakthroughs are well underway, the sheer number of studies, treatments, and associated reports makes it extremely challenging to keep track of the rapidly growing COVID-19 research landscape. While existing scientific literature search systems provide basic document retrieval, they fundamentally lack the ability to explore data, and in addition, do not help develop a deeper understanding of COVID-19 related clinical experiments and findings. As research expands, results do so as well, resulting in a position that is complicated and overwhelming. To address this issue, we present a named entity recognition based framework that accurately extracts COVID-19 related information from clinical test results articles, and generates an efficient and interactive visual knowledge graph. This knowledge graph platform is user friendly, and provides intuitive and convenient tools to explore and analyze COVID-19 research data and results including medicinal performances, side effects and target populations.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1609–1612},
numpages = {4},
keywords = {clinical results, covid-19, knowledge graph, named entity recognition},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3508230.3508252,
author = {Mehta, Shreyansh and Radke, Mansi and Sunkle, Sagar},
title = {Named Entity Recognition using Knowledge Graph Embeddings and DistilBERT},
year = {2022},
isbn = {9781450387354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508230.3508252},
doi = {10.1145/3508230.3508252},
abstract = {Named Entity Recognition (NER) is a Natural Language Processing (NLP) task of identifying entities from a natural language text and classifies them into categories like Person, Location, Organization etc. Pre-trained neural language models (PNLM) based on transformers are state-of-the-art in many NLP task including NER. Analysis of output of DistilBERT, a popular PNLM, reveals that mis-classifications occur when a non-entity word is at a place contextually suitable for an entity. The paper is based on the hypothesis that the performance of a PNLM can be improved by combining it with Knowledge Graph Embeddings (KGE). We show that fine-tuning of DistilBERT along with NumberBatch KGE gives performance improvement over various Open-domain as well as Biomedical-domain datasets.},
booktitle = {Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval},
pages = {146–150},
numpages = {5},
keywords = {Contextualized Word Representation, Knowledge Graph Embeddings, Named Entity Recognition, Neural Networks},
location = {Sanya, China},
series = {NLPIR '21}
}

@inproceedings{10.1145/3460210.3493582,
author = {Jaradeh, Mohamad Yaser and Singh, Kuldeep and Stocker, Markus and Auer, S\"{o}ren},
title = {Triple Classification for Scholarly Knowledge Graph Completion},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493582},
doi = {10.1145/3460210.3493582},
abstract = {structured information representing knowledge encoded in scientific publications. With the sheer volume of published scientific literature comprising a plethora of inhomogeneous entities and relations to describe scientific concepts, these KGs are inherently incomplete. We present exBERT, a method for leveraging pre-trained transformer language models to perform scholarly knowledge graph completion. We model triples of a knowledge graph as text and perform triple classification (i.e., belongs to KG or not). The evaluation shows that exBERT outperforms other baselines on three scholarly KG completion datasets in the tasks of triple classification, link prediction, and relation prediction. Furthermore, we present two scholarly datasets as resources for the research community, collected from public KGs and online resources.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {225–232},
numpages = {8},
keywords = {link prediction, relation prediction, scholarly knowledge graphs, triple classification},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@inproceedings{10.1145/3624062.3624172,
author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
title = {HPC-GPT: Integrating Large Language Model for High-Performance Computing},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624172},
doi = {10.1145/3624062.3624172},
abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model’s responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT’s potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {951–960},
numpages = {10},
keywords = {Data Race Detection, High-performance Computing, Large Language Model, Neural Network., OpenMP},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@article{10.1109/TASLP.2023.3331096,
author = {Wang, Yile and Zhang, Yue and Li, Peng and Liu, Yang},
title = {Gradual Syntactic Label Replacement for Language Model Pre-Training},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3331096},
doi = {10.1109/TASLP.2023.3331096},
abstract = {Pre-training serves as a foundation of recent NLP models, where language modeling tasks are performed over large texts. Typical models like BERT and GPT take the corpus as a whole and treat each word equally for language modeling. However, recent works show that the naturally existing frequency bias in the raw corpus may limit the power of the language model. In this article, we propose a multi-stage training strategy that gradually increases the training vocabulary by modifying the training data. Specifically, we leverage the syntactic structure as a bridge for infrequent words and replace them with the corresponding syntactic labels, then we recover their original lexical surface for further training. Such strategy results in an easy-to-hard curriculum learning process, where the model learns the most common words and some basic syntax concepts, before recognizing a large number of uncommon words via their specific usages and the previously learned category knowledge. Experimental results show that such a method can improve the performance of both discriminative and generative pre-trained language models on benchmarks and various downstream tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {486–496},
numpages = {11}
}

@article{10.1145/3617174,
author = {Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and Peng, Xin and Xu, Xiwei and Lu, Qinghua},
title = {FQN Inference in Partial Code by Prompt-tuned Language Model of Code},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3617174},
doi = {10.1145/3617174},
abstract = {Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this article, we propose using a prompt-tuned code masked language model (MLM) as a neural knowledge base for type inference, called POME, which is lightweight and has minimal requirements on code compilation. Unlike the existing symbol name and context matching for type inference, POME infers the FQNs syntax and usage knowledge encapsulated in prompt-tuned code MLM through a colze-style fill-in-blank strategy. POME is integrated as a plug-in into web and integrated development environments (IDE) to assist developers in inferring FQNs in the real world. We systematically evaluate POME on a large amount of source code from GitHub and Stack Overflow, and explore its generalization and hybrid capability. The results validate the effectiveness of the POME design and its applicability for partial code type inference, and they can be easily extended to different programming languages (PL). POME can also be used to generate a PL-hybrid type inference model for providing a one-for-all solution. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {31},
numpages = {32},
keywords = {Type inference, fully qualified names, code masked language model, neural knowledge base}
}

@inproceedings{10.1145/3511808.3557318,
author = {Jin, Xin and Sun, Xia and Chen, Jiacheng and Sutcliffe, Richard},
title = {Extracting Drug-drug Interactions from Biomedical Texts using Knowledge Graph Embeddings and Multi-focal Loss},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557318},
doi = {10.1145/3511808.3557318},
abstract = {The field of Drug-drug interaction (DDI) aims to detect descriptions of interactions between drugs from biomedical texts. Currently, researchers have extracted DDIs using pre-trained language models such as BERT, which often misclassify two kinds of DDI types, "Effect" and "Int", on the DDIExtraction 2013 corpus because of highly similar expressions. The use of knowledge graphs can alleviate this problem by incorporating different relationships for each, thus allowing them to be distinguished. Thus, we propose a novel framework to integrate the neural network with a knowledge graph, where the features from these components are complementary. Specifically, we take text features at different levels into account in the neural network part. This is done by firstly obtaining a word-level position feature using PubMedBERT together with a convolution neural network, secondly, getting a phrase-level key path feature using a dependency parsing tree, thirdly, using PubMedBERT with an attention mechanism to obtain a sentence-level language feature, and finally, fusing these three kinds of representation into a synthesized feature. We also extract a knowledge feature from a drug knowledge graph which takes just a few minutes to construct, then concatenate the synthesized feature with the knowledge feature, feed the result into a multi-layer perceptron and obtain the result by a softmax classifier. In order to achieve a good integration of the synthesized feature and the knowledge feature, we train the model using a novel multifocal loss function, KGE-MFL, which is based on a knowledge graph embedding. Finally we attain state-of-the-art results on the DDIExtraction 2013 dataset (micro F-score 86.24%) and on the ChemProt dataset (micro F-score 77.75%), which proves our framework to be effective for biomedical relation extraction tasks. In particular, we fill the performance gap (more than 5.57%) between methods that rely on and do not rely on knowledge graph embedding on the DDIExtraction 2013 corpus, when predicting the "Int" type. The implementation code is available at https://github.com/NWU-IPMI/DDIE-KGE-MFL.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {884–893},
numpages = {10},
keywords = {drug-drug interactions, imbalance problem, knowledge graph, pubmedbert},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3579051.3579065,
author = {Wu, Zhanglin and Zhang, Min and Zhu, Ming and Li, Yinglu and Zhu, Ting and Yang, Hao and Peng, Song and Qin, Ying},
title = {KG-BERTScore: Incorporating Knowledge Graph into BERTScore for Reference-Free Machine Translation Evaluation},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579065},
doi = {10.1145/3579051.3579065},
abstract = {BERTScore is an effective and robust automatic metric for reference-based machine translation evaluation. In this paper, we incorporate multilingual knowledge graph into BERTScore and propose a metric named KG-BERTScore, which linearly combines the results of BERTScore and bilingual named entity matching for reference-free machine translation evaluation. From the experimental results on WMT19 QE as a metric without references shared tasks, our metric KG-BERTScore gets higher overall correlation with human judgements than the current state-of-the-art metrics for reference-free machine translation evaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore and the parameter for linear combination are also studied in this paper.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {121–125},
numpages = {5},
keywords = {BERTScore, KG-BERTScore, machine translation evaluation, multilingual knowledge graph, pre-trained multilingual model},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@inproceedings{10.1145/3543507.3584185,
author = {Gautam, Nikita and Shumway, David and Kowalcyk, Megan and Khanal, Sarthak and Caragea, Doina and Caragea, Cornelia and Mcginty, Hande and Dorevitch, Samuel},
title = {Leveraging Existing Literature on the Web and Deep Neural Models to Build a Knowledge Graph Focused on Water Quality and Health Risks},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3584185},
doi = {10.1145/3543507.3584185},
abstract = {A knowledge graph focusing on water quality in relation to health risks posed by water activities (such as diving or swimming) is not currently available. To address this limitation, we first use existing resources to construct a knowledge graph relevant to water quality and health risks using KNowledge Acquisition and Representation Methodology (KNARM). Subsequently, we explore knowledge graph completion approaches for maintaining and updating the graph. Specifically, we manually identify a set of domain-specific UMLS concepts and use them to extract a graph of approximately 75,000 semantic triples from the Semantic MEDLINE database (which contains head-relation-tail triples extracted from PubMed). Using the resulting knowledge graph, we experiment with the KG-BERT approach for graph completion by employing pre-trained BERT/RoBERTa models and also models fine-tuned on a collection of water quality and health risks abstracts retrieved from the Web of Science. Experimental results show that KG-BERT with BERT/RoBERTa models fine-tuned on a domain-specific corpus improves the performance of KG-BERT with pre-trained models. Furthermore, KG-BERT gives better results than several translational distance or semantic matching baseline models.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {4161–4171},
numpages = {11},
keywords = {BERT, Knowledge graph, diving, health risks, knowledge graph completion, water quality, water recreation},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3587716.3587723,
author = {Huang, Xinyi and Cheng, Lianglun and Deng, Jianfeng and Wang, Tao},
title = {Binocular attention-based stacked BiLSTM NER model for Supply chain management event knowledge graph construction},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587723},
doi = {10.1145/3587716.3587723},
abstract = {Extracting fine-grained event ontology knowledge based on supply chain management (SCM) related corpus and constructing knowledge graph (KG) has important guiding significance and knowledge support for the efficient implementation and development of SCM in manufacturing enterprises. Recently, research on the KG of SCM has not gained sufficient attention. This paper aims to propose an event logical KG construction approach for SCM. Specifically, a stacked BiLSTM entity recognition model based on the binocular attention mechanism is proposed, called the SBBAN model. Firstly, the character feature attention mechanism is used to infer the key information that contributes greatly to entity recognition in the text sequence. Character weighted features and character features splicing are used as new character input features. Then the deep semantic abstract features of text sequence are obtained by stacked BiLSTM. In addition, a self-attention mechanism is added to obtain the deep context relevant features. Experimental results show that the model shows better performance in in comparison with the state-of-the-art algorithms to complete the matching of event argument entities and offer knowledge support for SCM.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {40–46},
numpages = {7},
keywords = {event logic knowledge graph, named entity recognition, stacked BiLSTM, supply chain management ontology},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3539618.3591698,
author = {Atif, Farah and El Khatib, Ola and Difallah, Djellel},
title = {BeamQA: Multi-hop Knowledge Graph Question Answering with Sequence-to-Sequence Prediction and Beam Search},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591698},
doi = {10.1145/3539618.3591698},
abstract = {Knowledge Graph Question Answering (KGQA) is a task that aims to answer natural language queries by extracting facts from a knowledge graph. Current state-of-the-art techniques for KGQA rely on text-based information from graph entity and relations labels, as well as external textual corpora. By reasoning over multiple edges in the graph, these can accurately rank and return the most relevant entities. However, one of the limitations of these methods is that they cannot handle the inherent incompleteness of real-world knowledge graphs and may lead to inaccurate answers due to missing edges. To address this issue, recent advances in graph representation learning have led to the development of systems that can use link prediction techniques to handle missing edges probabilistically, allowing the system to reason with incomplete information. However, existing KGQA frameworks that use such techniques often depend on learning a transformation from the query representation to the graph embedding space, which requires access to a large training dataset. We present BeamQA, an approach that overcomes these limitations by combining a sequence-to-sequence prediction model with beam search execution in the embedding space. Our model uses a pre-trained large language model and synthetic question generation. Our experiments demonstrate the effectiveness of BeamQA when compared to other KGQA methods on two knowledge graph question-answering datasets.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {781–790},
numpages = {10},
keywords = {knowledge graphs, question answering},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3596219,
author = {Zhao, Shuai and Li, Qing and Yang, Yuer and Wen, Jinming and Luo, Weiqi},
title = {From Softmax to Nucleusmax: A Novel Sparse Language Model for Chinese Radiology Report Summarization},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3596219},
doi = {10.1145/3596219},
abstract = {The Chinese radiology report summarization is a crucial component in smart healthcare that employs language models to summarize key findings in radiology reports and communicate these findings to physicians. However, most language models for radiology report summarization utilize a softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is inefficient, reducing model interpretability and giving probability mass to many unrealistic outputs. To tackle this issue, we propose a novel approach named nucleusmax. Nucleusmax is able to mitigate dense outputs and improve model interpretability by truncating the unreliable tail of the probability distribution. In addition, we incorporate nucleusmax with a copy mechanism, a useful technique to avoid professional errors in the generated diagnostic opinions. To further promote the research of radiology report summarization, we also have created a Chinese radiology report summarization dataset, which is freely available. Experimental results showed via both automatic and human evaluation that the proposed approach substantially improves the sparsity and overall quality of outputs over competitive softmax models, producing radiology summaries that approach the quality of those authored by physicians. In general, our work demonstrates the feasibility and prospect of the language model to the domain of radiology and smart healthcare.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {180},
numpages = {21},
keywords = {Chinese radiology report summarization, language model, softmax, abstractive summarization}
}

@inproceedings{10.1145/3605801.3605806,
author = {Gao, Shangsheng and Gao, Li and Li, Qi and Xu, Jianjun},
title = {Application of large language model in intelligent Q&amp;A of digital government},
year = {2023},
isbn = {9798400700620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605801.3605806},
doi = {10.1145/3605801.3605806},
abstract = {LLM (Large Language Model) is developing rapidly today, and it is very important to use LLM to help existing question answering systems improve. The government question answering system is of great value in improving government administrative efficiency. Existing intelligent questions and answers mostly use the combination of keyword matching and machine learning. But keyword matching is difficult to understand complex and multi-round dialogue questions, resulting in limited quality of reply content. Based on machine learning method, it has further improved the understanding of semantics, but because the understanding of words in the field of government affairs is too difficult and professional; and the semantic recognition of colloquial questions is difficult, the performance is not even as good as the question answering system based on keyword matching. This paper uses the large language model as a tool to help understand user questions, and integrates it into the existing government question answering system. On the premise of effectively utilizing the advantages of the large language model, it avoids its defects. And it is demonstrated by experiments that the above system has a great improvement compared with the previous method.},
booktitle = {Proceedings of the 2023 2nd International Conference on Networks, Communications and Information Technology},
pages = {24–27},
numpages = {4},
keywords = {Digital government, Government intelligence Q&amp;A, LLM},
location = {Qinghai, China},
series = {CNCIT '23}
}

@inproceedings{10.1145/3543873.3587335,
author = {Su\'{a}rez, Francisca and Hogan, Aidan},
title = {Templet: A Collaborative System for Knowledge Graph Question Answering over Wikidata},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587335},
doi = {10.1145/3543873.3587335},
abstract = {We present Templet: an online question answering (QA) system for Wikidata. Templet is based on the collaboratively-edited repository QAWiki, which collects questions in multiple natural languages along with their corresponding structured queries. Templet generates templates from question–query pairs on QAWiki by replacing key entities with identifiers. Using autocompletion, the user can type a question in natural language, select a template, and again using autocompletion, select the entities they wish to insert into the template’s placeholders, generating a concrete question, query and results. The main objectives of Templet are: (i) to enable users to answer potentially complex questions over Wikidata using natural language templates and autocompletion; (ii)&nbsp;to encourage users to collaboratively create new templates via QAWiki, which in turn can benefit not only Templet, but other QA systems.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {152–155},
numpages = {4},
keywords = {Wikidata, knowledge graphs, question answering, user interfaces},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3565387.3565405,
author = {Chen, Wentong and Fan, Chunxiao and Wu, Yuexin and Wang, Yitong},
title = {Chinese Machine Reading Comprehension Based on Language Model Containing Knowledge},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565405},
doi = {10.1145/3565387.3565405},
abstract = {Machine reading comprehension (MRC) is a task that requires machines to answer relevant questions based on a given context. In recent years, it has attracted extensive attention with the development of deep learning and big data. Considering that human beings will associate some external relevant knowledge when understanding the text, researchers have proposed a method of introducing knowledge outside the given context to assist reading and this method is called Knowledge-Based Machine Reading Comprehension (KBMRC). However, the current research on this method is still scattered, and the retrieval and fusion of relevant knowledge are still two challenges in application, especially in Chinese MRC. The contribution of this paper mainly on the following three points: Firstly, in order to resolve the problem of related knowledge retrieval, we build up a related knowledge set. Secondly, in order to resolve the problem of related knowledge fusion, we propose a negative sample generation strategy and train a language model containing knowledge. Finally, a twin-tower fusion model is constructed based on this model. The experiments on Chinese reading comprehension dataset CMRC2018 show that our method has a certain improvement compared with the baseline method without external knowledge.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {18},
numpages = {7},
keywords = {knowledge fusion, machine reading comprehension, natural language processing, pre-trained language model},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.1145/3442381.3450043,
author = {Wang, Bo and Shen, Tao and Long, Guodong and Zhou, Tianyi and Wang, Ying and Chang, Yi},
title = {Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450043},
doi = {10.1145/3442381.3450043},
abstract = {Human-curated knowledge graphs provide critical supportive information to various natural language processing tasks, but these graphs are usually incomplete, urging auto-completion of them (a.k.a. knowledge graph completion). Prevalent graph embedding approaches, e.g., TransE, learn structured knowledge via representing graph elements (i.e., entities/relations) into dense embeddings and capturing their triple-level relationship with spatial distance. However, they are hardly generalizable to the elements never visited in training and are intrinsically vulnerable to graph incompleteness. In contrast, textual encoding approaches, e.g., KG-BERT, resort to graph triple’s text and triple-level contextualized representations. They are generalizable enough and robust to the incompleteness, especially when coupled with pre-trained encoders. But two major drawbacks limit the performance: (1) high overheads due to the costly scoring of all possible triples in inference, and (2) a lack of structured knowledge in the textual encoder. In this paper, we follow the textual encoding paradigm and aim to alleviate its drawbacks by augmenting it with graph embedding techniques – a complementary hybrid of both paradigms. Specifically, we partition each triple into two asymmetric parts as in translation-based graph embedding approach, and encode both parts into contextualized representations by a Siamese-style textual encoder. Built upon the representations, our model employs both deterministic classifier and spatial measurement for representation and structure learning respectively. It thus reduces the overheads by reusing graph elements’ embeddings to avoid combinatorial explosion, and enhances structured knowledge by exploring the spatial characteristics. Moreover, we develop a self-adaptive ensemble scheme to further improve the performance by incorporating triple scores from an existing graph embedding model. In experiments, we achieve state-of-the-art performance on three benchmarks and a zero-shot dataset for link prediction, with highlights of inference costs reduced by 1-2 orders of magnitude compared to a sophisticated textual encoding method.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1737–1748},
numpages = {12},
keywords = {Contextualized Representation, Knowledge Graph Completion, Knowledge Graph Embedding, Link Prediction, Structured Knowledge},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3500931.3500947,
author = {Sun, Yue and Li, Yuxuan and Zheng, Bo and Zhu, ShaoJun and Wu, MaoNian},
title = {An Intelligent Question-Answering System for Myopia Prevention and Control based on Knowledge Graph},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500947},
doi = {10.1145/3500931.3500947},
abstract = {China is facing a serious visual health crisis, especially the trend of high incidence of myopia and low age. The Chinese government pays special attention to the vision health of young people. The prevention and treatment of myopia in children and adolescents has become a public health issue of general concern. In order to further promote the prevention and control of myopia and the management of vision health, this research actively tracks social hot issues, expounding the construction process of the knowledge graph from the three aspects of knowledge acquisition, knowledge fusion, knowledge storage and visualization. Based on the knowledge graph, design intelligent question-answering robots, in-depth research on user intentions, and realize knowledge retrieval and utilization. The intelligent question answering application related to myopia prevention and control proposed in this article can provide references for researchers in related fields.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {80–87},
numpages = {8},
keywords = {Knowledge Graph, Myopia, Q&amp;A(question-answering) system},
location = {Beijing, China},
series = {ISAIMS '21}
}

@article{10.1145/3451167,
author = {Wang, Yashen and Zhang, Huanhuan and Liu, Zhirun and Zhou, Qiang},
title = {Hierarchical Concept-Driven Language Model},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3451167},
doi = {10.1145/3451167},
abstract = {For guiding natural language generation, many semantic-driven methods have been proposed. While clearly improving the performance of the end-to-end training task, these existing semantic-driven methods still have clear limitations: for example, (i) they only utilize shallow semantic signals (e.g., from topic models) with only a single stochastic hidden layer in their data generation process, which suffer easily from noise (especially adapted for short-text etc.) and lack of interpretation; (ii) they ignore the sentence order and document context, as they treat each document as a bag of sentences, and fail to capture the long-distance dependencies and global semantic meaning of a document. To overcome these problems, we propose a novel semantic-driven language modeling framework, which is a method to learn a Hierarchical Language Model and a Recurrent Conceptualization-enhanced Gamma Belief Network, simultaneously. For scalable inference, we develop the auto-encoding Variational Recurrent Inference, allowing efficient end-to-end training and simultaneously capturing global semantics from a text corpus. Especially, this article introduces concept information derived from high-quality lexical knowledge graph Probase, which leverages strong interpretability and anti-nose capability for the proposed model. Moreover, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence concept dependence. Experiments conducted on several NLP tasks validate the superiority of the proposed approach, which could effectively infer meaningful hierarchical concept structure of document and hierarchical multi-scale structures of sequences, even compared with latest state-of-the-art Transformer-based models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {104},
numpages = {22},
keywords = {Language modeling, text generation, concept semantic information, interpretation, recurrent conceptualization-enhanced gamma belief network, hierarchical language modeling, representation learning}
}

@inproceedings{10.1145/3491102.3502087,
author = {Lee, Yoonjoo and Chung, John Joon Young and Kim, Tae Soo and Song, Jean Y and Kim, Juho},
title = {Promptiverse: Scalable Generation of Scaffolding Prompts Through Human-AI Hybrid Knowledge Graph Annotation},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502087},
doi = {10.1145/3491102.3502087},
abstract = {Online learners are hugely diverse with varying prior knowledge, but most instructional videos online are created to be one-size-fits-all. Thus, learners may struggle to understand the content by only watching the videos. Providing scaffolding prompts can help learners overcome these struggles through questions and hints that relate different concepts in the videos and elicit meaningful learning. However, serving diverse learners would require a spectrum of scaffolding prompts, which incurs high authoring effort. In this work, we introduce Promptiverse, an approach for generating diverse, multi-turn scaffolding prompts at scale, powered by numerous traversal paths over knowledge graphs. To facilitate the construction of the knowledge graphs, we propose a hybrid human-AI annotation tool, Grannotate. In our study (N=24), participants produced 40 times more on-par quality prompts with higher diversity, through Promptiverse and Grannotate, compared to hand-designed prompts. Promptiverse presents a model for creating diverse and adaptive learning experiences online.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {96},
numpages = {18},
keywords = {Scaffolding prompt, human-AI hybrid annotation, knowledge graph},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3500931.3501011,
author = {Guo, Chaohui and Lin, Shaofu and Huang, Zhisheng and Yao, Yahong},
title = {Mental Health Question and Answering System Based on Bert Model and Knowledge Graph Technology},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501011},
doi = {10.1145/3500931.3501011},
abstract = {With the development and progress of society, people are facing increasing pressure. The emergence of this phenomenon has led to a rapid increase in the incidence of mental illness. In order to deal with this phenomenon, this paper proposes a system of question and answering on the basic knowledge of mental health (MHQ&amp;A) by using deep learning retrieval technology and knowledge graph technology. The system MHQ&amp;A is designed mainly for the general public, to answer the basic knowledge of mental health, especially the field of depression. First of all, the basic and the professional question and answer data about mental health were respectively obtained by the reptilian bot from the "IASK" website knowledge and the "Dr. Dingxiang" website. Then, the questions and answers obtained through the crawler are made into a Question and Answering Knowledge Graph of Basic Health Knowledge in the mental health field, which is combined with semantic data of antidepressants and the semantic data of depression papers. Finally, a set of template matching rules is designed to determine the type of problem of users. If the questions are about the professional knowledge of medicine or thesis, the reasoning template will be used to reason and search the answer in the "Question and Answering Knowledge Graph of Basic Health Knowledge in the Mental Health Field". If the questions are about other basic knowledge in the field of mental health, the BERT model is used to vectorize the questions of users, and the matching questions and corresponding answers in the MHQ&amp;A are found through cosine similarity calculation. Through the test of system accuracy, it is proved that the system can effectively combine deep learning technology and knowledge.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {472–476},
numpages = {5},
keywords = {Deep learning, Knowledge Graph, Mental illness, Question and answering system},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.1145/3562007.3562036,
author = {Tian, Hao and Zhang, Xiaoxiong and Liu, Liu and Liu, Shanshan and Ding, Kun},
title = {Knowledge Graph Completion based on Multi-task Learning and Extractive Text Summarization},
year = {2022},
isbn = {9781450396851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562007.3562036},
doi = {10.1145/3562007.3562036},
abstract = {Knowledge completion graph is an important technology to supplement knowledge graph and improve data quality. However, existing knowledge graph completion methods ignored the characteristics of triple's relation and introduced redundant entity description information. In order to improve the above problems, this paper proposed an ALBERT-KGC model. The key contexts were extracted from redundant entity descriptions by the extraction summarization technology. Then ALBERT was used as an encoder to reduce the number of model parameters, and the multi-task learning method was applied to fine-tune the model, effectively integrating entity and relation features. The results of linking prediction experiments on data sets WN18RR and FB15k-237 showed that the proposed method can improve the MeanRank (MR) index by 31 and 2, and the top 10 hit ratio (Hit@10) index by 1.2% and 5.8% compared with the traditional methods, verifying the validity of the model.},
booktitle = {Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System},
pages = {159–164},
numpages = {6},
keywords = {ALBERT, Extractive summary generation, Knowledge completion, Multi-task learning},
location = {Virtual Event, China},
series = {CCRIS '22}
}

@article{10.1109/TASLP.2022.3225537,
author = {Li, Yu and Hu, Bojie and Liu, Jian and Chen, Yufeng and Xu, Jinan},
title = {A Neighborhood Re-Ranking Model With Relation Constraint for Knowledge Graph Completion},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3225537},
doi = {10.1109/TASLP.2022.3225537},
abstract = {Knowledge graph completion (KGC) aims to predict missing links based on observed triples. However, current KGC models are still limited by the following two aspects. (1) the entity semantics is implicitly learned by neural network and merely depends on existing facts, which mostly suffers from less additional specific knowledge. Although previous studies have noticed that entity type information can effectively improve KGC task, most of them rely on labeled type-specific data. (2) the recent graph-based models mainly concentrate on Graph Neural Network (GNN) to update source entity representation, regardless of the separate role that neighborhood information plays and may mix noisy neighbor features for target prediction. To address the above two issues, we propose a neighborhood re-ranking model with relation constraint for KGC task. We suggest that both relation constraint and structured information located in triples can boost the model performance. More importantly, we automatically generate explicit constraints as additional type feature to enrich entity representation instead of depending on human annotated labels. Meanwhile, we construct a neighborhood completion module to re-rank candidate entities for full use of the neighbor structure rather than traditional GNN updating manner. Extensive experiments on seven benchmarks demonstrate that our model achieves the competitive results in comparison to the recent advanced baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {411–425},
numpages = {15}
}

@inproceedings{10.1145/3487351.3489484,
author = {Pan, Zhenhe and Jiang, Shuang and Su, Juntao and Guo, Muzhe and Zhang, Yuanlin},
title = {Knowledge graph based platform of COVID-19 drugs and symptoms},
year = {2022},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3489484},
doi = {10.1145/3487351.3489484},
abstract = {Since the first cased of COVID-19 was identified in December 2019, a plethora of different drugs have been tested for COVID-19 treatment, making it a daunting task to keep track of the rapid growth of COVID-19 research landscape. Using the existing scientific literature search systems to develop a deeper understanding of COVID-19 related clinical experiments and results turns to be increasingly complicated. In this paper, we build a named entity recognition-based framework to extract information accurately and generate knowledge graph efficiently from a myriad of clinical test results articles. Of the tested drugs to treat COVID-19, we also develop a question answering system answers to medical questions regarding COVID-19 related symptoms using Wikipedia articles. We combine the state-of-the-art question answering model - Bidirectional Encoder Representations from Transformers (BERT), with Knowledge Graph to answer patients' questions about treatment options for their symptoms. This generated knowledge graph is user-friendly with intuitive and convenient tools to find the supporting and/or contradictory references of certain drugs with properties such as side effects, target population, etc. The trained question answering platform provides a straightforward and error-tolerant way to query for treatment suggestions given uses' input symptoms.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {313–316},
numpages = {4},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@inproceedings{10.1145/3503928.3503936,
author = {Yang, Shihan and Tang, Rui},
title = {Learning Knowledge Uncertainty from the Pretrained Language Model},
year = {2022},
isbn = {9781450385220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503928.3503936},
doi = {10.1145/3503928.3503936},
booktitle = {Proceedings of the 6th International Conference on Information Systems Engineering},
pages = {37–42},
numpages = {6},
keywords = {BERT, knowledge graph embedding, knowledge reasoning, knowledge representation, uncertainty},
location = {Shanghai, China},
series = {ICISE '21}
}

@inproceedings{10.1109/ASE51524.2021.9678574,
author = {Su, Yanqi and Xing, Zhenchang and Peng, Xin and Xia, Xin and Wang, Chong and Xu, Xiwei and Zhu, Liming},
title = {Reducing bug triaging confusion by learning from mistakes with a bug tossing knowledge graph},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678574},
doi = {10.1109/ASE51524.2021.9678574},
abstract = {Assigning bugs to the right components is the prerequisite to get the bugs analyzed and fixed. Classification-based techniques have been used in practice for assisting bug component assignments, for example, the BugBug tool developed by Mozilla. However, our study on 124,477 bugs in Mozilla products reveals that erroneous bug component assignments occur frequently and widely. Most errors are repeated errors and some errors are even misled by the BugBug tool. Our study reveals that complex component designs and misleading component names and bug report keywords confuse bug component assignment not only for bug reporters but also developers and even bug triaging tools. In this work, we propose a learning to rank framework that learns to assign components to bugs from correct, erroneous and irrelevant bug-component assignments in the history. To inform the learning, we construct a bug tossing knowledge graph which incorporates not only goal-oriented component tossing relationships but also rich information about component tossing community, component descriptions, and historical closed and tossed bugs, from which three categories and seven types of features for bug, component and bug-component relation can be derived. We evaluate our approach on a dataset of 98,587 closed bugs (including 29,100 tossed bugs) of 186 components in six Mozilla products. Our results show that our approach significantly improves bug component assignments for both tossed and non-tossed bugs over the BugBug tool and the BugBug tool enhanced with component tossing relationships, with &gt;20% Top-k accuracies and &gt;30% NDCG@k (k=1,3,5,10).},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {191–202},
numpages = {12},
keywords = {bug triaging, knowledge graph, learning to rank},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3551349.3556898,
author = {Lee, Jaehyung and Han, Kisun and Yu, Hwanjo},
title = {A Light Bug Triage Framework for Applying Large Pre-trained Language Model},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556898},
doi = {10.1145/3551349.3556898},
abstract = {Assigning appropriate developers to the bugs is one of the main challenges in bug triage. Demands for automatic bug triage are increasing in the industry, as manual bug triage is labor-intensive and time-consuming in large projects. The key to the bug triage task is extracting semantic information from a bug report. In recent years, large Pre-trained Language Models (PLMs) including BERT [4] have achieved dramatic progress in the natural language processing (NLP) domain. However, applying large PLMs to the bug triage task for extracting semantic information has several challenges. In this paper, we address the challenges and propose a novel framework for bug triage named LBT-P, standing for Light Bug Triage framework with a Pre-trained language model. It compresses a large PLM into small and fast models using knowledge distillation techniques and also prevents catastrophic forgetting of PLM by introducing knowledge preservation fine-tuning. We also develop a new loss function exploiting representations of earlier layers as well as deeper layers in order to handle the overthinking problem. We demonstrate our proposed framework on the real-world private dataset and three public real-world datasets [11]: Google Chromium, Mozilla Core, and Mozilla Firefox. The result of the experiments shows the superiority of LBT-P.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {3},
numpages = {11},
keywords = {BERT, Bug triage, Catastrophic forgetting, Knowledge distillation, Overthinking, Pre-trained language model},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3607188,
author = {Huang, Qing and Sun, Yanbang and Xing, Zhenchang and Yu, Min and Xu, Xiwei and Lu, Qinghua},
title = {API Entity and Relation Joint Extraction from Text via Dynamic Prompt-tuned Language Model},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3607188},
doi = {10.1145/3607188},
abstract = {Extraction of Application Programming Interfaces (APIs) and their semantic relations from unstructured text (e.g., Stack Overflow) is a fundamental work for software engineering tasks (e.g., API recommendation). However, existing approaches are rule based and sequence labeling based. They must manually enumerate the rules or label data for a wide range of sentence patterns, which involves a significant amount of labor overhead and is exacerbated by morphological and common-word ambiguity. In contrast to matching or labeling API entities and relations, this article formulates heterogeneous API extraction and API relation extraction task as a sequence-to-sequence generation task and proposes the API Entity-Relation Joint Extraction framework (AERJE), an API entity-relation joint extraction model based on the large pre-trained language model. After training on a small number of ambiguous but correctly labeled data, AERJE builds a multi-task architecture that extracts API entities and relations from unstructured text using dynamic prompts. We systematically evaluate AERJE on a set of long and ambiguous sentences from Stack Overflow. The experimental results show that AERJE achieves high accuracy and discrimination ability in API entity-relation joint extraction, even with zero or few-shot fine-tuning.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {6},
numpages = {25},
keywords = {API entity, API relation, joint extraction, dynamic prompt}
}

@inproceedings{10.1145/3627915.3628028,
author = {Chen, Zipeng and Sun, Peixia and Chang, Qian and Zhao, Longgang},
title = {Dynamic Recommendation System Based on Event Knowledge Graph in Telecom O&amp;M (Telecom Operations and Maintenance Field)},
year = {2023},
isbn = {9798400700590},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627915.3628028},
doi = {10.1145/3627915.3628028},
abstract = {With the accumulation of time and the increasing volume of telecommunications business, the company has accumulated a large number of network fault work orders. These work orders record the process of various fault resolution through textual information, which is of great reference value to the existing network fault resolution. In order to make full use of the work order data, this paper designs the work order event mapping structure and combines the information extraction algorithm to extract the information of unstructured work order data and its inner connection. To address the difficulty of extracting long text with low entity coverage, a text summarization algorithm is used to remove redundant information and then a pointer network, globalpointer, is applied to extract entities. For event information, the UIE (Unified Structure Generation for Universal Information Extraction) framework is used for event extraction. It is also applied to the dynamic recommendation field of similar work orders in dialogue scenarios. The experimental results verify the advantages of the main design of work order event mapping and the effectiveness of the recommendation algorithm in this paper, which is of great significance for forming a structured knowledge base in the telecom operation and maintenance industry and supporting the intelligent development of telecom operation and maintenance.},
booktitle = {Proceedings of the 7th International Conference on Computer Science and Application Engineering},
articleno = {10},
numpages = {6},
keywords = {Mapping Entity Extraction, Similar Work Order Recommendations, Work Order Event},
location = {Virtual Event, China},
series = {CSAE '23}
}

@article{10.1109/TCBB.2023.3248797,
author = {Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
title = {Prediction of Protein-Protein Interactions Using Vision Transformer and Language Model},
year = {2023},
issue_date = {Sept.-Oct. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2023.3248797},
doi = {10.1109/TCBB.2023.3248797},
abstract = {The knowledge of protein-protein interaction (PPI) helps us to understand proteins’ functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the &lt;italic&gt;S. cerevisiae&lt;/italic&gt; dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {3215–3225},
numpages = {11}
}

@inproceedings{10.1145/3511808.3557564,
author = {Ju, Jinhao and Yang, Deqing and Liu, Jingping},
title = {Commonsense Knowledge Base Completion with Relational Graph Attention Network and Pre-trained Language Model},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557564},
doi = {10.1145/3511808.3557564},
abstract = {Many commonsense knowledge graphs (CKGs) still suffer from incompleteness although they have been applied in many natural language processing tasks successfully. Due to the scale and sparsity of CKGs, existing knowledge base completion models are not still competent for CKGs. In this paper, we propose a commonsense knowledge base completion (CKBC) model which learns the structural representations and contextual representations of CKG nodes and relations, respectively by a relational graph attention network and a pre-trained language model. Based on these two types of representations, the scoring decoder in our model achieves a more accurate prediction for a given triple. Our empirical studies on the representative CKG ConceptNet demonstrate our model's superiority over the state-of-the-art CKBC models.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4104–4108},
numpages = {5},
keywords = {commonsense knowledge base completion, pre-trained language model, relational graph attention},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3488933.3489030,
author = {Shu, Xinfeng and Yan, Jing and Gao, Weiran and Zhang, Fan},
title = {Research on Military Equipment Entity Recognition and Knowledge Graph Construction Method Based on ALBERT-Bi-LSTM-CRF},
year = {2022},
isbn = {9781450384087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488933.3489030},
doi = {10.1145/3488933.3489030},
abstract = {Aiming at the problems of complex and sparse military weapon data and weak association between data, this paper proposes a research method for unstructured data named entity recognition based on the ALBERT-Bi-LSTM-CRF model. Preprocess the ALBERT model to generate a word vector based on contextual information, and then input the trained word vector into the BiLSTM-CRF model for further training processing, and finally build a knowledge map based on the military weapon field and complete the knowledge question and answer function. The experiment evaluates the performance of entity recognition by adding training data to the ALBERT model several times. The results show that the model has a good advantage in entity relationship recognition in Baidu Baike and Canglang Corpus, with F1 values reaching 93.76% and 92.76%, respectively. It provides a better way to solve the existing problems.},
booktitle = {Proceedings of the 2021 4th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {273–279},
numpages = {7},
keywords = {ALBERT, Bi-LSTM, Knowledge Graph, Knowledge Graph Questions and Answers, NER, Word vector},
location = {Xiamen, China},
series = {AIPR '21}
}

@inproceedings{10.1145/3580305.3599921,
author = {Zhang, Xinyang and Malkov, Yury and Florez, Omar and Park, Serim and McWilliams, Brian and Han, Jiawei and El-Kishky, Ahmed},
title = {TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599921},
doi = {10.1145/3580305.3599921},
abstract = {Pre-trained language models (PLMs) are fundamental for natural language processing applications. Most existing PLMs are not tailored to the noisy user-generated text on social media, and the pre-training does not factor in the valuable social engagement logs available in a social network. We present TwHIN-BERT, a multilingual language model productionized at Twitter, trained on in-domain data from the popular social network. TwHIN-BERT differs from prior pre-trained language models as it is trained with not only text-based self-supervision but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages, providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on various multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established pre-trained language models. We open-source TwHIN-BERT and our curated hashtag prediction and social engagement benchmark datasets to the research community.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5597–5607},
numpages = {11},
keywords = {language models, social engagement, social media},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3581783.3612838,
author = {Li, Ruizhe and Guo, Jiahao and Li, Mingxi and Wu, Zhengqian and Liang, Chao},
title = {A Hierarchical Deep Video Understanding Method with Shot-Based Instance Search and Large Language Model},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612838},
doi = {10.1145/3581783.3612838},
abstract = {Deep video understanding (DVU) is often considered a challenge due to the aim of interpreting a video with storyline, which is designed to solve two levels of problems: predicting the human interaction in scene-level and identifying the relationship between two entities in movie-level. Based on our understanding of the movie characteristics and analysis of DVU tasks, in this paper, we propose a four-stage method to solve the task, which includes video structuring, shot based instance search, interaction &amp; relation prediction and shot-scene summary &amp; Question Answering (QA) with ChatGPT. In these four stages, shot based instance search allows accurate identification and tracking of characters at an appropriate video granularity. Using ChatGPT in QA, on the one hand, can narrow the answer space, on the other hand, with the help of the powerful text understanding ability, ChatGPT can help us answer the questions by giving background knowledge. We rank first in movie-level group 2 and scene-level group 1, second in movie-level group 1 and scene-level group 2 in ACM MM 2023 Grand Challenge.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9425–9429},
numpages = {5},
keywords = {instance search, multi-modal feature, vedio understanding},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.1145/3483524,
author = {Liao, Xianwen and Huang, Yongzhong and Yang, Peng and Chen, Lei},
title = {A Statistical Language Model for Pre-Trained Sequence Labeling: A Case Study on Vietnamese},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3483524},
doi = {10.1145/3483524},
abstract = {By defining the computable word segmentation unit and studying its probability characteristics, we establish an unsupervised statistical language model (SLM) for a new pre-trained sequence labeling framework in this article. The proposed SLM is an optimization model, and its objective is to maximize the total binding force of all candidate word segmentation units in sentences under the condition of no annotated datasets and vocabularies. To solve SLM, we design a recursive divide-and-conquer dynamic programming algorithm. By integrating SLM with the popular sequence labeling models, Vietnamese word segmentation, part-of-speech tagging and named entity recognition experiments are performed. The experimental results show that our SLM can effectively promote the performance of sequence labeling tasks. Just using less than 10% of training data and without using a dictionary, the performance of our sequence labeling framework is better than the state-of-the-art Vietnamese word segmentation toolkit VnCoreNLP on the cross-dataset test. SLM has no hyper-parameter to be tuned, and it is completely unsupervised and applicable to any other analytic language. Thus, it has good domain adaptability.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {50},
numpages = {21},
keywords = {Unsupervised, statistical language model, sequence labeling}
}

@article{10.1109/TASLP.2023.3302238,
author = {Chang, Youngjae and Ko, Youngjoong},
title = {Two-Step Masked Language Model for Domain-Adapting Multi-Modal Task-Oriented Dialogue Systems},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3302238},
doi = {10.1109/TASLP.2023.3302238},
abstract = {The next generation of artificial intelligence (AI) is required to be capable of proper communication to enable eloquent interaction with human beings. Thus, AI models require powerful language understanding and generation capabilities. Therefore, designing an adequate dialogue-specific task is an important topic in dialogue research. In this article, we improve our model that won in two out of four subtasks in SIMMC2.0 challenge using a dialogue-specific pre-training task, which learns the distinctive features of dialogues. In addition, we introduce an efficient method to pre-train the encoder-decoder transformer using an auxiliary task for the encoder. Experimental results indicate that superior performance is achieved compared to our previous model, revealing a new approach to improve task-oriented dialogue systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2938–2943},
numpages = {6}
}

@inproceedings{10.1145/3581641.3584037,
author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
title = {The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584037},
doi = {10.1145/3581641.3584037},
abstract = {Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {491–514},
numpages = {24},
keywords = {code-fluent large language models, conversational interaction, foundation models, human-centered AI},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3511808.3557355,
author = {Liu, Yang and Sun, Zequn and Li, Guangyao and Hu, Wei},
title = {I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557355},
doi = {10.1145/3511808.3557355},
abstract = {Knowledge graph (KG) embedding seeks to learn vector representations for entities and relations. Conventional models reason over graph structures, but they suffer from the issues of graph incompleteness and long-tail entities. Recent studies have used pre-trained language models to learn embeddings based on the textual information of entities and relations, but they cannot take advantage of graph structures. In the paper, we show empirically that these two kinds of features are complementary for KG embedding. To this end, we propose CoLE, a Co-distillation Learning method for KG Embedding that exploits the complementarity of graph structures and text information. Its graph embedding model employs Transformer to reconstruct the representation of an entity from its neighborhood subgraph. Its text embedding model uses a pre-trained language model to generate entity representations from the soft prompts of their names, descriptions and relational neighbors. To let the two models promote each other, we propose co-distillation learning that allows them to distill selective knowledge from each other's prediction logits. In our co-distillation learning, each model serves as both a teacher and a student. Experiments on benchmark datasets demonstrate that the two models outperform their related baselines, and the ensemble method CoLE with co-distillation learning advances the state-of-the-art of KG embedding.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {1329–1338},
numpages = {10},
keywords = {co-distillation learning, knowledge graph, link prediction},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1109/TASLP.2022.3153268,
author = {Wang, Chengyu and Dai, Suyang and Wang, Yipeng and Yang, Fei and Qiu, Minghui and Chen, Kehan and Zhou, Wei and Huang, Jun},
title = {ARoBERT: An ASR Robust Pre-Trained Language Model for Spoken Language Understanding},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153268},
doi = {10.1109/TASLP.2022.3153268},
abstract = {Spoken Language Understanding (SLU) aims to interpret the meanings of human speeches in order to support various human-machine interaction systems. A key technique for SLU is Automatic Speech Recognition (ASR), which transcribes speech signals into text contents. As the output texts of modern ASR systems unavoidably contain errors, mainstream SLU models either trained or tested on texts transcribed by ASR systems would not be sufficiently error robust. We present ARoBERT, an ASR Robust BERT model, which can be fine-tuned to solve a variety of SLU tasks with noisy inputs. To guarantee the robustness of ARoBERT, during pretraining, we decrease the fluctuations of language representations when some parts of the input texts are replaced by homophones or synophones. Specifically, we propose two novel self-supervised pre-training tasks for ARoBERT, namely Phonetically-aware Masked Language Modeling (PMLM) and ASR Model-adaptive Masked Language Modeling (AMMLM). The PMLM task explicitly fuses the knowledge of word phonetic similarities into the pre-training process, which forces homophones and synophones to share similar representations. In AMMLM, a data-driven algorithm is further introduced to mine typical ASR errors such that ARoBERT can tolerate ASR model errors. In the experiments, we evaluate ARoBERT over multiple datasets. The results show the superiority of ARoBERT, which consistently outperforms strong baselines. We have also shown that ARoBERT outperforms state-of-the-arts on a public benchmark. Currently, ARoBERT has been deployed in an online production system with significant improvements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1207–1218},
numpages = {12}
}

@inproceedings{10.1145/3447548.3467128,
author = {Liu, Lihui and Du, Boxin and Fung, Yi Ren and Ji, Heng and Xu, Jiejun and Tong, Hanghang},
title = {KompaRe: A Knowledge Graph Comparative Reasoning System},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467128},
doi = {10.1145/3447548.3467128},
abstract = {Reasoning is a fundamental capability for harnessing valuable insight, knowledge and patterns from knowledge graphs. Existing work has primarily been focusing on point-wise reasoning, including search, link prediction, entity prediction, subgraph matching and so on. This paper introduces comparative reasoning over knowledge graphs, which aims to infer the commonality and inconsistency with respect to multiple pieces of clues. We envision that the comparative reasoning will complement and expand the existing point-wise reasoning over knowledge graphs. In detail, we develop KompaRe, the first of its kind prototype system that provides comparative reasoning capability over large knowledge graphs. We present both the system architecture and its core algorithms, including knowledge segment extraction, pairwise reasoning and collective reasoning. Empirical evaluations demonstrate the efficacy of the proposed KompaRe.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3308–3318},
numpages = {11},
keywords = {comparative reasoning, knowledge graph fact checking, knowledge graph reasoning system},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3459637.3482382,
author = {Kim, Bosung and Choi, Hyewon and Yu, Haeun and Ko, Youngjoong},
title = {Query Reformulation for Descriptive Queries of Jargon Words Using a Knowledge Graph based on a Dictionary},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482382},
doi = {10.1145/3459637.3482382},
abstract = {Query reformulation (QR) is a key factor in overcoming the problems faced by the lexical chasm in information retrieval (IR) systems. In particular, when searching for jargon, people tend to use descriptive queries, such as "a medical examination of the colon" rather than "colonoscopy," or they often use them interchangeably. Thus, transforming users' descriptive queries into appropriate jargon queries helps to retrieve more relevant documents. In this paper, we propose a new graph-based QR system that uses a dictionary, where the model does not require human-labeled data. Given a descriptive query, our system predicts the corresponding jargon word over a graph consisting of pairs of a headword and its description in the dictionary. First, we train a graph neural network to represent the relational properties between words and to infer a jargon word using compositional information of the descriptive query's words. Moreover, we propose a graph search model that finds the target node in real time using the relevance scores of neighborhood nodes. By adding this fast graph search model to the front of the proposed system, we reduce the reformulating time significantly. Experimental results on two datasets show that the proposed method can effectively reformulate descriptive queries to corresponding jargon words as well as improve retrieval performance under several search frameworks.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {854–862},
numpages = {9},
keywords = {graph neural networks, graph search, query reformulation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3488560.3498437,
author = {Zhu, Yushan and Zhang, Wen and Chen, Mingyang and Chen, Hui and Cheng, Xu and Zhang, Wei and Chen, Huajun},
title = {DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498437},
doi = {10.1145/3488560.3498437},
abstract = {Knowledge Graph Embedding (KGE) is a popular method for KG reasoning and training KGEs with higher dimension are usually preferred since they have better reasoning capability. However, high-dimensional KGEs pose huge challenges to storage and computing resources and are not suitable for resource-limited or time-constrained applications, for which faster and cheaper reasoning is necessary. To address this problem, we propose DualDE, a knowledge distillation method to build low-dimensional student KGE from pre-trained high-dimensional teacher KGE. DualDE considers the dual-influence between the teacher and the student. In DualDE, we propose a soft label evaluation mechanism to adaptively assign different soft label and hard label weights to different triples, and a two-stage distillation approach to improve the student's acceptance of the teacher. Our DualDE is general enough to be applied to various KGEs. Experimental results show that our method can successfully reduce the embedding parameters of a high-dimensional KGE by 7\texttimes{} - 15\texttimes{} and increase the inference speed by 2\texttimes{} - 6\texttimes{} while retaining a high performance. We also experimentally prove the effectiveness of our soft label evaluation mechanism and two-stage distillation approach via ablation study.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1516–1524},
numpages = {9},
keywords = {fast embedding, knowledge distillation, knowledge graph embedding},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3551349.3556912,
author = {Huang, Qing and Yuan, Zhiqiang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Lu, Qinghua},
title = {Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556912},
doi = {10.1145/3551349.3556912},
abstract = {Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this paper, we formulate type inference as a cloze-style fill-in-blank language task. Built on source code naturalness, our approach fine-tunes a code masked language model (MLM) as a neural knowledge base of code elements with a novel “pre-train, prompt and predict” paradigm from raw source code. Our approach is lightweight and has minimum requirements on code compilation. Unlike existing symbolic name and context matching for type inference, our prompt-tuned code MLM packs FQN syntax and usage in its parameters and supports fuzzy neural type inference. We systematically evaluate our approach on a large amount of source code from GitHub and Stack Overflow. Our results confirm the effectiveness of our approach design and the practicality for partial code type inference. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {79},
numpages = {13},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3487553.3524923,
author = {Negreanu, Carina and Karaoglu, Alperen and Williams, Jack and Chen, Shuang and Fabian, Daniel and Gordon, Andrew and Lin, Chin-Yew},
title = {Rows from Many Sources: Enriching row completions from Wikidata with a pre-trained Language Model},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524923},
doi = {10.1145/3487553.3524923},
abstract = {Row completion is the task of augmenting a given table of text and numbers with additional, relevant rows. The task divides into two steps: subject suggestion, the task of populating the main column; and gap filling, the task of populating the remaining columns. We present state-of-the-art results for subject suggestion and gap filling measured on a standard benchmark (WikiTables). Our idea is to solve this task by harmoniously combining knowledge base table interpretation and free text generation. We interpret the table using the knowledge base to suggest new rows and generate metadata like headers through property linking. To improve candidate diversity, we synthesize additional rows using free text generation via GPT-3, and crucially, we exploit the metadata we interpret to produce better prompts for text generation. Finally, we verify that the additional synthesized content can be linked to the knowledge base or a trusted web source such as Wikipedia.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1272–1280},
numpages = {9},
keywords = {free text generation, knowledge base linking, language models, natural language applications, semantic knowledge, tabular data},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3459637.3482491,
author = {Xie, Chenhao and Huang, Wenhao and Liang, Jiaqing and Huang, Chengsong and Xiao, Yanghua},
title = {WebKE: Knowledge Extraction from Semi-structured Web with Pre-trained Markup Language Model},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482491},
doi = {10.1145/3459637.3482491},
abstract = {The World Wide Web contains rich up-to-date information for knowledge graph construction. However, most current relation extraction techniques are designed for free text and thus do not handle well semi-structured web content. In this paper, we propose a novel multi-phase machine reading framework, called WebKE. It processes the web content on different granularity by first detecting areas of interest at DOM tree node level and then extracting relational triples for each area. We also propose HTMLBERT as an encoder the web content. It is a pre-trained markup language model that fully leverages the visual layout information and DOM-tree structure, without the need of hand engineered features. Experimental results show that the proposed approach outperforms state-of- the-art methods by a considerable gain. The source code is available at https://github.com/redreamality/webke.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2211–2220},
numpages = {10},
keywords = {htmlbert, knowledge extraction, knowledge graph construction, pre-trained markup language model, relation extraction, semi-structured web extraction, webke},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3580305.3599233,
author = {Yu, Wenhao and Tong, Lingbo and Shi, Weijia and Peng, Nanyun and Jiang, Meng},
title = {The Second Workshop on Knowledge-Augmented Methods for Natural Language Processing},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599233},
doi = {10.1145/3580305.3599233},
abstract = {Language models are being developed and deployed in many applications, "small"-scale and large-scale, generic and specialized, text-only and multimodal, etc. Meanwhile, the missingness of important knowledge causes limitations and safety challenges. The knowledge includes commonsense, world facts, domain expertise, personalization, and especially the unique patterns that need to be discovered from big data applications. Training and inference processes of the language models can be and should be augmented with the knowledge. The first KnowledgeNLP at AAAI 2023 attracted scientists on knowledge augmentation methods towards higher language intelligence. This workshop offers a broad platform to share ideas and discuss various topics, such as (1) synergy between knowledge and language model, (2) scalable architectures that integrate NLP, knowledge graph, and graph learning technologies, (3) KnowledgeNLP for e-commerce, education, and healthcare, (4) human factors and social good in KnowledgeNLP.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5899–5900},
numpages = {2},
keywords = {knowledge augmentation, knowledge graph, language model, question answering, text generation},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1145/3492855,
author = {Kou, Ziyi and Shang, Lanyu and Zhang, Yang and Wang, Dong},
title = {HC-COVID: A Hierarchical Crowdsource Knowledge Graph Approach to Explainable COVID-19 Misinformation Detection},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {GROUP},
url = {https://doi.org/10.1145/3492855},
doi = {10.1145/3492855},
abstract = {The proliferation of social media has promoted the spread of misinformation that raises many concerns in our society. This paper focuses on a critical problem of explainable COVID-19 misinformation detection that aims to accurately identify and explain misleading COVID-19 claims on social media. Motivated by the lack of COVID-19 relevant knowledge in existing solutions, we construct a novel crowdsource knowledge graph based approach to incorporate the COVID-19 knowledge facts by leveraging the collaborative efforts of expert and non-expert crowd workers. Two important challenges exist in developing our solution: i) how to effectively coordinate the crowd efforts from both expert and non-expert workers to generate the relevant knowledge facts for detecting COVID-19 misinformation; ii) How to leverage the knowledge facts from the constructed knowledge graph to accurately explain the detected COVID-19 misinformation. To address the above challenges, we develop HC-COVID, a hierarchical crowdsource knowledge graph based framework that explicitly models the COVID-19 knowledge facts contributed by crowd workers with different levels of expertise and accurately identifies the related knowledge facts to explain the detection results. We evaluate HC-COVID using two public real-world datasets on social media. Evaluation results demonstrate that HC-COVID significantly outperforms state-of-the-art baselines in terms of the detection accuracy of misleading COVID-19 claims and the quality of the explanations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {36},
numpages = {25},
keywords = {covid19, explainable misinformation detection, human-ai collaboration}
}

@inproceedings{10.1145/3587259.3627571,
author = {Hertling, Sven and Paulheim, Heiko},
title = {OLaLa: Ontology Matching with Large Language Models},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627571},
doi = {10.1145/3587259.3627571},
abstract = {Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {131–139},
numpages = {9},
keywords = {Entity Resolution, Large Language Model, Ontology Matching},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3583780.3615301,
author = {Gupta, Rajeev and Srinivasa, Srinath},
title = {Workshop on Enterprise Knowledge Graphs using Large Language Models},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615301},
doi = {10.1145/3583780.3615301},
abstract = {Knowledge graphs are used for organizing and connecting individual entities to integrate the information extracted from different data sources. Typically, knowledge graphs are used to connect various real-world entities like persons, places, things, actions, etc. For the knowledge graphs created using the enterprise data, the knowledge graph entities can be of different types-static entities (e.g., people, projects), communication entities (e.g., emails, meetings, documents), derived entities (e.g., rules, definitions, entities from emails), etc. The graphs are used to connect these entities with enriched context (as edges and node attributes) and used for powering various search and recommendations applications.With the advent of large language models, the whole lifecycle of knowledge graphs involving -information extraction, graph construction, application of graphs, querying knowledge graphs, using the graph for recommendations, etc., - is impacted. With large language models such as GPT, LLaMA, PALM, etc., entity and relationship extraction can be improved. Similarly, one can answer different types of queries with the help of LLMs which were very difficult without them. This workshop is about improving the enterprise knowledge graphs and its applications using large language models.Enterprise graphs can be of different scopes-whether they contain data from individual users/customers, a sub-organization, or the whole enterprise. This workshop will also cover various privacy and access control related issues which are typical for any enterprise graph. These include privacy preserving federated learning, using LLMs to extract information from private data, querying the knowledge graph in a privacy preserving manner, etc.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5271–5272},
numpages = {2},
keywords = {entity extraction, knowledge graph, large language model, recommendations, relationship extraction},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3581783.3612252,
author = {Zhai, Jianyang and Zheng, Xiawu and Wang, Chang-Dong and Li, Hui and Tian, Yonghong},
title = {Knowledge Prompt-tuning for Sequential Recommendation},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612252},
doi = {10.1145/3581783.3612252},
abstract = {Pre-trained language models (PLMs) have demonstrated strong performance in sequential recommendation (SR), which are utilized to extract general knowledge. However, existing methods still lack domain knowledge and struggle to capture users' fine-grained preferences. Meanwhile, many traditional SR methods improve this issue by integrating side information while suffering from information loss. To summarize, we believe that a good recommendation system should utilize both general and domain knowledge simultaneously. Therefore, we introduce an external knowledge base and propose Knowledge Prompt-tuning for Sequential Recommendation (KP4SR). Specifically, we construct a set of relationship templates and transform a structured knowledge graph (KG) into knowledge prompts to solve the problem of the semantic gap. However, knowledge prompts disrupt the original data structure and introduce a significant amount of noise. We further construct a knowledge tree and propose a knowledge tree mask, which restores the data structure in a mask matrix form, thus mitigating the noise problem. We evaluate KP4SR on three real-world datasets, and experimental results show that our approach outperforms state-of-the-art methods on multiple evaluation metrics. Specifically, compared with PLM-based methods, our method improves NDCG@5 and HR@5 by 40.65% and 36.42% on the books dataset, 11.17% and 11.47% on the music dataset, and 22.17% and 19.14% on the movies dataset, respectively. Our code is publicly available at the link: https://github.com/zhaijianyang/KP4SR.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {6451–6461},
numpages = {11},
keywords = {knowledge graph, pre-trained language model, prompt learning, sequential recommendation},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3589132.3625641,
author = {Zhou, Zhilun and Ding, Jingtao and Liu, Yu and Jin, Depeng and Li, Yong},
title = {Towards Generative Modeling of Urban Flow through Knowledge-enhanced Denoising Diffusion},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625641},
doi = {10.1145/3589132.3625641},
abstract = {Although generative AI has been successful in many areas, its ability to model geospatial data is still underexplored. Urban flow, a typical kind of geospatial data, is critical for a wide range of applications from public safety and traffic management to urban planning. Existing studies mostly focus on predictive modeling of urban flow that predicts the future flow based on historical flow data, which may be unavailable in data-sparse areas or newly planned regions. Some other studies aim to predict OD flow among regions but they fail to model dynamic changes of urban flow over time. In this work, we study a new problem of urban flow generation that generates dynamic urban flow for regions without historical flow data. To capture the effect of multiple factors on urban flow, such as region features and urban environment, we employ diffusion model to generate urban flow for regions under different conditions. We first construct an urban knowledge graph (UKG) to model the urban environment and relationships between regions, based on which we design a knowledge-enhanced spatio-temporal diffusion model (KSTDiff) to generate urban flow for each region. Specifically, to accurately generate urban flow for regions with different flow volumes, we design a novel diffusion process guided by a volume estimator, which is learnable and customized for each region. Moreover, we propose a knowledge-enhanced denoising network to capture the spatio-temporal dependencies of urban flow as well as the impact of urban environment in the denoising process. Extensive experiments on four real-world datasets validate the superiority of our model over state-of-the-art baselines in urban flow generation. Further in-depth studies demonstrate the utility of generated urban flow data and the ability of our model for long-term flow generation and urban flow prediction. Our code is released at: https://github.com/tsinghua-fib-lab/KSTDiff-Urban-flow-generation.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {91},
numpages = {12},
keywords = {generative model, urban flow, knowledge graph, diffusion model},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@inproceedings{10.1145/3487553.3524622,
author = {Barik, Anab Maulana and Hsu, Wynne and Lee, Mong Li},
title = {Incorporating External Knowledge for Evidence-based Fact Verification},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524622},
doi = {10.1145/3487553.3524622},
abstract = {Existing fact verification methods employ pre-trained language models such as BERT for the contextual representation of evidence sentences. However, such representations do not take into account commonsense knowledge and these methods often conclude that there is not enough information to predict whether a claim is supported or refuted by the evidence sentences. In this work, we propose a framework called CGAT that incorporates external knowledge from ConceptNet to enrich the contextual representations of evidence sentences. We employ graph attention models to propagate the information among the evidence sentences before predicting the veracity of the claim. Experiment results on the benchmark FEVER dataset and UKP Snopes Corpus indicate that the proposed approach leads to higher accuracy and FEVER score compared to state-of-the-art claim verification methods.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {429–437},
numpages = {9},
keywords = {fact verification, knowledge graph, language model},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3485447.3511921,
author = {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen, Xiang and Chen, Hui and Xiong, Feiyu and Chen, Xi and Chen, Huajun},
title = {Ontology-enhanced Prompt-tuning for Few-shot Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511921},
doi = {10.1145/3485447.3511921},
abstract = {Few-shot Learning (FSL) is aimed to make predictions based on a limited number of samples. Structured data such as knowledge graphs and ontology libraries has been leveraged to benefit the few-shot setting in various tasks. However, the priors adopted by the existing methods suffer from challenging knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder the performance for few-shot learning. In this study, we explore knowledge injection for FSL with pre-trained language models and propose ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the ontology transformation based on the external knowledge graph to address the knowledge missing issue, which fulfills and converts structure knowledge to text. We further introduce span-sensitive knowledge injection via a visible matrix to select informative knowledge to handle the knowledge noise issue. To bridge the gap between knowledge and text, we propose a collective training algorithm to optimize representations jointly. We evaluate our proposed OntoPrompt in three tasks, including relation extraction, event extraction, and knowledge graph completion, with eight datasets. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {778–787},
numpages = {10},
keywords = {Event Extraction, Few-shot Learning, Knowledge Graph Completion, Ontology, Prompt-tuning, Relation Extraction},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3485447.3511937,
author = {Geng, Shijie and Fu, Zuohui and Tan, Juntao and Ge, Yingqiang and de Melo, Gerard and Zhang, Yongfeng},
title = {Path Language Modeling over Knowledge Graphsfor Explainable Recommendation},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511937},
doi = {10.1145/3485447.3511937},
abstract = {To facilitate human decisions with credible suggestions, personalized recommender systems should have the ability to generate corresponding explanations while making recommendations. Knowledge graphs (KG), which contain comprehensive information about users and products, are widely used to enable this. By reasoning over a KG in a node-by-node manner, existing explainable models provide a KG-grounded path for each user-recommended item. Such paths serve as an explanation and reflect the historical behavior pattern of the user. However, not all items can be reached following the connections within the constructed KG under finite hops. Hence, previous approaches are constrained by a recall bias in terms of existing connectivity of KG structures. To overcome this, we propose a novel Path Language Modeling Recommendation (PLM-Rec) framework, learning a language model over KG paths consisting of entities and edges. Through path sequence decoding, PLM-Rec unifies recommendation and explanation in a single step and fulfills them simultaneously. As a result, PLM-Rec not only captures the user behaviors but also eliminates the restriction to pre-existing KG connections, thereby alleviating the aforementioned recall bias. Moreover, the proposed technique makes it possible to conduct explainable recommendation even when the KG is sparse or possesses a large number of relations. Experiments and extensive ablation studies on three Amazon e-commerce datasets demonstrate the effectiveness and explainability of the PLM-Rec framework.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {946–955},
numpages = {10},
keywords = {Explainable Recommendation, Knowledge Graph, Path Language Model, Recall Bias, Recommender Systems},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3624062.3624094,
author = {Kousha, Pouya and Sathu, Vivekananda and Lieber, Matthew and Subramoni, Hari and Panda, Dhabaleswar K.},
title = {Democratizing HPC Access and Use with Knowledge Graphs},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624094},
doi = {10.1145/3624062.3624094},
abstract = {The field of High-Performance Computing (HPC) is undergoing rapid evolution, with an expanding and diverse user base harnessing its unparalleled computational capabilities. As the range of HPC applications grows, newcomers to the field are faced with the daunting task of optimizing their applications for efficient execution on HPC systems. Traditional documentation, often spanning dozens of pages, is cumbersome for finding answers and ill-suited for integration with emerging conversational AI-powered user interfaces like chatbots. Addressing this challenge, we propose a novel HPC ontology crafted to encapsulate HPC runtime relations in a scalable fashion. Our proposed ontology not only facilitates the transfer and querying of this knowledge but also serves as a foundational pillar for our AI-powered Speech Assistant Interface (SAI)[13]. This ensures reproducibility, reliability, and optimal performance when executing tasks. In this paper, we elucidate the relationships and properties underpinning our ontology and showcase how users can interact with knowledge graphs based on our proposed ontology to derive insights.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {243–251},
numpages = {9},
keywords = {Documentation, HPC, Knowledge Graph, Ontology},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3579370.3594759,
author = {Alfasi, Daniel and Shapira, Tal and Bremler-Barr, Anat},
title = {Next-Generation Security Entity Linkage: Harnessing the Power of Knowledge Graphs and Large Language},
year = {2023},
isbn = {9781450399623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579370.3594759},
doi = {10.1145/3579370.3594759},
abstract = {With the continuous increase in reported Common Vulnerabilities and Exposures (CVEs), security teams are overwhelmed by vast amounts of data, which are often analyzed manually, leading to a slow and inefficient process. To address cybersecurity threats effectively, it is essential to establish connections across multiple security entity databases, including CVEs, Common Weakness Enumeration (CWEs), and Common Attack Pattern Enumeration and Classification (CAPECs). In this study, we introduce a new approach that leverages the RotatE [4] knowledge graph embedding model, initialized with embeddings from Ada language model developed by OpenAI [3]. Additionally, we extend this approach by initializing the embeddings for the relations.},
booktitle = {Proceedings of the 16th ACM International Conference on Systems and Storage},
pages = {150},
numpages = {1},
keywords = {CVE, CWE, CAPEC, knowledge graph embedding},
location = {Haifa, Israel},
series = {SYSTOR '23}
}

@inproceedings{10.1145/3543873.3587667,
author = {Lin, Shiyong and Yuan, Yiping and Jin, Carol and Pan, Yi},
title = {Skill Graph Construction From Semantic Understanding},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587667},
doi = {10.1145/3543873.3587667},
abstract = {LinkedIn is building a skill graph to power a skill-first talent marketplace. Constructing a skill graph from a flat list is not an trivial task, especially by human curation. In this paper, we leverage the pre-trained large language model BERT to achieve this through semantic understanding on synthetically generated texts as training data. We automatically create positive and negative labels from the seed skill graph. The training data are encoded by pre-trained language models into embeddings and they are consumed by the downstream classification module to classify the relationships between skill pairs.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {978–982},
numpages = {5},
keywords = {NLP, knowledge graph, link prediction},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3583780.3614938,
author = {Wang, Zihan and Zhao, Kai and He, Yongquan and Chen, Zhumin and Ren, Pengjie and de Rijke, Maarten and Ren, Zhaochun},
title = {Iteratively Learning Representations for Unseen Entities with Inter-Rule Correlations},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614938},
doi = {10.1145/3583780.3614938},
abstract = {Recent work on knowledge graph completion (KGC) focuses on acquiring embeddings of entities and relations in knowledge graphs. These embedding methods necessitate that all test entities be present during the training phase, resulting in a time-consuming retraining process for out-of-knowledge-graph (OOKG) entities. To tackle this predicament, current inductive methods employ graph neural networks (GNNs) to represent unseen entities by aggregating information of the known neighbors, and enhance the performance with additional information, such as attention mechanisms or logic rules. Nonetheless, Two key challenges continue to persist: (i) identifying inter-rule correlations to further facilitate the inference process, and (ii) capturing interactions among rule mining, rule inference, and embedding to enhance both rule and embedding learning.In this paper, we propose a virtual neighbor network with inter-rule correlations (VNC) to address the above challenges. VNC consists of three main components: (i) rule mining, (ii) rule inference, and (iii) embedding. To identify useful complex patterns in knowledge graphs, both logic rules and inter-rule correlations are extracted from knowledge graphs based on operations over relation embeddings. To reduce data sparsity, virtual networks for OOKG entities are predicted and assigned soft labels by optimizing a rule-constrained problem. We also devise an iterative framework to capture the underlying interactions between rule and embedding learning. Experimental results on both link prediction and triple classification tasks show that the proposed VNC framework achieves state-of-the-art performance on four widely-used knowledge graphs.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2534–2543},
numpages = {10},
keywords = {inductive learning, knowledge graph, representation learning},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3605943,
author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
title = {Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3605943},
doi = {10.1145/3605943},
abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {30},
numpages = {40},
keywords = {Large language models, foundational models, generative AI, neural networks}
}

@inproceedings{10.1145/3580305.3599853,
author = {Hui, Shuodi and Wang, Huandong and Li, Tong and Yang, Xinghao and Wang, Xing and Feng, Junlan and Zhu, Lin and Deng, Chao and Hui, Pan and Jin, Depeng and Li, Yong},
title = {Large-scale Urban Cellular Traffic Generation via Knowledge-Enhanced GANs with Multi-Periodic Patterns},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599853},
doi = {10.1145/3580305.3599853},
abstract = {With the rapid development of the cellular network, network planning is increasingly important. Generating large-scale urban cellular traffic contributes to network planning via simulating the behaviors of the planned network. Existing methods fail in simulating the long-term temporal behaviors of cellular traffic while cannot model the influences of the urban environment on the cellular networks. We propose a knowledge-enhanced GAN with multi-periodic patterns to generate large-scale cellular traffic based on the urban environment. First, we design a GAN model to simulate the multi-periodic patterns and long-term aperiodic temporal dynamics of cellular traffic via learning the daily patterns, weekly patterns, and residual traffic between long-term traffic and periodic patterns step by step. Then, we leverage urban knowledge to enhance traffic generation via constructing a knowledge graph containing multiple factors affecting cellular traffic in the surrounding urban environment. Finally, we evaluate our model on a real cellular traffic dataset. Our proposed model outperforms three state-of-art generation models by over 32.77%, and the urban knowledge enhancement improves the performance of our model by 4.71%. Moreover, our model achieves good generalization and robustness in generating traffic for urban cellular networks without training data in the surrounding areas.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4195–4206},
numpages = {12},
keywords = {cellular traffic, gan, generation, knowledge graph},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3459637.3482436,
author = {Zhang, Taolin and Cai, Zerui and Wang, Chengyu and Li, Peng and Li, Yang and Qiu, Minghui and Tang, Chengguang and He, Xiaofeng and Huang, Jun},
title = {HORNET: Enriching Pre-trained Language Representations with Heterogeneous Knowledge Sources},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482436},
doi = {10.1145/3459637.3482436},
abstract = {Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the language understanding abilities of deep language models by leveraging the rich semantic knowledge from knowledge graphs, other than plain pre-training texts. However, previous efforts mostly use homogeneous knowledge (especially structured relation triples in knowledge graphs) to enhance the context-aware representations of entity mentions, whose performance may be limited by the coverage of knowledge graphs. Also, it is unclear whether these KEPLMs truly understand the injected semantic knowledge due to the "black-box'' training mechanism. In this paper, we propose a novel KEPLM named HORNET, which integrates Heterogeneous knowledge from various structured and unstructured sources into the Roberta NETwork and hence takes full advantage of both linguistic and factual knowledge simultaneously. Specifically, we design a hybrid attention heterogeneous graph convolution network (HaHGCN) to learn heterogeneous knowledge representations based on the structured relation triplets from knowledge graphs and the unstructured entity description texts. Meanwhile, we propose the explicit dual knowledge understanding tasks to help induce a more effective infusion of the heterogeneous knowledge, promoting our model for learning the complicated mappings from the knowledge graph embedding space to the deep context-aware embedding space and vice versa. Experiments show that our HORNET model outperforms various KEPLM baselines on knowledge-aware tasks including knowledge probing, entity typing and relation extraction. Our model also achieves substantial improvement over several GLUE benchmark datasets, compared to other KEPLMs.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2608–2617},
numpages = {10},
keywords = {heterogeneous graph attention network, knowledge graph, natural language processing, pre-trained language model},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3600160.3604991,
author = {Tailhardat, Lionel and Troncy, Rapha\"{e}l and Chabot, Yoan},
title = {Leveraging Knowledge Graphs For Classifying Incident Situations in ICT Systems},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600160.3604991},
doi = {10.1145/3600160.3604991},
abstract = {The complexity of Information and Communications Technology (ICT) systems, such as enterprise or Internet access provider networks, entails uncertainty in causal reasoning for efficient incident management. In this work, we propose to use knowledge graphs and explicit representation of incident context to enable support teams to provide a quick and effective response to complex incident situations. Formal analysis and expert opinions are used to analyze challenges in providing knowledge about relationships between events and incidents in network operations. We make use of an RDF knowledge graph generated from a real industrial settings and representing the network topology in terms of equipments and applications, past incidents and their resolutions. We then demonstrate the effectiveness of using a graph embeddings-based classifier to categorize incident tickets based on context and link anomaly models with their logical representation.},
booktitle = {Proceedings of the 18th International Conference on Availability, Reliability and Security},
articleno = {83},
numpages = {9},
keywords = {Anomaly Detection, Graph Embeddings, Graph Query, Incident Management, Knowledge Graph},
location = {Benevento, Italy},
series = {ARES '23}
}

@inproceedings{10.1145/3583780.3614739,
author = {Chuang, Yu-Neng and Wang, Guanchu and Chang, Chia-Yuan and Lai, Kwei-Herng and Zha, Daochen and Tang, Ruixiang and Yang, Fan and Reyes, Alfredo Costilla and Zhou, Kaixiong and Jiang, Xiaoqian and Hu, Xia},
title = {DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614739},
doi = {10.1145/3583780.3614739},
abstract = {The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical User Interface that provides an intuitive visualization of the KG, query recommendations, and detailed article information, enabling efficient article retrieval, thus fostering interdisciplinary knowledge exploration.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5021–5025},
numpages = {5},
keywords = {biomedical, healthcare, information retrieval system, knowledge graph, recommender system},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3600230,
author = {Katwe, Praveen Kumar and Khamparia, Aditya and Gupta, Deepak and Dutta, Ashit Kumar},
title = {Methodical Systematic Review of Abstractive Summarization and Natural Language Processing Models for Biomedical Health Informatics: Approaches, Metrics and Challenges},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3600230},
doi = {10.1145/3600230},
abstract = {Text summarization tasks are primarily very useful for decision support systems and provide a source for useful data for training of bots as they can reduce and retain the useful information from the large corpus. This review article is for studying the literature that already exists in context of abstractive summarization and application of NLP language models in biomedical and associated healthcare applications. In past decade with trends like bigdata, IOT, enormous amount of data is getting processed in all structured, unstructured and semi structured formats. This review provides a comprehensive literature survey in research trends for abstractive summarization, foundations of machine translation and evolution of language models. This review identifies the potential of language model to provide a possible methodology for improving the performance and accuracy of various tasks in summarization. Deep neural network-based language models have now been the widely accepted state of art for various abstractive summarization and there exists an enormous scope to improvise and tune the language models for domain specific use case. This study shows current systems lack in faithfulness to original content and control of degree of hallucination. This review also details on the evaluation criteria and need for automated metrics and attempts to provide guideline for evaluation for abstractive summarization for health informatics.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
keywords = {NLP, abstractive summarization, sentence compression, sentence fusion, document summarization, language model, ROUGE}
}

@inproceedings{10.1145/3580305.3599852,
author = {Xiang, Tingyan and Li, Ao and Ji, Yugang and Li, Dong},
title = {Knowledge Based Prohibited Item Detection on Heterogeneous Risk Graphs},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599852},
doi = {10.1145/3580305.3599852},
abstract = {With the popularity of online shopping in recent years, various prohibited items are continuously attacking e-commerce portals. Searching and deleting such risk items online has played a fundamental role in protecting the health of e-commerce trades. To mitigate negative impact of limited supervision and adversarial behaviors of malicious sellers, current state-of-the-art work mainly introduces heterogeneous graph neural network with further improvements such as graph structure learning, pairwise training mechanism, etc. However, performance of these models is highly limited since domain knowledge is indispensable for identifying prohibited items but ignored by these methods. In this paper, we propose a novel Knowledge Based Prohibited item Detection system (named KBPD) to break through this limitation. To make full use of rich risk knowledge, the proposed method introduces the Risk-Domain Knowledge Graph (named RDKG), which is encoded by a path-based graph neural network method. Furthermore, to utilize information from both the RDKG and the Heterogeneous Risk Graph (named HRG), an interactive fusion framework is proposed and further improves the detection performance. We collect real-world datasets from the largest Chinese second-hand commodity trading platform, Xianyu. Both offline and online experimental results consistently demonstrate that KBPD outperforms the state-of-the-art baselines. The improvement over the second-best method is up to 22.67% in the AP metric.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5260–5269},
numpages = {10},
keywords = {heterogeneous graph neural network, knowledge graph, prohibited item detection},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3580305.3599439,
author = {Luo, Pengfei and Xu, Tong and Wu, Shiwei and Zhu, Chen and Xu, Linli and Chen, Enhong},
title = {Multi-Grained Multimodal Interaction Network for Entity Linking},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599439},
doi = {10.1145/3580305.3599439},
abstract = {Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network (MIMIC) framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction units to comprehensively explore the intra-modal interaction and inter-modal fusion among features of entities and mentions. In particular, three modules, namely the Text-based Global-Local interaction Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based interaction Unit (CMFU) are designed to capture and integrate the fine-grained representation lying in abbreviated text and implicit visual cues. Afterwards, we introduce a unit-consistency objective function via contrastive learning to avoid inconsistency and model degradation. Experimental results on three public benchmark datasets demonstrate that our solution outperforms various state-of-the-art baselines, and ablation studies verify the effectiveness of designed modules.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1583–1594},
numpages = {12},
keywords = {knowledge graph, multimodal entity linking, multimodal interaction},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3587716.3587787,
author = {Qian, Jing and Chen, Qi and Yue, Yong and Atkinson, Katie and Li, Gangmin},
title = {Injecting Commonsense Knowledge into Prompt Learning for Zero-Shot Text Classification},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587787},
doi = {10.1145/3587716.3587787},
abstract = {The combination of pre-training and fine-tuning has become a default solution to Natural Language Processing (NLP) tasks. The emergence of prompt learning breaks such routine, especially in the scenarios of low data resources. Insufficient labelled data or even unseen classes are frequent problems in text classification, equipping Pre-trained Language Models (PLMs) with task-specific prompts helps get rid of the dilemma. However, general PLMs are barely provided with commonsense knowledge. In this work, we propose a KG-driven verbalizer that leverages commonsense Knowledge Graph (KG) to map label words with predefined classes. Specifically, we transform the mapping relationships into semantic relevance in the commonsense-injected embedding space. For zero-shot text classification task, experimental results exhibit the effectiveness of our KG-driven verbalizer on a Twitter dataset for natural disasters (i.e. HumAID) compared with other baselines.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {427–432},
numpages = {6},
keywords = {knowledge graph, prompt learning, zero-shot text classification},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@article{10.1145/3567829,
author = {Li, Jia and Song, Dandan and Wu, Zhijing},
title = {A Semantically Driven Hybrid Network for Unsupervised Entity Alignment},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3567829},
doi = {10.1145/3567829},
abstract = {The major challenge in the task of entity alignment (EA) lies in the heterogeneity of the knowledge graph. The traditional solution to EA is to first map entities to the same space via knowledge embedding and then calculate the similarity between entities from different knowledge graphs. However, these methods mainly rely on manually labeled seeds of EA, which limits their applicability. Some researchers have begun using pseudo-labels rather than seeds for unsupervised EA. However, directly using pseudo-labels causes new problems, such as noise in the pseudo-labels. In this article, we propose a model called the Semantically Driven Hybrid Network (SDHN) to reduce the impact of noise in the pseudo-labels on the performance of EA models. The SDHN consists of two modules: a Teacher–Student Network (TSN) and a Rotation and Penalty (RAP) module. The TSN module reduces the impact of noise in two ways: (1) The TSN’s teacher network guides its student network to construct pseudo-labels based on semantic information instead of directly creating pseudo-labels. (2) It adaptively fuses semantic information into student networks to improve the final representation of entity embedding. Finally, the TSN enhances the performance of models of entity alignment via the RAP module. The results of experiments on multiple benchmark datasets showed that the SDHN outperforms state-of-the-art models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {20},
numpages = {21},
keywords = {Knowledge graph, graph neural networks, entity alignment}
}

@inproceedings{10.1145/3539618.3594246,
author = {Dietz, Laura and Bast, Hannah and Chatterjee, Shubham and Dalton, Jeffrey and Nie, Jian-Yun and Nogueira, Rodrigo},
title = {Neuro-Symbolic Representations for Information Retrieval},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3594246},
doi = {10.1145/3539618.3594246},
abstract = {This tutorial will provide an overview of recent advances on neuro-symbolic approaches for information retrieval. A decade ago, knowledge graphs and semantic annotations technology led to active research on how to best leverage symbolic knowledge. At the same time, neural methods have demonstrated to be versatile and highly effective.From a neural network perspective, the same representation approach can service document ranking or knowledge graph reasoning. End-to-end training allows to optimize complex methods for downstream tasks.We are at the point where both the symbolic and the neural research advances are coalescing into neuro-symbolic approaches. The underlying research questions are how to best combine symbolic and neural approaches, what kind of symbolic/neural approaches are most suitable for which use case, and how to best integrate both ideas to advance the state of the art in information retrieval.Materials are available online: https://github.com/laura-dietz/neurosymbolic-representations-for-IR},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3436–3439},
numpages = {4},
keywords = {document representation, entities, knowledge graph, neural networks, neuro-symbolic representation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3562007.3562032,
author = {Zhang, Xiaoqiao},
title = {Prediction of User Ratings of Dianping Based on K-BERT Model},
year = {2022},
isbn = {9781450396851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3562007.3562032},
doi = {10.1145/3562007.3562032},
abstract = {In the Internet era, people usually consider the ratings and reviews of stores on review platforms when choosing travel locations. Today, the mainstream rating scheme for total store scores is weighted by review scores, but this scoring system can be negatively affected by malicious scoring and uneven scoring. Problems such as incompleteness and other issues will affect the authenticity of the store's rating. To this end, this paper designs a K-BERT Dianping user rating prediction based on K-BERT model to reflect real review ratings. Compared with the traditional BERT pre-training model, the K-BERT model can solve knowledge-driven problems faster through knowledge graph injection. In this paper, a Dianping knowledge map is established. Through the steps of text preprocessing, text pre-training, and Dianping dataset fine-tuning, it is found that the accuracy rate of the K-BERT model in the Dianping rating classification is about 95%. By comparing the model with BERT, Logistic Regression , it is found that the predicted effect of the K-BERT model is significantly better than the above two models.},
booktitle = {Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System},
pages = {133–140},
numpages = {8},
keywords = {Bert, Knowledge Graph, Reviews, Text Categorization},
location = {Virtual Event, China},
series = {CCRIS '22}
}

@inproceedings{10.1145/3487553.3524199,
author = {Kuculo, Tin},
title = {Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524199},
doi = {10.1145/3487553.3524199},
abstract = {Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {359–363},
numpages = {5},
keywords = {event extraction, event geotagging, event representation, knowledge graph, quotes},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3544548.3580907,
author = {Petridis, Savvas and Diakopoulos, Nicholas and Crowston, Kevin and Hansen, Mark and Henderson, Keren and Jastrzebski, Stan and Nickerson, Jeffrey V and Chilton, Lydia B},
title = {AngleKindling: Supporting Journalistic Angle Ideation with Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580907},
doi = {10.1145/3544548.3580907},
abstract = {News media often leverage documents to find ideas for stories, while being critical of the frames and narratives present. Developing angles from a document such as a press release is a cognitively taxing process, in which journalists critically examine the implicit meaning of its claims. Informed by interviews with journalists, we developed AngleKindling, an interactive tool which employs the common sense reasoning of large language models to help journalists explore angles for reporting on a press release. In a study with 12 professional journalists, we show that participants found AngleKindling significantly more helpful and less mentally demanding to use for brainstorming ideas, compared to a prior journalistic angle ideation tool. AngleKindling helped journalists deeply engage with the press release and recognize angles that were useful for multiple types of stories. From our findings, we discuss how to help journalists customize and identify promising angles, and extending AngleKindling to other knowledge-work domains.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {225},
numpages = {16},
keywords = {Brainstorming, Generative AI, Ideation, Journalism, Large Language Models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3583780.3615514,
author = {Colas, Anthony and Ma, Haodi and He, Xuanli and Bai, Yang and Wang, Daisy Zhe},
title = {Can Knowledge Graphs Simplify Text?},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615514},
doi = {10.1145/3583780.3615514},
abstract = {Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplification models which start with a given complex text. Our code is available on GitHub.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {379–389},
numpages = {11},
keywords = {KG-to-text, data-to-text, knowledge graph, natural language generation, simulated annealing, text simplification},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3457682.3457745,
author = {ZHOU, JIALE and WANG, TAO and DENG, JIANFENG},
title = {Corpus Construction and Entity Recognition for the Field of Industrial Robot Fault Diagnosis},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457745},
doi = {10.1145/3457682.3457745},
abstract = {The fault logs record the fault information generated during the operation process of industrial robots. It contains a large amount of fault knowledge and solution information. It is necessary to extract this information and build the fault diagnosis knowledge graph of industrial robots, which can support remote fault diagnosis of industrial robots without human help. At present, the research of fault diagnosis knowledge graph is still relatively scarce. In this paper, we propose a method of named entity recognition for extracting the knowledge of industrial robot fault diagnosis. The contribution of our paper is to establish the fault field dataset Fault-Data, propose the ontology concept of the fault diagnosis field, and obtain a good field recognition effect through the verification of the entity recognition model of fault diagnosis. Experimental results show that the F value of named entity recognition reaches 91.99%, which provides a certain reference significance for subsequent knowledge extraction and knowledge graph construction.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {410–416},
numpages = {7},
keywords = {Industrial robots, entity recognition, fault diagnosis, knowledge graph},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@inproceedings{10.1145/3511808.3557246,
author = {Xiong, Guanming and Bao, Junwei and Zhao, Wen and Wu, Youzheng and He, Xiaodong},
title = {AutoQGS: Auto-Prompt for Low-Resource Knowledge-based Question Generation from SPARQL},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557246},
doi = {10.1145/3511808.3557246},
abstract = {This study investigates the task of knowledge-based question generation (KBQG). Conventional KBQG works generated questions from fact triples in the knowledge graph, which could not express complex operations like aggregation and comparison in SPARQL. Moreover, due to the costly annotation of large-scale SPARQL-question pairs, KBQG from SPARQL under low-resource scenarios urgently needs to be explored. Recently, since the generative pre-trained language models (PLMs) typically trained in natural language (NL)-to-NL paradigm have been proven effective for low-resource generation, e.g., T5 and BART, how to effectively utilize them to generate NL-question from non-NL SPARQL is challenging. To address these challenges, AutoQGS, an auto-prompt approach for low-resource KBQG from SPARQL, is proposed. Firstly, we put forward to generate questions directly from SPARQL for KBQG task to handle complex operations. Secondly, we propose an auto-prompter trained on large-scale unsupervised data to rephrase SPARQL into NL description, smoothing the low-resource transformation from non-NL SPARQL to NL question with PLMs. Experimental results on the WebQuestionsSP, ComlexWebQuestions 1.1, and PathQuestions show that our model achieves state-of-the-art performance, especially in low-resource settings. Furthermore, a corpora of 330k factoid complex question-SPARQL pairs is generated for further KBQG research.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {2250–2259},
numpages = {10},
keywords = {complex question generation, knowledge graph, low resource},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3586163,
author = {Breit, Anna and Waltersdorfer, Laura and Ekaputra, Fajar J. and Sabou, Marta and Ekelhart, Andreas and Iana, Andreea and Paulheim, Heiko and Portisch, Jan and Revenko, Artem and Teije, Annette Ten and Van Harmelen, Frank},
title = {Combining Machine Learning and Semantic Web: A Systematic Mapping Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3586163},
doi = {10.1145/3586163},
abstract = {In line with the general trend in artificial intelligence research to create intelligent systems that combine learning and symbolic components, a new sub-area has emerged that focuses on combining Machine Learning components with techniques developed by the Semantic Web community—Semantic Web Machine Learning (SWeML). Due to its rapid growth and impact on several communities in thepast two decades, there is a need to better understand the space of these SWeML Systems, their characteristics, and trends. Yet, surveys that adopt principled and unbiased approaches are missing. To fill this gap, we performed a systematic study and analyzed nearly 500 papers published in the past decade in this area, where we focused on evaluating architectural and application-specific features. Our analysis identified a rapidly growing interest in SWeML Systems, with a high impact on several application domains and tasks. Catalysts for this rapid growth are the increased application of deep learning and knowledge graph technologies. By leveraging the in-depth understanding of this area acquired through this study, a further key contribution of this article is a classification system for SWeML Systems that we publish as ontology.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {313},
numpages = {41},
keywords = {Semantic Web, Machine Learning, Artificial Intelligence, knowledge graph, Knowledge Representation and Reasoning, neuro-symbolic integration, Systematic Mapping Study}
}

@inproceedings{10.1145/3604915.3608885,
author = {Zhang, Gangyi},
title = {User-Centric Conversational Recommendation: Adapting the Need of User with Large Language Models},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608885},
doi = {10.1145/3604915.3608885},
abstract = {Conversational recommender systems (CRS) promise to provide a more natural user experience for exploring and discovering items of interest through ongoing conversation. However, effectively modeling and adapting to users’ complex and changing preferences remains challenging. This research develops user-centric methods that focus on understanding and adapting to users throughout conversations to provide the most helpful recommendations. First, a graph-based Conversational Path Reasoning (CPR) framework is proposed that represents dialogs as interactive reasoning over a knowledge graph to capture nuanced user interests and explain recommendations. To further enhance relationship modeling, graph neural networks are incorporated for improved representation learning. Next, to address uncertainty in user needs, the Vague Preference Multi-round Conversational Recommendation (VPMCR) scenario and matching Adaptive Vague Preference Policy Learning (AVPPL) solution are presented using reinforcement learning to tailor recommendations to evolving preferences. Finally, opportunities to leverage large language models are discussed to further advance user experiences via advanced user modeling, policy learning, and response generation. Overall, this research focuses on designing conversational recommender systems that continuously understand and adapt to users’ ambiguous, complex and changing needs during natural conversations.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1349–1354},
numpages = {6},
keywords = {conversational recommendation, large language model, user-centric},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3581783.3613848,
author = {Rao, Jiahua and Shan, Zifei and Liu, Longpo and Zhou, Yao and Yang, Yuedong},
title = {Retrieval-based Knowledge Augmented Vision Language Pre-training},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613848},
doi = {10.1145/3581783.3613848},
abstract = {With the recent progress in large-scale vision and language representation learning, Vision Language Pre-training (VLP) models have achieved promising improvements on various multi-modal downstream tasks. Albeit powerful, these models have not fully leveraged world knowledge to their advantage. A key challenge of knowledge-augmented VLP is the lack of clear connections between knowledge and multi-modal data. Moreover, not all knowledge present in images/texts is useful, therefore prior approaches often struggle to effectively integrate knowledge, visual, and textual information. In this study, we propose REtrieval-based knowledge Augmented Vision Language (REAVL), a novel knowledge-augmented pre-training framework to address the above issues. For the first time, we introduce a knowledge-aware self-supervised learning scheme that efficiently establishes the correspondence between knowledge and multi-modal data and identifies informative knowledge to improve the modeling of alignment and interactions between visual and textual modalities. By adaptively integrating informative knowledge with visual and textual information, REAVL achieves new state-of-the-art performance uniformly on knowledge-based vision-language understanding and multi-modal entity linking tasks, as well as competitive results on general vision-language tasks while only using 0.2% pre-training data of the best models. Our model shows strong sample efficiency and effective knowledge utilization.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5399–5409},
numpages = {11},
keywords = {knowledge graph, knowledge retrieval, knowledge-augmented model, vision-language pre-training},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3579895.3579936,
author = {Qiao, Lin and Qu, Ruiting and Liu, Shenglong and Zhang, Yu and Wang, Huiying and Liu, Biqi},
title = {Roadmap on Industrial Knowledge System for Data-Oriented Intelligent Operation and Maintenance in Chinese Power Industry},
year = {2023},
isbn = {9781450398039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579895.3579936},
doi = {10.1145/3579895.3579936},
abstract = {To effectively and efficiently manage the information of power industry, especially in the State Grid of China, data-oriented intelligent operation and maintenance have always been a crucial task. Hence, in this paper, the roadmap on industrial knowledge system is presented for data-oriented intelligent operation and maintenance in Chinese power industry. Firstly, the background of the data-oriented intelligent operation and maintenance is described, and it has been pointed out that the core problem is data explosion and lack of knowledge in the data-oriented intelligent operation and maintenance in the State Grid of China. This is important not only for the construction of smart grid, but also for global energy savings. Secondly, as for data-oriented intelligent operation and maintenance, the State Grid data-oriented knowledge graph can be constructed. Then, we study the knowledge-driven multi-scenario intelligent operation decision technologies and information deployment technologies. Finally, the possible research direction of industrial knowledge systems is briefly presented for data-oriented intelligent operation and maintenance in State Grid.},
booktitle = {Proceedings of the 2022 11th International Conference on Networks, Communication and Computing},
pages = {267–276},
numpages = {10},
keywords = {Industrial knowledge system, Knowledge Graph, data-oriented Intelligent maintenance, data-oriented Intelligent operation},
location = {Beijing, China},
series = {ICNCC '22}
}

@inproceedings{10.1145/3543507.3583236,
author = {Guo, Kunpeng and Diefenbach, Dennis and Gourru, Antoine and Gravier, Christophe},
title = {Wikidata as a seed for Web Extraction},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583236},
doi = {10.1145/3543507.3583236},
abstract = {Wikidata has grown to a knowledge graph with an impressive size. To date, it contains more than 17 billion triples collecting information about people, places, films, stars, publications, proteins, and many more. On the other side, most of the information on the Web is not published in highly structured data repositories like Wikidata, but rather as unstructured and semi-structured content, more concretely in HTML pages containing text and tables. Finding, monitoring, and organizing this data in a knowledge graph is requiring considerable work from human editors. The volume and complexity of the data make this task difficult and time-consuming. In this work, we present a framework that is able to identify and extract new facts that are published under multiple Web domains so that they can be proposed for validation by Wikidata editors. The framework is relying on question-answering technologies. We take inspiration from ideas that are used to extract facts from textual collections and adapt them to extract facts from Web pages. For achieving this, we demonstrate that language models can be adapted to extract facts not only from textual collections but also from Web pages. By exploiting the information already contained in Wikidata the proposed framework can be trained without the need for any additional learning signals and can extract new facts for a wide range of properties and domains. Following this path, Wikidata can be used as a seed to extract facts on the Web. Our experiments show that we can achieve a mean performance of 84.07 at F1-score. Moreover, our estimations show that we can potentially extract millions of facts that can be proposed for human validation. The goal is to help editors in their daily tasks and contribute to the completion of the Wikidata knowledge graph.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2402–2411},
numpages = {10},
keywords = {Knowledge Graph Completion, Linking, Question Answering, Web Crawling, Web Extraction, Web Scraping, Wikidata},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3477495.3531683,
author = {Yang, Zuoxi},
title = {Generating Knowledge-based Explanation for Recommendation from Review},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531683},
doi = {10.1145/3477495.3531683},
abstract = {Reasonable explanation is helpful to increase the trust and satisfaction of user to the recommender system. Among many previous studies, there is growing concern about generating explanation based on review text.Collaborative filtering is one of the most successful approaches to predict user's preference. However, most of them suffer from data sparsity problem. Researcher often utilizes auxiliary data to address this problem, such as review, knowledge graph (KG), image and so on. Some researchers have proven that recommendation accuracy can be improved via incorporating rating and review data. Besides, neural network is also applied to learn more powerful representations for user and item from the review data. For example, convolution neural network (CNN) is used to extract representation from review text by using convolutional filters. Recurrent neural network (RNN) is another widely used model, which can encode the sequential behaviours as hidden states. However, most of them lack the ability to generate explanation.In order to generate explanation, there are two main approaches are used, i.e., template-based approach and generation-based approach. It is usually necessary for the templated-based approach to define serval templates. Then, these templates will be further filled with different personalized features/words. Although they can offer readable explanations, they rely heavily on pre-defined templates. It causes large manual efforts, limiting their explanation expression. Due to the strong generation ability of natural language model, the generation-based approach is capable to generate explanation without templates, which can largely enhance the expression of the generated sentence. Although they can generate more free and flexible explanation, the explanation might tend to be uninformative.To tackle these challenges of the above-mentioned work, we propose a Generating Knowldge-based Explanation for Recommendation from Review (GKER) to provide informative explanation. Unlike the traditional generation-based approach with a multi-task framework, we design a single-task framework to simultaneously model user's preference and explanation generation. The multi-task training usually needs more manual effort and time overhead. In this unitary framework, we inject the user's sentiment preference into the explanation generation, aiming at capturing the user's interest while producing high-quality explanation. Specifically, we build three graphs, including a bipartite graph, a KG and a co-occur graph. All of them are integrated to form a unitary graph, thus bringing the semantic among user-item interaction, KG and review. Based on this integrated graph, it is possible to learn more effective representations for user and item. To make better use of the integrated KG, a graph convolution network (GCN) is utilized to obtain improved embeddings due to its superior representation learning ability. We argue that these embeddings can contain more semantic interaction signals with the help of the integrated KG and GCN. After obtaining these extensive embeddings, a multilayer perceptron (MLP) layer is further employed to capture non-linear interaction signals between user and item, aiming at predicting user's rating accurately. The predicted rating would be regarded as a sentiment indicator to explore why the user likes or dislikes the target item. To investigate the association between sentiment indicator and the related review data, a transformer-enhanced encoder-decoder architecture is designed to produce informative and topic-relevant explanation. Besides, the aspect semantic is added in this architecture through an attention mechanism. In this framework, the transformer is utilized as a "teacher" model to supervise the generation of the encoder-decoder process. Finally, experiments conducted on three datasets have shown the state-of-the-art performance of GKER.There are some research issues for discussion: 1) although KG is a useful tool for recommendation accuracy and explainability, it is always incomplete in the real world. Hence, it is worth completing it for the recommendation. 2) Besides, as for explainable, it still needs more metrics to evaluate the quality of its explanation.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3494},
numpages = {1},
keywords = {generating explanation, graph convolution network, knowledge graph, review},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3511808.3557459,
author = {Li, Jiacheng and Katsis, Yannis and Baldwin, Tyler and Kim, Ho-Cheol and Bartko, Andrew and McAuley, Julian and Hsu, Chun-Nan},
title = {SPOT: Knowledge-Enhanced Language Representations for Information Extraction},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557459},
doi = {10.1145/3511808.3557459},
abstract = {Knowledge-enhanced pre-trained models for language representation have been shown to be more effective in knowledge base construction tasks (i.e.,~relation extraction) than language models such as BERT. These knowledge-enhanced language models incorporate knowledge into pre-training to generate representations of entities or relationships. However, existing methods typically represent each entity with a separate embedding. As a result, these methods struggle to represent out-of-vocabulary entities and a large amount of parameters, on top of their underlying token models (i.e., the transformer), must be used and the number of entities that can be handled is limited in practice due to memory constraints. Moreover, existing models still struggle to represent entities and relationships simultaneously. To address these problems, we propose a new pre-trained model that learns representations of both entities and relationships from token spans and span pairs in the text respectively. By encoding spans efficiently with span modules, our model can represent both entities and their relationships but requires fewer parameters than existing models. We pre-trained our model with the knowledge graph extracted from Wikipedia and test it on a broad range of supervised and unsupervised information extraction tasks. Results show that our model learns better representations for both entities and relationships than baselines, while in supervised settings, fine-tuning our model outperforms RoBERTa consistently and achieves competitive results on information extraction tasks.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {1124–1134},
numpages = {11},
keywords = {information extraction, knowledge representation, language model, pre-trained model, representation learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3607199.3607208,
author = {Alam, Md Tanvirul and Bhusal, Dipkamal and Park, Youngja and Rastogi, Nidhi},
title = {Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607199.3607208},
doi = {10.1145/3607199.3607208},
abstract = {Public and commercial organizations extensively share cyberthreat intelligence (CTI) to prepare systems to defend against existing and emerging cyberattacks. However, traditional CTI has primarily focused on tracking known threat indicators such as IP addresses and domain names, which may not provide long-term value in defending against evolving attacks. To address this challenge, we propose to use more robust threat intelligence signals called attack patterns. LADDER is a knowledge extraction framework that can extract text-based attack patterns from CTI reports at scale. The framework characterizes attack patterns by capturing the phases of an attack in Android and enterprise networks and systematically maps them to the MITRE ATT&amp;CK pattern framework. LADDER can be used by security analysts to determine the presence of attack vectors related to existing and emerging threats, enabling them to prepare defenses proactively. We also present several use cases to demonstrate the application of LADDER in real-world scenarios. Finally, we provide a new, open-access benchmark malware dataset to train future cyberthreat intelligence models.},
booktitle = {Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
pages = {92–108},
numpages = {17},
keywords = {Attack Patterns, Knowledge Graph, LADDER, Threat Intelligence},
location = {Hong Kong, China},
series = {RAID '23}
}

@inproceedings{10.1145/3581783.3612274,
author = {Liang, Kongming and Wang, Xinran and Zhang, Haiwen and Ma, Zhanyu and Guo, Jun},
title = {Hierarchical Visual Attribute Learning in the Wild},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612274},
doi = {10.1145/3581783.3612274},
abstract = {Observing objects' attributes at different levels of detail is a fundamental aspect of how humans perceive and understand the world around them. Existing studies focused on attribute prediction in a flat way, but they overlook the underlying attribute hierarchy, e.g., navy blue is a subcategory of blue. In recent years, large language models, e.g., ChatGPT, have emerged with the ability to perform an extensive range of natural language processing tasks like text generation and classification. The factual knowledge learned by LLM can assist us build the hierarchical relations of visual attributes in the wild. Based on that, we propose a model called the object-specific attribute relation net, which takes advantage of three types of relations among attributes - positive, negative, and hierarchical - to better facilitate attribute recognition in images. Guided by the extracted hierarchical relations, our model can predict attributes from coarse to fine. Additionally, we introduce several evaluation metrics for attribute hierarchy to comprehensively assess the model's ability to comprehend hierarchical relations. Our extensive experiments demonstrate that our proposed hierarchical annotation brings improvements to the model's understanding of hierarchical relations of attributes, and the object-specific attribute relation net can recognize visual attributes more accurately.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {3415–3423},
numpages = {9},
keywords = {attribute learning, hierarchical multi-label learning, large language model},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3447548.3467203,
author = {Luo, Xusheng and Bo, Le and Wu, Jinhang and Li, Lin and Luo, Zhiy and Yang, Yonghua and Yang, Keping},
title = {AliCoCo2: Commonsense Knowledge Extraction, Representation and Application in E-commerce},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467203},
doi = {10.1145/3447548.3467203},
abstract = {Commonsense knowledge used by humans while doing online shopping is valuable but difficult to be captured by existing systems running on e-commerce platforms. While construction of common- sense knowledge graphs in e-commerce is non-trivial, representation learning upon such graphs poses unique challenge compared to well-studied open-domain knowledge graphs (e.g., Freebase). By leveraging the commonsense knowledge and representation techniques, various applications in e-commerce can be benefited. Based on AliCoCo, the large-scale e-commerce concept net assisting a series of core businesses in Alibaba, we further enrich it with more commonsense relations and present AliCoCo2, the first commonsense knowledge graph constructed for e-commerce use. We propose a multi-task encoder-decoder framework to provide effective representations for nodes and edges from AliCoCo2. To explore the possibility of improving e-commerce businesses with commonsense knowledge, we apply newly mined commonsense relations and learned embeddings to e-commerce search engine and recommendation system in different ways. Experimental results demonstrate that our proposed representation learning method achieves state-of-the-art performance on the task of knowledge graph completion (KGC), and applications on search and recommendation indicate great potential value of the construction and use of commonsense knowledge graph in e-commerce. Besides, we propose an e-commerce QA task with a new benchmark during the construction of AliCoCo2, for testing machine common sense in e-commerce, which can benefit research community in exploring commonsense reasoning.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3385–3393},
numpages = {9},
keywords = {common sense, e-commerce, knowledge graph embedding},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3582197.3582250,
author = {Xu, Sa},
title = {A Knowledge Reasoning Model Based on Non-Factoid Information Enhancement},
year = {2023},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582197.3582250},
doi = {10.1145/3582197.3582250},
abstract = {Q&amp;A system plays an increasingly important role in the modern society with information explosion, and knowledge reasoning model (KRM) is the main research content of Q&amp;A system. Existing knowledge reasoning models are mainly divided into text-based information retrieval (IR) and knowledge graph embedding (KGE). KGE is superior to IR in terms of storage and reasoning capabilities for massive factoid information, but lacks the ability to reason non-factoid information, merely focus on the mining of structural information without the semantic information. We proposed a knowledge reasoning model based on non-factoid information enhancement (NFE-KRM) in scenic Q&amp;A. It realizes the KGE integrates semantic information (SIKGE) and the unified semantic embedding space (USES), so that NFE-KRM has the ability to answer both factoid and non-factoid questions. We have used a large number of experiments to prove that SIKGE gets a better performance on Mean Rank and Hits@10. NFE-KRM's F1 score and accuracy on the mixed dataset are both competitive.},
booktitle = {Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
pages = {318–323},
numpages = {6},
keywords = {Q&amp;A system, Information Retrieval, Knowledge Graph Embedding, NLP.},
location = {Shanghai, China},
series = {ICIT '22}
}

@inproceedings{10.1145/3500931.3500953,
author = {Dong, Andi and Wang, Chao and Tong, Pan and Yang, Dan and Yong, Cuo},
title = {Research on Tibetan medicine intelligent question answering system integrating confrontation training and reinforcement learning},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500953},
doi = {10.1145/3500931.3500953},
abstract = {In this study, a knowledge graph (KG) based Tibetan medicine intelligent question answering (QA) system model was proposed based on an adversarial learning generative network model, in an attempt to alleviate the scarcity of medical resources, promote the heritage and innovation of Tibetan medicine, and ease the shortage of Tibetan medical information. In this model, the simulated answers were generated via adversarial learning, and subsequently the reinforcement learning was applied for feedback-based optimization, with the ultimate aim of enhancing the accuracy rate of this model. Besides, a triple extraction method based on Tibetan features was proposed to construct a KG dialog set. Finally, this model was subjected to an experiment in Chinese and Tibetan datasets, with the results indicating that the accuracy of this intelligent QA model incorporating adversarial networks and reinforcement learning was higher than other models.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {120–125},
numpages = {6},
keywords = {Generate countermeasure network, Knowledge extraction, Knowledge graph, Reinforcement learning, Tibetan medicine},
location = {Beijing, China},
series = {ISAIMS '21}
}

@article{10.1145/3457533,
author = {Deng, Yang and Xie, Yuexiang and Li, Yaliang and Yang, Min and Lam, Wai and Shen, Ying},
title = {Contextualized Knowledge-aware Attentive Neural Network: Enhancing Answer Selection with Knowledge},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3457533},
doi = {10.1145/3457533},
abstract = {Answer selection, which is involved in many natural language processing applications, such as dialog systems and question answering (QA), is an important yet challenging task in practice, since conventional methods typically suffer from the issues of ignoring diverse real-world background knowledge. In this article, we extensively investigate approaches to enhancing the answer selection model with external knowledge from knowledge graph (KG). First, we present a context-knowledge interaction learning framework, Knowledge-aware Neural Network, which learns the QA sentence representations by considering a tight interaction with the external knowledge from KG and the textual information. Then, we develop two kinds of knowledge-aware attention mechanism to summarize both the context-based and knowledge-based interactions between questions and answers. To handle the diversity and complexity of KG information, we further propose a Contextualized Knowledge-aware Attentive Neural Network, which improves the knowledge representation learning with structure information via a customized Graph Convolutional Network and comprehensively learns context-based and knowledge-based sentence representation via the multi-view knowledge-aware attention mechanism. We evaluate our method on four widely used benchmark QA datasets, including WikiQA, TREC QA, InsuranceQA, and Yahoo QA. Results verify the benefits of incorporating external knowledge from KG and show the robust superiority and extensive applicability of our method.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {2},
numpages = {33},
keywords = {Answer selection, knowledge graph, attention mechanism, graph convolutional network}
}

@inproceedings{10.1145/3543873.3587617,
author = {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
title = {Automated Ontology Evaluation: Evaluating Coverage and Correctness using a Domain Corpus},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587617},
doi = {10.1145/3543873.3587617},
abstract = {Ontologies conceptualize domains and are a crucial part of web semantics and information systems. However, re-using an existing ontology for a new task requires a detailed evaluation of the candidate ontology as it may cover only a subset of the domain concepts, contain information that is redundant or misleading, and have inaccurate relations and hierarchies between concepts. Manual evaluation of large and complex ontologies is a tedious task. Thus, a few approaches have been proposed for automated evaluation, ranging from concept coverage to ontology generation from a corpus. Existing approaches, however, are limited by their dependence on external structured knowledge sources, such as a thesaurus, as well as by their inability to evaluate semantic relationships. In this paper, we propose a novel framework to automatically evaluate the domain coverage and semantic correctness of existing ontologies based on domain information derived from text. The approach uses a domain-tuned named-entity-recognition model to extract phrasal concepts. The extracted concepts are then used as a representation of the domain against which we evaluate the candidate ontology’s concepts. We further employ a domain-tuned language model to determine the semantic correctness of the candidate ontology’s relations. We demonstrate our automated approach on several large ontologies from the oceanographic domain and show its agreement with a manual evaluation by domain experts and its superiority over the state-of-the-art.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1127–1137},
numpages = {11},
keywords = {BERT, knowledge engineering, natural language processing, ontology},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3584371.3613016,
author = {Oduro-Afriyie, Joel and Jamil, Hasan M},
title = {Enabling the Informed Patient Paradigm with Secure and Personalized Medical Question Answering},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3613016},
doi = {10.1145/3584371.3613016},
abstract = {Quality patient care is a complex and multifaceted problem requiring the integration of data from multiple sources. We propose Medicient, a knowledge-graph-based question answering system that processes heterogeneous data sources, including patient health records, drug databases, and medical literature, into a unified knowledge graph with zero training. The knowledge graph is then utilized to provide personalized recommendations for treatment or medication. The system leverages the power of large language models for question understanding and natural language response generation, while hiding sensitive patient information. We compare our system to a large language model (ChatGPT), which does not have access to patient health records, and show that our system provides better recommendations. This study contributes to a growing body of research on knowledge graphs and their applications in healthcare.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {33},
numpages = {6},
keywords = {knowledge graphs, large language models, semantic graph search, data integration, personal health library, informed patients},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3539597.3570426,
author = {Zhang, Xiaoyu and Xin, Xin and Li, Dongdong and Liu, Wenxuan and Ren, Pengjie and Chen, Zhumin and Ma, Jun and Ren, Zhaochun},
title = {Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3570426},
doi = {10.1145/3539597.3570426},
abstract = {Conversational recommender systems (CRSs) often utilize external knowledge graphs (KGs) to introduce rich semantic information and recommend relevant items through natural language dialogues. However, original KGs employed in existing CRSs are often incomplete and sparse, which limits the reasoning capability in recommendation. Moreover, only few of existing studies exploit the dialogue context to dynamically refine knowledge from KGs for better recommendation. To address the above issues, we propose the Variational Reasoning over Incomplete KGs Conversational Recommender (VRICR). Our key idea is to incorporate the large dialogue corpus naturally accompanied with CRSs to enhance the incomplete KGs; and perform dynamic knowledge reasoning conditioned on the dialogue context. Specifically, we denote the dialogue-specific subgraphs of KGs as latent variables with categorical priors for adaptive knowledge graphs refactor. We propose a variational Bayesian method to approximate posterior distributions over dialogue-specific subgraphs, which not only leverages the dialogue corpus for restructuring missing entity relations but also dynamically selects knowledge based on the dialogue context. Finally, we infuse the dialogue-specific subgraphs to decode the recommendation and responses. We conduct experiments on two benchmark CRSs datasets. Experimental results confirm the effectiveness of our proposed method.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {231–239},
numpages = {9},
keywords = {conversational recommender systems, knowledge graph enhancement, knowledge refinemen, variational inference},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3587259.3627555,
author = {Verkijk, Stella and Roothaert, Ritten and Pernisch, Romana and Schlobach, Stefan},
title = {Do you catch my drift? On the usage of embedding methods to measure concept shift in knowledge graphs},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627555},
doi = {10.1145/3587259.3627555},
abstract = {Automatically detecting and measuring differences between evolving Knowledge Graphs (KGs) has been a topic of investigation for years. With the rising popularity of embedding methods, we investigate the possibility of using embeddings to detect Concept Shift in evolving KGs. Specifically, we go deeper into the usage of nearest neighbour set comparison as the basis for a similarity measure, and show why this approach is conceptually problematic. As an alternative, we explore the possibility of using clustering methods. This paper serves to (i) inform the community about the challenges that arise when using KG embeddings for the comparison of different versions of a KG specifically, (ii) investigate how this is supported by theories on knowledge representation and semantic representation in NLP and (iii) take the first steps into the direction of valuable representation of semantics within KGs for comparison.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {70–74},
numpages = {5},
keywords = {Concept Shift, Knowledge Graph Embeddings, NLP, Semantics},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3404835.3462865,
author = {Li, Junyi and Zhao, Wayne Xin and Wei, Zhicheng and Yuan, Nicholas Jing and Wen, Ji-Rong},
title = {Knowledge-based Review Generation by Coherence Enhanced Text Planning},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462865},
doi = {10.1145/3404835.3462865},
abstract = {As a natural language generation task, it is challenging to generate informative and coherent review text. In order to enhance the informativeness of the generated text, existing solutions typically learn to copy entities or triples from knowledge graphs (KGs). However, they lack overall consideration to select and arrange the incorporated knowledge, which tends to cause text incoherence. To address the above issue, we focus on improving entity-centric coherence of the generated reviews by leveraging the semantic structure of KGs. In this paper, we propose a novel Coherence Enhanced Text Planning model (CETP) based on knowledge graphs (KGs) to improve both global and local coherence for review generation. The proposed model learns a two-level text plan for generating a document: (1) the document plan is modeled as a sequence of sentence plans in order, and (2) the sentence plan is modeled as an entity-based subgraph from KG. Local coherence can be naturally enforced by KG subgraphs through intra-sentence correlations between entities. For global coherence, we design a hierarchical self-attentive architecture with both subgraph- and node-level attention to enhance the correlations between subgraphs. To our knowledge, we are the first to utilize a KG-based text planning model to enhance text coherence for review generation. Extensive experiments on three datasets confirm the effectiveness of our model on improving the content coherence of generated texts.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {183–192},
numpages = {10},
keywords = {knowledge graph, review generation, text planning},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3603719.3603736,
author = {Xie, Bingbing and Ma, Xiaoxiao and Wu, Jia and Yang, Jian and Xue, Shan and Fan, Hao},
title = {Heterogeneous Graph Neural Network via Knowledge Relations for Fake News Detection},
year = {2023},
isbn = {9798400707469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603719.3603736},
doi = {10.1145/3603719.3603736},
abstract = {The proliferation of fake news in social media has been recognized as a severe problem for society, and substantial attempts have been devoted to fake news detection to alleviate the detrimental impacts. Knowledge graphs (KGs) comprise rich factual relations among real entities, which could be utilized as ground-truth databases and enhance fake news detection. However, most of the existing methods only leveraged natural language processing and graph mining techniques to extract features of fake news for detection and rarely explored the ground knowledge in knowledge graphs. In this work, we propose a novel Heterogeneous Graph Neural Network via Knowledge Relations for Fake News Detection (HGNNR4FD). The devised framework has four major components: 1) A heterogeneous graph (HG) built upon news content, including three types of nodes, i.e., news, entities, and topics, and their relations. 2) A KG that provides the factual basis for detecting fake news by generating embeddings via relations in the KG. 3) A novel attention-based heterogeneous graph neural network that can aggregate information from HG and KG, and 4) a fake news detector, which is capable of identifying fake news based on the news embeddings generated by HGNNR4FD. We further validate the performance of our method by comparison with seven state-of-art baselines and verify the effectiveness of the components through a thorough ablation analysis. From the results, we empirically demonstrate that our framework achieves superior results and yields improvement over the baselines regarding evaluation metrics of accuracy, precision, recall, and F1-score on four real-world datasets.},
booktitle = {Proceedings of the 35th International Conference on Scientific and Statistical Database Management},
articleno = {15},
numpages = {11},
keywords = {Anomaly detection, Fake news detection, Graph mining, Knowledge graph},
location = {Los Angeles, CA, USA},
series = {SSDBM '23}
}

@article{10.1145/3505138,
author = {Zhang, Peng and Hui, Wenjie and Wang, Benyou and Zhao, Donghao and Song, Dawei and Lioma, Christina and Simonsen, Jakob Grue},
title = {Complex-valued Neural Network-based Quantum Language Models},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3505138},
doi = {10.1145/3505138},
abstract = {Language modeling is essential in Natural Language Processing and Information Retrieval related tasks. After the statistical language models, Quantum Language Model (QLM) has been proposed to unify both single words and compound terms in the same probability space without extending term space exponentially. Although QLM achieved good performance in ad hoc retrieval, it still has two major limitations: (1) QLM cannot make use of supervised information, mainly due to the iterative and non-differentiable estimation of the density matrix, which represents both queries and documents in QLM. (2) QLM assumes the exchangeability of words or word dependencies, neglecting the order or position information of words.This article aims to generalize QLM and make it applicable to more complicated matching tasks (e.g., Question Answering) beyond ad hoc retrieval. We propose a complex-valued neural network-based QLM solution called C-NNQLM to employ an end-to-end approach to build and train density matrices in a light-weight and differentiable manner, and it can therefore make use of external well-trained word vectors and supervised labels. Furthermore, C-NNQLM adopts complex-valued word vectors whose phase vectors can directly encode the order (or position) information of words. Note that complex numbers are also essential in the quantum theory. We show that the real-valued NNQLM (R-NNQLM) is a special case of C-NNQLM.The experimental results on the QA task show that both R-NNQLM and C-NNQLM achieve much better performance than the vanilla QLM, and C-NNQLM’s performance is on par with state-of-the-art neural network models. We also evaluate the proposed C-NNQLM on text classification and document retrieval tasks. The results on most datasets show that the C-NNQLM can outperform R-NNQLM, which demonstrates the usefulness of the complex representation for words and sentences in C-NNQLM.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {84},
numpages = {31},
keywords = {Quantum theory, language model, question answering, neural network}
}

@inproceedings{10.1145/3583780.3615484,
author = {Tu, Shangqing and Zhang, Zheyuan and Yu, Jifan and Li, Chunyang and Zhang, Siyu and Yao, Zijun and Hou, Lei and Li, Juanzi},
title = {LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615484},
doi = {10.1145/3583780.3615484},
abstract = {Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. We test the system's performance via both offline evaluation and online deployment. Since May 2020, our LittleMu system has served over 80,000 users with over 300,000 queries from over 500 courses on XuetangX MOOC platform, which continuously contributes to a more convenient and fair education. Our code, services, and dataset will be available at https://github.com/THU-KEG/VTA.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4843–4849},
numpages = {7},
keywords = {dialogue system, educational support, language model prompts, virtual teaching assistant},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3477495.3531661,
author = {Ge, Congcong and Zeng, Xiaocan and Chen, Lu and Gao, Yunjun},
title = {ZeroMatcher: A Cost-Off Entity Matching System},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531661},
doi = {10.1145/3477495.3531661},
abstract = {Entity Matching (EM) aims to find data instances from different sources that refer to the same real-world entity. The existing EM techniques can be either costly or tailored for a specific data type. We present ZeroMatcher, a cost-off entity matching system, which supports (i) handling EM tasks with different data types, including relational tables and knowledge graphs; (ii) keeping its EM performance always competitive by enabling the sub-modules to be updated in a lightweight manner, thus reducing development costs; and (iii) performing EM without human annotations to further slash the labor costs. First, ZeroMatcher automatically suggests users a set of appropriate modules for EM according to the data types of the input datasets. Users could specify the modules for the subsequent EM process according to their preferences. Alternatively, users are able to customize the modules of ZeroMatcher. Then, the system proceeds to the EM task, where users can track the entire EM process and monitor the memory usage changes in real-time. When the EM process is completed, ZeroMatcher visualizes the EM results from different aspects to ease the understanding for users. Finally, ZeroMatcher provides EM results evaluation, enabling users to compare the effectiveness among different parameter settings.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3262–3266},
numpages = {5},
keywords = {entity matching, knowledge graph, relational table},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3534678.3539382,
author = {Wang, Xiaolei and Zhou, Kun and Wen, Ji-Rong and Zhao, Wayne Xin},
title = {Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539382},
doi = {10.1145/3534678.3539382},
abstract = {Conversational recommender systems (CRS) aim to proactively elicit user preference and recommend high-quality items through natural language conversations. Typically, a CRS consists of a recommendation module to predict preferred items for users and a conversation module to generate appropriate responses. To develop an effective CRS, it is essential to seamlessly integrate the two modules. Existing works either design semantic alignment strategies, or share knowledge resources and representations between the two modules. However, these approaches still rely on different architectures or techniques to develop the two modules, making it difficult for effective module integration. To address this problem, we propose a unified CRS model named UniCRS based on knowledge-enhanced prompt learning. Our approach unifies the recommendation and conversation subtasks into the prompt learning paradigm, and utilizes knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to fulfill both subtasks in a unified approach. In the prompt design, we include fused knowledge representations, task-specific soft tokens, and the dialogue context, which can provide sufficient contextual information to adapt the PLM for the CRS task. Besides, for the recommendation subtask, we also incorporate the generated response template as an important part of the prompt, to enhance the information interaction between the two subtasks. Extensive experiments on two public CRS datasets have demonstrated the effectiveness of our approach. Our code is publicly available at the link: https://github.com/RUCAIBox/UniCRS.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1929–1937},
numpages = {9},
keywords = {conversational recommender system, pre-trained language model, prompt learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3488560.3498516,
author = {Li, Xiangsheng and Mao, Jiaxin and Ma, Weizhi and Wu, Zhijing and Liu, Yiqun and Zhang, Min and Ma, Shaoping and Wang, Zhaowei and He, Xiuqiang},
title = {A Cooperative Neural Information Retrieval Pipeline with Knowledge Enhanced Automatic Query Reformulation},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498516},
doi = {10.1145/3488560.3498516},
abstract = {This paper presents a neural information retrieval pipeline that integrates cooperative learning of query reformulation and neural retrieval models. Our pipeline first exploits an automatic query reformulator to reformulate the user-issued query and then submits the reformulated query to the neural retrieval model. We simultaneously optimize the quality of reformulated queries and ranking performance with an alternate training strategy where query reformulator and neural retrieval model learn from the feedback of each other. Besides, we incorporate knowledge information into automatic query reformulation. The reformulated queries are further improved and contribute to a better ranking performance of the following neural retrieval model. We study two representative neural retrieval models KNRM and BERT in our pipeline. Experiments on two datasets show that our pipeline consistently improves the retrieval performance of the original neural retrieval models while only increases negligible time on automatic query reformulation.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {553–561},
numpages = {9},
keywords = {knowledge graph, neural ir, query reformulation},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3543507.3583317,
author = {Liu, Yu and Hua, Wen and Xin, Kexuan and Hosseini, Saeid and Zhou, Xiaofang},
title = {TEA: Time-aware Entity Alignment in Knowledge Graphs},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583317},
doi = {10.1145/3543507.3583317},
abstract = {Entity alignment (EA) aims to identify equivalent entities between knowledge graphs (KGs), which is a key technique to improve the coverage of existing KGs. Current EA models largely ignore the importance of time information contained in KGs and treat relational facts or attribute values of entities as time-invariant. However, real-world entities could evolve over time, making the knowledge of the aligned entities very different in multiple KGs. This may cause incorrect matching between KGs if such entity dynamics is ignored. In this paper, we propose a time-aware entity alignment (TEA) model that discovers the entity evolving behaviour by exploring the time contexts in KGs and aggregates various contextual information to make the alignment decision. In particular, we address two main challenges in the TEA model: 1) How to identify highly-correlated temporal facts; 2) How to capture entity dynamics and incorporate it to learn a more informative entity representation for the alignment task. Experiments on real-world datasets1 verify the superiority of our TEA model over state-of-the-art entity aligners.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {2591–2599},
numpages = {9},
keywords = {Entity alignment, context evolving, knowledge graph, predicate clustering, time context encoder},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3563657.3595996,
author = {Kim, Jeongyeon and Suh, Sangho and Chilton, Lydia B and Xia, Haijun},
title = {Metaphorian: Leveraging Large Language Models to Support Extended Metaphor Creation for Science Writing},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3595996},
doi = {10.1145/3563657.3595996},
abstract = {Science writers commonly use extended metaphors to communicate unfamiliar concepts in a more accessible way to a wider audience. However, creating metaphors for science writing is challenging even for professional writers; according to our formative study (n=6), finding inspiration and extending metaphors with coherent structures were critical yet significantly challenging tasks for them. We contribute Metaphorian, a system that supports science writers with the creation of scientific metaphors by facilitating the search, extension, and iterative revision of metaphors. Metaphorian uses a large language model-based workflow inspired by the heuristic rules revealed from a study with six professional writers. A user study (n=16) revealed that Metaphorian significantly enhances satisfaction, confidence, and inspiration in metaphor writing without decreasing writers’ sense of agency. We discuss design implications for creativity support for figurative writing in science.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {115–135},
numpages = {21},
keywords = {Creativity Support Tools, GPT-3, Large Language Model, Metaphors, Science Writing, Writing Support},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@article{10.1145/3588947,
author = {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and Huang, Weiming and Hai, Zhen},
title = {Mining Geospatial Relationships from Text},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588947},
doi = {10.1145/3588947},
abstract = {A geospatial Knowledge Graph (KG) is a heterogeneous information network, capable of representing relationships between spatial entities in a machine-interpretable format, and has tremendous applications in logistics and social networks. Existing efforts to build a geospatial KG, have mainly used sparse spatial relationships, e.g., a district located inside a city, which provide only marginal benefits compared to a traditional database. In spite of the substantial advances in the tasks of link prediction and knowledge graph completion, identifying geospatial relationships remains challenging, particularly due to the fact that spatial entities are represented with single-point geometries, and textual attributes are frequently missing. In this study, we present GTMiner, a novel framework capable of jointly modeling Geospatial and Textual information to construct a knowledge graph, by mining three useful spatial relationships from a geospatial database, in an end-to-end fashion. The system is divided into three components: (1) a Candidate Selection module, to efficiently select a small number of candidate pairs; (2) a Relation Prediction component to predict spatial relationships between the entities; (3) a KG Refinement procedure, to improve both coverage and correctness of a geospatial knowledge graph. We carry out experiments on four cities' geospatial databases, from publicly-available sources and compare with existing algorithms for link prediction and geospatial data integration. Finally, we conduct an ablation study to motivate our design choices and an efficiency analysis to show that the time required by GTMiner for training and inference is comparable, or even shorter, than existing solutions.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {93},
numpages = {26},
keywords = {area, geokg, geometry, geospatial, graph, information, interest, kg, knowledge, language, learning, model, of, ontology, poi, point, pois, prediction, relation, relationship, relationships, representation, spatial, text}
}

@inproceedings{10.1145/3539597.3573038,
author = {Yong, Shan Jie and Dong, Kuicai and Sun, Aixin},
title = {DOCoR: Document-level OpenIE with Coreference Resolution},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3573038},
doi = {10.1145/3539597.3573038},
abstract = {Open Information Extraction (OpenIE) extracts relational fact tuples in the form of &lt;subject, relation, object&gt; from text. Most existing OpenIE solutions operate at sentence level and extract relational tuples solely from a sentence. However, many sentences exist as a part of paragraph or a document, where coreferencing is common. In this demonstration, we present a system which refines the semantic tuples generated by OpenIE with the aid of a coreference resolution tool. Specifically, all coreferential mentions across the entire document are identified and grouped into coreferential clusters. Objects and subjects in the extracted tuples from OpenIE which match any coreferential mentions are then resolved with a suitable representative term. In this way, our system is able to resolve both anaphoric and cataphoric references, to achieve Document-level OpenIE with Coreference Resolution (DOCoR). The demonstration video can be viewed at https://youtu.be/o9ZSWCBvlDs},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1204–1207},
numpages = {4},
keywords = {coreference resolution, knowledge graph construction},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3543873.3587318,
author = {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
title = {OntoEval: an Automated Ontology Evaluation System},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587318},
doi = {10.1145/3543873.3587318},
abstract = {Developing semantically-aware web services requires comprehensive and accurate ontologies. Evaluating an existing ontology or adapting it is a labor-intensive and complex task for which no automated tools exist. Nevertheless, in this paper we propose a tool that aims at making this vision come true, i.e., we present a tool for the automated evaluation of ontologies that allows one to rapidly assess an ontology’s coverage of a domain and identify specific problems in the ontology’s structure. The tool evaluates the domain coverage and correctness of parent-child relations of a given ontology based on domain information derived from a text corpus representing the domain. The tool provides both overall statistics and detailed analysis of sub-graphs of the ontology. In the demo, we show how these features can be used for the iterative improvement of an ontology.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {82–85},
numpages = {4},
keywords = {BERT, knowledge engineering, natural language processing, ontology},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3579051.3579053,
author = {Chen, Zhuo and Huang, Yufeng and Chen, Jiaoyan and Geng, Yuxia and Fang, Yin and Pan, Jeff Z. and Zhang, Ningyu and Zhang, Wen},
title = {LaKo: Knowledge-driven Visual Question Answering via Late Knowledge-to-Text Injection},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579053},
doi = {10.1145/3579051.3579053},
abstract = {Visual question answering (VQA) often requires an understanding of visual concepts and language semantics, which relies on external knowledge. Most existing methods exploit pre-trained language models or/and unstructured text, but the knowledge in these resources are often incomplete and noisy. Some other methods prefer to use knowledge graphs (KGs) which often have intensive structured knowledge, but the research is still quite preliminary. In this paper, we propose LaKo, a knowledge-driven VQA method via Late Knowledge-to-text Injection. To effectively incorporate an external KG, we transfer triples into textual format and propose a late injection mechanism for knowledge fusion. Finally we address VQA as a text generation task with an effective encoder-decoder paradigm, which achieves state-of-the-art results on OKVQA datasets.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {20–29},
numpages = {10},
keywords = {Knowledge Graph, Knowledge-to-Text, Late Knowledge Injection, Visual Question Answering},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@inproceedings{10.1145/3594778.3594877,
author = {Chai, Andrew and Vezvaei, Alireza and Golab, Lukasz and Kargar, Mehdi and Srivastava, Divesh and Szlichta, Jaroslaw and Zihayat, Morteza},
title = {EAGER: Explainable Question Answering Using Knowledge Graphs},
year = {2023},
isbn = {9798400702013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594778.3594877},
doi = {10.1145/3594778.3594877},
abstract = {We present EAGER: a tool for answering questions expressed in natural language. Core to EAGER is a modular pipeline for generating a knowledge graph from raw text without human intervention. Notably, EAGER uses the knowledge graph to answer questions and to explain the reasoning behind the derivation of answers. Our demonstration will showcase both the automated knowledge graph generation pipeline and the explainable question answering functionality. Lastly, we outline open problems and directions for future work.},
booktitle = {Proceedings of the 6th Joint Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
articleno = {4},
numpages = {5},
location = {Seattle, WA, USA},
series = {GRADES-NDA '23}
}

@inproceedings{10.1145/3544548.3581260,
author = {Zhang, Xiaoyu and Li, Jianping and Chi, Po-Wei and Chandrasegaran, Senthil and Ma, Kwan-Liu},
title = {ConceptEVA: Concept-Based Interactive Exploration and Customization of Document Summaries},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581260},
doi = {10.1145/3544548.3581260},
abstract = {With the most advanced natural language processing and artificial intelligence approaches, effective summarization of long and multi-topic documents—such as academic papers—for readers from different domains still remains a challenge. To address this, we introduce ConceptEVA, a mixed-initiative approach to generate, evaluate, and customize summaries for long and multi-topic documents. ConceptEVA incorporates a custom multi-task longformer encoder decoder to summarize longer documents. Interactive visualizations of document concepts as a network reflecting both semantic relatedness and co-occurrence help users focus on concepts of interest. The user can select these concepts and automatically update the summary to emphasize them. We present two iterations of ConceptEVA evaluated through an expert review and a within-subjects study. We find that participants’ satisfaction with customized summaries through ConceptEVA is higher than their own manually-generated summary, while incorporating critique into the summaries proved challenging. Based on our findings, we make recommendations for designing summarization systems incorporating mixed-initiative interactions.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {204},
numpages = {16},
keywords = {Document Summarization, Interactive Visual Analytics, Knowledge Graph, Mixed-Initiative Interfaces},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3583780.3615126,
author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Horrocks, Ian},
title = {Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615126},
doi = {10.1145/3583780.3615126},
abstract = {Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based methods.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5316–5320},
numpages = {5},
keywords = {SNOMED CT, biomedical ontologies, concept placement, entity linking, language models, ontology enrichment, text mining},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3539618.3591973,
author = {Feng, Jiazhan and Tao, Chongyang and Shen, Tao and Liu, Chang and Zhao, Dongyan},
title = {Dimension-Prompts Boost Commonsense Consolidation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591973},
doi = {10.1145/3539618.3591973},
abstract = {Neural knowledge models emerged and advanced common-sense-centric knowledge grounding. They parameterize a small seed curated commonsense knowledge graph (CS-KG) in a language model to generalize more. A current trend is to scale the seed up by directly mixing multiple sources of CS-KG (e.g., ATOMIC, ConceptNet) into one model. But, such brute-force mixing inevitably hinders effective knowledge consolidation due to i) ambiguous, polysemic, and/or inconsistent relations across sources and ii) knowledge learned in an entangled manner despite distinct types (e.g., causal, temporal). To mitigate this, we adopt a concept of commonsense knowledge dimension and propose a brand-new dimension-disentangled knowledge model (D2KM) learning paradigm with multiple sources. That is, a generative language model with dimension-specific soft prompts is trained to disentangle knowledge acquisitions along with different dimensions and facilitate potential intra-dimension consolidation across CS-KG sources. Experiments show our knowledge model outperforms its baselines in both standard and zero-shot scenarios.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1934–1938},
numpages = {5},
keywords = {commonsense knowledge construction, neural knowledge models},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3486001.3486240,
author = {Hagen, Morten and Arora, Piyush and Ghosh, Rahul and Thomas, Dawn and Joshi, Salil R},
title = {Class-Based Order-Independent Models of Natural Language for Bayesian Auto-Complete Inference},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486240},
doi = {10.1145/3486001.3486240},
abstract = {We introduce a model for auto-complete of general queries via Bayesian inference. To that end, we address three issues: First, the problem of predicting a word given previous words in a text. Usually, the context words are treated as a directional sequence. In our approach, we introduce a set-based class language model with order-independence, modeling the context words as a set of classes. Second, towards the task of predicting the next word’s class based on the classes of previous words plus an incomplete word prefix, we present a Bayesian framework that incorporates the set-based class language model in conjunction with an ontology. Third, regarding the auto-complete problem, we provide complete query suggestions via abstract class-space search which determines similar historical queries that contain the classes of previous words plus the next word’s predicted class. Subsequently, we apply the model to auto-complete inference in a system setting, in which users can access data via natural language queries.},
booktitle = {Proceedings of the First International Conference on AI-ML Systems},
articleno = {20},
numpages = {7},
keywords = {Bayesian inference, Class-based language model, auto-complete, order-independence},
location = {Bangalore, India},
series = {AIMLSystems '21}
}

@inproceedings{10.1145/3535508.3545550,
author = {Wei, Anqi and Wang, Liangjiang},
title = {Deep sequence representation learning for predicting human proteins with liquid-liquid phase separation propensity and synaptic functions},
year = {2022},
isbn = {9781450393867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535508.3545550},
doi = {10.1145/3535508.3545550},
abstract = {With advancements in next-generation sequencing techniques, the whole protein sequence repertoire has increased to a great extent. In the meantime, deep learning techniques have promoted the development of computational methods to interpret large-scale proteomic data and facilitate functional studies of proteins. Inferring properties from protein amino acid sequences has been a long-standing problem in Bioinformatics. Extensive studies have successfully applied natural language processing (NLP) techniques for the representation learning of protein sequences. In this paper, we applied the deep sequence model - UDSMProt, to fine-tune and evaluate two protein prediction tasks: (1) predict proteins with liquid-liquid phase separation propensity and (2) predict synaptic proteins. Our results have shown that, without prior domain knowledge and only based on protein sequences, the fine-tuned language models achieved high classification accuracies and outperformed baseline models using compositional k-mer features in both tasks. Hence, it is promising to apply the protein language model to some learning tasks and the fine-tuned models can be used to predict protein candidates for biological studies.},
booktitle = {Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {41},
numpages = {8},
keywords = {liquid-liquid phase separation, protein language model, synaptic proteins},
location = {Northbrook, Illinois},
series = {BCB '22}
}

@inproceedings{10.1145/3474085.3475648,
author = {Zhu, Yushan and Zhao, Huaixiao and Zhang, Wen and Ye, Ganqiang and Chen, Hui and Zhang, Ningyu and Chen, Huajun},
title = {Knowledge Perceived Multi-modal Pretraining in E-commerce},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475648},
doi = {10.1145/3474085.3475648},
abstract = {In this paper, we address multi-modal pretraining of product data in the field of E-commerce. Current multi-modal pretraining methods proposed for image and text modalities lack robustness in the face of modality-missing and modality-noise, which are two pervasive problems of multi-modal product data in real E-commerce scenarios. To this end, we propose a novel method, K3M, which introduces knowledge modality in multi-modal pretraining to correct the noise and supplement the missing of image and text modalities. The modal-encoding layer extracts the features of each modality. The modal-interaction layer is capable of effectively modeling the interaction of multiple modalities, where an initial-interactive feature fusion model is designed to maintain the independence of image modality and text modality, and a structure aggregation module is designed to fuse the information of image, text, and knowledge modalities. We pretrain K3M with three pretraining tasks, including masked object modeling (MOM), masked language modeling (MLM), and link prediction modeling (LPM). Experimental results on a real-world E-commerce dataset and a series of product-based downstream tasks demonstrate that K3M achieves significant improvements in performances than the baseline and state-of-the-art methods when modality-noise or modality-missing exists.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2744–2752},
numpages = {9},
keywords = {knowledge graph, modality missing, modality noise, multi-modal pretraining},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3587259.3627560,
author = {Schneider, Florian and Dash, Sarthak and Bagchi, Sugato and Mihindukulasooriya, Nandana and Gliozzo, Alfio Massimiliano},
title = {NLFOA: Natural Language Focused Ontology Alignment},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627560},
doi = {10.1145/3587259.3627560},
abstract = {For Ontology Alignment (OA), the task is to align semantically equivalent concepts and relations from different ontologies. This task plays a crucial role in many downstream tasks and applications in academia and industry. Since manually aligning ontologies is inefficient and costly, numerous approaches exist to do this automatically. However, most approaches are tailored to specific domains, are rule-based systems or based on feature engineering, and require external knowledge. The most recent advances in the field of OA rely on the widely proven effectiveness of pre-trained language models to represent the human-generated language that describes the entities in an ontology. However, these approaches additionally require sophisticated algorithms or Graph Neural Networks to exploit an ontology’s graphical structure to achieve state-of-the-art performance. In this work, we present NLFOA, or Natural Language Focused Ontology Alignment, which purely focuses on the natural language contained in ontologies to process the ontology’s semantics as well as graphical structure. An evaluation of our approach on common OA datasets shows superior results when finetuning with only a small number of training samples. Additionally, it demonstrates strong results in a zero-shot setting which could be employed in an active learning setup to reduce human labor when manually aligning ontologies significantly.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {114–121},
numpages = {8},
keywords = {Ontology Alignment Sentence Transformers Zero-Shot},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3512527.3531361,
author = {Wang, Xuan and Chen, Jiajun and Tang, Hao and Zhu, Zhigang},
title = {MultiCLU: Multi-stage Context Learning and Utilization for Storefront Accessibility Detection and Evaluation},
year = {2022},
isbn = {9781450392389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512527.3531361},
doi = {10.1145/3512527.3531361},
abstract = {In this work, a storefront accessibility image dataset is collected from Google street view and is labeled with three main objects for storefront accessibility: doors (for store entrances), doorknobs (for accessing the entrances) and stairs (for leading to the entrances). Then MultiCLU, a new multi-stage context learning and utilization approach, is proposed with the following four stages: Context in Labeling (CIL), Context in Training (CIT), Context in Detection (CID) and Context in Evaluation (CIE). The CIL stage automatically extends the label for each knob to include more local contextual information. In the CIT stage, a deep learning method is used to project the visual information extracted by a Faster R-CNN based object detector to semantic space generated by a Graph Convolutional Network. The CID stage uses the spatial relation reasoning between categories to refine the confidence score. Finally in the CIE stage, a new loose evaluation metric for storefront accessibility, especially for knob category, is proposed to efficiently help BLV users to find estimated knob locations. Our experiment results show that the proposed MultiCLU framework can achieve significantly better performance than the baseline detector using Faster R-CNN, with +13.4% on mAP and +15.8% on recall, respectively. Our new evaluation metric also introduces a new way to evaluate storefront accessibility objects, which could benefit BLV group in real life.},
booktitle = {Proceedings of the 2022 International Conference on Multimedia Retrieval},
pages = {304–312},
numpages = {9},
keywords = {context learning, convolutional neural networks, graph convolutional network, knowledge graph, object detection},
location = {Newark, NJ, USA},
series = {ICMR '22}
}

@article{10.1145/3418598,
author = {Li, Yamin and Zhang, Jun and Yang, Zhongliang and Zhang, Ru},
title = {Topic-aware Neural Linguistic Steganography Based on Knowledge Graphs},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {2691-1922},
url = {https://doi.org/10.1145/3418598},
doi = {10.1145/3418598},
abstract = {The core challenge of steganography is always how to improve the hidden capacity and the concealment. Most current generation-based linguistic steganography methods only consider the probability distribution between text characters, and the emotion and topic of the generated steganographic text are uncontrollable. Especially for long texts, generating several sentences related to a topic and displaying overall coherence and discourse-relatedness can ensure better concealment. In this article, we address the problem of generating coherent multi-sentence texts for better concealment, and a topic-aware neural linguistic steganography method that can generate a steganographic paragraph with a specific topic is present. We achieve a topic-controllable steganographic long text generation by encoding the related entities and their relationships from Knowledge Graphs. Experimental results illustrate that the proposed method can guarantee both the quality of the generated steganographic text and its relevance to a specific topic. The proposed model can be widely used in covert communication, privacy protection, and many other areas of information security.},
journal = {ACM/IMS Trans. Data Sci.},
month = apr,
articleno = {10},
numpages = {13},
keywords = {Neural networks, linguistic steganography, knowledge graph, topic aware, text generation}
}

@inproceedings{10.1145/3472634.3472667,
author = {Xu, Hanchen and Chen, Zhenxiang and Wang, Shanshan and Jiang, Xiaoqing},
title = {Chinese NER Using ALBERT and Multi-word Information},
year = {2021},
isbn = {9781450385671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472634.3472667},
doi = {10.1145/3472634.3472667},
abstract = {Recently, many Chinese Named Entity Recognition (NER) problems which utilize the character-based approach have a good performance, the character-based approach becomes the dominant current of the approaches, but there are two issues in the existing models. Firstly, the traditional character vector representation is too single to express the polysemia of characters. Secondly, words contain more information than characters, but the existing models cannot use the word information sufficiently. To address the above issues, the AM-BiLSTM model is provided in this work. With the introduction of ALBERT pre-training language model and Multi-word Information(MWI), the enhanced character embedding is composed. We perform experiments on two datasets and the results demonstrate that the capability of our method is improved outstandingly in comparison with the existing NER models.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China},
pages = {141–145},
numpages = {5},
keywords = {ALBERT, BiLSTM, Named Entity Recognition, pre-training language model},
location = {Hefei, China},
series = {ACM TURC '21}
}

@inproceedings{10.1145/3404835.3463100,
author = {Huang, Zhiqi and Rahimi, Razieh and Yu, Puxuan and Shang, Jingbo and Allan, James},
title = {AutoName: A Corpus-Based Set Naming Framework},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463100},
doi = {10.1145/3404835.3463100},
abstract = {We propose AutoName, an unsupervised framework that extracts a name for a set of query entities from a large-scale text corpus. Entity-set naming is useful in many tasks related to natural language processing and information retrieval such as session-based and conversational information seeking. Previous studies mainly extract set names from knowledge bases which provide highly reliable entity relations, but suffer from limited coverage of entities and set names that represent broad semantic classes. To address these problems, AutoName generates hypernym-anchored candidate phrases via probing a pre-trained language model and the entities' context in documents. Phrases are then clustered to identify ones that describe common concepts among query entities. Finally, AutoName ranks refined phrases based on the co-occurrences of their words with query entities and the conceptual integrity of their respective clusters. We built a new benchmark dataset for this task, consisting of 130 entity sets with name labels. Experimental results show that AutoName generates coherent and meaningful set names and significantly outperforms all baselines.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2101–2105},
numpages = {5},
keywords = {conceptual clustering, entity set naming, language model probing},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3474085.3479220,
author = {Anand, Vishal and Ramesh, Raksha and Jin, Boshen and Wang, Ziyin and Lei, Xiaoxiao and Lin, Ching-Yung},
title = {MultiModal Language Modelling on Knowledge Graphs for Deep Video Understanding},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3479220},
doi = {10.1145/3474085.3479220},
abstract = {The natural language processing community has had a major interest in auto-regressive [4, 13] and span-prediction based language models [7] recently, while knowledge graphs are often referenced for common-sense based reasoning and fact-checking models. In this paper, we present an equivalence representation of span-prediction based language models and knowledge-graphs to better leverage recent developments of language modelling for multi-modal problem statements. Our method performed well, especially with sentiment understanding for multi-modal inputs, and discovered potential bias in naturally occurring videos when compared with movie-data interaction-understanding. We also release a dataset of an auto-generated questionnaire with ground-truths consisting of labels spanning across 120 relationships, 99 sentiments, and 116 interactions, among other labels for finer-grained analysis of model comparisons in the community.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4868–4872},
numpages = {5},
keywords = {intent detection, knowledge graphs, language model, scene description, slot filling, speaker diarization, transformers},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3442381.3449943,
author = {Li, Xiangsheng and Mao, Jiaxin and Ma, Weizhi and Liu, Yiqun and Zhang, Min and Ma, Shaoping and Wang, Zhaowei and He, Xiuqiang},
title = {Topic-enhanced knowledge-aware retrieval model for diverse relevance estimation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449943},
doi = {10.1145/3442381.3449943},
abstract = {Relevance measures the relation between query and document which contains several different dimensions, e.g., semantic similarity, topical relatedness, cognitive relevance (the relations in the aspect of knowledge), usefulness, timeliness, utility and so on. However, existing retrieval models mainly focus on semantic similarity and cognitive relevance while ignore other possible dimensions to model relevance. Topical relatedness, as an important dimension to measure relevance, is not well studied in existing neural information retrieval. In this paper, we propose a Topic Enhanced Knowledge-aware retrieval Model (TEKM) that jointly learns semantic similarity, knowledge relevance and topical relatedness to estimate relevance between query and document. We first construct a neural topic model to learn topical information and generate topic embeddings of a query. Then we combine the topic embeddings with a knowledge-aware retrieval model to estimate different dimensions of relevance. Specifically, we exploit kernel pooling to soft match topic embeddings with word and entity in a unified embedding space to generate fine-grained topical relatedness. The whole model is trained in an end-to-end manner. Experiments on a large-scale publicly available benchmark dataset show that TEKM outperforms existing retrieval models. Further analysis also shows how topic relatedness is modeled to improve traditional retrieval model with semantic similarity and knowledge relevance.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {756–767},
numpages = {12},
keywords = {Kernel pooling, Knowledge graph, Neural IR, Neural topic model},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3442442.3452326,
author = {Lees, Alyssa and Barbosa, Luciano and Korn, Flip and Souza Silva, Levy de and Wu, You and Yu, Cong},
title = {Collocating News Articles with Structured Web Tables✱},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452326},
doi = {10.1145/3442442.3452326},
abstract = {In today’s news deluge, it can often be overwhelming to understand the significance of a news article or verify the facts within. One approach to address this challenge is to identify relevant data so that crucial statistics or facts can be highlighted for the user to easily digest, and thus improve the user’s comprehension of the news story in a larger context. In this paper, we look toward structured tables on the Web, especially the high quality data tables from Wikipedia, to assist in news understanding. Specifically, we aim to automatically find tables related to a news article. For that, we leverage the content and entities extracted from news articles and their matching tables to fine-tune a Bidirectional Transformers (BERT) model. The resulting model is, therefore, an encoder tailored for article-to-table match. To find the matching tables for a given news article, the fine-tuned BERT model encodes each table in the corpus and the news article into their respective embedding vectors. The tables with the highest cosine similarities to the news article in this new representation space are considered the possible matches. Comprehensive experimental analyses show that the new approach significantly outperforms the baselines over a large, weakly-labeled, dataset obtained from Web click logs as well as a small, crowdsourced, evaluation set. Specifically, our approach achieves near 90% accuracy@5 as opposed to baselines varying between 30% and 64%.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {393–401},
numpages = {9},
keywords = {Bidirectional Encoders with Transformers, Encoders, Knowledge Graph, Structured Data, WebTables},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3485447.3511928,
author = {Zhou, Yucheng and Geng, Xiubo and Shen, Tao and Long, Guodong and Jiang, Daxin},
title = {EventBERT: A Pre-Trained Model for Event Correlation Reasoning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511928},
doi = {10.1145/3485447.3511928},
abstract = {Event correlation reasoning infers whether a natural language paragraph containing multiple events conforms to human common sense. For example, “Andrew was very drowsy, so he took a long nap, and now he is very alert” is sound and reasonable. In contrast, “Andrew was very drowsy, so he stayed up a long time, now he is very alert” does not comply with human common sense. Such reasoning capability is essential for many downstream tasks, such as script reasoning, abductive reasoning, narrative incoherence, story cloze test, etc. However, conducting event correlation reasoning is challenging due to a lack of large amounts of diverse event-based knowledge and difficulty in capturing correlation among multiple events. In this paper, we propose EventBERT, a pre-trained model to encapsulate eventuality knowledge from unlabeled text. Specifically, we collect a large volume of training examples by identifying natural language paragraphs that describe multiple correlated events and further extracting event spans in an unsupervised manner. We then propose three novel event- and correlation-based learning objectives to pre-train an event correlation model on our created training corpus. Experimental results show EventBERT outperforms strong baselines on four downstream tasks, and achieves state-of-the-art results on most of them. Moreover, it outperforms existing pre-trained models by a large margin, e.g., 6.5 ∼ 23%, in zero-shot learning of these tasks.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {850–859},
numpages = {10},
keywords = {Contrastive learning, Event correlation reasoning, Language model, Pre-Training model, Zero-shot learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3584371.3612942,
author = {Kabir, Anowarul and Moldwin, Asher and Shehu, Amarda},
title = {A Comparative Analysis of Transformer-based Protein Language Models for Remote Homology Prediction},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612942},
doi = {10.1145/3584371.3612942},
abstract = {Protein language models based on the transformer architecture are increasingly shown to learn rich representations from protein sequences that improve performance on a variety of downstream protein prediction tasks. These tasks encompass a wide range of predictions, including prediction of secondary structure, subcellular localization, evolutionary relationships within protein families, as well as superfamily and family membership. There is recent evidence that such models also implicitly learn structural information. In this paper we put this to the test on a hallmark problem in computational biology, remote homology prediction. We employ a rigorous setting, where, by lowering sequence identity, we clarify whether the problem of remote homology prediction has been solved. Among various interesting findings, we report that current state-of-the-art, large models are still underperforming in the "twilight zone" of very low sequence identity.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {97},
numpages = {9},
keywords = {remote homology, transformer, large language model},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3485447.3511941,
author = {Islam, Sk Mainul and Bhattacharya, Sourangshu},
title = {AR-BERT: Aspect-relation enhanced Aspect-level Sentiment Classification with Multi-modal Explanations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511941},
doi = {10.1145/3485447.3511941},
abstract = {Aspect level sentiment classification (ALSC) is a difficult problem with state-of-the-art models showing less than 80% macro-F1 score on benchmark datasets. Existing models do not incorporate information on aspect-aspect relations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem from inaccurate disambiguation of aspects to KG entities, and the inability to learn aspect representations from the large KGs in joint training with ALSC models. We propose AR-BERT, a novel two-level global-local entity embedding scheme that allows efficient joint training of KG-based aspect embeddings and ALSC models. A novel incorrect disambiguation detection technique addresses the problem of inaccuracy in aspect disambiguation. We also introduce the problem of determining mode significance in multi-modal explanation generation, and propose a two step solution. The proposed methods show a consistent improvement of 2.5 − 4.1 percentage points, over the recent BERT-based baselines on benchmark datasets.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {987–998},
numpages = {12},
keywords = {Explainable Deep Learning, Knowledge Graph Embedding, Sentiment Analysis},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3570991.3571058,
author = {Gupta, Akshay and Kumar, Suresh and Kumar P, Sreenivasa},
title = {Solving age-word problems using domain ontology and BERT},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571058},
doi = {10.1145/3570991.3571058},
abstract = {An age word problem (ageWP) typically involves sentences that express relationships between the age of the agents and asks for the age of one of them. Automatically solving ageWPs is a challenging task as we need to tackle temporal relationships between the agent’s ages, frame and solve the equations for the required unknowns. To the best of our knowledge, there exists only one ageWP dataset consisting of just 124 examples. The dataset is too small to employ a learning-based solver, mainly consisting of ageWPs with simple temporal relationships. To address this issue, in our earlier work, we designed a description-logic based ontology (ageWP-ont) for the domain of age word problems and utilized it to automatically generate a large number of ageWPs. Sentences in these ageWPs relate the ages of agents in a temporally complex manner. In this paper, we focus on solving these problems. We analyzed an existing learning-based solver of algebraic word-problems that uses a traditional machine learning approach and found that the solver can be adapted to our domain. But we found that this approach does not seem to perform well, perhaps due to the complex nature of the ageWPs. As we have the ontology of the domain on hand, we propose a new approach of utilizing it in the deep-learning based NLU component of the solution. We annotate parts of the ageWP sentences with class-names from ageWP-ont and train a BERT-based language model (LM) that learns to predict the instances for these classes in the given sentences. An RDF graph is populated with these values and serves as a concrete problem-specific instance of the ontology. The dataset for training the LM is automatically generated with the help of ageWP-ont. Finally, for the actual solving of a given ageWP, we make use of its RDF graph and employ Semantic Web Rule Language (SWRL) rules. We implemented the proposed system and achieved 68.8% accuracy. The work demonstrates that combining deep learning with ontologies can give impressive results.},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {95–103},
numpages = {9},
keywords = {Age-word problem solver, BERT, OWL-DL ontology, SWRL},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@inproceedings{10.1145/3543507.3583300,
author = {Ko, Yunyong and Ryu, Seongeun and Han, Soeun and Jeon, Youngseung and Kim, Jaehoon and Park, Sohyun and Han, Kyungsik and Tong, Hanghang and Kim, Sang-Wook},
title = {KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583300},
doi = {10.1145/3543507.3583300},
abstract = {The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect – people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierarchical attention networks (HAN) to learn the relationships among words and sentences in three different levels and (2) knowledge encoding (KE) to incorporate external knowledge for real-world entities into the process of political stance prediction. Also, to take into account the subtle and important difference between opposite political stances, we build two independent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by ourselves and learn to fuse the different political knowledge. Through extensive evaluations on three real-world datasets, we demonstrate the superiority of KHAN in terms of (1) accuracy, (2) efficiency, and (3) effectiveness.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1572–1583},
numpages = {12},
keywords = {echo chamber effect, hierarchical attention networks, knowledge graph embedding, political stance prediction},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3604915.3608812,
author = {Zhao, Zhipeng and Zhou, Kun and Wang, Xiaolei and Zhao, Wayne Xin and Pan, Fan and Cao, Zhao and Wen, Ji-Rong},
title = {Alleviating the Long-Tail Problem in Conversational Recommender Systems},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608812},
doi = {10.1145/3604915.3608812},
abstract = {Conversational recommender systems (CRS) aim to provide the recommendation service via natural language conversations. To develop an effective CRS, high-quality CRS datasets are very crucial. However, existing CRS datasets suffer from the long-tail issue, i.e., a large proportion of items are rarely (or even never) mentioned in the conversations, which are called long-tail items. As a result, the CRSs trained on these datasets tend to recommend frequent items, and the diversity of the recommended items would be largely reduced, making users easier to get bored. To address this issue, this paper presents LOT-CRS, a novel framework that focuses on simulating and utilizing a balanced CRS dataset (i.e., covering all the items evenly) for improving LOng-Tail recommendation performance of CRSs. In our approach, we design two pre-training tasks to enhance the understanding of simulated conversation for long-tail items, and adopt retrieval-augmented fine-tuning with label smoothness strategy to further improve the recommendation of long-tail items. Extensive experiments on two public CRS datasets have demonstrated the effectiveness and extensibility of our approach, especially on long-tail recommendation. Our code is publicly available at the link: https://github.com/Oran-Ac/LOT-CRS.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {374–385},
numpages = {12},
keywords = {Conversational Recommender System, Long-tail Problem, Pre-trained Language Model},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{10.1109/TASLP.2023.3316422,
author = {Gao, Nan and Wang, Yongjian and Chen, Peng and Tang, Jijun},
title = {Boosting Short Text Classification by Solving the OOV Problem},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3316422},
doi = {10.1109/TASLP.2023.3316422},
abstract = {In the field of natural language processing, text classification has received a lot of attention. Compared with long texts, short texts have fewer words and lack contextual semantic information. Existing approaches enrich short text information by linking the external knowledge graph, but they ignore the out-of-vocabulary (OOV) problem during entity linking, especially when dealing with domain-oriented data, which has some rare words or domain-specific nouns. In this article, to alleviate the OOV problem caused by linking the external knowledge graph(KG), we propose a domain knowledge graph and entity complementation strategy to improve the performance of short text classification. Specifically, the external knowledge graph is used to enrich the information of short texts. The self-build domain knowledge graph is used to solve the problem of entities failing to link to the external knowledge graph. Finally, we conduct experiments on various datasets: 1. a labeled Chinese electronic domain dataset; 2. an open-source dataset to test the performance of our algorithm in different data distribution scenarios. The results demonstrate our dual knowledge graph model outperforms the state-of-the-art short text classification methods, especially when the OOV problem is severe.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4014–4024},
numpages = {11}
}

@article{10.1145/3580488,
author = {Li, Lei and Zhang, Yongfeng and Chen, Li},
title = {Personalized Prompt Learning for Explainable Recommendation},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3580488},
doi = {10.1145/3580488},
abstract = {Providing user-understandable explanations to justify recommendations could help users better understand the recommended items, increase the system’s ease of use, and gain users’ trust. A typical approach to realize it is natural language generation. However, previous works mostly adopt recurrent neural networks to meet the ends, leaving the potentially more effective pre-trained Transformer models under-explored. In fact, user and item IDs, as important identifiers in recommender systems, are inherently in different semantic space as words that pre-trained models were already trained on. Thus, how to effectively fuse IDs into such models becomes a critical issue. Inspired by recent advancement in prompt learning, we come up with two solutions: find alternative words to represent IDs (called discrete prompt learning) and directly input ID vectors to a pre-trained model (termed continuous prompt learning). In the latter case, ID vectors are randomly initialized but the model is trained in advance on large corpora, so they are actually in different learning stages. To bridge the gap, we further propose two training strategies: sequential tuning and recommendation as regularization. Extensive experiments show that our continuous prompt learning approach equipped with the training strategies consistently outperforms strong baselines on three datasets of explainable recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {103},
numpages = {26},
keywords = {Explainable recommendation, Transformer, pre-trained language model, prompt learning}
}

@inproceedings{10.1145/3600100.3623720,
author = {He, Fang and Wang, Dan and Sun, Yaojie},
title = {Ontology Integration for Building Systems and Energy Storage Systems},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3623720},
doi = {10.1145/3600100.3623720},
abstract = {A building ontology defines the concepts and organization of building data. Such knowledge can be assistance with automatic data access and support data-driven applications in buildings. With technological advances in batteries and energy storage, an increasing number of data-driven building applications now involve both building systems and energy storage systems (ESS), e.g., peak load shaving (PLS). However, existing building ontologies, e.g., Brick, are not designed to include concepts from ESS systems. Given the emergence of building-ESS applications, it has become important to develop ontologies that can cover knowledge about both building and ESS systems. Building systems and ESS systems fall under different industry sectors and there are building ontologies and ESS ontologies that have been developed independently. To maximally reuse existing knowledge, we leverage ontology integration technologies. We present a building-energy storage ontology integration (BESOI) system that can extend a building ontology with appropriate ESS ontologies. Our system handles ambiguity, incoherence, and redundancy problems in ontology integration. We evaluate BESOI on four building-ESS applications by extending Brick, a notable building ontology, with different ESS ontologies. The results show that BESOI can extend the coverage of Brick from 68.09% to 95.74% on the concepts of applications.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {212–215},
numpages = {4},
keywords = {Building Application, Energy Storage System, Metadata, Ontology Integration},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@inproceedings{10.1145/3534678.3539258,
author = {Liu, Fenglin and Yang, Bang and You, Chenyu and Wu, Xian and Ge, Shen and Woicik, Adelaide and Wang, Sheng},
title = {Graph-in-Graph Network for Automatic Gene Ontology Description Generation},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539258},
doi = {10.1145/3534678.3539258},
abstract = {Gene Ontology (GO) is the primary gene function knowledge base that enables computational tasks in biomedicine. The basic element of GO is a term, which includes a set of genes with the same function. Existing research efforts of GO mainly focus on predicting gene term associations. Other tasks, such as generating descriptions of new terms, are rarely pursued. In this paper, we propose a novel task: GO term description generation. This task aims to automatically generate a sentence that describes the function of a GO term belonging to one of the three categories, i.e., molecular function, biological process, and cellular component. To address this task, we propose a Graph-in-Graph network that can efficiently leverage the structural information of GO. The proposed network introduces a two-layer graph: the first layer is a graph of GO terms where each node is also a graph (gene graph). Such a Graph-in-Graph network can derive the biological functions of GO terms and generate proper descriptions. To validate the effectiveness of the proposed network, we build three large-scale benchmark datasets. By incorporating the proposed Graph-in-Graph network, the performances of seven different sequence-to-sequence models can be substantially boosted across all evaluation metrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU, ROUGE-L, and METEOR, respectively.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1060–1068},
numpages = {9},
keywords = {bioinformatics, gene ontology, graph representations, natural language generation, sequence-to-sequence learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3487553.3524250,
author = {GUO, Kunpeng and Defretiere, Clement and Diefenbach, Dennis and Gravier, Christophe and Gourru, Antoine},
title = {QAnswer: Towards Question Answering Search over Websites},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524250},
doi = {10.1145/3487553.3524250},
abstract = {Question Answering (QA) is increasingly used by search engines to provide results to their end-users, yet very few websites currently use QA technologies for their search functionality. To illustrate the potential of QA technologies for the website search practitioner, we demonstrate web searches that combine QA over knowledge graphs and QA over free text – each being usually tackled separately. We also discuss the different benefits and drawbacks of both approaches for web site searches. We use the case studies made of websites hosted by the Wikimedia Foundation (namely Wikipedia and Wikidata). Differently from a search engine (e.g. Google, Bing, etc), the data are indexed integrally, i.e. we do not index only a subset, and they are indexed exclusively, i.e. we index only data available on the corresponding website.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {252–255},
numpages = {4},
keywords = {Knowledge Graph Question Answering, Open Domain Question Answering, Question Answering, Website Search},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3511808.3557319,
author = {Lyu, Yuanjie and Zhu, Chen and Xu, Tong and Yin, Zikai and Chen, Enhong},
title = {Faithful Abstractive Summarization via Fact-aware Consistency-constrained Transformer},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557319},
doi = {10.1145/3511808.3557319},
abstract = {Abstractive summarization is a classic task in Natural Language Generation (NLG), which aims to produce a concise summary of the original document. Recently, great efforts have been made on sequence-to-sequence neural networks to generate abstractive sum- maries with a high level of fluency. However, prior arts mainly focus on the optimization of token-level likelihood, while the rich semantic information in documents has been largely ignored. In this way, the summarization results could be vulnerable to hallucinations, i.e., the semantic-level inconsistency between a summary and corresponding original document. To deal with this challenge, in this paper, we propose a novel fact-aware abstractive summarization model, named Entity-Relation Pointer Generator Network (ERPGN). Specially, we attempt to formalize the facts in original document as a factual knowledge graph, and then generate the high-quality summary via directly modeling consistency between summary and the factual knowledge graph. To that end, we first leverage two pointer net- work structures to capture the fact in original documents. Then, to enhance the traditional token-level likelihood loss, we design two extra semantic-level losses to measure the disagreement between a summary and facts from its original document. Extensive experi- ments on public datasets demonstrate that our ERPGN framework could outperform both classic abstractive summarization models and the state-of-the-art fact-aware baseline methods, with significant improvement in terms of faithfulness.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {1410–1419},
numpages = {10},
keywords = {abstractive summarization, factual consistency, transformer},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3543507.3583314,
author = {Luo, Yun and Liu, Zihan and Li, Stan Z. and Zhang, Yue},
title = {Improving (Dis)agreement Detection with Inductive Social Relation Information From Comment-Reply Interactions},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583314},
doi = {10.1145/3543507.3583314},
abstract = {(Dis)agreement detection aims to identify the authors’ attitudes or positions (agree, disagree, neutral) towards a specific text. It is limited for existing methods merely using textual information for identifying (dis)agreements, especially for cross-domain settings. Social relation information can play an assistant role in the (dis)agreement task besides textual information. We propose a novel method to extract such relation information from (dis)agreement data into an inductive social relation graph, merely using the comment-reply pairs without any additional platform-specific information. The inductive social relation globally considers the historical discussion and the relation between authors. Textual information based on a pre-trained language model and social relation information encoded by pre-trained RGCN are jointly considered for (dis)agreement detection. Experimental results show that our model achieves state-of-the-art performance for both the in-domain and cross-domain tasks on the benchmark – DEBAGREEMENT. We find social relations can boost the performance of the (dis)agreement detection model, especially for the long-token comment-reply pairs, demonstrating the effectiveness of the social relation graph. We also explore the effect of the knowledge graph embedding methods, the information fusing method, and the time interval in constructing the social relation graph, which shows the effectiveness of our model.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1584–1593},
numpages = {10},
keywords = {Disagreement Detection, Opinion Mining, Social Relation, Stance Detection},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.1145/3624734,
author = {Sun, Jiamou and Xing, Zhenchang and Xia, Xin and Lu, Qinghua and Xu, Xiwei and Zhu, Liming},
title = {Aspect-level Information Discrepancies across Heterogeneous Vulnerability Reports: Severity, Types and Detection Methods},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624734},
doi = {10.1145/3624734},
abstract = {Vulnerable third-party libraries pose significant threats to software applications that reuse these libraries. At an industry scale of reuse, manual analysis of third-party library vulnerabilities can be easily overwhelmed by the sheer number of vulnerabilities continually collected from diverse sources for thousands of reused libraries. Our study of four large-scale, actively maintained vulnerability databases (NVD, IBM X-Force, ExploitDB, and Openwall) reveals the wide presence of information discrepancies, in terms of seven vulnerability aspects, i.e., product, version, component, vulnerability type, root cause, attack vector, and impact, between the reports for the same vulnerability from heterogeneous sources. It would be beneficial to integrate and cross-validate multi-source vulnerability information, but it demands automatic aspect extraction and aspect discrepancy detection. In this work, we experimented with a wide range of NLP methods to extract named entities (e.g., product) and free-form phrases (e.g., root cause) from textual vulnerability reports and to detect semantically different aspect mentions between the reports. Our experiments confirm the feasibility of applying NLP methods to automate aspect-level vulnerability analysis and identify the need for domain customization of general NLP methods. Based on our findings, we propose a discrepancy-aware, aspect-level vulnerability knowledge graph and a KG-based web portal that integrates diversified vulnerability key aspect information from heterogeneous vulnerability databases. Our conducted user study proves the usefulness of our web portal. Our study opens the door to new types of vulnerability integration and management, such as vulnerability portraits of a product and explainable prediction of silent vulnerabilities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {49},
numpages = {38},
keywords = {Vulnerability key aspect, information discrepancy, hetergeneous vulnerability reports}
}

@inproceedings{10.1145/3543873.3587361,
author = {Choudhury, Sutanay and Agarwal, Khushbu and Ham, Colby and Tamang, Suzanne},
title = {MediSage: An AI Assistant for Healthcare via Composition of Neural-Symbolic Reasoning Operators},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587361},
doi = {10.1145/3543873.3587361},
abstract = {We introduce MediSage, an AI decision support assistant for medical professionals and caregivers that simplifies the way in which they interact with different modalities of electronic health records (EHRs) through a conversational interface. It provides step-by-step reasoning support to an end-user to summarize patient health, predict patient outcomes and provide comprehensive and personalized healthcare recommendations. MediSage provides these reasoning capabilities by using a knowledge graph that combines general purpose clinical knowledge resources with recent-most information from the EHR data. By combining the structured representation of knowledge with the predictive power of neural models trained over both EHR and knowledge graph data, MediSage brings explainability by construction and represents a stepping stone into the future through further integration with biomedical language models.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {258–261},
numpages = {4},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3578741.3578781,
author = {Song, Yu and Zhang, Wenxuan and Ye, Yajuan and Zhang, Chenghao and Zhang, Kunli},
title = {Knowledge-Enhanced Relation Extraction in Chinese EMRs},
year = {2023},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578741.3578781},
doi = {10.1145/3578741.3578781},
abstract = {Electronic Medical Records (EMRs) is one of the important data sources of clinical information. Relation extraction is a key step to extract rich medical knowledge from EMRs, which has been studied by many scholars. However, there are some problems in EMRs corpus, such as entity nesting and relation overlapping, which make it difficult to achieve ideal results in EMRs relation extraction task. Previous studies rarely considered the fusion of knowledge graph containing rich and valuable structured knowledge, which leads to semantic ambiguity and other issues. Aiming at the above problems, Relation Extraction model based on Knowledge Graph and Chinese character Radical information(RE-KGR) model is proposed in this paper to study the relation extraction of EMRs in diabetic patients. Firstly, knowledge information is extracted from knowledge graph and embedded by GCN. At the same time, the corresponding radical features of Chinese characters are fused to enhance the semantic information of the input text. Compared with other baseline models, the DEMRC and DiaKG experiments of EMRs datasets of diabetic patients were improved by 1.32% and 2.19%.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
pages = {196–201},
numpages = {6},
keywords = {Electronic medical records, Integrating knowledge, Relation extraction},
location = {Sanya, China},
series = {MLNLP '22}
}

@inproceedings{10.1145/3584376.3584482,
author = {Zhao, Jingsheng and Cui, Mingyu and Gao, Xiang and Yan, Shuai and Ni, Qihui},
title = {Chinese Named Entity Recognition Based on BERT and Lexicon Enhancement},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584482},
doi = {10.1145/3584376.3584482},
abstract = {Named entity recognition is an important part of information extraction and knowledge graph construction, and is the basic work of natural language processing. Chinese named entity recognition mainly adopts word-based and character-based methods, word-based methods rely on word segmentation and common word segmentation methods have word segmentation errors, which easily cause error propagation, character-based methods avoid this error but do not make full use of lexicon information. The performance of Chinese named entity recognition can be effectively improved by introducing lexicon information into character-based named entity recognition. In this paper, we propose a BERT-IDCNN-CRF model combined with the SoftLexicon method. First, the BERT pre-training language model is used to train the character embedding vector, and the lexicon information is obtained by the SoftLexicon method. Then, the lexicon information is combined with the character vector representation obtained by training. Next, the fused vector representation is input to the IDCNN model for further training. Finally, the recognition results of Chinese named entities are obtained by the CRF model. The experimental results show that the F1 value can reach 95.95%, 70.63% and 95.28% on Resume, Weibo and MSRA datasets, and the training speed is faster than BERT-BiLSTM-CRF.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {597–604},
numpages = {8},
location = {Dongguan, China},
series = {RICAI '22}
}

@article{10.1145/3582262,
author = {Touma, Roudy and Hajj, Hazem and El-Hajj, Wassim and Shaban, Khaled},
title = {Automated Generation of Human-readable Natural Arabic Text from RDF Data},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3582262},
doi = {10.1145/3582262},
abstract = {With the advances in Natural Language Processing (NLP), the industry has been moving towards human-directed artificial intelligence (AI) solutions. Recently, chatbots and automated news generation have captured a lot of attention. The goal is to automatically generate readable text from tabular data or web data commonly represented in Resource Description Framework (RDF) format. The problem can then be formulated as Data-to-text (D2T) generation from structured non-linguistic data into human-readable natural language. Despite the significant work done for the English language, no efforts are being directed towards low-resource languages like the Arabic language. This work promotes the development of the first RDF data-to-text (D2T) generation system for the Arabic language while trying to address the low-resource limitation. We develop several models for the Arabic D2T task using transfer learning from large language models (LLM) such as AraBERT, AraGPT2, and mT5. These models include a baseline Bi-LSTM Sequence-to-Sequence (Seq2Seq) model, as well as encoder-decoder transformers like BERT2BERT, BERT2GPT, and T5. We then provide a detailed comparative study highlighting the strengths and limitations of these methods setting the stage for further advancement in the field. We also introduce a new Arabic dataset (AraWebNLG) that can be used for new model development in the field. To ensure a comprehensive evaluation, general-purpose automated metrics (BLEU and Perplexity scores) are used as well as task-specific human evaluation metrics related to the accuracy of the content selection and fluency of the generated text. The results highlight the importance of pre-training on a large corpus of Arabic data and show that transfer learning from AraBERT gives the best performance. Text-to-text pre-training using mT5 achieves second best performance results even with multilingual weights.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {98},
numpages = {13},
keywords = {Low-resource languages, data-to-text, RDF, language models, neural networks, datasets}
}

@inproceedings{10.1145/3477495.3531997,
author = {Dong, Qian and Liu, Yiding and Cheng, Suqi and Wang, Shuaiqiang and Cheng, Zhicong and Niu, Shuzi and Yin, Dawei},
title = {Incorporating Explicit Knowledge in Pre-trained Language Models for Passage Re-ranking},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531997},
doi = {10.1145/3477495.3531997},
abstract = {Passage re-ranking is to obtain a permutation over the candidate passage set from retrieval stage. Re-rankers have been boomed by Pre-trained Language Models (PLMs) due to their overwhelming advantages in natural language understanding. However, existing PLM based re-rankers may easily suffer from vocabulary mismatch and lack of domain specific knowledge. To alleviate these problems, explicit knowledge contained in knowledge graph is carefully introduced in our work. Specifically, we employ the existing knowledge graph which is incomplete and noisy, and first apply it in passage re-ranking task. To leverage a reliable knowledge, we propose a novel knowledge graph distillation method and obtain a knowledge meta graph as the bridge between query and passage. To align both kinds of embedding in the latent space, we employ PLM as text encoder and graph neural network over knowledge meta graph as knowledge encoder. Besides, a novel knowledge injector is designed for the dynamic interaction between text and knowledge encoder. Experimental results demonstrate the effectiveness of our method especially in queries requiring in-depth domain knowledge.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1490–1501},
numpages = {12},
keywords = {language models, learning to rank, semantic matching},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3511808.3557219,
author = {Tong, Hanwen and Xie, Chenhao and Liang, Jiaqing and He, Qianyu and Yue, Zhiang and Liu, Jingping and Xiao, Yanghua and Wang, Wenguang},
title = {A Context-Enhanced Generate-then-Evaluate Framework for Chinese Abbreviation Prediction},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557219},
doi = {10.1145/3511808.3557219},
abstract = {As a popular form of lexicalization, abbreviation is widely used in both oral and written language and plays an important role in various Natural Language Processing applications. However, current approaches cannot ensure that the predicted abbreviation preserves the meaning of its full form and maintains fluency. In this paper, we introduce a fresh perspective to evaluate the quality of abbreviations within their textual contexts with pre-trained language model. To this end, we propose a novel two-stage generate-then-evaluate framework enhanced by context, which consists of a generation model to generate multiple candidate abbreviations and an evaluation model to evaluate their quality within their contexts. Experimental results show that our framework consistently outperforms all the existing approaches, achieving 53.2% Hit@1 performance with a 5.6 points improvement compared to its previous best result. Our code and data are publicly available at https://github.com/HavenTong/CEGE.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {1945–1954},
numpages = {10},
keywords = {chinese abbreviation prediction, context-enhanced framework, generate-then-evaluate framework},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3512467,
author = {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
title = {A Survey of Knowledge-enhanced Text Generation},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3512467},
doi = {10.1145/3512467},
abstract = {The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {227},
numpages = {38},
keywords = {Natural language generation, Knowledge-enhanced Methods}
}

@inproceedings{10.1145/3459637.3482007,
author = {Wang, Haiwen and Zhou, Le and Zhang, Weinan and Wang, Xinbing},
title = {LiteratureQA: A Qestion Answering Corpus with Graph Knowledge on Academic Literature},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482007},
doi = {10.1145/3459637.3482007},
abstract = {In this paper, we introduce LiteratureQA, a large question answering (QA) corpus consisting of publicly available academic papers. Different from other QA corpus, LiteratureQA has its unique challenges such as how to leverage the structured knowledge of citation networks. We further examine some popular QA method and present a benchmark approach of answering academic questions by combining both semantic text and graph knowledge to improve the prevalent pre-training model. We hope this resource could help research and development of tasks for machine reading over academic text.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4623–4632},
numpages = {10},
keywords = {academic corpus, academic knowledge graph, academic network, machine reading comprehension, question answering},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1145/3593804,
author = {Huang, Tao and Hu, Shengze and Lin, Keke and Yang, Huali and Zhang, Hao and Song, Houbing and Lv, Zhihan},
title = {Sequence Generation Model Integrating Domain Ontology for Mathematical question tagging},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3593804},
doi = {10.1145/3593804},
abstract = {In online learning systems, tagging knowledge points for questions is a fundamental task. Automatic tagging technology uses intelligent algorithms to automatically tag knowledge points for questions to reduce manpower and time costs. However, the current knowledge point tagging technology cannot satisfy the situation that mathematics questions often involve a variable number of knowledge points, lacks the consideration of the characteristics of the mathematics field, and ignores the internal connection between knowledge points. To address the above issues, we propose a Sequence Generation Model Integrating Domain Ontology for Mathematical question tagging (SOMPT). SOMPT performs data augmentation for text and then obtains intermediate text based on domain ontology replacement to facilitate deep learning model to understand mathematical question text. SOMPT is able to obtain dynamic word vector embedding to optimize the textual representation for math questions. What’s more, our model can capture the relationship between tags to generate knowledge points more accurately in the way of sequence generation. The comparative experimental results show that our proposed model has an excellent tagging ability for mathematical questions. Moreover, the sequence generation module in SOMPT can be applied on other multi-label classification tasks and be on par with the state-of-the-art performance models.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
keywords = {Mathematical question tagging, Deep learning, Language models, Sequence generation}
}

@inproceedings{10.1145/3544549.3583931,
author = {Sun, Yuqian and Xu, Ying and Cheng, Chenhang and Li, Yihua and Lee, Chang Hee and Asadipour, Ali},
title = {Explore the Future Earth with Wander 2.0: AI Chatbot Driven By Knowledge-base Story Generation and Text-to-image Model},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3583931},
doi = {10.1145/3544549.3583931},
abstract = {People always envision the future of earth through science fiction (Sci-fi), so can we create a unique experience of "visiting the future earth" through the lens of artificial intelligence (AI)? We introduce Wander 2.0, an AI chatbot that co-creates sci-fi stories through knowledge-based story generation on daily communication platforms like WeChat and Discord. Using location information from Google Maps, Wander generates narrative travelogues about specific locations (e.g. Paris) through a large-scale language model (LLM). Additionally, using the large-scale text-to-image model (LTGM) Stable Diffusion, Wander transfers future scenes that match both the text description and location photo, facilitating future imagination. The project also includes a real-time visualization of the human-AI collaborations on a future map. Through journeys with visitors from all over the world, Wander demonstrates how AI can serve as a subjective interface linking fiction and reality. Our research shows that multi-modal AI systems have the potential to extend the artistic experience and creative world-building through adaptive and unique content generation for different people. Wander 2.0 is available at http://wander001.com/},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {450},
numpages = {5},
keywords = {Artificial intelligence, chatbot, design fiction, gaming, human-AI interaction, interactive fiction},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3587259.3627574,
author = {Ram\'{o}n-Ferrer, Virginia and Badenes-Olmedo, Carlos and Corcho, Oscar},
title = {Automatic Topic Label Generation using Conversational Models},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627574},
doi = {10.1145/3587259.3627574},
abstract = {In probabilistic topic models, a topic is characterised by a set of words, with a probability associated to each of them. Even though it is not necessary to understand the meaning of topics to perform common downstream tasks where topic models are used, such as topic inference or document similarity, there have been attempts to uncover the semantics of topics by providing labels to them, consisting in a couple of concepts. In this paper we propose a methodology, Conversational Probabilistic Topic Labelling (CPTL), to study whether conversational models can be used to generate labels that describe probabilistic topics given their most representative keywords. We evaluate and compare the performance of a selection of conversational models for the topic label generation task with the performance of a task-specific language model trained to generate topic labels.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {17–24},
numpages = {8},
keywords = {conversational model, probabilistic topic labelling, topic label, topic label generation},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3503161.3548387,
author = {Li, Rengang and Xu, Cong and Guo, Zhenhua and Fan, Baoyu and Zhang, Runze and Liu, Wei and Zhao, Yaqian and Gong, Weifeng and Wang, Endong},
title = {AI-VQA: Visual Question Answering based on Agent Interaction with Interpretability},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548387},
doi = {10.1145/3503161.3548387},
abstract = {Visual Question Answering (VQA) serves as a proxy for evaluating the scene understanding of an intelligent agent by answering questions about images. Most VQA benchmarks to date are focused on those questions that can be answered through understanding visual content in the scene, such as simple counting, visual attributes, and even a little challenging questions that require extra encyclopedic knowledge. However, humans have a remarkable capacity to reason dynamic interaction on the scene, which is beyond the literal content of an image and has not been investigated so far. In this paper, we propose Agent Interaction Visual Question Answering (AI-VQA), a task investigating deep scene understanding if the agent takes a certain action. For this task, a model not only needs to answer action-related questions but also to locate the objects in which the interaction occurs for guaranteeing it truly comprehends the action. Accordingly, we make a new dataset based on Visual Genome and ATOMIC knowledge graph, including more than 19,000 manually annotated questions, and will make it publicly available. Besides, we also provide an annotation of the reasoning path while developing the answer for each question. Based on the dataset, we further propose a novel method, called ARE, that can comprehend the interaction and explain the reason based on a given event knowledge base. Experimental results show that our proposed method outperforms the baseline by a clear margin.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5274–5282},
numpages = {9},
keywords = {dataset, vision and language, visual question answer},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2021.3120636,
author = {Zhu, Biru and Zhang, Xingyao and Gu, Ming and Deng, Yangdong},
title = {Knowledge Enhanced Fact Checking and Verification},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3120636},
doi = {10.1109/TASLP.2021.3120636},
abstract = {As the Internet and social media offer increasing opportunities for organizations and individuals to publicize online contents, it has become essential to develop effective means to identify misinformation like fake news. Recently, fact checking systems have been regarded as a promising tool to automatically deal with large amounts of information. How to effectively take advantage of existing unstructured document knowledge bases and structured knowledge graphs to build robust fact checking systems, however, remains to be a challenge. In this paper, we propose a knowledge enhanced fact checking system, which leverages the Wikidata5M knowledge graph and Wikipedia documents to incorporate external knowledge into the claim to be checked for more robust and accurate fact checking. First, we devise a contextualized knowledge graph selection method to identify the most relevant sub-graph with the checked claim from the large knowledge graph. We then construct a novel claim-evidence-knowledge graph and use a graph attention network to integrate natural language evidence with structured knowledge triplets by allowing them to propagate information among each other. By integrating the claim, retrieved evidence and selected knowledge triplets in a unified claim-evidence-knowledge graph, our method improves the label accuracy of predicted claims by more than 4% on the FEVER dataset over state-of-the-art fact checking models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3132–3143},
numpages = {12}
}

@inproceedings{10.1145/3485447.3511943,
author = {Wang, Suyuchen and Zhao, Ruihui and Zheng, Yefeng and Liu, Bang},
title = {QEN: Applicable Taxonomy Completion via Evaluating Full Taxonomic Relations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511943},
doi = {10.1145/3485447.3511943},
abstract = {Taxonomy is a fundamental type of knowledge graph for a wide range of web applications like searching and recommendation systems. To keep a taxonomy automatically updated with the latest concepts, the taxonomy completion task matches a pair of proper hypernym and hyponym in the original taxonomy with the new concept as its parent and child. Previous solutions utilize term embeddings as input and only evaluate the parent-child relations between the new concept and the hypernym-hyponym pair. Such methods ignore the important sibling relations, and are not applicable in reality since term embeddings are not available for the latest concepts. They also suffer from the relational noise of the “pseudo-leaf” node, which is a null node acting as a node’s hyponym to enable the new concept to be a leaf node. To tackle the above drawbacks, we propose the Quadruple Evaluation Network (QEN), a novel taxonomy completion framework that utilizes easily accessible term descriptions as input, and applies pretrained language model and code attention for accurate inference while reducing online computation. QEN evaluates both parent-child and sibling relations to both enhance the accuracy and reduce the noise brought by pseudo-leaf. Extensive experiments on three real-world datasets in different domains with different sizes and term description sources prove the effectiveness and robustness of QEN on overall performance and especially the performance for adding non-leaf nodes, which largely surpasses previous methods and achieves the new state-of-the-art of the task.1},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1008–1017},
numpages = {10},
keywords = {Self-supervised Learning, Taxonomic Relations, Taxonomy Completion},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3477495.3532077,
author = {Ren, Zhaochun and Tian, Zhi and Li, Dongdong and Ren, Pengjie and Yang, Liu and Xin, Xin and Liang, Huasheng and de Rijke, Maarten and Chen, Zhumin},
title = {Variational Reasoning about User Preferences for Conversational Recommendation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532077},
doi = {10.1145/3477495.3532077},
abstract = {Conversational recommender systems (CRSs) provide recommendations through interactive conversations. CRSs typically provide recommendations through relatively straightforward interactions, where the system continuously inquires about a user's explicit attribute-aware preferences and then decides which items to recommend. In addition, topic tracking is often used to provide naturally sounding responses. However, merely tracking topics is not enough to recognize a user's real preferences in a dialogue.In this paper, we address the problem of accurately recognizing and maintaining user preferences in CRSs. Three challenges come with this problem: (1) An ongoing dialogue only provides the user's short-term feedback; (2) Annotations of user preferences are not available; and (3) There may be complex semantic correlations among items that feature in a dialogue. We tackle these challenges by proposing an end-to-end variational reasoning approach to the task of conversational recommendation. We model both long-term preferences and short-term preferences as latent variables with topical priors for explicit long-term and short-term preference exploration, respectively. We use an efficient stochastic gradient variational Bayesian (SGVB) estimator for optimizing the derived evidence lower bound. A policy network is then used to predict topics for a clarification utterance or items for a recommendation response. The use of explicit sequences of preferences with multi-hop reasoning in a heterogeneous knowledge graph helps to provide more accurate conversational recommendation results.Extensive experiments conducted on two benchmark datasets show that our proposed method outperforms state-of-the-art baselines in terms of both objective and subjective evaluation metric},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {165–175},
numpages = {11},
keywords = {conversational recommendation, task-oriented dialogue systems, user preference tracking, variational inference},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3511808.3557617,
author = {Anelli, Vito Walter and Biancofiore, Giovanni Maria and De Bellis, Alessandro and Di Noia, Tommaso and Di Sciascio, Eugenio},
title = {Interpretability of BERT Latent Space through Knowledge Graphs},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557617},
doi = {10.1145/3511808.3557617},
abstract = {The advent of pretrained language have renovated the ways of handling natural languages, improving the quality of systems that rely on them. BERT played a crucial role in revolutionizing the Natural Language Processing (NLP) area. However, the deep learning framework it implements lacks interpretability. Thus, recent research efforts aimed to explain what BERT learns from the text sources exploited to pre-train its linguistic model. In this paper, we analyze the latent vector space resulting from the BERT context-aware word embeddings. We focus on assessing whether regions of the BERT vector space hold an explicit meaning attributable to a Knowledge Graph (KG). First, we prove the existence of explicitly meaningful areas through the Link Prediction (LP) task. Then, we demonstrate these regions being linked to explicit ontology concepts of a KG by learning classification patterns. To the best of our knowledge, this is the first attempt at interpreting the BERT learned linguistic knowledge through a KG relying on its pretrained context-aware word embeddings.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {3806–3810},
numpages = {5},
keywords = {deep learning, knowledge graphs, natural language processing},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3447548.3467138,
author = {Hao, Junheng and Lei, Chuan and Efthymiou, Vasilis and Quamar, Abdul and \"{O}zcan, Fatma and Sun, Yizhou and Wang, Wei},
title = {MEDTO: Medical Data to Ontology Matching Using Hybrid Graph Neural Networks},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467138},
doi = {10.1145/3447548.3467138},
abstract = {Medical ontologies are widely used to describe and organize medical terminologies and to support many critical applications on healthcare databases. These ontologies are often manually curated (e.g., UMLS, SNOMED CT, and MeSH) by medical experts. Medical databases, on the other hand, are often created by database administrators, using different terminology and structures. The discrepancies between medical ontologies and databases compromise interoperability between them. Data to ontology matching is the process of finding semantic correspondences between tables in databases to standard ontologies. Existing solutions such as ontology matching have mostly focused on engineering features from terminological, structural, and semantic model information extracted from the ontologies. However, this is often labor intensive and the accuracy varies greatly across different ontologies. Worse yet, the ontology capturing a medical database is often not given in practice. In this paper, we propose MEDTO, a novel end-to-end framework that consists of three innovative techniques: (1) a lightweight yet effective method that bootstrap a semantically rich ontology from a given medical database, (2) a hyperbolic graph convolution layer that encodes hierarchical concepts in the hyperbolic space, and (3) a heterogeneous graph layer that encodes both local and global context information of a concept. Experiments on two real-world medical datasets matching against SNOMED CT show significant improvements compared to the state-of-the-art methods. MEDTO also consistently achieves competitive results on a benchmark from the Ontology Alignment Evaluation Initiative.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2946–2954},
numpages = {9},
keywords = {graph neural network, medical data, ontology matching},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3436369.3436390,
author = {Wang, Qingchuan and E, Haihong},
title = {A BERT-Based Named Entity Recognition in Chinese Electronic Medical Record},
year = {2021},
isbn = {9781450387835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436369.3436390},
doi = {10.1145/3436369.3436390},
abstract = {Named entity recognition, aiming at identifying and classifying named entity mentioned in the structured or unstructured text, is a fundamental subtask for information extraction in natural language processing (NLP). With the development of electronic medical records, obtaining the key and effective information in electronic document through named entity identification has become an increasingly popular research direction. In this article, we adapt a recently introduced pre-trained language model BERT for named entity recognition in electronic medical records to solve the problem of missing context information and we add an extra mechanism to capture the relationship between words. Based on this, (1) the entities can be represented by sentence-level vector, with the forward as well as backward information of the sentence, which can be directly used by downstream tasks; (2) the model acquires the representation of word in context and learn the potential relation between words to decrease the influence of inconsistent entity markup problem of a text. We conduct experiments an electronic medical record dataset proposed by China Conference on Knowledge Graph and Semantic Computing in 2019. The experimental result shows that our proposed method has an improvement compared with the traditional methods.},
booktitle = {Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition},
pages = {13–17},
numpages = {5},
keywords = {BERT, Named entity recognition, attention mechanism, electronic medical records},
location = {Xiamen, China},
series = {ICCPR '20}
}

@inproceedings{10.1145/3507548.3507591,
author = {Wang, Chenxi and Luo, Xudong},
title = {A Legal Question Answering System Based on BERT&nbsp;},
year = {2022},
isbn = {9781450384155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507548.3507591},
doi = {10.1145/3507548.3507591},
abstract = {With the development of artificial intelligence technology, intelligent question-answering systems in general fields have been widely accepted by people. However, the development of intelligent question-answering systems in limited areas is not very satisfactory. Moreover, due to the diversification of Chinese expressions, matching user input problems with prior problems is very important. This paper proposes a scheme to obtain the problem vector representation based on the BERT model. In addition, the Milvus vector search engine is used in this paper, which can not only provide store vector representation information but also calculate vector similarity. Finally, we return the answer through the database. When the threshold value of our proposed scheme is 0.2, the recall rate reaches 86%, and the mismatch rate reaches 84%. The results verify that the system has relatively good performance.},
booktitle = {Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence},
pages = {278–283},
numpages = {6},
keywords = {Intelligent question answering,&nbsp;BERT,&nbsp;Pre-trained language model, Milvus&nbsp;vector search engine, Similarity calculation},
location = {Beijing, China},
series = {CSAI '21}
}

@inproceedings{10.1145/3539618.3591781,
author = {Li, Mingchen and Huang, Lifu},
title = {Understand the Dynamic World: An End-to-End Knowledge Informed Framework for Open Domain Entity State Tracking},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591781},
doi = {10.1145/3539618.3591781},
abstract = {Open domain entity state tracking aims to predict reasonable state changes of entities (i.e., [attribute] of [entity] was [before_state] and [after_state] afterwards) given the action descriptions. It's important to many reasoning tasks to support human everyday activities. However, it's challenging as the model needs to predict an arbitrary number of entity state changes caused by the action while most of the entities are implicitly relevant to the actions and their attributes as well as states are from open vocabularies. To tackle these challenges, we propose a novel end-to-end Knowledge Informed framework for open domain Entity State Tracking, namely KIEST, which explicitly retrieves the relevant entities and attributes from external knowledge graph (i.e., ConceptNet) and incorporates them to autoregressively generate all the entity state changes with a novel dynamic knowledge grained encoder-decoder framework. To enforce the logical coherence among the predicted entities, attributes, and states, we design a new constraint decoding strategy and employ a coherence reward to improve the decoding process. Experimental results show that our proposed KIEST framework significantly outperforms the strong baselines on the public benchmark dataset - OpenPI},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {842–851},
numpages = {10},
keywords = {coherent reward, constraint decoding, knowledge informed genera- tion, open domain entity state tracking},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3580305.3599819,
author = {Hu, Xinyue and Gu, Lin and An, Qiyuan and Zhang, Mengliang and Liu, Liangchen and Kobayashi, Kazuma and Harada, Tatsuya and Summers, Ronald M. and Zhu, Yingying},
title = {Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599819},
doi = {10.1145/3580305.3599819},
abstract = {To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Different Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4156–4165},
numpages = {10},
keywords = {datasets, medical imaging, visual question answering},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3587716.3587743,
author = {Chan, Chunkit and Chan, Tszho},
title = {Discourse-Aware Prompt for Argument Impact Classification},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587743},
doi = {10.1145/3587716.3587743},
abstract = {Discourse information behind the arguments attracts a lot of attention from the field of Natural Language Processing (NLP) and computational argumentation. Durmus et al.&nbsp;[10] launched a new study on the influence of discourse contexts on determining argument impact. Argument Impact Classification is an intriguing but challenging task to classify whether the argumentative unit or an argument is impactful in a conversation. This paper empirically demonstrates that the discourse marker (e.g., "for example," "in other words") can be represented by the learnable continuous prompt to align with discourse information existing in Pre-trained Language Model (PLM). This discourse information helps the Pre-trained Language Model understand the input template and elicit the discourse information to improve the performance on this task. Therefore, based on this intuition, we propose a prompt model DAPA and surpass the previous state-of-the-art model with a 2.5% F1 score.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {165–171},
numpages = {7},
keywords = {argument mining, natural language processing, neural networks},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3569951.3597596,
author = {Wang, Mei-Yu and Uran, Julian and Buitrago, Paola},
title = {Deep Learning Benchmark Studies on an Advanced AI Engineering Testbed from the Open Compass Project},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569951.3597596},
doi = {10.1145/3569951.3597596},
abstract = {We present the Open Compass project’s pilot deep learning benchmark results with various AI accelerators. Those accelerators are NVIDIA V-100 and A-100, AMD MI100, as well as emerging novel accelerators such as Cerebras CS-2 and Graphcore. We evaluate their performance on various deep learning training tasks. We then discuss key insights from our experiments and share experiences about evaluating and integrating those novel AI accelerators with our supercomputing systems.},
booktitle = {Practice and Experience in Advanced Research Computing 2023: Computing for the Common Good},
pages = {255–259},
numpages = {5},
keywords = {Artificial Intelligence, BERT, Benchmarking, Bridges-2, Graphics Processing Unit, Intelligence Processing Unit, Large Language Model, Neocortex, UNet, Wafer-Scale Engine},
location = {Portland, OR, USA},
series = {PEARC '23}
}

@inproceedings{10.1145/3580305.3599246,
author = {Zhang, Jiarui and Ilievski, Filip and Ma, Kaixin and Kollaa, Aravinda and Francis, Jonathan and Oltramari, Alessandro},
title = {A Study of Situational Reasoning for Traffic Understanding},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599246},
doi = {10.1145/3580305.3599246},
abstract = {Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work, based on natural language inference, commonsense knowledge-graph self-supervision, multi-QA joint training, and dense retrieval of domain information. We associate each method with a relevant knowledge source, including knowledge graphs, relevant benchmarks, and driving manuals. In extensive experiments, we benchmark various knowledge-aware methods against the three datasets, under zero-shot evaluation; we provide in-depth analyses of model performance on data partitions and examine model predictions categorically, to yield useful insights on traffic understanding, given different background knowledge and reasoning strategies.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3262–3272},
numpages = {11},
keywords = {language models, question answering, traffic understanding, zero-shot evaluation},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3500931.3501009,
author = {Yu, Tao and Yong, Cuo},
title = {Deep Convolutional Neural Network Diabetic Entity Relationship Extraction Model Based on Enhanced Semantic Representation},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501009},
doi = {10.1145/3500931.3501009},
abstract = {Diabetes, as the number one chronic disease in China, plagues the lives of people of different age groups and causes great distress to people. This paper extracts the semantic relationships among diabetes entities through the entity relationship extraction technique, which helps to build the knowledge graph of diabetes domain. The experimental results show that the deep convolutional neural network (ESPDCNN) based on enhanced semantic representation proposed in this paper effectively improves the accuracy of the diabetic entity relationship extraction model.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {460–465},
numpages = {6},
keywords = {DiaKG, Diabetes, ESPDCNN, Entity relationship extraction},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.1145/3578741.3578791,
author = {Yu, Yishu},
title = {Beyond Natural Language Processing: Building Knowledge Graphs to Assist Scientists Understand COVID-19 Concepts},
year = {2023},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578741.3578791},
doi = {10.1145/3578741.3578791},
abstract = {To combat COVID-19, scientists must digest the vast amount of relevant biomedical knowledge in the literature to understand disease mechanisms and related biological functions. Nearly 3,000 scientific papers are published on PubMed every day. This knowledge bottleneck has resulted in severe delays in developing COVID-19 vaccines and drugs. Our research produces a hierarchy of knowledge concepts related to COVID-19, designed to assist scientists in answering questions and generating summaries. It aims to discover scientific and comprehensive knowledge to extract fine-grained multimedia elements (i.e., physical and visual structures, relational events and events, and chemical knowledge). Our project is toward one step in natural language understanding: detailed contextual sentences, subgraphs, and knowledge subgraphs are the first time to be automatically generated, and relations and coreferences of COVID-19 mentions will be sketched. Extensive results show that our method outperforms other state-of-the-art methods. In addition, we have published the generated knowledge graph on Google Drive1 and released the source in the Github2.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
pages = {245–251},
numpages = {7},
keywords = {COVID-19, Knowledge Graphs, Natural Language Processing},
location = {Sanya, China},
series = {MLNLP '22}
}

@article{10.1145/3502720,
author = {Lo, Pei-Chi and Lim, Ee-Peng},
title = {Contextual Path Retrieval: A Contextual Entity Relation Embedding-based Approach},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3502720},
doi = {10.1145/3502720},
abstract = {Contextual path retrieval (CPR) refers to the task of finding contextual path(s) between a pair of entities in a knowledge graph that explains the connection between them in a given context. For this novel retrieval task, we propose the Embedding-based Contextual Path Retrieval (ECPR) framework. ECPR is based on a three-component structure that includes a context encoder and path encoder that encode query context and path, respectively, and a path ranker that assigns a ranking score to each candidate path to determine the one that should be the contextual path. For context encoding, we propose two novel context encoding methods, i.e., context-fused entity embeddings and contextualized embeddings. For path encoding, we propose PathVAE, an inductive embedding approach to generate path representations. Finally, we explore two path-ranking approaches. In our evaluation, we construct a synthetic dataset from Wikipedia and two real datasets of Wikinews articles constructed through crowdsourcing. Our experiments show that methods based on ECPR framework outperform baseline methods, and that our two proposed context encoders yield significantly better performance than baselines. We also analyze a few case studies to show the distinct features of ECPR-based methods.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {1},
numpages = {38},
keywords = {Knowledge base, reasoning, information retrieval, embedding learning}
}

@inproceedings{10.1145/3580305.3599387,
author = {Wang, Xiaolei and Zhou, Kun and Tang, Xinyu and Zhao, Wayne Xin and Pan, Fan and Cao, Zhao and Wen, Ji-Rong},
title = {Improving Conversational Recommendation Systems via Counterfactual Data Simulation},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599387},
doi = {10.1145/3580305.3599387},
abstract = {Conversational recommender systems~(CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data.To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage recommendation dialogue simulator based on a conversation flow language model. Under the guidance of the learned user preference and dialogue schema, the flow language model can produce reasonable, coherent conversation flows, which can be further realized into complete dialogues. Based on the simulator, we perform the intervention at the representations of the interacted entities of target users, and design an adversarial training method with a curriculum schedule that can gradually optimize the data augmentation strategy. Extensive experiments show that our approach can consistently boost the performance of several competitive CRSs, and outperform other data augmentation methods, especially when the training data is limited. Our code is publicly available at https://github.com/RUCAIBox/CFCRS.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2398–2408},
numpages = {11},
keywords = {conversational recommender system, counterfactual data augmentation, prompt learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3511808.3557446,
author = {Burgdorf, Andreas and Paulus, Alexander and Pomp, Andr\'{e} and Meisen, Tobias},
title = {DocSemMap 2.0: Semantic Labeling based on Textual Data Documentations Using Seq2Seq Context Learner},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557446},
doi = {10.1145/3511808.3557446},
abstract = {Methods for automated semantic labeling of data are an indispensable basis for increasing the usability of data. On the one hand, they contribute to the homogenization of the annotations and thus to the increase in quality; on the other hand, they reduce the modeling effort, provided that the quality of the used methodology is sufficient. In the past, research has focused primarily on data- and label-based methods. Another approach that has received recent attention is the incorporation of textual data documentations to support the automatic mapping of datasets to a knowledge graph. However, upon deeper analysis, our recent approach called DocSemMap gives away potential in a number of places. In this paper, we extend the current state of the art approach by uncovering existing shortcomings and presenting our own improvements. Using a sequence-to-sequence model (Seq2Seq), we exploit the context of datasets. An additional introduced classifier provides the linkage of documentation and labels for prediction. Our extended approach achieves a sustainable improvement in comparison to the reference approach.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {98–107},
numpages = {10},
keywords = {classifier, natural language processing, semantic mapping, seq2seq},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3581783.3612403,
author = {Sun, Hongbo and He, Xiangteng and Zhou, Jiahuan and Peng, Yuxin},
title = {Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612403},
doi = {10.1145/3581783.3612403},
abstract = {Large-scale pre-trained vision-language (VL) models have shown powerful generic representation capabilities for adapting to downstream tasks with limited training data, which are data-efficient solutions to various applications such as image recognition. In order to enhance the adaption performance, most existing methods attempt to introduce learnable vectors into the text prompt to generate adaptive classification weights for the class in the downstream task. However, they generally focus on the text side while neglecting adaptive visual feature generation on the image side, which is insufficient to fit the downstream task data. In this paper, we propose fine-grained visual prompt learning (FG-VPL) of vision-language models for image recognition with few training samples, and the main contributions are: (1) Fine-grained visual prompt is introduced into the image encoder of the vision-language model for focusing on the target object and conducting information interaction within the object, which facilitates generating discriminative visual features for image recognition. (2) A two-pathway adaptive recognition module is proposed to narrow the domain gap and utilize both the cross-modal knowledge of the vision-language model and the visual information of the few-sample training set for classifying images with the help of feature adapters. We conduct extensive experiments on 11 image recognition benchmark datasets under the few training samples setting, which demonstrate that our proposed approach can achieve state-of-the-art performance. The code is available at https://github.com/PKU-ICST-MIPL/FG-VPL_ACMMM2023.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5828–5836},
numpages = {9},
keywords = {fine-grained visual prompt learning, image recognition with few training samples, vision-language models},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3581783.3611964,
author = {Yang, Qian and Chen, Qian and Wang, Wen and Hu, Baotian and Zhang, Min},
title = {Enhancing Multi-modal Multi-hop Question Answering via Structured Knowledge and Unified Retrieval-Generation},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611964},
doi = {10.1145/3581783.3611964},
abstract = {Multi-modal multi-hop question answering involves answering a question by reasoning over multiple input sources from different modalities. Existing methods often retrieve evidences separately and then use a language model to generate an answer based on the retrieved evidences, and thus do not adequately connect candidates and are unable to model the interdependent relations during retrieval. Moreover, the pipelined approaches of retrieval and generation might result in poor generation performance when retrieval performance is low. To address these issues, we propose a Structured Knowledge and Unified Retrieval-Generation (SKURG) approach. SKURG employs an Entity-centered Fusion Encoder to align sources from different modalities using shared entities. It then uses a unified Retrieval-Generation Decoder to integrate intermediate retrieval results for answer generation and also adaptively determine the number of retrieval steps. Extensive experiments on two representative multi-modal multi-hop QA datasets MultimodalQA and WebQA demonstrate that SKURG outperforms the state-of-the-art models in both source retrieval and answer generation performance with fewer parameters1.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5223–5234},
numpages = {12},
keywords = {cross-modal reasoning, multi-modal retrieval, question answering},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3534678.3539080,
author = {El-Kishky, Ahmed and Markovich, Thomas and Park, Serim and Verma, Chetan and Kim, Baekjin and Eskander, Ramy and Malkov, Yury and Portman, Frank and Samaniego, Sof\'{\i}a and Xiao, Ying and Haghighi, Aria},
title = {TwHIN: Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539080},
doi = {10.1145/3534678.3539080},
abstract = {Social networks, such as Twitter, form a heterogeneous information network (HIN) where nodes represent domain entities (e.g., user, content, advertiser, etc.) and edges represent one of many entity interactions (e.g, a user re-sharing content or "following" another). Interactions from multiple relation types can encode valuable information about social network entities not fully captured by a single relation; for instance, a user's preference for accounts to follow may depend on both user-content engagement interactions and the other users they follow. In this work, we investigate knowledge-graph embeddings for entities in the Twitter HIN (TwHIN); we show that these pretrained representations yield significant offline and online improvement for a diverse range of downstream recommendation and classification tasks: personalized ads rankings, account follow-recommendation, offensive content detection, and search ranking. We discuss design choices and practical challenges of deploying industry-scale HIN embeddings, including compressing them to reduce end-to-end model latency and handling parameter drift across versions.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2842–2850},
numpages = {9},
keywords = {Twitter, embedding, graph embedding, heterogeneous information network, recommendation system, social network},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3588938,
author = {Tu, Jianhong and Fan, Ju and Tang, Nan and Wang, Peng and Li, Guoliang and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song},
title = {Unicorn: A Unified Multi-tasking Model for Supporting Matching Tasks in Data Integration},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588938},
doi = {10.1145/3588938},
abstract = {Data matching - which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match) - is a key concept in data integration, such as entity matching and schema matching. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Unicorn can enable knowledge sharing by learning from multiple tasks and multiple datasets, and can also support zero-shot prediction for new tasks with zero labeled matching/non-matching pairs. However, building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on seven well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {84},
numpages = {26},
keywords = {data integration, data matching, multi-task learning}
}

@inproceedings{10.1145/3600211.3604702,
author = {Rismani, Shalaleh and Moon, AJung},
title = {What does it mean to be a responsible AI practitioner: An ontology of roles and skills},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604702},
doi = {10.1145/3600211.3604702},
abstract = {With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {584–595},
numpages = {12},
keywords = {Competency Framework, Education, Responsible AI Practitioner},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3584684.3597263,
author = {Ilani, Arnon and Dolev, Shlomi},
title = {Invited Paper: Common Public Knowledge for Enhancing Machine Learning Data Sets},
year = {2023},
isbn = {9798400701283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584684.3597263},
doi = {10.1145/3584684.3597263},
abstract = {In this study, we show the advantages of incorporating multi-source knowledge from publicly available sources, such as ChatGPT and Wikipedia, into existing datasets to enhance the performance of machine learning models for routine tasks, such as classification. specifically, we propose the utilization of supplementary data from external sources and demonstrate the utility of widely accessible knowledge in the context of the Forest Cover Type Prediction task launched by the Roosevelt National Forest of Northern Colorado. Additionally, we exhibit an improvement in classification accuracy for the Isolated Letter Speech Recognition dataset when incorporating information on regional accents in the prediction of spoken English letter names.},
booktitle = {Proceedings of the 5th Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating Algorithms for Distributed Systems},
articleno = {2},
numpages = {10},
keywords = {ontology, machine learning, random forests, feature engineering, world knowledge, speech recognition, isolated letter, forest management, tree cover type, ChatGPT},
location = {Orlando, FL, USA},
series = {ApPLIED 2023}
}

@inproceedings{10.1145/3469213.3470406,
author = {Cai, Xinhui and Fang, Jiandong and Zhao, Yudong},
title = {Joint extraction of Chinese entitys and relations based on hierarchical labeling},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470406},
doi = {10.1145/3469213.3470406},
abstract = {Entitys and relations triples are the basic information units that make up the knowledge graph, and represent some simple and specific information. This paper proposes a hierarchical annotation model based on BERT to jointly extract entity relation triples from text. First, the subject of the triples is marked by the labeling method. On this basis, for each predefined relationship, the half-pointer and half-marking method is used, and the double pointer is used to mark the beginning and end positions of the corresponding object in the text, and then the Complete extraction of triples can effectively solve the problem that traditional methods are difficult to extract triples with overlapping entities.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {199},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3543507.3583457,
author = {Zhao, Mingjun and Wang, Mengzhen and Ma, Yinglong and Niu, Di and Wu, Haijiang},
title = {CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583457},
doi = {10.1145/3543507.3583457},
abstract = {Text clustering, as one of the most fundamental challenges in unsupervised learning, aims at grouping semantically similar text segments without relying on human annotations. With the rapid development of deep learning, deep clustering has achieved significant advantages over traditional clustering methods. Despite the effectiveness, most existing deep text clustering methods rely heavily on representations pre-trained in general domains, which may not be the most suitable solution for clustering in specific target domains. To address this issue, we propose CEIL, a novel Classification-Enhanced Iterative Learning framework for short text clustering, which aims at generally promoting the clustering performance by introducing a classification objective to iteratively improve feature representations. In each iteration, we first adopt a language model to retrieve the initial text representations, from which the clustering results are collected using our proposed Category Disentangled Contrastive Clustering (CDCC) algorithm. After strict data filtering and aggregation processes, samples with clean category labels are retrieved, which serve as supervision information to update the language model with the classification objective via a prompt learning approach. Finally, the updated language model with improved representation ability is used to enhance clustering in the next iteration. Extensive experiments demonstrate that the CEIL framework significantly improves the clustering performance over iterations, and is generally effective on various clustering algorithms. Moreover, by incorporating CEIL on CDCC, we achieve the state-of-the-art clustering performance on a wide range of short text clustering benchmarks outperforming other strong baseline methods.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1784–1792},
numpages = {9},
keywords = {Classification-enhanced Clustering, Iterative Framework, Text Clustering},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3485447.3512039,
author = {Weinzierl, Maxwell and Harabagiu, Sanda},
title = {Identifying the Adoption or Rejection of Misinformation Targeting COVID-19 Vaccines in Twitter Discourse},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512039},
doi = {10.1145/3485447.3512039},
abstract = {Although billions of COVID-19 vaccines have been administered, too many people remain hesitant. Misinformation about the COVID-19 vaccines, propagating on social media, is believed to drive hesitancy towards vaccination. However, exposure to misinformation does not necessarily indicate misinformation adoption. In this paper we describe a novel framework for identifying the stance towards misinformation, relying on attitude consistency and its properties. The interactions between attitude consistency, adoption or rejection of misinformation and the content of microblogs are exploited in a novel neural architecture, where the stance towards misinformation is organized in a knowledge graph. This new neural framework is enabling the identification of stance towards misinformation about COVID-19 vaccines with state-of-the-art results. The experiments are performed on a new dataset of misinformation towards COVID-19 vaccines, called CoVaxLies, collected from recent Twitter discourse. Because CoVaxLies provides a taxonomy of the misinformation about COVID-19 vaccines, we are able to show which type of misinformation is mostly adopted and which is mostly rejected.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3196–3205},
numpages = {10},
keywords = {COVID-19, misinformation, social media, stance, twitter, vaccine},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3584371.3612953,
author = {Quintana, Felix and Treangen, Todd and Kavraki, Lydia},
title = {Leveraging Large Language Models for Predicting Microbial Virulence from Protein Structure and Sequence},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612953},
doi = {10.1145/3584371.3612953},
abstract = {In the aftermath of COVID-19, screening for pathogens has never been a more relevant problem. However, computational screening for pathogens is challenging due to a variety of factors, including (i) the complexity and role of the host, (ii) virulence factor divergence and dynamics, and (iii) population and community-level dynamics. Considering a potential pathogen's molecular interactions, specifically individual proteins and protein interactions can help pinpoint a potential protein of a given microbe to cause disease. However, existing tools for pathogen screening rely on existing annotations (KEGG, GO, etc), making the assessment of novel and unannotated proteins more challenging. Here, we present an LLM-inspired approach that considers protein sequence and structure to predict protein virulence. We present a two-stage model incorporating evolutionary features captured from the DistilProtBert language model and protein structure in a graph convolutional network. Our model performs better than sequence alone for virulence function when high-quality structures are present, thus representing a path forward for virulence prediction of novel and unannotated proteins.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {103},
numpages = {6},
keywords = {protein function, virulence prediction, graph-based models, large language models},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3477495.3531954,
author = {Li, Yinghui and Li, Yangning and He, Yuxin and Yu, Tianyu and Shen, Ying and Zheng, Hai-Tao},
title = {Contrastive Learning with Hard Negative Entities for Entity Set Expansion},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531954},
doi = {10.1145/3477495.3531954},
abstract = {Entity Set Expansion (ESE) is a promising task which aims to expand entities of the target semantic class described by a small seed entity set. Various NLP and IR applications will benefit from ESE due to its ability to discover knowledge. Although previous ESE methods have achieved great progress, most of them still lack the ability to handle hard negative entities (i.e., entities that are difficult to distinguish from the target entities), since two entities may or may not belong to the same semantic class based on different granularity levels we analyze on. To address this challenge, we devise an entity-level masked language model with contrastive learning to refine the representation of entities. In addition, we propose the ProbExpan, a novel probabilistic ESE framework utilizing the entity representation obtained by the aforementioned language model to expand entities. Extensive experiments and detailed analyses on three datasets show that our method outperforms previous state-of-the-art methods.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1077–1086},
numpages = {10},
keywords = {contrastive learning, entity set expansion, knowledge discovery},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3490322.3490336,
author = {Li, Zhengmin and Yun, Hongyan and Guo, Zhenbo and Qi, Jianjun},
title = {Medical Named Entity Recognition Based on Multi Feature Fusion of BERT},
year = {2022},
isbn = {9781450385091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490322.3490336},
doi = {10.1145/3490322.3490336},
abstract = {In order to solve the problem that traditional word vectors are difficult to express the context semantics and the feature extraction of traditional model is single, a multi-feature fusion model named BERT-BiLSTM-IDCNN-Attention-CRF for Named Entity Recognition is proposed, which uses BERT to model the context semantic relationship of word vectors and fuse the context features and local features extracted by BiLSTM and IDCNN respectively. The proposed model is tested on Chinese Electronic Medical Record (EMR) dataset issued by China Conference on Knowledge Graph and Semantic Computing 2020 (CCKS2020).Compared with the baseline models such as BiLSTM-CRF, the experiment on CCKS2020 data shows that BERT-BiLSTM-IDCNN-Attention-CRF achieves 1.27% improvement in F1. The experimental results show that the proposed model can better identify the medical entities in EMR.},
booktitle = {Proceedings of the 4th International Conference on Big Data Technologies},
pages = {86–91},
numpages = {6},
keywords = {BERT, BiLSTM, CCKS2020, IDCNN, Named Entity Recognition, multi feature fusion},
location = {Zibo, China},
series = {ICBDT '21}
}

@inproceedings{10.1145/3460210.3493573,
author = {Li, Xue and Magliacane, Sara and Groth, Paul},
title = {The Challenges of Cross-Document Coreference Resolution for Email},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493573},
doi = {10.1145/3460210.3493573},
abstract = {Long-form conversations such as email are an important source of information for knowledge capture. For tasks such as knowledge graph construction, conversational search, and entity linking, being able to resolve entities from across documents is important. Building on recent work on within document coreference resolution for email, we study for the first time a cross-document formulation of the problem. Our results show that the current state-of-the-art deep learning models for general cross-document coreference resolution are insufficient for email conversations. Our experiments show that the general task is challenging and, importantly for knowledge intensive tasks, coreference resolution models that only treat entity mentions perform worse. Based on these results, we outline the work needed to address this challenging task.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {273–276},
numpages = {4},
keywords = {challenges, conversational data, cross-document coreference resolution, email conversations, entity resolution},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@inproceedings{10.1145/3580305.3599401,
author = {Kim, Taeho and Yu, Juwon and Shin, Won-Yong and Lee, Hyunyoung and Im, Ji-hui and Kim, Sang-Wook},
title = {LATTE: A Framework for Learning Item-Features to Make a Domain-Expert for Effective Conversational Recommendation},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599401},
doi = {10.1145/3580305.3599401},
abstract = {For high-quality conversational recommender systems (CRS), it is important to recommend the suitable items by capturing the items' features mentioned in the dialog and to explain the appropriate ones among the various features of the recommended item. We argue that the CRS model should be a domain-expert who is (1) knowledgeable about the relationships between items and their various features and (2) able to explain the recommended item with its features relevant to dialog context. To this end, we propose a novel framework, named as LATTE, to pre-train each core module in CRS (i.e., the recommendation and the conversation module) through abundant external data. For the recommendation module, we pre-train the recommendation module to comprehensively understand the relationships between items and their various features by leveraging both multi-reviews and a knowledge graph. For pre-training the conversation module, we create the synthetic dialogs, which contain responses providing the explanation relevant to the dialog context by using all the items' features and dialog templates. Through extensive experiments on two public CRS datasets, we demonstrate that LATTE exhibits (1) the effectiveness of each module in LATTE, (2) the superiority over 7 state-of-the art methods, and (3) the interpretations based on visualization.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1144–1153},
numpages = {10},
keywords = {conversational recommender systems, domain-expert, explanation.},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3404835.3463259,
author = {Jain, Aman and Kothyari, Mayank and Kumar, Vishwajeet and Jyothi, Preethi and Ramakrishnan, Ganesh and Chakrabarti, Soumen},
title = {Select, Substitute, Search: A New Benchmark for Knowledge-Augmented Visual Question Answering},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463259},
doi = {10.1145/3404835.3463259},
abstract = {Multimodal IR, spanning text corpus, knowledge graph and images, called outside knowledge visual question answering (OKVQA), is of much recent interest. However, the popular data set has serious limitations. A surprisingly large fraction of queries do not assess the ability to integrate cross-modal information. Instead, some are independent of the image, some depend on speculation, some require OCR or are otherwise answerable from the image alone. To add to the above limitations, frequency-based guessing is very effective because of (unintended) widespread answer overlaps between the train and test folds. Overall, it is hard to determine when state-of-the-art systems exploit these weaknesses rather than really infer the answers, because they are opaque and their 'reasoning' process is uninterpretable. An equally important limitation is that the dataset is designed for the quantitative assessment only of the end-to-end answer retrieval task, with no provision for assessing the correct(semantic) interpretation of the input query. In response, we identify a key structural idiom in OKVQA ,viz., S3 (select, substitute and search), and build a new data set and challenge around it. Specifically, the questioner identifies an entity in the image and asks a question involving that entity which can be answered only by consulting a knowledge graph or corpus passage mentioning the entity. Our challenge consists of (i)OKVQA_S3, a subset of OKVQA annotated based on the structural idiom and (ii)S3VQA, a new dataset built from scratch. We also present a neural but structurally transparent OKVQA system, S3, that explicitly addresses our challenge dataset, and outperforms recent competitive baselines. We make our code and data available at https://s3vqa.github.io/.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2491–2498},
numpages = {8},
keywords = {multimodal question answering, open-domain question answering, query reformulation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3539618.3591700,
author = {Chen, Zhongwu and Xu, Chengjin and Su, Fenglong and Huang, Zhen and Dou, Yong},
title = {Incorporating Structured Sentences with Time-enhanced BERT for Fully-inductive Temporal Relation Prediction},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591700},
doi = {10.1145/3539618.3591700},
abstract = {Temporal relation prediction in incomplete temporal knowledge graphs (TKGs) is a popular temporal knowledge graph completion (TKGC) problem in both transductive and inductive settings. Traditional embedding-based TKGC models (TKGE) rely on structured connections and can only handle a fixed set of entities, i.e., the transductive setting. In the inductive setting where test TKGs contain emerging entities, the latest methods are based on symbolic rules or pre-trained language models (PLMs). However, they suffer from being inflexible and not time-specific, respectively. In this work, we extend the fully-inductive setting, where entities in the training and test sets are totally disjoint, into TKGs and take a further step towards a more flexible and time-sensitive temporal relation prediction approach SST-BERT,incorporating Structured Sentences with Time-enhanced BERT. Our model can obtain the entity history and implicitly learn rules in the semantic space by encoding structured sentences, solving the problem of inflexibility. We propose to use a time masking MLM task to pre-train BERT in a corpus rich in temporal tokens specially generated for TKGs, enhancing the time sensitivity of SST-BERT. To compute the probability of occurrence of a target quadruple, we aggregate all its structured sentences from both temporal and semantic perspectives into a score. Experiments on the transductive datasets and newly generated fully-inductive benchmarks show that SST-BERT successfully improves over state-of-the-art baselines.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {889–899},
numpages = {11},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3460231.3478882,
author = {F\"{a}rber, Michael and Leisinger, Ann-Kathrin},
title = {DataHunter: A System for Finding Datasets Based on Scientific Problem Descriptions},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3478882},
doi = {10.1145/3460231.3478882},
abstract = {The number of datasets is steadily rising, making it increasingly difficult for researchers and practitioners in the various scientific disciplines to be aware of all datasets, particularly of the most relevant datasets for a given research problem. To this end, dataset search engines have been proposed. However, they are based on the users’ keywords and thus have difficulties in determining precisely fitting datasets for complex research problems. In this paper, we propose the system at http://data-hunter.io that recommends suitable datasets to users based on given research problem descriptions. It is based on fastText for the text representation and text classification, the Data Set Knowledge Graph (DSKG) with metadata about almost 1,700 unique datasets, as well as 88,000 paper abstracts as research problem descriptions for training the model. Overall, our system demonstrates that recommending datasets facilitates data provisioning and reuse according to the FAIR principles and that dataset recommendation is a promising future research direction.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {749–752},
numpages = {4},
keywords = {FAIR principles, datasets, machine learning, recommendation, text classification},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/3543507.3583220,
author = {Sakota, Marija and Peyrard, Maxime and West, Robert},
title = {Descartes: Generating Short Descriptions of Wikipedia Articles},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583220},
doi = {10.1145/3543507.3583220},
abstract = {Wikipedia is one of the richest knowledge sources on the Web today. In order to facilitate navigating, searching, and maintaining its content, Wikipedia’s guidelines state that all articles should be annotated with a so-called short description indicating the article’s topic (e.g., the short description of beer is “Alcoholic drink made from fermented cereal grains”). Nonetheless, a large fraction of articles (ranging from 10.2% in Dutch to 99.7% in Kazakh) have no short description yet, with detrimental effects for millions of Wikipedia users. Motivated by this problem, we introduce the novel task of automatically generating short descriptions for Wikipedia articles and propose Descartes, a multilingual model for tackling it. Descartes integrates three sources of information to generate an article description in a target language: the text of the article in all its language versions, the already-existing descriptions (if any) of the article in other languages, and semantic type information obtained from a knowledge graph. We evaluate a Descartes model trained for handling 25 languages simultaneously, showing that it beats baselines (including a strong translation-based baseline) and performs on par with monolingual models tailored for specific languages. A human evaluation on three languages further shows that the quality of Descartes’s descriptions is largely indistinguishable from that of human-written descriptions; e.g., 91.3% of our English descriptions (vs. 92.1% of human-written descriptions) pass the bar for inclusion in Wikipedia, suggesting that Descartes is ready for production, with the potential to support human editors in filling a major gap in today’s Wikipedia across languages.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1446–1456},
numpages = {11},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3570991.3571028,
author = {Kumar, Suresh and Kumar P, Sreenivasa},
title = {Using domain ontology to identify consistent and inconsistent cases from LSTM-generated transfer type AWPs},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571028},
doi = {10.1145/3570991.3571028},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {289–290},
numpages = {2},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@article{10.1109/TASLP.2021.3110126,
author = {Zhang, Ningyu and Ye, Hongbin and Deng, Shumin and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Huang, Fei and Chen, Huajun},
title = {Contrastive Information Extraction With Generative Transformer},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3110126},
doi = {10.1109/TASLP.2021.3110126},
abstract = {Information extraction tasks such as entity relation extraction and event extraction are of great importance for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end information extraction task for sequence generation. Since generative information extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive information extraction with a generative transformer. Specifically, we introduce a single shared transformer module for an encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, we introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on five datasets (i.e., NYT, WebNLG, MIE, ACE-2005, and MUC-4) show that our approach achieves better performance than baselines.&lt;sup&gt;1&lt;/sup&gt;},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3077–3088},
numpages = {12}
}

@inproceedings{10.1145/3487553.3524648,
author = {Garcia-Olano, Diego and Onoe, Yasumasa and Ghosh, Joydeep},
title = {Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524648},
doi = {10.1145/3487553.3524648},
abstract = {Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task requiring external world knowledge in order to correctly answer a text question and associated image. Recent single modality text work has shown knowledge injection into pre-trained language models, specifically entity enhanced knowledge graph embeddings, can improve performance on downstream entity-centric tasks. In this work, we empirically study how and whether such methods, applied in a bi-modal setting, can improve an existing VQA system’s performance on the KBVQA task. We experiment with two large publicly available VQA datasets, (1) KVQA which contains mostly rare Wikipedia entities and (2) OKVQA which is less entity-centric and more aligned with common sense reasoning. Both lack explicit entity spans, and we study the effect of different weakly supervised and manual methods for obtaining them. Additionally, we analyze how recently proposed bi-modal and single modal attention explanations are affected by the incorporation of such entity enhanced representations. Our results show substantially improved performance on the KBVQA task without the need for additional costly pre-training, and we provide insights for when entity knowledge injection helps improve a model’s understanding. We provide code and enhanced datasets for reproducibility1.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {705–715},
numpages = {11},
keywords = {entity learning, explainability, knowledge injection, multi-modal learning, visual question answering, weak supervision},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3545801.3545806,
author = {Su, Hailong and Di, Jin and Yin, Xinhong and Li, Xianbo},
title = {Research on Intelligent Recommendation of Science and Technology Resource Data Based on Semantic Intelligence Analysis},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545801.3545806},
doi = {10.1145/3545801.3545806},
abstract = {Aiming at the practical problems such as imperfect semantic analysis function of recommendation service and low sharing degree among scientific data in the existing science and technology resources sharing, the center of knowledge association algorithm with intelligence is built through the mutual reflection evidence among different data subjects such as talents, enterprises, platforms, industries and scientific payoffs to realize multi-angle, multi-dimensional and multi-association data portrait. Through business understanding, data extraction, data processing, feature extraction, model construction, model deduction, model application, model evaluation and other mining steps, the data mining algorithm center with intelligence is established. Based on knowledge graph, an intelligent recommendation model for scientific data is proposed, and we construct vector model bank through machine learning, and realize intelligent recommendation and accurate pushing of scientific resources based on massive data of scientific research such as papers, patents, projects, and scientific reports. The problems of data centralization and unification, data systematization mapping, data application convenience, and data multiform and multi-scene application are effectively solved in the actual research work. By summarizing the theories, technologies and methods related to data sharing and application of S&amp;T resources, this study aims to provide useful references for the wisdom upgrading of S&amp;T resource sharing services and scientific and technical information service model innovation under the big data environment.},
booktitle = {Proceedings of the 7th International Conference on Big Data and Computing},
pages = {29–36},
numpages = {8},
keywords = {data sharing, intelligent recommendation, science and technology resources, semantic analysis},
location = {Shenzhen, China},
series = {ICBDC '22}
}

@article{10.5555/3648699.3648939,
author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
title = {PaLM: scaling language modeling with pathways},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540- billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {240},
numpages = {113},
keywords = {large language models, few-shot learning, natural language processing, scalable deep learning}
}

@inproceedings{10.1145/3459930.3469524,
author = {Mohan, Sunil and Angell, Rico and Monath, Nicholas and McCallum, Andrew},
title = {Low resource recognition and linking of biomedical concepts from a large ontology},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469524},
doi = {10.1145/3459930.3469524},
abstract = {Tools to explore scientific literature are essential for scientists, especially in biomedicine, where about a million new papers are published every year. Many such tools provide users the ability to search for specific entities (e.g. proteins, diseases) by tracking their mentions in papers. PubMed, the most well known database of biomedical papers, relies on human curators to add these annotations. This can take several weeks for new papers, and not all papers get tagged. Machine learning models have been developed to facilitate the semantic indexing of scientific papers. However their performance on the more comprehensive ontologies of biomedical concepts does not reach the levels of typical entity recognition problems studied in NLP. In large part this is due to their low resources, where the ontologies are large, there is a lack of descriptive text defining most entities, and labeled data can only cover a small portion of the ontology. In this paper, we develop a new model that overcomes these challenges by (1) generalizing to entities unseen at training time, and (2) incorporating linking predictions into the mention segmentation decisions. Our approach achieves new state-of-the-art results for the UMLS ontology in both traditional recognition/linking (+8 F1 pts) as well as semantic indexing-based evaluation (+10 F1 pts).},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {54},
numpages = {10},
keywords = {UMLS, biomedical concept recognition, deep learning, named entity recognition and linking},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inproceedings{10.1145/3503161.3547889,
author = {Ge, Jiannan and Xie, Hongtao and Min, Shaobo and Li, Pandeng and Zhang, Yongdong},
title = {Dual Part Discovery Network for Zero-Shot Learning},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547889},
doi = {10.1145/3503161.3547889},
abstract = {Zero-Shot Learning (ZSL) aims to recognize unseen classes by transferring knowledge from seen classes. Recent methods focus on learning a common semantic space to align visual and attribute information. However, they always over-relied on provided attributes and ignored the category discriminative information that contributes to accurate unseen class recognition, resulting in weak transferability. To this end, we propose a novel Dual Part Discovery Network (DPDN) that considers both attribute and category discriminative information by discovering attribute-guided parts and category-guided parts simultaneously to improve knowledge transfer. Specifically, for attribute-guided parts discovery, DPDN can localize the regions with specific attribute information and significantly bridge the gap between visual and semantic information guided by the given attributes. For category-guided parts discovery, the local parts are explored to discover other important regions that bring latent crucial details ignored by attributes, with the guidance of adaptive category prototypes. To better mine the transferable knowledge, we impose class correlations constraints to regularize the category prototypes. Finally, attribute- and category-guided parts complement each other and provide adequate discriminative subtle information for more accurate unseen class recognition. Extensive experimental results demonstrate that DPDN can discover discriminative parts and outperform state-of-the-art methods on three standard benchmarks.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {3244–3252},
numpages = {9},
keywords = {joint embedding, object recognition, zero-shot learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3603781.3603812,
author = {Chen, Yi and Song, Xingshen and Deng, Jinsheng and Cao, Jihao},
title = {Few-shot Question Answering with Entity-Aware Prompt},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603812},
doi = {10.1145/3603781.3603812},
abstract = {Providing simple task descriptions or prompts in natural language for large pre-trained language models yields impressive few-shot learning results in different tasks, such as text classification, knowledge probing, machine translation, and named entity recognition. In this paper, we apply this idea to question-answering task to fine-tune pre-trained language models by constructing entity-type prompts. Specifically, we augment the context sequences with semantic labels to enhance the understanding of pre-trained models, and dynamically adjust the prompts via intention recognition of the questions. Our proposition is simple yet powerful over traditional fine-tune training strategies and robust under few-shot conditions. The contributions of our work are as follows: 1. We proposed a few-shot learning method with entity-aware prompts for question-answering tasks to fine-tune the pre-trained language model. 2. Based on the SQuAD dataset, we extract a subset with 1,131 samples containing different categories of answer type, in which the answers to all questions are entities. 3. Experiments on multiple pre-trained language models validate that our method can effectively improve the performance of few-shot learning of question-answering tasks over the promptless ones.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {185–190},
numpages = {6},
keywords = {Question Answering, entity awareness, prompt learning},
location = {Xiamen, China},
series = {CNIOT '23}
}

@article{10.1145/3573201,
author = {Ma, Xuan and Yang, Xiaoshan and Xu, Changsheng},
title = {Multi-Source Knowledge Reasoning Graph Network for Multi-Modal Commonsense Inference},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {1551-6857},
url = {https://doi.org/10.1145/3573201},
doi = {10.1145/3573201},
abstract = {As a crucial part of natural language processing, event-centered commonsense inference task has attracted increasing attention. With a given observed event, the intention and reaction of the people involved in the event are required to be inferred with artificial intelligent algorithms. To solve this problem, sequence-to-sequence methods are widely studied, where the event is first encoded into a specific representation and then decoded to generate the results. However, all the existing methods learn the event representation only with the textual information, while the visual information is ignored, which is actually helpful for the commonsense reference. In this article, we first define a new task of multi-modal commonsense reference with both textual and visual information. A new event-centered multi-modal dataset is also provided. Then we propose a multi-source knowledge reasoning graph network to solve this task, where three kinds of relational knowledge are considered. Multi-modal correlations are learned to get the event’s multi-modal representation from a global perspective. Intra-event object relations are explored to capture the fine-grained event feature with an object graph. Inter-event semantic relations are also explored through the external knowledge to understand the semantic associations among events with an event graph. We conduct extensive experiments on the new dataset, and the results show the effectiveness of our method.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = mar,
articleno = {141},
numpages = {17},
keywords = {Knowledge reasoning, multi-modal commonsense inference, graph neural network}
}

@inproceedings{10.1145/3486713.3486730,
author = {Koutsomitropoulos, Dimitrios},
title = {Validating Ontology-based Annotations of Biomedical Resources using Zero-shot Learning},
year = {2021},
isbn = {9781450385107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486713.3486730},
doi = {10.1145/3486713.3486730},
abstract = {Authoritative thesauri in the form of web ontologies offer a sound representation of domain knowledge and can act as a reference point for automated semantic tagging. On the other hand, current language models achieve to capture contextualized semantics of text corpora and can be leveraged towards this goal. We present an approach for injecting subject annotations using query term expansion against such ontologies in the biomedical domain. For the user to have an indication of the usefulness of these suggestions we further propose an online method for validating the quality of annotations using NLI models such as BART and XLM-R. To circumvent training barriers posed by very large label sets and scarcity of data we rely on zero-shot classification and show that semantic matching can contribute above-average thematic annotations. Also, a web-based validation service can be attractive for human curators vs. the overhead of pretraining large, domain-tailored classification models.},
booktitle = {The 12th International Conference on Computational Systems-Biology and Bioinformatics},
pages = {37–43},
numpages = {7},
keywords = {MeSH, Thesaurus, biomedical indexing, classification, language models, machine learning, semantic matching},
location = {Virtual (GMT+7 Bangkok Time), Thailand},
series = {CSBio2021}
}

@inproceedings{10.5555/3545946.3598706,
author = {Xu, Weilai and Charles, Fred and Hargood, Charlie},
title = {Generating Stylistic and Personalized Dialogues for Virtual Agents in Narratives},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Virtual agents interact with each other through dialogues in various types of narratives (e.g. narrative films). In this paper, we propose an approach on the basis of DialoGPT pre-trained language model, which explores the impact of dialogue generation with different levels of agents' personalities derived from narrative films based on the Big-Five model, as well as with three different embedding methods. From the experimental results using automatic metrics and human judgments, we investigate and analyze the impact of different settings on narrative dialogue generation. Also, we demonstrate that our approach is able to generate dialogues with increased variety that correctly reflect the corresponding target personality.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {737–746},
numpages = {10},
keywords = {deep learning, dialogue generation, narratives, virtual agents},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3459637.3482387,
author = {Wisniewski, Dawid and Potoniec, Jedrzej and Lawrynowicz, Agnieszka},
title = {SeeQuery: An Automatic Method for Recommending Translations of Ontology Competency Questions into SPARQL-OWL},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482387},
doi = {10.1145/3459637.3482387},
abstract = {Ontology authoring is a complicated and error-prone process since the knowledge being modeled is expressed using logic-based formalisms, in which logical consequences of the knowledge have to be foreseen. To make that process easier, competency questions (CQs), being questions expressed in natural language are often stated to trace both the correctness and completeness of the ontology at a given time. However, CQs have to be translated into a formal language, like ontology query language (SPARQL-OWL), to query the ontology. Since the translation step is time-consuming and requires familiarity with the query language used, in this paper, we propose an automatic method named SeeQuery, which recommends SPARQL-OWL queries being translations of CQs stated against a given ontology. It consists of a pipeline of transformations based on template matching and filling, being motivated by the biggest to date publicly available CQ to SPARQL-OWL datasets. We provide a detailed description of SeeQuery and evaluate the method on a separate set of 2 ontologies with their CQs. It is, to date, the only automatic method available for recommending SPARQL-OWL queries out of CQs. The source code of SeeQuery is available at: https://github.com/dwisniewski/SeeQuery.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2119–2128},
numpages = {10},
keywords = {automatic translation, competency questions, ontology authoring, semantic similarity, sparql-owl},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3459637.3481902,
author = {Zhang, Chao and Zhou, Jingbo and Zang, Xiaoling and Xu, Qing and Yin, Liang and He, Xiang and Liu, Lin and Xiong, Haoyi and Dou, Dejing},
title = {CHASE: Commonsense-Enriched Advertising on Search Engine with Explicit Knowledge},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481902},
doi = {10.1145/3459637.3481902},
abstract = {While online advertising is one of the major sources of income for search engines, pumping up the incomes from business advertisements while ensuring the user experience becomes a challenging but emerging area. Designing high-quality advertisements with persuasive content has been proved as a way to increase revenues through improving the Click-Through Rate (CTR). However, it is difficult to scale up the design of high-quality ads, due to the lack of automation in creativity. In this paper, we present Commonsense-Enriched Advertisement on Search Engine (CHASE) --- a system for the automatic generation of persuasive ads. CHASE adopts a specially designed language model that fuses the keywords, commonsense-related texts, and marketing contents to generate persuasive advertisements. Specifically, the language model has been pre-trained using massive contents of explicit knowledge and fine-tuned with well-constructed quasi-parallel corpora with effective control of the proportion of commonsense in the generated ads and fitness to the ads' keywords. The effectiveness of the proposed method CHASE has been verified by real-world web traffics for search and manual evaluation. In A/B tests, the advertisements generated by CHASE would bring 11.13% CTR improvement. The proposed model has been deployed to cover three advertisement domains (which are kid education, psychological counseling, and beauty e-commerce) at Baidu, the world's largest Chinese search engine, with adding revenue of about 1 million RMB (Chinese Yuan) per day.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4352–4361},
numpages = {10},
keywords = {advertising, commonsense-enriched advertisement, description generation, persuasive advertisement, quasi-parallel corpora},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3539618.3594250,
author = {Liao, Lizi and Yang, Grace Hui and Shah, Chirag},
title = {Proactive Conversational Agents in the Post-ChatGPT World},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3594250},
doi = {10.1145/3539618.3594250},
abstract = {ChatGPT and similar large language model (LLM) based conversational agents have brought shock waves to the research world. Although astonished by their human-like performance, we find they share a significant weakness with many other existing conversational agents in that they all take a passive approach in responding to user queries. This limits their capacity to understand the users and the task better and to offer recommendations based on a broader context than a given conversation. Proactiveness is still missing in these agents, including their ability to initiate a conversation, shift topics, or offer recommendations that take into account a more extensive context. To address this limitation, this tutorial reviews methods for equipping conversational agents with proactive interaction abilities.The full-day tutorial is divided into four parts, including multiple interactive exercises. We will begin the tutorial with an interactive exercise and cover the design of existing conversational systems architecture and challenges. The content includes coverage of LLM-based recent advancements such as ChatGPT and Bard, along with reinforcement learning with human feedback (RLHF) technique. Then we will introduce the concept of proactive conversation agents and preset recent advancements in proactiveness of conversational agents, including actively driving conversations by asking questions, topic shifting, and methods that support strategic planning of conversation. Next, we will discuss important issues in conversational responses' quality control, including safety, appropriateness, language detoxication, hallucination, and alignment. Lastly, we will launch another interactive exercise and discussion with the audience to arrive at concluding remarks, prospecting open challenges and new directions. By exploring new techniques for enhancing conversational agents' proactive behavior to improve user engagement, this tutorial aims to help researchers and practitioners develop more effective conversational agents that can better understand and respond to user needs proactively and safely.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3452–3455},
numpages = {4},
keywords = {conversational ai, conversational search, proactive conversation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3543507.3583541,
author = {Mosharrof, Adib and Fereidouni, Moghis and Siddique, A.B.},
title = {Toward Open-domain Slot Filling via Self-supervised Co-training},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583541},
doi = {10.1145/3543507.3583541},
abstract = {Slot filling is one of the critical tasks in modern conversational systems. The majority of existing literature employs supervised learning methods, which require labeled training data for each new domain. Zero-shot learning and weak supervision approaches, among others, have shown promise as alternatives to manual labeling. Nonetheless, these learning paradigms are significantly inferior to supervised learning approaches in terms of performance. To minimize this performance gap and demonstrate the possibility of open-domain slot filling, we propose a Self-supervised Co-training framework, called , that requires zero in-domain manually labeled training examples and works in three phases. Phase one acquires two sets of complementary pseudo labels automatically. Phase two leverages the power of the pre-trained language model BERT, by adapting it for the slot filling task using these sets of pseudo labels. In phase three, we introduce a self-supervised co-training mechanism, where both models automatically select high-confidence soft labels to further improve the performance of the other in an iterative fashion. Our thorough evaluations show that outperforms state-of-the-art models by 45.57% and 37.56% on SGD and MultiWoZ datasets, respectively. Moreover, our proposed framework achieves comparable performance when compared to state-of-the-art fully supervised models.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1928–1937},
numpages = {10},
keywords = {co-training, open-domain slot filling, weak supervision.},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3442381.3450117,
author = {Fang, Tianqing and Zhang, Hongming and Wang, Weiqi and Song, Yangqiu and He, Bin},
title = {DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense Knowledge},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450117},
doi = {10.1145/3442381.3450117},
abstract = {Commonsense knowledge is crucial for artificial intelligence systems to understand natural language. Previous commonsense knowledge acquisition approaches typically rely on human annotations (for example, ATOMIC) or text generation models (for example, COMET.) Human annotation could provide high-quality commonsense knowledge, yet its high cost often results in relatively small scale and low coverage. On the other hand, generation models have the potential to automatically generate more knowledge. Nonetheless, machine learning models often fit the training data well and thus struggle to generate high-quality novel knowledge. To address the limitations of previous approaches, in this paper, we propose an alternative commonsense knowledge acquisition framework DISCOS (from DIScourse to COmmonSense), which automatically populates expensive complex commonsense knowledge to more affordable linguistic knowledge resources. Experiments demonstrate that we can successfully convert discourse knowledge about eventualities from ASER, a large-scale discourse knowledge graph, into if-then commonsense knowledge defined in ATOMIC without any additional annotation effort. Further study suggests that DISCOS significantly outperforms previous supervised approaches in terms of novelty and diversity with comparable quality. In total, we can acquire 3.4M ATOMIC-like inferential commonsense knowledge by populating ATOMIC on the core part of ASER. Codes and data are available at https://github.com/HKUST-KnowComp/DISCOS-commonsense.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2648–2659},
numpages = {12},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3543434.3543590,
author = {Ortiz-Rodriguez, Fernando and Tiwari, Sanju and Panchal, Ronak and Medina-Quintero, Jose Melchor and Barrera, Ruben},
title = {MEXIN: Multidialectal Ontology supporting NLP approach to improve government electronic communication with the Mexican Ethnic Groups},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543590},
doi = {10.1145/3543434.3543590},
abstract = {The government services usually target all citizens, but sometimes physical services nor technology-based services do not cover all people. This research aims to tackle services given to underrepresented citizens in Mexico (Indigenous people) and apply NLP techniques supported by ontologies to achieve accurate translation to most dialects spoken in Mexico. The scope of this paper only tests with Mayan dialect spoken primarily in the Mexican peninsula.},
booktitle = {Proceedings of the 23rd Annual International Conference on Digital Government Research},
pages = {461–463},
numpages = {3},
keywords = {NLP, Ontologies, Semantic Web, eGovernment},
location = {Virtual Event, Republic of Korea},
series = {dg.o '22}
}

@inproceedings{10.1145/3447548.3467144,
author = {Xia, Yuan and Wang, Chunyu and Shi, Zhenhui and Zhou, Jingbo and Lu, Chao and Huang, Haifeng and Xiong, Hui},
title = {Medical Entity Relation Verification with Large-scale Machine Reading Comprehension},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467144},
doi = {10.1145/3447548.3467144},
abstract = {Medical entity relation verification is a crucial step to build a practical and enterprise medical knowledge graph (MKG) because high-precision medical entity relation is a key requirement for many MKG-based applications. Existing relation verification approaches for general knowledge graphs are not designed for considering medical domain knowledge, although it is central to achieve high-quality entity relation verification for MKG. To this end, in this paper, we introduce a system for medical entity relation verification with large-scale machine reading comprehension. The proposed system is tailored to overcome the unique challenges of medical relation verification including high variants of medical terms, the high difficulty of evidence searching in complex medical documents, and the lack of evidence labels for supervision. To deal with the problem of variants of medical terms, we introduce a synonym-aware retrieve model to retrieve the potential evidence implicitly verifying the given claim. To better utilize the medical domain knowledge, a relation-aware evidence detector and a medical ontology-enhanced aggregator are developed to improve the performance of the relation verification module. Moreover, to overcome the challenge of providing high-quality evidence due to the lack of labels, we introduce an interactive collaborative-training method to iteratively improve the evidence accuracy. Finally, we conduct extensive experiments to demonstrate that the performance of our proposed system is superior to all comparable models. We also demonstrate that our system can significantly reduce the annotation time by medical experts in real-world verification tasks. It can help to improve the efficiency by nearly 300%. In particular, our system has been embedded into the Baidu Clinical Decision Support System.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3765–3774},
numpages = {10},
keywords = {clinical decision support, fact verification, relation extraction},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3534678.3539020,
author = {Ren, Houxing and Wang, Jingyuan and Zhao, Wayne Xin},
title = {Generative Adversarial Networks Enhanced Pre-training for Insufficient Electronic Health Records Modeling},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539020},
doi = {10.1145/3534678.3539020},
abstract = {In recent years, automatic computational systems based on deep learning are widely used in medical fields, such as automatic diagnosing and disease prediction. Most of these systems are designed for data sufficient scenarios. However, due to the disease rarity or privacy, the medical data are always insufficient. When applying these data-hungry deep learning models with insufficient data, it is likely to lead to issues of over-fitting and cause serious performance problems. Many data augmentation methods have been proposed to solve the data insufficiency problem, such as using GAN (Generative Adversarial Networks) to generate training data. However, the augmented data usually contains lots of noise. Directly using them to train sensitive medical models is very difficult to achieve satisfactory results.To overcome this problem, we propose a novel deep model learning method for insufficient EHR (Electronic Health Record) data modeling, namely GRACE, which stands GeneRative Adversarial networks enhanCed prE-training. In the method, we propose an item-relation-aware GAN to capture changing trends and correlations among data for generating high-quality EHR records. Furthermore, we design a pre-training mechanism consisting of a masked records prediction task and a real-fake contrastive learning task to learn representations for EHR data using both generated and real data. After the pre-training, only the representations of real data is used to train the final prediction model. In this way, we can fully exploit useful information in generated data through pre-training, and also avoid the problems caused by directly using noisy generated data to train the final prediction model. The effectiveness of the proposed method is evaluated using extensive experiments on three healthcare-related real-world datasets. We also deploy our method in a maternal and child health care hospital for the online test. Both offline and online experimental results demonstrate the effectiveness of the proposed method. We believe doctors and patients can benefit from our effective learning method in various healthcare-related applications.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3810–3818},
numpages = {9},
keywords = {healthcare informatics, pre-training, representation learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3591106.3592223,
author = {Nebbia, Giacomo and Kovashka, Adriana},
title = {Hypernymization of named entity-rich captions for grounding-based multi-modal pretraining},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592223},
doi = {10.1145/3591106.3592223},
abstract = {Named entities are ubiquitous in text that naturally accompanies images, especially in domains such as news or Wikipedia articles. In previous work, named entities have been identified as a likely reason for low performance of image-text retrieval models pretrained on Wikipedia and evaluated on named entities-free benchmark datasets. Because they are rarely mentioned, named entities could be challenging to model. They also represent missed learning opportunities for self-supervised models: the link between named entity and object in the image may be missed by the model, but it would not be if the object were mentioned using a more common term. In this work, we investigate hypernymization as a way to deal with named entities for pretraining grounding-based multi-modal models and for fine-tuning on open-vocabulary detection. We propose two ways to perform hypernymization: (1) a “manual” pipeline relying on a comprehensive ontology of concepts, and (2) a “learned” approach where we train a language model to learn to perform hypernymization. We run experiments on data from Wikipedia and from The New York Times. We report improved pretraining performance on objects of interest following hypernymization, and we show the promise of hypernymization on open-vocabulary detection, specifically on classes not seen during training.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {67–75},
numpages = {9},
keywords = {grounding, hypernymization, named entities, open-vocabulary detection},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3444370.3444559,
author = {Hou, Ruihui and Li, Hanhao and Feng, Honghai and Li, Yunpeng and Li, Jun and Shen, Yatian},
title = {An entity relation extraction algorithm based on BERT(wwm-ext)-BiGRU-Attention},
year = {2021},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444559},
doi = {10.1145/3444370.3444559},
abstract = {Entity relation extraction is one of the basic steps of knowledge Graph. It identifies the relations between entities. A BERT-Bidirectional gated recurrent units-Attention mechanism (BERT-BiGRU-Attention) model has been proposed, but it is based on the single Chinese character based masking. Due to the complexity of Chinese grammar structure and the semantic diversity, a BERT(wwm-ext) was proposed based on the whole Chinese word masking. In this paper we propose a BERT(wwm-ext)-BiGRU-Attention model. The experimental result shows that for the purpose of entity relation extraction the precision is 93.60%, recall rate is 91.90%, and F1 value 92.53%, which are higher than the BERT-BiGRU-Attention and its precision is 91.80%, recall rate is 90.16%, and F1 value is 90.97%. Since BERT(wwm-ext)-BIGRU-Attention gets higher precision, F1 value, and higher recall rate, it has better effects on the Chinese entity relation extraction tasks.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {130–135},
numpages = {6},
keywords = {Attention mechanism, Bidirectional gated recurrent units, Entity relation extraction, Natural language processing},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@inproceedings{10.1145/3404835.3462870,
author = {Ge, Congcong and Liu, Xiaoze and Chen, Lu and Zheng, Baihua and Gao, Yunjun},
title = {Make It Easy: An Effective End-to-End Entity Alignment Framework},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462870},
doi = {10.1145/3404835.3462870},
abstract = {Entity alignment (EA) is a prerequisite for enlarging the coverage of a unified knowledge graph. Previous EA approaches either restrain the performance due to inadequate information utilization or need labor-intensive pre-processing to get external or reliable information to perform the EA task. This paper proposes EASY, an effective end-to-end EA framework, which is able to (i) remove the labor-intensive pre-processing by fully discovering the name information provided by the entities themselves; and (ii) jointly fuse the features captured by the names of entities and the structural information of the graph to improve the EA results. Specifically, EASY first introduces NEAP, a highly effective name-based entity alignment procedure, to obtain an initial alignment that has reasonable accuracy and meanwhile does not require much memory consumption or any complex training process. Then, EASY invokes SRS, a novel structure-based refinement strategy, to iteratively correct the misaligned entities generated by NEAP to further enhance the entity alignment. Extensive experiments demonstrate the superiority of our proposed EASY with significant improvement against 13 existing state-of-the-art competitors.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {777–786},
numpages = {10},
keywords = {entity alignment, entity name, graph structure, iterative training},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3477495.3531809,
author = {Yan, Guojun and Pei, Jiahuan and Ren, Pengjie and Ren, Zhaochun and Xin, Xin and Liang, Huasheng and de Rijke, Maarten and Chen, Zhumin},
title = {ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531809},
doi = {10.1145/3477495.3531809},
abstract = {AcpMDS aim to assist doctors and patients with a range of professional medical services, i.e., diagnosis, treatment and consultation. The development of acpMDS is hindered because of a lack of resources. In particular. beginenumerate* [label=(arabic*) ] item there is no dataset with large-scale medical dialogues that covers multiple medical services and contains fine-grained medical labels (i.e., intents, actions, slots, values), and item there is no set of established benchmarks for acpMDS for multi-domain, multi-service medical dialogues. endenumerate*  In this paper, we present acsReMeDi, a set of aclReMeDi acusedReMeDi. \O{}urResources consists of two parts, the \O{}urResources dataset and the \O{}urResources benchmarks. The \O{}urResources dataset contains 96,965 conversations between doctors and patients, including 1,557 conversations with fine-gained labels. It covers 843 types of diseases, 5,228 medical entities, and 3 specialties of medical services across 40 domains. To the best of our knowledge, the \O{}urResources dataset is the only medical dialogue dataset that covers multiple domains and services, and has fine-grained medical labels.  The second part of the \O{}urResources resources consists of a set of state-of-the-art models for (medical) dialogue generation. The \O{}urResources benchmark has the following methods: beginenumerate* item pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5) trained, validated, and tested on the \O{}urResources dataset, and item a acfSCL method to expand the \O{}urResources dataset and enhance the training of the state-of-the-art pretrained models. endenumerate* We describe the creation of the \O{}urResources dataset, the \O{}urResources benchmarking methods, and establish experimental results using the \O{}urResources benchmarking methods on the \O{}urResources dataset for future research to compare against. With this paper, we share the dataset, implementations of the benchmarks, and evaluation scripts.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3013–3024},
numpages = {12},
keywords = {dialogue benchmarks, dialogue dataset, medical dialogues},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3631504.3631518,
author = {Amer-Yahia, Sihem and Bonifati, Angela and Chen, Lei and Li, Guoliang and Shim, Kyuseok and Xu, Jianliang and Yang, Xiaochun},
title = {From Large Language Models to Databases and Back: A Discussion on Research and Education},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3631504.3631518},
doi = {10.1145/3631504.3631518},
abstract = {In recent years, large language models (LLMs) have garnered increasing attention from both academia and industry due to their potential to facilitate natural language processing (NLP) and generate highquality text. Despite their benefits, however, the use of LLMs is raising concerns about the reliability of knowledge extraction. The combination of DB research and data science has advanced the state of the art in solving real-world problems, such as merchandise recommendation and hazard prevention [30]. In this discussion, we explore the challenges and opportunities related to LLMs in DB and data science research and education.},
journal = {SIGMOD Rec.},
month = nov,
pages = {49–56},
numpages = {8}
}

@inproceedings{10.1145/3622896.3622920,
author = {Cui, Ronghui and Li, Xudong},
title = {A Comprehensive Survey on Text Filling Algorithms: A Research Review},
year = {2023},
isbn = {9798400708190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622896.3622920},
doi = {10.1145/3622896.3622920},
abstract = {Starting from the concept, application scenarios and research significance of text filling, this paper divides text filling algorithms into two categories: traditional methods and deep learning, summarizes the development process of text filling algorithms in detail, focuses on describing the algorithm principles of text filling algorithms in recent years, and summarizes and evaluates text filling algorithms from the aspects of experimental results, advantages and disadvantages. In addition, this paper details the most commonly used text datasets and the most popular evaluation indicators - BLEU and Perplexity. Finally, this paper summarizes the research on text filling algorithm and looks forward to the future development trend. This paper aims to provide researchers with a comprehensive review of text filling algorithms to guide and prospect future development.},
booktitle = {Proceedings of the 2023 4th International Conference on Control, Robotics and Intelligent System},
pages = {141–147},
numpages = {7},
keywords = {Deep learning, Generative adversarial network (GAN), Recurrent neural network (RNN), Text filling},
location = {Guangzhou, China},
series = {CCRIS '23}
}

@inproceedings{10.1145/3544548.3581566,
author = {Liu, Xingyu "Bruce" and Kirilyuk, Vladimir and Yuan, Xiuxiu and Olwal, Alex and Chi, Peggy and Chen, Xiang "Anthony" and Du, Ruofei},
title = {Visual Captions: Augmenting Verbal Communication with On-the-fly Visuals},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581566},
doi = {10.1145/3544548.3581566},
abstract = {Video conferencing solutions like Zoom, Google Meet, and Microsoft Teams are becoming increasingly popular for facilitating conversations, and recent advancements such as live captioning help people better understand each other. We believe that the addition of visuals based on the context of conversations could further improve comprehension of complex or unfamiliar concepts. To explore the potential of such capabilities, we conducted a formative study through remote interviews (N=10) and crowdsourced a dataset of over 1500 sentence-visual pairs across a wide range of contexts. These insights informed Visual Captions, a real-time system that integrates with a video conferencing platform to enrich verbal communication. Visual Captions leverages a fine-tuned large language model to proactively suggest relevant visuals in open-vocabulary conversations. We present findings from a lab study (N=26) and an in-the-wild case study (N=10), demonstrating how Visual Captions can help improve communication through visual augmentation in various scenarios.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {108},
numpages = {20},
keywords = {AI agent, augmented communication, augmented reality, collaborative work, dataset, large language models, online meeting, text-to-visual, video-mediated communication},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3587259.3627561,
author = {Chhikara, Prateek and Zhang, Jiarui and Ilievski, Filip and Francis, Jonathan and Ma, Kaixin},
title = {Knowledge-enhanced Agents for Interactive Text Games},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627561},
doi = {10.1145/3587259.3627561},
abstract = {Communication via natural language is a key aspect of machine intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. Significant progress has been made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding. Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a knowledge-injection framework for improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports two representative model classes: reinforcement learning agents and language model agents. Furthermore, we devise multiple injection strategies for the above domain knowledge types and agent architectures, including injection via knowledge graphs and augmentation of the existing input encoding strategies. We experiment with four models on the 10 tasks in the ScienceWorld&nbsp;text-based game environment, to illustrate the impact of knowledge injection on various model configurations and challenging task settings. Our findings provide crucial insights into the interplay between task properties, model architectures, and domain knowledge for interactive contexts.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {157–165},
numpages = {9},
keywords = {Interactive Task Learning, Knowledge Injection, Natural Language Communication, Text-based Games},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3500931.3500939,
author = {Gao, Feng and Zhou, LunSheng and Gu, JinGuang},
title = {Entity Pair Recognition using Semantic Enrichment and Adversarial Training for Chinese Drug Knowledge Extraction},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500939},
doi = {10.1145/3500931.3500939},
abstract = {Existing knowledge extraction methods in pharmacy often use natural language processing tools and deep learning model to identify drug entities and extract their relationships from drug instructions, thus obtaining drug-drug or drug-disease knowledge. However, sentences in drug instructions may contain multiple drug-related entities, and existing methods lack the capability of identifying valid the "drug-drug" or "drug-disease" entity pairs. This will introduce significant noise data in the subsequent tasks such as entity relationship extraction and knowledge graph construction. Meanwhile, some mentions in the sentence can have hierarchical relations even if they do not form valid entity pairs, such information is also crucial to knowledge extraction. To solve these two problems, this paper proposes an entity pair verification model based on entity semantic enhancement and adversarial training. Through the experiment on more than 2000 kinds of drug instructions data, the experimental results show that the F1 value of the model for entity pair verification is up to 98.65%, which is up to 9.37% compared with the existing methods.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {35–42},
numpages = {8},
keywords = {entity pair verification, knowledge induction, medical field, subclass and hyponym},
location = {Beijing, China},
series = {ISAIMS '21}
}

@article{10.1145/3436819,
author = {Wang, Yu and Sun, Yining and Ma, Zuchang and Gao, Lisheng and Xu, Yang},
title = {A Hybrid Model for Named Entity Recognition on Chinese Electronic Medical Records},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3436819},
doi = {10.1145/3436819},
abstract = {Electronic medical records (EMRs) contain valuable information about the patients, such as clinical symptoms, diagnostic results, and medications. Named entity recognition (NER) aims to recognize entities from unstructured text, which is the initial step toward the semantic understanding of the EMRs. Extracting medical information from Chinese EMRs could be a more complicated task because of the difference between English and Chinese. Some researchers have noticed the importance of Chinese NER and used the recurrent neural network or convolutional neural network (CNN) to deal with this task. However, it is interesting to know whether the performance could be improved if the advantages of the RNN and CNN can be both utilized. Moreover, RoBERTa-WWM, as a pre-training model, can generate the embeddings with word-level features, which is more suitable for Chinese NER compared with Word2Vec. In this article, we propose a hybrid model. This model first obtains the entities identified by bidirectional long short-term memory and CNN, respectively, and then uses two hybrid strategies to output the final results relying on these entities. We also conduct experiments on raw medical records from real hospitals. This dataset is provided by the China Conference on Knowledge Graph and Semantic Computing in 2019 (CCKS 2019). Results demonstrate that the hybrid model can improve performance significantly.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {35},
numpages = {12},
keywords = {Named entity recognition, Chinese electronic medical records, neural networks, hybrid models}
}

@article{10.1109/TASLP.2021.3058616,
author = {Zhang, Zhuosheng and Li, Junlong and Zhao, Hai},
title = {Multi-Turn Dialogue Reading Comprehension With Pivot Turns and Knowledge},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3058616},
doi = {10.1109/TASLP.2021.3058616},
abstract = {Multi-turn dialogue reading comprehension aims to teach machines to read dialogue contexts and solve tasks such as response selection and answering questions. The major challenges involve noisy history contexts and especial prerequisites of commonsense knowledge that is unseen in the given material. Existing works mainly focus on context and response matching approaches. This work thus makes the first attempt to tackle the above two challenges by extracting substantially important turns as pivot utterances and utilizing external knowledge to enhance the representation of context. We propose a pivot-oriented deep selection model (PoDS) on top of the Transformer-based language models for dialogue comprehension. In detail, our model first picks out the pivot utterances from the conversation history according to the semantic matching with the candidate response or question, if any. Besides, knowledge items related to the dialogue context are extracted from a knowledge graph as external knowledge. Then, the pivot utterances and the external knowledge are combined together with a well-designed mechanism for refining predictions. Experimental results on four dialogue comprehension benchmark tasks show that our proposed model achieves great improvements on baselines. A series of empirical comparisons are conducted to show how our selection strategies and the extra knowledge injection influence the results.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1161–1173},
numpages = {13}
}

@inproceedings{10.1145/3539618.3592092,
author = {Lin, Hsien-Chin and Feng, Shutong and Geishauser, Christian and Lubis, Nurul and van Niekerk, Carel and Heck, Michael and Ruppik, Benjamin and Vukovic, Renato and Gasi\'{c}, Milica},
title = {EmoUS: Simulating User Emotions in Task-Oriented Dialogues},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592092},
doi = {10.1145/3539618.3592092},
abstract = {Existing user simulators (USs) for task-oriented dialogue systems only model user behaviour on semantic and natural language levels without considering the user persona and emotions. Optimising dialogue systems with generic user policies, which cannot model diverse user behaviour driven by different emotional states, may result in a high drop-off rate when deployed in the real world. Thus, we present EmoUS, a user simulator that learns to simulate user emotions alongside user behaviour. EmoUS generates user emotions, semantic actions, and natural language responses based on the user goal, the dialogue history, and the user persona. By analysing what kind of system behaviour elicits what kind of user emotions, we show that EmoUS can be used as a probe to evaluate a variety of dialogue systems and in particular their effect on the user's emotional state. Developing such methods is important in the age of large language model chat-bots and rising ethical concerns.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2526–2531},
numpages = {6},
keywords = {dialogue system, emotion simulation, user simulation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1109/TASLP.2022.3145320,
author = {Liu, Yongkang and Huang, Qingbao and Li, Jing and Mo, Linzhang and Cai, Yi and Li, Qing},
title = {SSAP: Storylines and Sentiment Aware Pre-Trained Model for Story Ending Generation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3145320},
doi = {10.1109/TASLP.2022.3145320},
abstract = {As an interesting but under-explored task, story ending generation aims at generating an appropriate ending for an incomplete story. The challenges of the task are to deeply understand the story context, mine the storylines hidden in the story, and generate rational endings in logic and sentiment. Although existing pre-trained approaches have been proven effective to this task, how to learn to generate endings with appropriate plots and sufficient sentimental information still remains a major challenge. One possible reason is that an over reliance on external commonsense knowledge beyond the storylines and sentimental trends information hidden in the story context could lead to generation deviating from the main theme. To address this issue, we propose a two-stage &lt;bold&gt;S&lt;/bold&gt;troylines and &lt;bold&gt;S&lt;/bold&gt;entiment &lt;bold&gt;A&lt;/bold&gt;ware &lt;bold&gt;P&lt;/bold&gt;re-trained model (SSAP) for generating sentimentally relevant story endings. We apply a classifier for discriminating the sentiment of the story, and then employ a pre-trained language model, combining with storylines information, to conditionally generate sentences that match both the logic and sentiment of the story. Automatic and manual evaluations show that, without integrating external knowledge, our model can produce more consistent and diverse story endings than state-of-the-art baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {686–694},
numpages = {9}
}

@inproceedings{10.1145/3487351.3489443,
author = {Paschalides, Demetris and Pallis, George and Dikaiakos, Marios D.},
title = {POLAR: a holistic framework for the modelling of polarization and identification of polarizing topics in news media},
year = {2022},
isbn = {9781450391283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487351.3489443},
doi = {10.1145/3487351.3489443},
abstract = {Polarization is an alarming trend in modern societies with serious implications on social cohesion and the democratic process. Typically, polarization manifests itself in the public discourse in politics, governance and ideology. In recent years, however, polarization arises increasingly in a wider range of issues, from identity and culture to healthcare and the environment. As the public and private discourse moves online, polarization feeds in and is fed by phenomena like fake news and hate speech. The identification and analysis of online polarization is challenging because of the massive scale, diversity, and unstructured nature of online content, and the rapid and unpredictable evolution of polarizing issues. Therefore, we need effective ways to identify, quantify, and represent polarization and polarizing topics algorithmically and at scale. In this work, we introduce POLAR - an unsupervised, large-scale framework for modeling and identifying polarizing topics in any domain, without prior domain-specific knowledge. POLAR comprises a processing pipeline that analyzes a corpus of an arbitrary number of news articles to construct a hierarchical knowledge graph that models polarization and identify polarizing topics discussed in the corpus. Our evaluation shows that POLAR is able to identify and rank polarizing topics accurately and efficiently.},
booktitle = {Proceedings of the 2021 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {348–355},
numpages = {8},
keywords = {inter-group conflict, natural language processing, polarization, polarizing topic extraction, signed networks},
location = {Virtual Event, Netherlands},
series = {ASONAM '21}
}

@article{10.1145/3582688,
author = {Song, Yisheng and Wang, Ting and Cai, Puyu and Mondal, Subrota K. and Sahoo, Jyoti Prakash},
title = {A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3582688},
doi = {10.1145/3582688},
abstract = {Few-shot learning (FSL) has emerged as an effective learning method and shows great potential. Despite the recent creative works in tackling FSL tasks, learning valid information rapidly from just a few or even zero samples remains a serious challenge. In this context, we extensively investigated 200+ FSL papers published in top journals and conferences in the past three years, aiming to present a timely and comprehensive overview of the most recent advances in FSL with a fresh perspective and to provide an impartial comparison of the strengths and weaknesses of existing work. To avoid conceptual confusion, we first elaborate and contrast a set of relevant concepts including few-shot learning, transfer learning, and meta-learning. Then, we inventively extract prior knowledge related to few-shot learning in the form of a pyramid, which summarizes and classifies previous work in detail from the perspective of challenges. Furthermore, to enrich this survey, we present in-depth analysis and insightful discussions of recent advances in each subsection. What is more, taking computer vision as an example, we highlight the important application of FSL, covering various research hotspots. Finally, we conclude the survey with unique insights into technology trends and potential future research opportunities to guide FSL follow-up research.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {271},
numpages = {40},
keywords = {Few-shot learning, one-shot learning, zero-shot learning, low-shot learning, meta-learning, prior knowledge}
}

@inproceedings{10.1145/3581783.3612516,
author = {Sun, Zhongfan and Hu, Yongli and Gao, Qingqing and Jiang, Huajie and Gao, Junbin and Sun, Yanfeng and Yin, Baocai},
title = {Breaking the Barrier Between Pre-training and Fine-tuning: A Hybrid Prompting Model for Knowledge-Based VQA},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612516},
doi = {10.1145/3581783.3612516},
abstract = {Considerable performance gains have been achieved for knowledge-based visual question answering due to the visual-language pre-training models with pre-training-then-fine-tuning paradigm. However, because the targets of the pre-training and fine-tuning stages are different, there is an evident barrier that prevents the cross-modal comprehension ability developed in the pre-training stage from fully endowing the fine-tuning task. To break this barrier, in this paper, we propose a novel hybrid prompting model for knowledge-based VQA, which inherits and incorporates the pre-training and fine-tuning tasks with a shared objective. Specifically, based on static declaration prompt, we construct a consistent goal with the fine-tuning via masked language modeling to inherit capabilities of pre-training task, while selecting the top-t relevant knowledge in a dense retrieval manner. Additionally, a dynamic knowledge prompt is learned from retrieved knowledge, which not only alleviates the length constraint on inputs for visual-language pre-trained models but also assists in providing answer features via fine-tuning. Combining and unifying the aims of the two stages could fully exploit the abilities of pre-training and fine-tuning to predict answer. We evaluate the proposed model on the OKVQA dataset, and the result shows that our model outperforms the state-of-the-art methods based on visual-language pre-training models with a noticeable performance gap and even exceeds the large-scale language model of GPT-3, which proves the benefits of the hybrid prompts and the advantages of unifying pre-training to fine-tuning.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {4065–4073},
numpages = {9},
keywords = {knowledge integration, multi-modal fusion, visual question answering},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3477495.3531971,
author = {Gerritse, Emma J. and Hasibi, Faegheh and de Vries, Arjen P.},
title = {Entity-aware Transformers for Entity Search},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531971},
doi = {10.1145/3477495.3531971},
abstract = {Pre-trained language models such as BERT have been a key ingredient to achieve state-of-the-art results on a variety of tasks in natural language processing and, more recently, also in information retrieval. Recent research even claims that BERT is able to capture factual knowledge about entity relations and properties, the information that is commonly obtained from knowledge graphs. This paper investigates the following question: Do BERT-based entity retrieval models benefit from additional entity information stored in knowledge graphs? To address this research question, we map entity embeddings into the same input space as a pre-trained BERT model and inject these entity embeddings into the BERT model. This entity-enriched language model is then employed on the entity retrieval task. We show that the entity-enriched BERT model improves effectiveness on entity-oriented queries over a regular BERT model, establishing a new state-of-the-art result for the entity retrieval task, with substantial improvements for complex natural language queries and queries requesting a list of entities with a certain property. Additionally, we show that the entity information provided by our entity-enriched model particularly helps queries related to less popular entities. Last, we observe empirically that the entity-enriched BERT models enable fine-tuning on limited training data, which otherwise would not be feasible due to the known instabilities of BERT in few-sample fine-tuning, thereby contributing to data-efficient training of BERT for entity search.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1455–1465},
numpages = {11},
keywords = {bert, entity embeddings, entity retrieval, transformers},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3603163.3609075,
author = {Ro\ss{}ner, Daniel and Atzenbeck, Claus and Brooker, Sam},
title = {SPORE: A Storybreaking Machine},
year = {2023},
isbn = {9798400702327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603163.3609075},
doi = {10.1145/3603163.3609075},
abstract = {The paper presents SPORE, a Spatial Recommender System. As we enter a period of unprecedented collaboration between authors and computers, where artificial intelligence in particular seems likely to act increasingly in a co-authoring capacity, SPORE offers a different approach to collaboration. More organic and exploratory than other automated or procedural systems, SPORE aims to mimic the process of storybreaking that already exists in the creative industries.},
booktitle = {Proceedings of the 34th ACM Conference on Hypertext and Social Media},
articleno = {1},
numpages = {6},
keywords = {Mother, education, hypertext, linguistics, recommender system, spatial hypertext, storytelling, tropes},
location = {Rome, Italy},
series = {HT '23}
}

@inproceedings{10.1145/3539597.3570415,
author = {Wu, Taiqiang and Bai, Xingyu and Guo, Weigang and Liu, Weijie and Li, Siheng and Yang, Yujiu},
title = {Modeling Fine-grained Information via Knowledge-aware Hierarchical Graph for Zero-shot Entity Retrieval},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3570415},
doi = {10.1145/3539597.3570415},
abstract = {Zero-shot entity retrieval, aiming to link mentions to candidate entities under the zero-shot setting, is vital for many tasks in Natural Language Processing. Most existing methods represent mentions/entities via the sentence embeddings of corresponding context from the Pre-trained Language Model. However, we argue that such coarse-grained sentence embeddings can not fully model the mentions/entities, especially when the attention scores towards mentions/entities are relatively low. In this work, we propose GER, a Graph enhanced Entity Retrieval framework, to capture more fine-grained information as complementary to sentence embeddings. We extract the knowledge units from the corresponding context and then construct a mention/entity centralized graph. Hence, we can learn the fine-grained information about mention/entity by aggregating information from these knowledge units. To avoid the graph bottleneck for the central mention/entity node, we construct a hierarchical graph and design a novel Hierarchical Graph Attention Network~(HGAN). Experimental results on popular benchmarks demonstrate that our proposed GER framework performs better than previous state-of-the-art models.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1021–1029},
numpages = {9},
keywords = {fine-grained information, zero-shot entity retrieval},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3581783.3611898,
author = {Xi, Nan and Meng, Jingjing and Yuan, Junsong},
title = {Chain-of-Look Prompting for Verb-centric Surgical Triplet Recognition in Endoscopic Videos},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611898},
doi = {10.1145/3581783.3611898},
abstract = {Surgical triplet recognition aims to recognize surgical activities as triplets (i.e.,&lt;instrument, verb, target &gt;), which provides fine-grained information essential for surgical scene understanding. Existing methods for surgical triplet recognition rely on compositional methods that recognize the instrument, verb, and target simultaneously. In contrast, our method, called chain-of-look prompting, casts the problem of surgical triplet recognition as visual prompt generation from large-scale vision-language (VL) models, and explicitly decomposes the task into a series of video reasoning processes. Chain-of-Look prompting is inspired by: (1) the chain-of-thought prompting in natural language processing, which divides a problem into a sequence of intermediate reasoning steps; (2) the inter-dependency between motion and visual appearance in the human vision system. Since surgical activities are conveyed by the actions of physicians, we regard the verbs as the carrier of semantics in surgical endoscopic videos. Additionally, we utilize the BioMed large language model to calibrate the generated visual prompt features for surgical scenarios. Our approach captures the visual reasoning processes underlying surgical activities and achieves better performance compared to the state-of-the-art methods on the largest surgical triplet recognition dataset, CholecT50. The code is available at https://github.com/southnx/CoLSurgical.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5007–5016},
numpages = {10},
keywords = {chain-of-look prompting, endoscopic videos, surgical triplet recognition, verb-centric},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.1109/TASLP.2021.3079812,
author = {Xie, Zhiwen and Zhu, Runjie and Liu, Jin and Zhou, Guangyou and Huang, Jimmy Xiangji},
title = {Hierarchical Neighbor Propagation With Bidirectional Graph Attention Network for Relation Prediction},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3079812},
doi = {10.1109/TASLP.2021.3079812},
abstract = {The graph attention network (GAT) &lt;xref ref-type="bibr" rid="ref1"&gt;[1]&lt;/xref&gt; has started to become a mainstream neural network architecture since 2018, yielding remarkable performance gains in various natural language processing (NLP) tasks. Although GAT has reached the state-of-the-art (SOTA) performance as a recent success in &lt;italic&gt;relation prediction&lt;/italic&gt; in knowledge graph, the current model is still limited by the following two aspects: (1) the existing model only considers the neighbors from the inbound-direction of the given entity, but ignores the rich neighborhood information from outbound-directions; (2) the existing model only uses the &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt;-th hop output to learn the multi-hop embeddings, which leads to the loss of a large amount of early-stage embedding information (e.g., one-hop) at the graph attention step. In this study, we propose a novel bidirectional graph attention network (BiGAT) to learn the hierarchical neighbor propagation. In our proposed BiGAT, an inbound-directional GAT and an outbound-directional GAT are introduced to capture sufficient neighborhood information before propagating the bidirectional neighborhood information to learn the multi-hop feature embeddings in a hierarchical manner. Experiments conducted on the four publicly available datasets show that BiGAT achieves the competitive results in comparison to other SOTA methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1762–1773},
numpages = {12}
}

@inproceedings{10.1145/3437963.3441748,
author = {Vakulenko, Svitlana and Longpre, Shayne and Tu, Zhucheng and Anantha, Raviteja},
title = {Question Rewriting for Conversational Question Answering},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441748},
doi = {10.1145/3437963.3441748},
abstract = {Conversational question answering (QA) requires the ability to correctly interpret a question in the context of previous conversation turns. We address the conversational QA task by decomposing it into question rewriting and question answering subtasks. The question rewriting (QR) subtask is specifically designed to reformulate ambiguous questions, which depend on the conversational context, into unambiguous questions that can be correctly interpreted outside of the conversational context. We introduce a conversational QA architecture that sets the new state of the art on the TREC CAsT 2019 passage retrieval dataset. Moreover, we show that the same QR model improves QA performance on the QuAC dataset with respect to answer span extraction, which is the next step in QA after passage retrieval. Our evaluation results indicate that the QR model we proposed achieves near human-level performance on both datasets and the gap in performance on the end-to-end conversational QA task is attributed mostly to the errors in QA.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {355–363},
numpages = {9},
keywords = {conversational search, question answering, question rewriting},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@article{10.1109/TASLP.2022.3140482,
author = {Mao, Qianren and Li, Jianxin and Lin, Chenghua and Chen, Congwen and Peng, Hao and Wang, Lihong and Yu, Philip S.},
title = {Adaptive Pre-Training and Collaborative Fine-Tuning: A Win-Win Strategy to Improve Review Analysis Tasks},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3140482},
doi = {10.1109/TASLP.2022.3140482},
abstract = {Summarizing user reviews and classifying user sentiment are two critical tasks for modern e-commerce platforms. These two tasks can benefit each other by capturing the shared linguistic features. However, such a relationship has not been fully exploited by existing research on domain-specific contextual representations. This work explores a win-win strategy for a multi-task framework with three stages: general pre-training, adaptive pre-training, and collaborative fine-tuning. The task-adaptive continual pre-training on a language model can obtain domain-specific contextual representations, further used to improve two related tasks, sentiment classification and review summarization during the collaborative fine-tuning. Meanwhile, to effectively capture sentiment-oriented domain-specific contextual representations, we introduce a novel task-adaptive pre-training procedure, which adds a sentiment prediction task during the adaptive pre-training. Extensive experiments conducted on two adaption scenarios of a general-to-single domain and a general-to-multiple domain show that our framework outperforms state-of-the-art methods.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {622–634},
numpages = {13}
}

@inproceedings{10.1145/3498891.3501259,
author = {Aranovich, Ra\'{u}l and Wu, Muting and Yu, Dian and Katsy, Katya and Ahmadnia, Benyamin and Bishop, Matthew and Filkov, Vladimir and Sagae, Kenji},
title = {Beyond NVD: Cybersecurity meets the Semantic Web.},
year = {2022},
isbn = {9781450385732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498891.3501259},
doi = {10.1145/3498891.3501259},
abstract = {Cybersecurity experts rely on the knowledge stored in databases like the NVD to do their work, but these are not the only sources of information about threats and vulnerabilities. Much of that information flows through social media channels. In this paper we argue that security experts and general users alike can benefit from the technologies of the Semantic Web, merging heterogeneous sources of knowledge in an ontological representation. We present a system that has an ontology of vulnerabilities at its core, but that is enhanced with NLP tools to identify cybersecurity-related information in social media and to launch queries over heterogeneous data sources. The transformative power of Semantic Web technologies for cybersecurity, which has been proven in the biomedical field, is evaluated and discussed.},
booktitle = {Proceedings of the 2021 New Security Paradigms Workshop},
pages = {59–69},
numpages = {11},
keywords = {cybersecurity, neural networks, nlp, ontology, social media},
location = {Virtual Event, USA},
series = {NSPW '21}
}

@inproceedings{10.1145/3586183.3606779,
author = {Veluri, Bandhav and Itani, Malek and Chan, Justin and Yoshioka, Takuya and Gollakota, Shyamnath},
title = {Semantic Hearing: Programming Acoustic Scenes with Binaural&nbsp;Hearables},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606779},
doi = {10.1145/3586183.3606779},
abstract = {Imagine being able to listen to the birds chirping in a park without hearing the chatter from other hikers, or being able to block out traffic noise on a busy street while still being able to hear emergency sirens and car honks. We introduce semantic hearing, a novel capability for hearable devices that enables them to, in real-time, focus on, or ignore, specific sounds from real-world environments, while also preserving the spatial cues. To achieve this, we make two technical contributions: 1) we present the first neural network that can achieve binaural target sound extraction in the presence of interfering sounds and background noise, and 2) we design a training methodology that allows our system to generalize to real-world use. Results show that our system can operate with 20 sound classes and that our transformer-based network has a runtime of 6.56 ms on a connected smartphone. In-the-wild evaluation with participants in previously unseen indoor and outdoor scenarios shows that our proof-of-concept system can extract the target sounds and generalize to preserve the spatial cues in its binaural output. Project page with code: https://semantichearing.cs.washington.edu},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {89},
numpages = {15},
keywords = {Spatial computing, attention, binaural target sound extraction, causal neural networks, earable computing, noise cancellation},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3442381.3449860,
author = {Ye, Muchao and Cui, Suhan and Wang, Yaqing and Luo, Junyu and Xiao, Cao and Ma, Fenglong},
title = {MedPath: Augmenting Health Risk Prediction via Medical Knowledge Paths},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449860},
doi = {10.1145/3442381.3449860},
abstract = {The broad adoption of electronic health records (EHR) data and the availability of biomedical knowledge graphs (KGs) on the web have provided clinicians and researchers unprecedented resources and opportunities for conducting health risk predictions to improve healthcare quality and medical resource allocation. Existing methods have focused on improving the EHR feature representations using attention mechanisms, time-aware models, or external knowledge. However, they ignore the importance of using personalized information to make predictions. Besides, the reliability of their prediction interpretations needs to be improved since their interpretable attention scores are not explicitly reasoned from disease progression paths. In this paper, we propose MedPath to solve these challenges and augment existing risk prediction models with the ability to use personalized information and provide reliable interpretations inferring from disease progression paths. Firstly, MedPath extracts personalized knowledge graphs (PKGs) containing all possible disease progression paths from observed symptoms to target diseases from a large-scale online medical knowledge graph. Next, to augment existing EHR encoders for achieving better predictions, MedPath learns a PKG embedding by conducting multi-hop message passing from symptom nodes to target disease nodes through a graph neural network encoder. Since MedPath reasons disease progression by paths existing in PKGs, it can provide explicit explanations for the prediction by pointing out how observed symptoms can finally lead to target diseases. Experimental results on three real-world medical datasets show that MedPath is effective in improving the performance of eight state-of-the-art methods with higher F1 scores and AUCs. Our case study also demonstrates that MedPath can greatly improve the explicitness of the risk prediction interpretation.1},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1397–1409},
numpages = {13},
keywords = {graph neural network, healthcare informatics, model interpretability, risk prediction},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3581641.3584088,
author = {Brachman, Michelle and Pan, Qian and Do, Hyo Jin and Dugan, Casey and Chaudhary, Arunima and Johnson, James M. and Rai, Priyanshu and Chakraborti, Tathagata and Gschwind, Thomas and Laredo, Jim A and Miksovic, Christoph and Scotton, Paolo and Talamadupula, Kartik and Thomas, Gegi},
title = {Follow the Successful Herd: Towards Explanations for Improved Use and Mental Models of Natural Language Systems},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584088},
doi = {10.1145/3581641.3584088},
abstract = {While natural language systems continue improving, they are still imperfect. If a user has a better understanding of how a system works, they may be able to better accomplish their goals even in imperfect systems. We explored whether explanations can support effective authoring of natural language utterances and how those explanations impact users’ mental models in the context of a natural language system that generates small programs. Through an online study (n=252), we compared two main types of explanations: 1) system-focused, which provide information about how the system processes utterances and matches terms to a knowledge base, and 2) social, which provide information about how other users have successfully interacted with the system. Our results indicate that providing social suggestions of terms to add to an utterance helped users to repair and generate correct flows more than system-focused explanations or social recommendations of words to modify. We also found that participants commonly understood some mechanisms of the natural language system, such as the matching of terms to a knowledge base, but they often lacked other critical knowledge, such as how the system handled structuring and ordering. Based on these findings, we make design recommendations for supporting interactions with and understanding of natural language systems.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {220–239},
numpages = {20},
keywords = {AI explainability, mental models, natural language interaction},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@article{10.1145/3611651,
author = {Wang, Benyou and Xie, Qianqian and Pei, Jiahuan and Chen, Zhihong and Tiwari, Prayag and Li, Zhao and Fu, Jie},
title = {Pre-trained Language Models in Biomedical Domain: A Systematic Survey},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3611651},
doi = {10.1145/3611651},
abstract = {Pre-trained language models (PLMs) have been the de facto paradigm for most natural language processing tasks. This also benefits the biomedical domain: researchers from informatics, medicine, and computer science communities propose various PLMs trained on biomedical datasets, e.g., biomedical text, electronic health records, protein, and DNA sequences for various biomedical tasks. However, the cross-discipline characteristics of biomedical PLMs hinder their spreading among communities; some existing works are isolated from each other without comprehensive comparison and discussions. It is nontrivial to make a survey that not only systematically reviews recent advances in biomedical PLMs and their applications but also standardizes terminology and benchmarks. This article summarizes the recent progress of pre-trained language models in the biomedical domain and their applications in downstream biomedical tasks. Particularly, we discuss the motivations of PLMs in the biomedical domain and introduce the key concepts of pre-trained language models. We then propose a taxonomy of existing biomedical PLMs that categorizes them from various perspectives systematically. Plus, their applications in biomedical downstream tasks are exhaustively discussed, respectively. Last, we illustrate various limitations and future trends, which aims to provide inspiration for the future research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {55},
numpages = {52},
keywords = {Biomedical domain, pre-trained language models, natural language processing}
}

@inproceedings{10.1145/3532106.3533533,
author = {Gero, Katy Ilonka and Liu, Vivian and Chilton, Lydia},
title = {Sparks: Inspiration for Science Writing using Language Models},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533533},
doi = {10.1145/3532106.3533533},
abstract = {Large-scale language models are rapidly improving, performing well on a wide variety of tasks with little to no customization. In this work we investigate how language models can support science writing, a challenging writing task that is both open-ended and highly constrained. We present a system for generating “sparks”, sentences related to a scientific concept intended to inspire writers. We find that our sparks are more coherent and diverse than a competitive language model baseline, and approach a human-written gold standard. We run a user study with 13 STEM graduate students writing on topics of their own selection and find three main use cases of sparks—inspiration, translation, and perspective—each of which correlates with a unique interaction pattern. We also find that while participants were more likely to select higher quality sparks, the average quality of sparks seen by a given participant did not correlate with their satisfaction with the tool. We end with a discussion about what impacts human satisfaction with AI support tools, considering participant attitudes towards influence, their openness to technology, as well as issues of plagiarism, trustworthiness, and bias in AI.},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {1002–1019},
numpages = {18},
keywords = {co-creativity, creativity support tools, natural language processing, science writing, writing support},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@article{10.1109/TASLP.2022.3153255,
author = {Gao, Silin and Takanobu, Ryuichi and Bosselut, Antoine and Huang, Minlie},
title = {End-to-End Task-Oriented Dialog Modeling With Semi-Structured Knowledge Management},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153255},
doi = {10.1109/TASLP.2022.3153255},
abstract = {Current task-oriented dialog (TOD) systems mostly manage structured knowledge (e.g. databases and tables) to guide the goal-oriented conversations. However, they fall short of handling dialogs which also involve unstructured knowledge (e.g. reviews and documents). In this article, we formulate a task of modeling TOD grounded on a fusion of structured and unstructured knowledge. To address this task, we propose a TOD system with semi-structured knowledge management, SeKnow, which extends the belief state to manage knowledge with both structured and unstructured contents. Furthermore, we introduce two implementations of SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained language model, respectively. Both implementations use the end-to-end manner to jointly optimize dialog modeling grounded on structured and unstructured knowledge. We conduct experiments on a modified version of MultiWOZ 2.1 dataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve semi-structured knowledge. Experimental results show that SeKnow has strong performances in both end-to-end dialog and intermediate knowledge management, compared to existing TOD systems and their extensions with pipeline knowledge management schemes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {2173–2187},
numpages = {15}
}

@inproceedings{10.1145/3488560.3498514,
author = {Zhou, Yuanhang and Zhou, Kun and Zhao, Wayne Xin and Wang, Cheng and Jiang, Peng and Hu, He},
title = {C²-CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498514},
doi = {10.1145/3488560.3498514},
abstract = {Conversational recommender systems (CRS) aim to recommend suitable items to users through natural language conversations. For developing effective CRSs, a major technical issue is how to accurately infer user preference from very limited conversation context. To address issue, a promising solution is to incorporate external data for enriching the context information. However, prior studies mainly focus on designing fusion models tailored for some specific type of external data, which is not general to model and utilize multi-type external data. To effectively leverage multi-type external data, we propose a novel coarse-to-fine contrastive learning framework to improve data semantic fusion for CRS. In our approach, we first extract and represent multi-grained semantic units from different data signals, and then align the associated multi-type semantic units in a coarse-to-fine way. To implement this framework, we design both coarse-grained and fine-grained procedures for modeling user preference, where the former focuses on more general, coarse-grained semantic fusion and the latter focuses on more specific, fine-grained semantic fusion. Such an approach can be extended to incorporate more kinds of external data. Extensive experiments on two public CRS datasets have demonstrated the effectiveness of our approach in both recommendation and conversation tasks.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1488–1496},
numpages = {9},
keywords = {contrastive learning, conversational recommender system},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3442381.3450090,
author = {Wang, Daheng and Shiralkar, Prashant and Lockard, Colin and Huang, Binxuan and Dong, Xin Luna and Jiang, Meng},
title = {TCN: Table Convolutional Network for Web Table Interpretation},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450090},
doi = {10.1145/3442381.3450090},
abstract = {Information extraction from semi-structured webpages provides valuable long-tailed facts for augmenting knowledge graph. Relational Web tables are a critical component containing additional entities and attributes of rich and diverse knowledge. However, extracting knowledge from relational tables is challenging because of sparse contextual information. Existing work linearize table cells and heavily rely on modifying deep language models such as BERT which only captures related cells information in the same table. In this work, we propose a novel relational table representation learning approach considering both the intra- and inter-table contextual information. On one hand, the proposed Table Convolutional Network model employs the attention mechanism to adaptively focus on the most informative intra-table cells of the same row or column; and, on the other hand, it aggregates inter-table contextual information from various types of implicit connections between cells across different tables. Specifically, we propose three novel aggregation modules for (i) cells of the same value, (ii) cells of the same schema position, and (iii) cells linked to the same page topic. We further devise a supervised multi-task training objective for jointly predicting column type and pairwise column relation, as well as a table cell recovery objective for pre-training. Experiments on real Web table datasets demonstrate our method can outperform competitive baselines by of F1 for column type prediction and by of F1 for pairwise column relation prediction.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {4020–4032},
numpages = {13},
keywords = {Web table, information extraction, knowledge extraction},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3523181.3523197,
author = {Jiang, Shan and Wu, Huanhuan and Luo, Lingyun},
title = {Infusing Biomedical Knowledge into BERT for Chinese Biomedical NLP Tasks with Adversarial Training},
year = {2022},
isbn = {9781450387453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523181.3523197},
doi = {10.1145/3523181.3523197},
abstract = {Biomedical text mining is becoming increasingly important. Recently, biomedical pre-trained language models such as BioBERT and SciBERT, which can capture biomedical knowledge from text, have achieved promising results in biomedical NLP tasks. However, most biomedical pre-trained language models rely on the traditional masked language model (MLM) pre-training strategy, which cannot fully capture the semantic relations of context. It is challenging to learn biomedical knowledge via language models in the Chinese biomedical fields due to the lack of training resources and the extreme complexity and diversity of Chinese medical terminologies. To this end, we propose MedBERT-adv, which utilizes a biomedical knowledge infusion method that can effectively complement BERT-like models. Instead of using time-consuming medical expert annotation and inaccurate automatic annotation, we use the article structure in Baidu Encyclopedia as a weakly supervised signal, utilizing each medical term and its category as labels to pre-train the model. We also leverage adversarial training strategies like FGM for fine-tuning downstream tasks to further improve the performance of MedBERT-adv. We experimented with MedBERT-adv on the Chinese biomedical dataset CBLUE using eight NLP tasks. Among all of them, our proposed model obtained an average 1.8% improvement in average score than four baseline models, demonstrating the effectiveness of MedBERT-adv on Chinese biomedical text mining.},
booktitle = {2022 3rd Asia Service Sciences and Software Engineering Conference},
pages = {108–114},
numpages = {7},
location = {Macau, Macao},
series = {ASSE' 22}
}

@inproceedings{10.1145/3532213.3532254,
author = {Zhang, Xiangliang and Jia, Yangli and Zhang, Zhenling and Kang, Qi and Zhang, Yongchen and Jia, Hongling},
title = {Improving End-to-End Biomedical Question Answering System},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532254},
doi = {10.1145/3532213.3532254},
abstract = {Biomedical question answering refers to extracting an answer based on given questions and related documents. Existing biomedical question answering research either focuses on a specific stage, such as machine reading comprehension, or uses traditional rule-based methods and ontology with complex construction processes. In this paper, we demonstrate the application of simple but powerful neural-based approaches in improving the end-to-end biomedical question answering system. We employ the BM25-based documents retriever, BERT-based neural ranker, and an answer extraction stage using the BioBERT pre-trained language model. In view of the lack of sufficient training data in the biomedical domain, domain adaptation and data augmentation are adopted to address the question answering task, so as to further reinforce the system performance. Based on our self-built standard large-volume retrieve corpus and neural ranker corpus, we get competitive results on BioASQ8b.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {274–279},
numpages = {6},
keywords = {Biomedical question answering, Data augmentation, Document re-ranking},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1145/3477495.3532037,
author = {Zhao, Mengxue and Yang, Yang and Li, Miao and Wang, Jingang and Wu, Wei and Ren, Pengjie and de Rijke, Maarten and Ren, Zhaochun},
title = {Personalized Abstractive Opinion Tagging},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532037},
doi = {10.1145/3477495.3532037},
abstract = {An opinion tag is a sequence of words on a specific aspect of a product or service. Opinion tags reflect key characteristics of product reviews and help users quickly understand their content in e-commerce portals. The task of abstractive opinion tagging has previously been proposed to automatically generate a ranked list of opinion tags for a given review. However, current models for opinion tagging are not personalized, even though personalization is an essential ingredient of engaging user interactions, especially in e-commerce. In this paper, we focus on the task of personalized abstractive opinion tagging. There are two main challenges when developing models for the end-to-end generation of personalized opinion tags: sparseness of reviews and difficulty to integrate multi-type signals, i.e., explicit review signals and implicit behavioral signals. To address these challenges, we propose an end-to-end model, named POT, that consists of three main components: (1) a review-based explicit preference tracker component based on a hierarchical heterogeneous review graph to track user preferences from reviews; (2)a behavior-based implicit preference tracker component using a heterogeneous behavior graph to track the user preferences from implicit behaviors; and (3) a personalized rank-aware tagging component to generate a ranked sequence of personalized opinion tags. In our experiments, we evaluate POT on a real-world dataset collected from e-commerce platforms and the results demonstrate that it significantly outperforms strong baselines.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1066–1076},
numpages = {11},
keywords = {abstractive summarization, e-commerce, personalization, review analysis},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3443279.3443314,
author = {Wan, Tianyu and Wang, Wenhui and Zhou, Hui},
title = {Research on Information Extraction of Municipal Solid Waste Crisis using BERT-LSTM-CRF},
year = {2021},
isbn = {9781450377607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443279.3443314},
doi = {10.1145/3443279.3443314},
abstract = {There is much research on the phenomenon of municipal solid waste (MSW) and its improvement measures, and the method of information extraction be adopted to obtain the potential knowledge of MSW from the existing relevant research literature. Due to the complexity and diversity of the MSW, unsupervised training of target texts can be achieved through information data based on manual annotation. According to the characteristics of the BERT language model, a common method in natural language processing(NLP), the pre-trained BERT(Bidirectional Encoder Representation from Transformers) model with LSTM-CRF(Long Short Term Memory-Conditional Random Field) architecture is used in the information extraction of MSW crisis to extract entities and relationships between entities from natural language texts. By the method of calculating and evaluating the extraction effect, it provided technical support for further study of its crisis conversion.},
booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
pages = {205–209},
numpages = {5},
keywords = {Bert-lstm-crf model, Information Extraction, Machine learning, Solid Waste Crisis},
location = {Seoul, Republic of Korea},
series = {NLPIR '20}
}

@article{10.1109/TASLP.2022.3197316,
author = {Gan, Leilei and Teng, Zhiyang and Zhang, Yue and Zhu, Linchao and Wu, Fei and Yang, Yi},
title = {SemGloVe: Semantic Co-Occurrences for GloVe From BERT},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3197316},
doi = {10.1109/TASLP.2022.3197316},
abstract = {GloVe learns word embeddings by leveraging statistical information from word co-occurrence matrices. However, word pairs in the matrices are extracted from a predefined local context window, which might lead to limited word pairs and potentially semantic irrelevant word pairs. In this paper, we propose &lt;italic&gt;SemGloVe&lt;/italic&gt;, which distills &lt;italic&gt;semantic co-occurrences&lt;/italic&gt; from BERT into static GloVe word embeddings. Particularly, we propose two models to extract co-occurrence statistics based on either the masked language model or the multi-head attention weights of BERT. Our methods can extract word pairs limited by the local window assumption, and can define the co-occurrence weights by directly considering the semantic distance between word pairs. Experiments on several word similarity datasets and external tasks show that SemGloVe can outperform GloVe.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2696–2704},
numpages = {9}
}

@article{10.1109/TASLP.2022.3224286,
author = {Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
title = {Minimising Biasing Word Errors for Contextual ASR With the Tree-Constrained Pointer Generator},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3224286},
doi = {10.1109/TASLP.2022.3224286},
abstract = {Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {345–354},
numpages = {10}
}

@article{10.1109/TCBB.2021.3108718,
author = {Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice C. and Mofrad, Mohammad R.K.},
title = {TripletProt: Deep Representation Learning of Proteins Based On Siamese Networks},
year = {2021},
issue_date = {Nov.-Dec. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3108718},
doi = {10.1109/TCBB.2021.3108718},
abstract = {Pretrained representations have recently gained attention in various machine learning applications. Nonetheless, the high computational costs associated with training these models have motivated alternative approaches for representation learning. Herein we introduce TripletProt, a new approach for protein representation learning based on the Siamese neural networks. Representation learning of biological entities which capture essential features can alleviate many of the challenges associated with supervised learning in bioinformatics. The most important distinction of our proposed method is relying on the protein-protein interaction (PPI) network. The computational cost of the generated representations for any potential application is significantly lower than comparable methods since the length of the representations is significantly smaller than that in other approaches. TripletProt offers great potentials for the protein informatics tasks and can be widely applied to similar tasks. We evaluate TripletProt comprehensively in protein functional annotation tasks including sub-cellular localization (14 categories) and gene ontology prediction (more than 2000 classes), which are both challenging multi-class, multi-label classification machine learning problems. We compare the performance of TripletProt with the state-of-the-art approaches including a recurrent language model-based approach (i.e., UniRep), as well as a protein-protein interaction (PPI) network and sequence-based method (i.e., DeepGO). Our TripletProt showed an overall improvement of F1 score in the above mentioned comprehensive functional annotation tasks, solely relying on the PPI network. Availability: The source code and datasets are available at &lt;uri&gt;https://github.com/EsmaeilNourani/TripletProt&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = aug,
pages = {3744–3753},
numpages = {10}
}

@article{10.1145/3588767,
author = {Huang, Hu and Zhang, Bowen and Li, Yangyang and Zhang, Baoquan and Sun, Yuxi and Luo, Chuyao and Peng, Cheng},
title = {Knowledge-enhanced Prompt-tuning for Stance Detection},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3588767},
doi = {10.1145/3588767},
abstract = {Investigating public attitudes on social media is important in opinion mining systems. Stance detection aims to analyze the attitude of an opinionated text (e.g., favor, neutral, or against) toward a given target. Existing methods mainly address this problem from the perspective of fine-tuning. Recently, prompt-tuning has achieved success in natural language processing tasks. However, conducting prompt-tuning methods for stance detection in real-world remains a challenge for several reasons: (1) The text form of stance detection is usually short and informal, which makes it difficult to design label words for the verbalizer. (2) The tweet text may not explicitly give the attitude. Instead, users may use various hashtags or background knowledge to express stance-aware perspectives. In this article, we first propose a prompt-tuning-based framework that performs stance detection in a cloze question manner. Specifically, a knowledge-enhanced prompt-tuning framework (KEprompt) method is designed, which consists of an automatic verbalizer (AutoV) and background knowledge injection (BKI). Specifically, in AutoV, we introduce a semantic graph to build a better mapping from the predicted word of the pretrained language model and detection labels. In BKI, we first propose a topic model for learning hashtag representation and introduce ConceptGraph as the supplement of the target. At last, we present a challenging dataset for stance detection, where all stance categories are expressed in an implicit manner. Extensive experiments on a large real-world dataset demonstrate the superiority of KEprompt over state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {159},
numpages = {20},
keywords = {Stance detection, deep learning, prompt-tuning framework}
}

@inproceedings{10.1145/3446132.3446397,
author = {Shi, Xin and Zeng, Xiaoyang and Wu, Jie and Hou, Mengshu and Zhu, Hao},
title = {Context Event Features and Event Embedding Enhanced Event Detection},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446397},
doi = {10.1145/3446132.3446397},
abstract = {Extracting valuable information from text has always been a hot point for research and event detection is an essential subtask of information extraction. Most existing methods of event detection only focus on sentence-level information and do not consider the correlation between different event types. To address these problems, in this paper, we propose a novel pre-trained language model based event detection framework named CFEE that utilizes document-level information and event correlation to enhance the event detection task. To obtain event correlation, we project all event types into a shared semantic space through a Skip-gram model, where the event correlation can be represented as the distance between event embeddings. In order to capture document-level information, we utilize a bidirectional recurrent neural network to fuse the context information. Experiments on the ACE2005 dataset demonstrate that our proposed model is better than most existing methods, and also demonstrate the effectiveness of event correlation and document-level information.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {70},
numpages = {6},
keywords = {Bert, Document-level Information, Event Embedding, Event detection},
location = {Sanya, China},
series = {ACAI '20}
}

@inproceedings{10.1145/3487553.3524237,
author = {Schelb, Julian and Ehrmann, Maud and Romanello, Matteo and Spitz, Andreas},
title = {ECCE:&nbsp;Entity-centric&nbsp;Corpus&nbsp;Exploration Using&nbsp;Contextual&nbsp;Implicit&nbsp;Networks},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524237},
doi = {10.1145/3487553.3524237},
abstract = {In the Digital Age, the analysis and exploration of unstructured document collections is of central importance to members of investigative professions, whether they might be scholars, journalists, paralegals, or analysts. In many of their domains, entities play a key role in the discovery of implicit relations between the contents of documents and thus serve as natural entry points to a detailed manual analysis, such as the prototypical 5Ws in journalism or stock symbols in finance. To assist in these analyses, entity-centric networks have been proposed as a language model that represents document collections as a cooccurrence graph of entities and terms, and thereby enables the visual exploration of corpora. Here, we present ECCE, a web-based application that implements entity-centric networks, augments them with contextual language models, and provides users with the ability to upload, manage, and explore document collections. Our application is available as a web-based service at http://dimtools.uni.kn/ecce.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {278–281},
numpages = {4},
keywords = {Entity network, cooccurrence network, corpus exploration},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3442442.3451384,
author = {Nguyen, Nhu Khoa and Boros, Emanuela and Lejeune, Ga\"{e}l and Doucet, Antoine and Delahaut, Thierry},
title = {L3i_LBPAM at the FinSim-2 task: Learning Financial Semantic Similarities with Siamese Transformers},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451384},
doi = {10.1145/3442442.3451384},
abstract = {In this paper, we present the different methods proposed for the FinSIM-2 Shared Task 2021 on Learning Semantic Similarities for the Financial domain. The main focus of this task is to evaluate the classification of financial terms into corresponding top-level concepts (also known as hypernyms) that were extracted from an external ontology. We approached the task as a semantic textual similarity problem. By relying on a siamese network with pre-trained language model encoders, we derived semantically meaningful term embeddings and computed similarity scores between them in a ranked manner. Additionally, we exhibit the results of different baselines in which the task is tackled as a multi-class classification problem. The proposed methods outperformed our baselines and proved the robustness of the models based on textual similarity siamese network.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {302–306},
numpages = {5},
keywords = {Hypernym detection, semantic similarities, siamese networks},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3581783.3612322,
author = {Wang, Jieming and Li, Ziyan and Yu, Jianfei and Yang, Li and Xia, Rui},
title = {Fine-Grained Multimodal Named Entity Recognition and Grounding with a Generative Framework},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612322},
doi = {10.1145/3581783.3612322},
abstract = {Multimodal Named Entity Recognition (MNER) aims to locate and classify named entities mentioned in a pair of text and image. However, most previous MNER works focus on extracting entities in the form of text but failing to ground text symbols to their corresponding visual objects. Moreover, existing MNER studies primarily classify entities into four coarse-grained entity types, which are often insufficient to map them to their real-world referents. To solve these limitations, we introduce a task named Fine-grained Multimodal Named Entity Recognition and Grounding (FMNERG) in this paper, which aims to simultaneously extract named entities in text, their fine-grained entity types, and their grounded visual objects in image. Moreover, we construct a Twitter dataset for the FMNERG task, and further propose a T5-based multImodal GEneration fRamework (TIGER), which formulates FMNERG as a generation problem by converting all the entity-type-object triples into a target sequence and adapts a pre-trained sequence-to-sequence model T5 to directly generate the target sequence from an image-text input pair. Experimental results demonstrate that TIGER performs significantly better than a number of baseline systems on the annotated Twitter dataset. Our dataset annotation and source code are publicly released at https://github.com/NUSTM/FMNERG.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {3934–3943},
numpages = {10},
keywords = {fine-grained named entity recognition, generative framework, multimodal named entity recognition, visual grounding},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@proceedings{10.1145/3622758,
title = {Onward! 2023: Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2023), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet fully proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.  

Onward! 2023 is co-located with SPLASH 2023, running from Sunday 22nd of October till Friday 27th of October, in Cascais, Portugal. We are delighted to have Felienne Hermans giving the Onward! keynote, on Wednesday 25th of October, on "Creating a learnable and inclusive programming language".  

All papers and essays that lie here before you received at least three reviews, leading to a decision of accept, reject, or conditional accept. Authors of conditionally accepted papers were provided with explicit requirements for acceptance, and were carefully re-reviewed in the second phase. The essays track received six submissions, out of which four were accepted. The papers track accepted nine out of nineteen submissions.  

We hope that the papers and essays in these proceedings will stimulate and challenge your thinking about programming and software engineering, and we are looking forward to many discussions at the conference.},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3447548.3467164,
author = {Lin, Rongmei and He, Xiang and Feng, Jie and Zalmout, Nasser and Liang, Yan and Xiong, Li and Dong, Xin Luna},
title = {PAM: Understanding Product Images in Cross Product Category Attribute Extraction},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467164},
doi = {10.1145/3447548.3467164},
abstract = {Understanding product attributes plays an important role in improving online shopping experience for customers and serves asan integral part for constructing a product knowledge graph. Most existing methods focus on attribute extraction from text description or utilize visual information from product images such as shape and color. Compared to the inputs considered in prior works, a product image in fact contains more information, represented by a rich mixture of words and visual clues with a layout carefully designed to impress customers. This work proposes a more inclusive framework that fully utilizes these different modalities for attribute extraction.Inspired by recent works in visual question answering, we use a transformer based sequence to sequence model to fuse representations of product text, Optical Character Recognition (OCR) tokens and visual objects detected in the product image. The framework is further extended with the capability to extract attribute value across multiple product categories with a single model, by training the decoder to predict both product category and attribute value and conditioning its output on product category. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. We evaluated the model on two product attributes, one with many possible values and one with a small set of possible values, over 14 product categories and found the model could achieve 15% gain on the Recall and 10% gain on the F1 score compared to existing methods using text-only features.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3262–3270},
numpages = {9},
keywords = {e-commerce, knowledge extraction, multi-modality learning},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3571884.3604313,
author = {Kernan Freire, Samuel and Foosherian, Mina and Wang, Chaofan and Niforatos, Evangelos},
title = {Harnessing Large Language Models for Cognitive Assistants in Factories},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3604313},
doi = {10.1145/3571884.3604313},
abstract = {As agile manufacturing expands and workforce mobility increases, the importance of efficient knowledge transfer among factory workers grows. Cognitive Assistants (CAs) with Large Language Models (LLMs), like GPT-3.5, can bridge knowledge gaps and improve worker performance in manufacturing settings. This study investigates the opportunities, risks, and user acceptance of LLM-powered CAs in two factory contexts: textile and detergent production. Several opportunities and risks are identified through a literature review, proof-of-concept implementation, and focus group sessions. Factory representatives raise concerns regarding data security, privacy, and the reliability of LLMs in high-stake environments. By following design guidelines regarding persistent memory, real-time data integration, security, privacy, and ethical concerns, LLM-powered CAs can become valuable assets in manufacturing settings and other industries.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {44},
numpages = {6},
keywords = {cognitive assistant, conversational user interfaces, human-centered AI, industry 5.0, knowledge management, knowledge sharing},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}

@article{10.1145/3589338,
author = {Storey, Veda C. and Lukyanenko, Roman and Castellanos, Arturo},
title = {Conceptual Modeling: Topics, Themes, and Technology Trends},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3589338},
doi = {10.1145/3589338},
abstract = {Conceptual modeling is an important part of information systems development and use that involves identifying and representing relevant aspects of reality. Although the past decades have experienced continuous digitalization of services and products that impact business and society, conceptual modeling efforts are still required to support new technologies as they emerge. This paper surveys research on conceptual modeling over the past five decades and shows how its topics and trends continue to evolve to accommodate emerging technologies, while remaining grounded in basic constructs. We survey over 5,300 papers that address conceptual modeling topics from the 1970s to the present, which are collected from 35 multidisciplinary journals and conferences, and use them as the basis from which to analyze the progression of conceptual modeling. The important role that conceptual modeling should play in our evolving digital world is discussed, and future research directions proposed.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {317},
numpages = {38},
keywords = {Conceptual modeling, digital world, database, information systems, information technology, structured literature review, clustering analysis}
}

@proceedings{10.1145/3578741,
title = {MLNLP '22: Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
year = {2022},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@article{10.1109/TASLP.2021.3065823,
author = {Li, Zekang and Li, Zongjia and Zhang, Jinchao and Feng, Yang and Zhou, Jie},
title = {Bridging Text and Video: A Universal Multimodal Transformer for Audio-Visual Scene-Aware Dialog},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3065823},
doi = {10.1109/TASLP.2021.3065823},
abstract = {Audio-Visual Scene-Aware Dialog (AVSD) is a task to generate responses when chatting about a given video, which is organized as a track of the 8th Dialog System Technology Challenge (DSTC8). There are two challenges in this task: 1) making effective interaction among different modalities; 2) better understanding dialogues and generating informative responses. To tackle the challenges, we propose a universal multimodal transformer and introduce the multi-task learning method to learn joint representations among different modalities as well as generate informative and fluent responses by leveraging the pre-trained language model. Our method extends the natural language generation pre-trained model to multimodal dialogue generation task, which allows fine-tuning language models to capture information across both visual and textual modalities. Our system achieves the best performance in the objective evaluation in both DSTC7-AVSD and DSTC8-AVSD dataset and achieves an impressive 98.4% of the human performance based on human ratings in the DSTC8-AVSD challenge.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {2476–2483},
numpages = {8}
}

@proceedings{10.1145/3605801,
title = {CNCIT '23: Proceedings of the 2023 2nd International Conference on Networks, Communications and Information Technology},
year = {2023},
isbn = {9798400700620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Qinghai, China}
}

@inproceedings{10.1145/3581783.3612425,
author = {Lin, Hongpeng and Ruan, Ludan and Xia, Wenke and Liu, Peiyu and Wen, Jingyuan and Xu, Yixin and Hu, Di and Song, Ruihua and Zhao, Wayne Xin and Jin, Qin and Lu, Zhiwu},
title = {TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612425},
doi = {10.1145/3581783.3612425},
abstract = {To facilitate the research on intelligent and human-like chatbots with multi-modal context, we introduce a new video-based multi-modal dialogue dataset, called TikTalk. We collect 38K videos from a popular video-sharing platform, along with 367K conversations posted by users beneath them. Users engage in spontaneous conversations based on their multi-modal experiences from watching videos, which helps recreate real-world chitchat context. Compared to previous multi-modal dialogue datasets, the richer context types in TikTalk lead to more diverse conversations, but also increase the difficulty in capturing human interests from intricate multi-modal information to generate personalized responses. Moreover, external knowledge is more frequently evoked in our dataset. These facts reveal new challenges for multi-modal dialogue models. We quantitatively demonstrate the characteristics of TikTalk, propose a video-based multi-modal chitchat task, and evaluate several dialogue baselines. Experimental results indicate that the models incorporating large language models (LLM) can generate more diverse responses, while the model utilizing knowledge graphs to introduce external knowledge performs the best overall. Furthermore, no existing model can solve all the above challenges well. There is still a large room for future improvements, even for LLM with visual extensions. Our dataset is available at https://ruc-aimind.github.io/projects/TikTalk/.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {1303–1313},
numpages = {11},
keywords = {chitchat, dataset, multi-modal dialogue, real world},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.1145/3576901,
author = {Sarkar, Souvika and Bijoy, Biddut Sarker and Saba, Syeda Jannatus and Feng, Dongji and Mahajan, Yash and Amin, Mohammad Ruhul and Islam, Sheikh Rabiul and Karmaker (“Santu”), Shubhra Kanti},
title = {Ad-Hoc Monitoring of COVID-19 Global Research Trends for Well-Informed Policy Making},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3576901},
doi = {10.1145/3576901},
abstract = {The COVID-19 pandemic has affected millions of people worldwide with severe health, economic, social, and political implications. Healthcare Policy Makers (HPMs) and medical experts are at the core of responding to this continuously evolving pandemic situation and are working hard to contain the spread and severity of this relatively unknown virus. Biomedical researchers are continually discovering new information about this virus and communicating the findings through scientific articles. As such, it is crucial for HPMs and funding agencies to monitor the COVID-19 research trend globally on a regular basis. However, given the influx of biomedical research articles, monitoring COVID-19 research trends has become more challenging than ever, especially when HPMs want on-demand guided search techniques with a set of topics of interest in mind. Unfortunately, existing topic trend modeling techniques are unable to serve this purpose as (1) traditional topic models are unsupervised, and (2) HPMs in different regions may have different topics of interest that they want to track. &nbsp;&nbsp; To address this problem, we introduce a novel computational task in this article called Ad-Hoc Topic Tracking, which is essentially a combination of zero-shot topic categorization and the spatio-temporal analysis task. We then propose multiple zero-shot classification methods to solve this task by building on state-of-the-art language understanding techniques. Next, we picked the best-performing method based on its accuracy on a separate validation dataset and then applied it to a corpus of recent biomedical research articles to track COVID-19 research endeavors across the globe using a spatio-temporal analysis. A demo website has also been developed for HPMs to create custom spatio-temporal visualizations of COVID-19 research trends. The research outcomes demonstrate that the proposed zero-shot classification methods can potentially facilitate further research on this important subject matter. At the same time, the spatio-temporal visualization tool will greatly assist HPMs and funding agencies in making well-informed policy decisions for advancing scientific research efforts.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {26},
numpages = {28},
keywords = {Topic models, zero-shot learning, COVID-19, policy making, spatio-temporal analysis}
}

@inproceedings{10.1145/3404835.3463232,
author = {Zhang, Qi and Jia, Qinglin and Wang, Chuyuan and Li, Jingjie and Wang, Zhaowei and He, Xiuqiang},
title = {AMM: Attentive Multi-field Matching for News Recommendation},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463232},
doi = {10.1145/3404835.3463232},
abstract = {Personalized news recommendation is a critical technology to help users find interested news, and how to precisely match users' interests and candidate news lies in the core of news recommendation. Existing studies generally learn user's interest vector by aggregating his/her browsed news and then match it with the candidate news vector, which may lose the textual semantic matching signals for recommendation. In this paper, we propose an Attentive Multi-field Matching (AMM) framework for news recommendation which captures the semantic matching representations between each browsed news and candidate news, and then aggregates them as final user-news matching signal. In addition, our method incorporates multi-field information and designs a within-field and cross-field matching mechanism, which leverages complementary information from different fields (e.g., titles, abstracts and bodies) and obtain the multi-field matching representations. To achieve a comprehensive semantic understanding, we employ the most popular language model BERT to learn the matching representation of each browsed-candidate news pair, and incorporate the attention mechanism in aggregating procedure to characterize the importance of each matching representation for the final user-news matching signal. Experiments on the real world datasets validate the effectiveness of AMM.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1588–1592},
numpages = {5},
keywords = {multi-field, news recommendation, semantic matching},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{10.1145/3622933,
author = {Jia, Qi and Liu, Yizhu and Ren, Siyu and Zhu, Kenny Q.},
title = {Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches, and Future Directions},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3622933},
doi = {10.1145/3622933},
abstract = {Abstractive dialogue summarization generates a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted significant attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures, and unclear topic boundaries. This survey provides a comprehensive investigation of existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dialogue features, designing auxiliary training tasks, and using additional data. A list of datasets under different scenarios and widely accepted evaluation metrics are summarized for completeness. After that, the trends of scenarios and techniques are summarized, together with deep insights into correlations between extensively exploited features and different scenarios. Based on these analyses, we recommend future directions, including more controlled and complicated scenarios, technical innovations and comparisons, publicly available datasets in special domains, and so on.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {67},
numpages = {38},
keywords = {Dialogue summarization, dialogue context modeling, abstractive summarization}
}

@proceedings{10.1145/3586182,
title = {UIST '23 Adjunct: Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@article{10.1145/3554727,
author = {Dong, Chenhe and Li, Yinghui and Gong, Haifan and Chen, Miaoxin and Li, Junxin and Shen, Ying and Yang, Min},
title = {A Survey of Natural Language Generation},
year = {2022},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3554727},
doi = {10.1145/3554727},
abstract = {This article offers a comprehensive review of the research on Natural Language Generation (NLG) over the past two decades, especially in relation to data-to-text generation and text-to-text generation deep learning methods, as well as new applications of NLG technology. This survey aims to (a) give the latest synthesis of deep learning research on the NLG core tasks, as well as the architectures adopted in the field; (b) detail meticulously and comprehensively various NLG tasks and datasets, and draw attention to the challenges in NLG evaluation, focusing on different evaluation methods and their relationships; (c) highlight some future emphasis and relatively recent research issues that arise due to the increasing synergy between NLG and other artificial intelligence areas, such as computer vision, text, and computational creativity.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {173},
numpages = {38},
keywords = {Natural language generation, data-to-text generation, text-to-text generation, deep learning, evaluation}
}

@inproceedings{10.1145/3442381.3449852,
author = {Liu, Qian and Geng, Xiubo and Lu, Jie and Jiang, Daxin},
title = {Pivot-based Candidate Retrieval for Cross-lingual Entity Linking},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449852},
doi = {10.1145/3442381.3449852},
abstract = {Entity candidate retrieval plays a critical role in cross-lingual entity linking (XEL). In XEL, entity candidate retrieval needs to retrieve a list of plausible candidate entities from a large knowledge graph in a target language given a piece of text in a sentence or question, namely a mention, in a source language. Existing works mainly fall into two categories: lexicon-based and semantic-based approaches. The lexicon-based approach usually creates cross-lingual and mention-entity lexicons, which is effective but relies heavily on bilingual resources (e.g. inter-language links in Wikipedia). The semantic-based approach maps mentions and entities in different languages to a unified embedding space, which reduces dependence on large-scale bilingual dictionaries. However, its effectiveness is limited by the representation capacity of fixed-length vectors. In this paper, we propose a pivot-based approach which inherits the advantages of the aforementioned two approaches while avoiding their limitations. It takes an intermediary set of plausible target-language mentions as pivots to bridge the two types of gaps: cross-lingual gap and mention-entity gap. Specifically, it first converts mentions in the source language into an intermediary set of plausible mentions in the target language by cross-lingual semantic retrieval and a selective mechanism, and then retrieves candidate entities based on the generated mentions by lexical retrieval. The proposed approach only relies on a small bilingual word dictionary, and fully exploits the benefits of both lexical and semantic matching. Experimental results on two challenging cross-lingual entity linking datasets spanning over 11 languages show that the pivot-based approach outperforms both the lexicon-based and semantic-based approach by a large margin.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1076–1085},
numpages = {10},
keywords = {Information extraction, cross-lingual retrieval, entity linking},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3578503.3583625,
author = {Sha, Alyssa Shuang and Nunes, Bernardo Pereira and Haller, Armin},
title = {Link Topics from Q&amp;A Platforms using Wikidata: A Tool for Cross-platform Hierarchical Classification},
year = {2023},
isbn = {9798400700897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578503.3583625},
doi = {10.1145/3578503.3583625},
abstract = {This paper proposes a novel rule-based topic classification tool for questions on Q&amp;A platforms mediated by the Wikidata ontology – an open and accessible multilingual ontology curated by a large community of online users. Q&amp;A platforms are important sources of information on the Web and often appear as part of Web search results. By adopting Wikidata taxonomic relations as references, our tool can categories the Web content from different platforms in a unified coarse-to-fine mode based on their domain coverage. To validate and demonstrate the potential applicability of our tool, a set of use cases and experiments are carried out on two popular Q&amp;A platforms – Zhihu and Quora, where the impact of topic categories on question lifecycles is explored. Furthermore, we compare our results with the output generated by GPT-3 classifier. This tool sheds light on how structured knowledge bases can enable data interoperability and serve as a filtering functionality to mitigate classification bias of OpenAI.},
booktitle = {Proceedings of the 15th ACM Web Science Conference 2023},
pages = {357–362},
numpages = {6},
keywords = {Entity Linking, Q&amp;A platforms, Topic classification, Wikidata ontology},
location = {Austin, TX, USA},
series = {WebSci '23}
}

@inproceedings{10.1145/3442381.3450141,
author = {Daza, Daniel and Cochez, Michael and Groth, Paul},
title = {Inductive Entity Representations from Text via Link Prediction},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450141},
doi = {10.1145/3442381.3450141},
abstract = {Knowledge Graphs (KG) are of vital importance for multiple applications on the web, including information retrieval, recommender systems, and metadata annotation. Regardless of whether they are built manually by domain experts or with automatic pipelines, KGs are often incomplete. To address this problem, there is a large amount of work that proposes using machine learning to complete these graphs by predicting new links. Recent work has begun to explore the use of textual descriptions available in knowledge graphs to learn vector representations of entities in order to preform link prediction. However, the extent to which these representations learned for link prediction generalize to other tasks is unclear. This is important given the cost of learning such representations. Ideally, we would prefer representations that do not need to be trained again when transferring to a different task, while retaining reasonable performance. Therefore, in this work, we propose a holistic evaluation protocol for entity representations learned via a link prediction objective. We consider the inductive link prediction and entity classification tasks, which involve entities not seen during training. We also consider an information retrieval task for entity-oriented search. We evaluate an architecture based on a pretrained language model, that exhibits strong generalization to entities not observed during training, and outperforms related state-of-the-art methods (22% MRR improvement in link prediction on average). We further provide evidence that the learned representations transfer well to other tasks without fine-tuning. In the entity classification task we obtain an average improvement of 16% in accuracy compared with baselines that also employ pre-trained models. In the information retrieval task, we obtain significant improvements of up to 8.8% in NDCG@10 for natural language queries. We thus show that the learned representations are not limited KG-specific tasks, and have greater generalization properties than evaluated in previous work.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {798–808},
numpages = {11},
keywords = {entity classification, entity representations, information retrieval, knowledge graphs, link prediction},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3617680,
author = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
title = {A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3617680},
doi = {10.1145/3617680},
abstract = {Controllable Text Generation (CTG) is an emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used Transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods needs to be guaranteed. To this end, controllable text generation using Transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the past 3 to 4 years, targeting different CTG tasks that require different types of controlled constraints. In this article, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey article to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {64},
numpages = {37},
keywords = {Controllable text generation, pre-trained language models, Transformer, controllability, systematic review}
}

@inproceedings{10.1145/3404835.3463000,
author = {Li, Yanran and Li, Wenjie and Wang, Zhitao},
title = {Graph-Structured Context Understanding for Knowledge-grounded Response Generation},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463000},
doi = {10.1145/3404835.3463000},
abstract = {In this work, we establish a context graph from both conversation utterances and external knowledge, and develop a novel graph-based encoder to better understand the conversation context. Specifically, the encoder fuses the information in the context graph stage-by-stage and provides global context-graph-aware representations of each node in the graph to facilitate knowledge-grounded response generation. On a large-scale conversation corpus, we validate the effectiveness of the proposed approach and demonstrate the benefit of knowledge in conversation understanding.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1930–1934},
numpages = {5},
keywords = {dialogue systems, knowledge-grounded response generation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{10.1145/3609483,
author = {Moscato, Vincenzo and Postiglione, Marco and Sperl\'{\i}, Giancarlo},
title = {Few-shot Named Entity Recognition: Definition, Taxonomy and Research Directions},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3609483},
doi = {10.1145/3609483},
abstract = {Recent years have seen an exponential growth (+98% in 2022 w.r.t. the previous year) of the number of research articles in the few-shot learning field, which aims at training machine learning models with extremely limited available data. The research interest toward few-shot learning systems for Named Entity Recognition (NER) is thus at the same time increasing. NER consists in identifying mentions of pre-defined entities from unstructured text, and serves as a fundamental step in many downstream tasks, such as the construction of Knowledge Graphs, or Question Answering. The need for a NER system able to be trained with few-annotated examples comes in all its urgency in domains where the annotation process requires time, knowledge and expertise (e.g., healthcare, finance, legal), and in low-resource languages. In this survey, starting from a clear definition and description of the few-shot NER (FS-NER) problem, we take stock of the current state-of-the-art and propose a taxonomy which divides algorithms in two macro-categories according to the underlying mechanisms: model-centric and data-centric. For each category, we line-up works as a story to show how the field is moving toward new research directions. Eventually, techniques, limitations, and key aspects are deeply analyzed to facilitate future studies.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {94},
numpages = {46},
keywords = {Few-shot learning, Named Entity Recognition}
}

@article{10.1145/3446343,
author = {Bashar, Md Abul and Nayak, Richi},
title = {Active Learning for Effectively Fine-Tuning Transfer Learning to Downstream Task},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3446343},
doi = {10.1145/3446343},
abstract = {Language model (LM) has become a common method of transfer learning in Natural Language Processing (NLP) tasks when working with small labeled datasets. An LM is pretrained using an easily available large unlabelled text corpus and is fine-tuned with the labelled data to apply to the target (i.e., downstream) task. As an LM is designed to capture the linguistic aspects of semantics, it can be biased to linguistic features. We argue that exposing an LM model during fine-tuning to instances that capture diverse semantic aspects (e.g., topical, linguistic, semantic relations) present in the dataset will improve its performance on the underlying task. We propose a Mixed Aspect Sampling (MAS) framework to sample instances that capture different semantic aspects of the dataset and use the ensemble classifier to improve the classification performance. Experimental results show that MAS performs better than random sampling as well as the state-of-the-art active learning models to abuse detection tasks where it is hard to collect the labelled data for building an accurate classifier.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {24},
numpages = {24},
keywords = {Misogynistic tweet, active learning, hate speech, imbalanced dataset, topic model, transfer learning}
}

@inproceedings{10.1145/3587259.3627572,
author = {Rula, Anisa and D'Souza, Jennifer},
title = {Procedural Text Mining with Large Language Models},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627572},
doi = {10.1145/3587259.3627572},
abstract = {Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {9–16},
numpages = {8},
keywords = {knowledge capture, knowledge representation},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@proceedings{10.1145/3591196,
title = {C&amp;C '23: Proceedings of the 15th Conference on Creativity and Cognition},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, USA}
}

@inproceedings{10.1145/3584931.3606951,
author = {Lim, Gionnieve and Kim, Hyunwoo and Choi, Yoonseo and Li, Toby Jia-Jun and Kulkarni, Chinmay and Subramonyam, Hariharan and Seering, Joseph and Bernstein, Michael S. and Zhang, Amy X. and Glassman, Elena L. and Perrault, Simon and Kim, Juho},
title = {Designing for AI-Powered Social Computing Systems},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3606951},
doi = {10.1145/3584931.3606951},
abstract = {The CSCW community has been active in designing, implementing, and evaluating novel social computing systems. In recent years, there has been a rise in using AI to empower social interactions and the capabilities of these systems. While these implementations charge ahead of the establishment of ethical and legal frameworks, it is timely to reflect on the state of AI-powered social computing systems and to identify new research agendas for the community. This Special Interest Group aims to bring in researchers and practitioners from different fields to foster discussions on the key considerations and challenges in designing for AI-powered social computing systems and to promote opportunities for new research collaborations.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {572–575},
numpages = {4},
keywords = {artificial intelligence, design, human-AI interaction, social computing systems},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}

@inproceedings{10.1145/3474085.3475545,
author = {Tian, Hongshuo and Xu, Ning and Liu, An-An and Yan, Chenggang and Mao, Zhendong and Zhang, Quan and Zhang, Yongdong},
title = {Mask and Predict: Multi-step Reasoning for Scene Graph Generation},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475545},
doi = {10.1145/3474085.3475545},
abstract = {Scene Graph Generation (SGG) aims to parse the image as a set of semantics, containing objects and their relations. Currently, the SGG methods only stay at presenting the intuitive detection in the image, such as the triplet "logo on board". Intuitively, we humans can further refine these intuitive detections as rational descriptions like "flower painted on surfboard". However, most of existing methods always formulate SGG as a straightforward task, only limited by the manner of one-time prediction, which focuses on a single-pass pipeline and predicts all the semantic. Therefore, to handle this problem, we propose a novel multi-step reasoning manner for SGG. Concretely, we break SGG into two explicit learning stages, including intuitive training stage (ITS) and rational training stage (RTS). In the first stage, we follow the traditional SGG processing to detect objects and relationships, yielding an intuitive scene graph. In the second stage, we perform multi-step reasoning to refine the intuitive scene graph. For each step of reasoning, it consists of two kinds of operations: mask and predict. According to primary predictions and their confidences, we constantly select and mask the low-confidence predictions, which features are optimized and predicted again. After several iterations, all of intuitive semantics will gradually tend to be revised with high confidences, yielding a rational scene graph. Extensive experiments on Visual Genome prove the superiority of the proposed method. Additional ablation studies and visualization cases further validate its effectiveness.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4128–4136},
numpages = {9},
keywords = {mask and predict, multi-step reasoning, scene graph},
location = {Virtual Event, China},
series = {MM '21}
}

@article{10.1109/TASLP.2021.3123885,
author = {Liao, Xianwen and Huang, Yongzhong and Wei, Yongzhuang and Zhang, Chenhao and Wang, Fu and Wang, Yong},
title = {Efficient Estimate of Sentence's Representation Based on the Difference Semantics Model},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3123885},
doi = {10.1109/TASLP.2021.3123885},
abstract = {Sentence representation is an important research hotspot in natural language processing (NLP) since it can map the semantics of sentences into semantics vectors, thereby effectively solving complex semantics computing problems. Recently, sentence representations are mainly obtained by indirect means. Specifically, for sentence representations obtained by unsupervised means, they are often calculated by the weighted sum of embeddings of tokens in sentences; for sentence representations obtained by self-supervised or supervised means, they are often derived from intermediate encodings of sentences in prediction tasks. For example, Google's BERT and MUSE respectively use the embedding of [CLS] in the next sentence prediction task and intermediate encodings of sentences in the translation bridge task as sentence representations. In this paper, we use the observed semantics increment feature of sentences to directly model the semantics function of sentences. To be able to use the existing neural network language model to approximate the semantics function, we first implement the first-order Taylor expansion on the semantics function to obtain a difference semantics model and then add it to BERT as a subtask to perform self-supervised fine-tuning. Finally, we get a new sentence representation model S-BERT. S-BERT achieves the state-of-the-art performance on many datasets in Chinese, English, and Vietnamese.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3384–3399},
numpages = {16}
}

@inproceedings{10.1145/3584931.3611284,
author = {Boonprakong, Nattapat and He, Gaole and Gadiraju, Ujwal and van Berkel, Niels and Wang, Danding and Chen, Si and Liu, Jiqun and Tag, Benjamin and Goncalves, Jorge and Dingler, Tilman},
title = {Workshop on Understanding and Mitigating Cognitive Biases in Human-AI Collaboration},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3611284},
doi = {10.1145/3584931.3611284},
abstract = {AI systems are increasingly incorporated into human decision-making. Yet, human decision-makers are often affected by their cognitive biases. In critical settings, such as medical diagnosis, criminal judgment, or information consumption, these cognitive biases hinder optimal decision outcomes, thereby resulting in dangerous decisions and negative societal impact. The use of AI systems can amplify and exacerbate cognitive biases in their users. In this workshop, we seek to foster discussions on ongoing research around cognitive biases in human-AI collaboration and identify future research directions to understand, quantify, and mitigate the effects of cognitive biases. We will explore cognitive biases appearing in various contexts of human-AI collaboration: what can cause them?; how can we measure, model, mitigate, and manage cognitive biases?; and how can we utilise cognitive biases for the greater good? We will reflect on workshop discussions to form a research community around cognitive biases and bias-aware systems.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {512–517},
numpages = {6},
keywords = {Cognitive Bias, Debiasing, Human-AI Collaboration},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}

@inproceedings{10.1145/3581641.3584065,
author = {Pataranutaporn, Pat and Danry, Valdemar and Blanchard, Lancelot and Thakral, Lavanay and Ohsugi, Naoki and Maes, Pattie and Sra, Misha},
title = {Living Memories: AI-Generated Characters as Digital Mementos},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584065},
doi = {10.1145/3581641.3584065},
abstract = {Every human culture has developed practices and rituals associated with remembering people of the past - be it for mourning, cultural preservation, or learning about historical events. In this paper, we present the concept of “Living Memories”: interactive digital mementos that are created from journals, letters and data that an individual have left behind. Like an interactive photograph, living memories can be talked to and asked questions, making accessing the knowledge, attitudes and past experiences of a person easily accessible. To demonstrate our concept, we created an AI-based system for generating living memories from any data source and implemented living memories of the three historical figures “Leonardo Da Vinci”, “Murasaki Shikibu”, and “Captain Robert Scott”. As a second key contribution, we present a novel metrics scheme for evaluating the accuracy of living memory architectures and show the accuracy of our pipeline to improve over baselines. Finally, we compare the user experience and learning effects of interacting with the living memory of Leonardo Da Vinci to reading his journal. Our results show that interacting with the living memory, in addition to simply reading a journal, increases learning effectiveness and motivation to learn about the character.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {889–901},
numpages = {13},
keywords = {AI, AI-Generated Characters, Human-AI Interaction},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@proceedings{10.1145/3571884,
title = {CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Eindhoven, Netherlands}
}

@inproceedings{10.1145/3442381.3450029,
author = {Yu, Bowen and Zhang, Zhenyu and Sheng, Jiawei and Liu, Tingwen and Wang, Yubin and Wang, Yucheng and Wang, Bin},
title = {Semi-Open Information Extraction},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450029},
doi = {10.1145/3442381.3450029},
abstract = {Open Information Extraction (OIE), the task aimed at discovering all textual facts organized in the form of (subject, predicate, object) found within a sentence, has gained much attention recently. However, in some knowledge-driven applications such as question answering, we often have a target entity and hope to obtain its structured factual knowledge for better understanding, instead of extracting all possible facts aimlessly from the corpus. In this paper, we define a new task, namely Semi-Open Information Extraction (SOIE), to address this need. The goal of SOIE is to discover domain-independent facts towards a particular entity from general and diverse web text. To facilitate research on this new task, we propose a large-scale human-annotated benchmark called SOIED, consisting of 61,984 facts for 8,013 subject entities annotated on 24,000 Chinese sentences collected from the web search engine. In addition, we propose a novel unified model called USE for this task. First, we introduce subject-guided sequence as input to a pre-trained language model and normalize the hidden representations conditioned on the subject embedding to encode the sentence in a subject-aware manner. Second, we decompose SOIE into three uncoupled subtasks: predicate extraction, object extraction, and boundary alignment. They can all be formulated as the problem of table filling by forming a two-dimensional tag table based on a task-specific tagging scheme. Third, we introduce a collaborative learning strategy that enables the interactive relations among subtasks to be better exploited by explicitly exchanging informative clues. Finally, we evaluate USE and several strong baselines on our new dataset. Experimental results demonstrate the advantages of the proposed method and reveal insight for future improvement.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {1661–1672},
numpages = {12},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3587259.3627545,
author = {Theodoropoulos, Christos and Mulligan, Natalia and Stappenbeck, Thaddeus and Bettencourt-Silva, Joao},
title = {Representation Learning for Person or Entity-Centric Knowledge Graphs: An Application in Healthcare},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627545},
doi = {10.1145/3587259.3627545},
abstract = {Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as training graph neural networks (GNNs) for a wide range of predictive tasks. This paper presents an end-to-end representation learning framework to extract entity-centric KGs from structured and unstructured data. We introduce a star-shaped ontology to represent the multiple facets of a person and use it to guide KG creation. Compact representations of the graphs are created leveraging GNNs and experiments are conducted using different levels of heterogeneity or explicitness. A readmission prediction task is used to evaluate the results of the proposed framework, showing a stable system, robust to missing data, that outperforms a range of baseline machine learning classifiers. We highlight that this approach has several potential applications across domains and is open-sourced.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {225–233},
numpages = {9},
keywords = {Entity-Centric Knowledge Graphs, Graph Neural Networks, Person-Centric Ontology, Representation Learning},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@proceedings{10.1145/3581754,
title = {IUI '23 Companion: Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@article{10.1145/3489142,
author = {Li, Qun and Xiao, Fu and Bhanu, Bir and Sheng, Biyun and Hong, Richang},
title = {Inner Knowledge-based Img2Doc Scheme for Visual Question Answering},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3489142},
doi = {10.1145/3489142},
abstract = {Visual Question Answering (VQA) is a research topic of significant interest at the intersection of computer vision and natural language understanding. Recent research indicates that attributes and knowledge can effectively improve performance for both image captioning and VQA. In this article, an inner knowledge-based Img2Doc algorithm for VQA is presented. The inner knowledge is characterized as the inner attribute relationship in visual images. In addition to using an attribute network for inner knowledge-based image representation, VQA scheme is associated with a question-guided Doc2Vec method for question–answering. The attribute network generates inner knowledge-based features for visual images, while a novel question-guided Doc2Vec method aims at converting natural language text to vector features. After the vector features are extracted, they are combined with visual image features into a classifier to provide an answer. Based on our model, the VQA problem is resolved by textual question answering. The experimental results demonstrate that the proposed method achieves superior performance on multiple benchmark datasets.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = mar,
articleno = {76},
numpages = {21},
keywords = {VQA, dense image captioning, Doc2Vec, inner knowledge-based, attribute network}
}

@article{10.1145/3624733,
author = {Deldjoo, Yashar and Nazary, Fatemeh and Ramisa, Arnau and McAuley, Julian and Pellegrini, Giovanni and Bellogin, Alejandro and Noia, Tommaso Di},
title = {A Review of Modern Fashion Recommender Systems},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3624733},
doi = {10.1145/3624733},
abstract = {The textile and apparel industries have grown tremendously over the past few years. Customers no longer have to visit many stores, stand in long queues, or try on garments in dressing rooms, as millions of products are now available in online catalogs. However, given the plethora of options available, an effective recommendation system is necessary to properly sort, order, and communicate relevant product material or information to users. Effective fashion recommender systems (RSs) can have a noticeable impact on billions of customers’ shopping experiences and increase sales and revenues on the provider side.The goal of this survey is to provide a review of RSs that operate in the specific vertical domain of garment and fashion products. We have identified the most pressing challenges in fashion RS research and created a taxonomy that categorizes the literature according to the objective they are trying to accomplish (e.g., item or outfit recommendation, size recommendation, and explainability, among others) and type of side information (users, items, context). We have also identified the most important evaluation goals and perspectives (outfit generation, outfit recommendation, pairing recommendation, and fill-in-the-blank outfit compatibility prediction) and the most commonly used datasets and evaluation metrics.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {87},
numpages = {37},
keywords = {Recommender systems, information retrieval, fashion retail, machine learning, artificial intelligence, computer vision, text mining, e-commerce}
}

@proceedings{10.1145/3591106,
title = {ICMR '23: Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Thessaloniki, Greece}
}

@proceedings{10.1145/3616961,
title = {Mindtrek '23: Proceedings of the 26th International Academic Mindtrek Conference},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@article{10.1145/3465074.3465080,
author = {Talamadupula, Kartik},
title = {Applied AI matters: AI4Code: applying artificial intelligence to source code},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
url = {https://doi.org/10.1145/3465074.3465080},
doi = {10.1145/3465074.3465080},
abstract = {The marriage of Artificial Intelligence (AI) techniques to problems surrounding the generation, maintenance, and use of source code has come to the fore in recent years as an important AI application area1. A large chunk of this recent attention can be attributed to contemporaneous advancements in Natural Language Processing (NLP) techniques and sub-fields. The naturalness hypothesis, which states that "software is a form of human communication" and that code exhibits patterns that are similar to (human) natural languages (Devanbu, 2015; Hindle, Barr, Gabel, Su, &amp; Devanbu, 2016), has allowed for the application of many of these NLP advances to code-centric usecases. This development has contributed to a spate of work in the community --- much of it captured in a survey by Allamanis, Barr, Devanbu, and Sutton (2018) that focuses on classifying these approaches by the type of probabilistic model applied to source code.This increase in the variety of AI techniques applied to source code has found various manifestations in the industry at large. Code and software form the backbone that underpins almost all modern technical advancements: it is thus natural that breakthroughs in this area should reflect in the emergence of real world deployments.},
journal = {AI Matters},
month = jul,
pages = {18–20},
numpages = {3}
}

@inproceedings{10.1145/3485447.3512018,
author = {Truong, Quoc-Tuan and Zhao, Tong and Yuan, Changhe and Li, Jin and Chan, Jim and Pantel, Soo-Min and Lauw, Hady W.},
title = {AmpSum: Adaptive Multiple-Product Summarization towards Improving Recommendation Captions},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512018},
doi = {10.1145/3485447.3512018},
abstract = {In e-commerce websites, multiple related product recommendations are usually organized into “widgets”, each given a name, as a recommendation caption, to describe the products within. These recommendation captions are usually manually crafted and generic in nature, making it difficult to attach meaningful and informative names at scale. As a result, the captions are inadequate in helping customers to better understand the connection between the multiple recommendations and make faster product discovery. We propose an Adaptive Multiple-Product Summarization framework (AmpSum) that automatically and adaptively generates widget captions based on different recommended products. The multiplicity of products to be summarized in a widget caption is particularly novel. The lack of well-developed labels motivates us to design a weakly supervised learning approach with distant supervision to bootstrap the model learning from pseudo labels, and then fine-tune the model with a small amount of manual labels. To validate the efficacy of this method, we conduct extensive experiments on several product categories of Amazon data. The results demonstrate that our proposed framework consistently outperforms state-of-the-art baselines over 9.47-29.14% on ROUGE and 27.31% on METEOR. With case studies, we illustrate how AmpSum could adaptively generate summarization based on different product recommendations.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2978–2988},
numpages = {11},
keywords = {Multiple-Product Summarization, Product Summarization, Recommendation Captions},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3576840.3578330,
author = {El Zein, Dima and C\^{a}mara, Arthur and Da Costa Pereira, C\'{e}lia and Tettamanzi, Andrea},
title = {RULKNE: Representing User Knowledge State in Search-as-Learning with Named Entities},
year = {2023},
isbn = {9798400700354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576840.3578330},
doi = {10.1145/3576840.3578330},
abstract = {A reliable representation of the user’s knowledge state during a learning search session is crucial to understand their real information needs. When a search system is aware of such a state, it can adapt the search results and provide greater support for the user’s learning objectives. A common practice to track the user’s knowledge state is to consider the content of the documents they read during their search session(s). However, most current work ignores entity mentions in the documents, which, when linked to knowledge graphs, can be a source of valuable information regarding the user’s knowledge. To fill this gap, we extend RULK—Representing User Knowledge in Search-as-Learning—with entity linking capabilities. The extended framework RULK represents and tracks user knowledge as a collection of such entities. It eventually estimates the user knowledge gain—learning outcome—by measuring the similarity between the represented knowledge and the learning objective. We show that our methods allow for up to 10% improvements when estimating user knowledge gains.},
booktitle = {Proceedings of the 2023 Conference on Human Information Interaction and Retrieval},
pages = {388–393},
numpages = {6},
keywords = {Interactive IR, Named Entities, Retrieval system, Search-As-Learning, User Knowledge},
location = {Austin, TX, USA},
series = {CHIIR '23}
}

@article{10.1145/3575803,
author = {Di, Donglin and Song, Xianyang and Zhang, Weinan and Zhang, Yue and Wang, Fanglin},
title = {Building Dialogue Understanding Models for Low-resource Language Indonesian from Scratch},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3575803},
doi = {10.1145/3575803},
abstract = {Using off-the-shelf resources from resource-rich languages to transfer knowledge to low-resource languages has received a lot of attention. The requirements of enabling the model to achieve the reliable performance, including the scale of required annotated data and the effective framework, are not well guided. To address the first question, we empirically investigate the cost-effectiveness of several methods for training intent classification and slot-filling models from scratch in Indonesia (ID) using English data. Confronting the second challenge, we propose a Bi-Confidence-Frequency Cross-Lingual transfer framework (BiCF), which consists of “BiCF Mixing”, “Latent Space Refinement” and “Joint Decoder”, respectively, to overcome the lack of low-resource language dialogue data. BiCF Mixing based on the word-level alignment strategy generates code-mixed data by utilizing the importance-frequency and translating-confidence. Moreover, Latent Space Refinement trains a new dialogue understanding model using code-mixed data and word embedding models. Joint Decoder based on Bidirectional LSTM (BiLSTM) and Conditional Random Field (CRF) is used to obtain experimental results of intent classification and slot-filling. We also release a large-scale fine-labeled Indonesia dialogue dataset (ID-WOZ1) and ID-BERT for experiments. BiCF achieves 93.56% and 85.17% (F1 score) on intent classification and slot filling, respectively. Extensive experiments demonstrate that our framework performs reliably and cost-efficiently on different scales of manually annotated Indonesian data.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {105},
numpages = {20},
keywords = {Dialogue datasets, intent classification, slot-filling, indonesian}
}

@inproceedings{10.1145/3571884.3597133,
author = {Addlesee, Angus and Damonte, Marco},
title = {Understanding and Answering Incomplete Questions},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3597133},
doi = {10.1145/3571884.3597133},
abstract = {Voice assistants interrupt people when they pause mid-question, a frustrating interaction that requires the full repetition of the entire question again. This impacts all users, but particularly people with cognitive impairments. In human-human conversation, these situations are recovered naturally as people understand the words that were uttered. In this paper we build answer pipelines which parse incomplete questions and repair them following human recovery strategies. We evaluated these pipelines on our new corpus, SLUICE. It contains 21,000 interrupted questions, from LC-QuAD 2.0 and QALD-9-plus, paired with their underspecified SPARQL queries. Compared to a system that is given the full question, our best partial understanding pipeline answered only 0.77% fewer questions. Results show that our pipeline correctly identifies what information is required to provide an answer but is not yet provided by the incomplete question. It also accurately identifies where that missing information belongs in the semantic structure of the question.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {10},
numpages = {9},
keywords = {accessibility, human agent interaction, knowledge base question answering, semantic parsing, voice user experience},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}

@proceedings{10.1145/3594778,
title = {GRADES-NDA '23: Proceedings of the 6th Joint Workshop on Graph Data Management Experiences &amp; Systems (GRADES) and Network Data Analytics (NDA)},
year = {2023},
isbn = {9798400702013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GRADES-NDA 2023 is the sixth joint meeting of the GRADES and NDA workshops, which were each independently organized at previous SIGMOD-PODS meetings, GRADES since 2013 and NDA since 2016. The focus of the GRADES-NDA workshop is the application areas, usage scenarios and open challenges in managing largescale graph-shaped data. The workshop is a forum for exchanging ideas and methods for mining, querying, and learning with real-world network data, developing new common understandings of the problems at hand, sharing of data sets and benchmarks where applicable, and leveraging existing knowledge from different disciplines. GRADES-NDA aims to present technical contributions inside graph, RDF, and other data management systems on massive graphs.The purpose of this workshop is to bring together researchers from academia, industry, and government to create a forum for discussing recent advances in large-scale graph data management and analytics systems, as well as propose and discuss novel methods and techniques towards addressing domain specific challenges and handling noise in real-world graphs.},
location = {Seattle, WA, USA}
}

@article{10.1145/3573204,
author = {Yao, Jing and Liu, Zheng and Yang, Junhan and Dou, Zhicheng and Xie, Xing and Wen, Ji-Rong},
title = {CDSM: Cascaded Deep Semantic Matching on Textual Graphs Leveraging Ad-hoc Neighbor Selection},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3573204},
doi = {10.1145/3573204},
abstract = {Deep semantic matching aims at discriminating the relationship between documents based on deep neural networks. In recent years, it becomes increasingly popular to organize documents with a graph structure, then leverage both the intrinsic document features and the extrinsic neighbor features to derive discrimination. Most of the existing works mainly care about how to utilize the presented neighbors, whereas limited effort is made to filter appropriate neighbors. We argue that the neighbor features could be highly noisy and partially useful. Thus, a lack of effective neighbor selection will not only incur a great deal of unnecessary computation cost but also restrict the matching accuracy severely. In this work, we propose a novel framework, Cascaded Deep Semantic Matching (CDSM), for accurate and efficient semantic matching on textual graphs. CDSM is highlighted for its two-stage workflow. In the first stage, a lightweight CNN-based ad-hod neighbor selector is deployed to filter useful neighbors for the matching task with a small computation cost. We design both one-step and multi-step selection methods. In the second stage, a high-capacity graph-based matching network is employed to compute fine-grained relevance scores based on the well-selected neighbors. It is worth noting that CDSM is a generic framework which accommodates most of the mainstream graph-based semantic matching networks. The major challenge is how the selector can learn to discriminate the neighbors’ usefulness which has no explicit labels. To cope with this problem, we design a weak-supervision strategy for optimization, where we train the graph-based matching network at first and then the ad-hoc neighbor selector is learned on top of the annotations from the matching network. We conduct extensive experiments with three large-scale datasets, showing that CDSM notably improves the semantic matching accuracy and efficiency thanks to the selection of high-quality neighbors. The source code is released at https://github.com/jingjyyao/CDSM.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {32},
numpages = {24},
keywords = {Semantic matching, textual graph, neighbor selection}
}

@article{10.1109/TASLP.2023.3340610,
author = {Zhao, Yaru and Cheng, Bo and Huang, Yakun and Wan, Zhiguo},
title = {FluGCF: A Fluent Dialogue Generation Model With Coherent Concept Entity Flow},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3340610},
doi = {10.1109/TASLP.2023.3340610},
abstract = {The integration of external knowledge graphs into dialogue systems effectively mitigates the generation of generic and uninteresting responses. This approach, particularly the explicit modeling of conversation flows from related concept entities, facilitates the generation of semantically rich and informative responses. However, recent models guided by concept entity flows present two primary limitations: (1) a limited semantic understanding of the post message, which complicates the selection of highly relevant 1-hop concept entities, and (2) an inability to extract dynamic and diverse semantic relations between the post message and 2-hop concept entities. To address these issues, we introduce FluGCF, a novel model that fluently generates dialogues with coherent guidance from concept entity flows. FluGCF employs a ternary fusion to explicitly model multi-hop concept entity flows using a post-aware knowledge encoding mechanism. This mechanism learns semantic concept entity features from both word and sentence-level text features. Additionally, we design a corresponding ternary decoding mechanism that dynamically selects concept entities or words from the vocabulary to enhance fluency and diversity in dialogue generation. FluGCF, implemented in PyTorch, was extensively evaluated on a large-scale dataset, revealing that it surpasses baseline models, including the state-of-the-art knowledge-aware model ConceptFlow, by nearly 15% in terms of fluency. Furthermore, it demonstrated notable enhancements in coherence, diversity and informativeness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {853–867},
numpages = {15}
}

@article{10.1145/3510030,
author = {Hu, Yang and Chapman, Adriane and Wen, Guihua and Hall, Dame Wendy},
title = {What Can Knowledge Bring to Machine Learning?—A Survey of Low-shot Learning for Structured Data},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3510030},
doi = {10.1145/3510030},
abstract = {Supervised machine learning has several drawbacks that make it difficult to use in many situations. Drawbacks include heavy reliance on massive training data, limited generalizability, and poor expressiveness of high-level semantics. Low-shot Learning attempts to address these drawbacks. Low-shot learning allows the model to obtain good predictive power with very little or no training data, where structured knowledge plays a key role as a high-level semantic representation of human. This article will review the fundamental factors of low-shot learning technologies, with a focus on the operation of structured knowledge under different low-shot conditions. We also introduce other techniques relevant to low-shot learning. Finally, we point out the limitations of low-shot learning, the prospects and gaps of industrial applications, and future research directions.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {48},
numpages = {45},
keywords = {Machine learning, low-shot learning, structured knowledge, industrial applications, future directions}
}

@inproceedings{10.1145/3539618.3591667,
author = {Li, Na and Kteich, Hanane and Bouraoui, Zied and Schockaert, Steven},
title = {Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591667},
doi = {10.1145/3539618.3591667},
abstract = {Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence from the neighbourhood structure of the contextualised word embeddings. The second strategy instead relies on a distant supervision signal from ConceptNet. Our experimental results show that the resulting vectors substantially outperform existing concept embeddings in predicting the semantic properties of concepts, with the ConceptNet-based strategy achieving the best results. These findings are furthermore confirmed in a clustering task and in the downstream task of ontology completion.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {216–226},
numpages = {11},
keywords = {commonsense knowledge, contrastive learning, language models, word embedding},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@proceedings{10.1145/3587259,
title = {K-CAP '23: Proceedings of the 12th Knowledge Capture Conference 2023},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 12th ACM International Conference on Knowledge Capture: K-CAP 2023, held in person on December 5th - 7th in Pensacola, Florida, US.Driven by the increasing demands for knowledge-based applications and the unprecedented availability of information from heterogeneous data sources, the study of knowledge capture is of crucial importance. Knowledge capture involves the extraction of useful knowledge from vast and diverse data sources as well as its acquisition directly from human experts.Nowadays knowledge is derived from an increasingly diverse set of data resources that differ with regard to their domain, format, quality, coverage, specificity, viewpoint, bias, and most importantly, consumers and producers of data. The heterogeneity, amount and complexity of data allow us to answer complex questions that could not be answered in isolation, requiring the interaction of different scientific fields and technologies. A goal of K-CAP is to develop such synergies using systematic and rigorous methodologies.The call for papers attracted 105 submissions from all over the world, covering a diverse range of topics spanning knowledge mining, large language models for information extraction, neuro-symbolic approaches for knowledge capture, knowledge engineering, question-answering, knowledge graphs, natural language processing, reasoning, entity linking, querying and knowledge-based applications. From a competitive set of high-quality submissions, we accepted 27 long research papers, 5 short papers, and 1 vision paper. The high-quality program is divided into 7 research sessions, in addition to 3 tutorials reflecting novel topics of interest in Knowledge Capture.We encourage everyone to attend the keynote talks that we have planned for K-CAP 2023. The highly anticipated talks by Dr. Robert R. Hoffman (Florida Institute for Human and Machine Cognition) and Dr. Jane Pinelis (Johns Hopkins University Applied Physics Laboratory) will guide us to a better understanding of the future of knowledge capture and explainable, resilient AI ecosystems, as they become commonplace in real world applications.},
location = {Pensacola, FL, USA}
}

@inproceedings{10.1145/3442381.3449838,
author = {Shao, Huajie and Wang, Jun and Lin, Haohong and Zhang, Xuezhou and Zhang, Aston and Ji, Heng and Abdelzaher, Tarek},
title = {Controllable and Diverse Text Generation in E-commerce},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449838},
doi = {10.1145/3442381.3449838},
abstract = {In E-commerce, a key challenge in text generation is to find a good trade-off between word diversity and accuracy (relevance) in order to make generated text appear more natural and human-like. In order to improve the relevance of generated results, conditional text generators were developed that use input keywords or attributes to produce the corresponding text. Prior work, however, do not finely control the diversity of automatically generated sentences. For example, it does not control the order of keywords to put more relevant ones first. Moreover, it does not explicitly control the balance between diversity and accuracy. To remedy these problems, we propose a fine-grained controllable generative model, called&nbsp;Apex, that uses an algorithm borrowed from automatic control (namely, a variant of the proportional, integral, and derivative (PID) controller) to precisely manipulate the diversity/accuracy trade-off of generated text. The algorithm is injected into a Conditional Variational Autoencoder (CVAE), allowing Apex to control both (i) the order of keywords in the generated sentences (conditioned on the input keywords and their order), and (ii) the trade-off between diversity and accuracy. Evaluation results on real world datasets&nbsp;1 show that the proposed method outperforms existing generative models in terms of diversity and relevance. Moreover, it achieves about 97% accuracy in the control of the order of keywords. Apex is currently deployed to generate production descriptions and item recommendation reasons in Taobao2, the largest E-commerce platform in China. The A/B production test results show that our method improves click-through rate (CTR) by 13.17% compared to the existing method for production descriptions. For item recommendation reason, it is able to increase CTR by 6.89% and 1.42% compared to user reviews and top-K item recommendation without reviews, respectively.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2392–2401},
numpages = {10},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3564746.3587001,
author = {Adatrao, Naga Sai Krishna and Gadireddy, Gowtham Reddy and Noh, Jiho},
title = {A Survey on Conversational Search and Applications in Biomedicine},
year = {2023},
isbn = {9781450399210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564746.3587001},
doi = {10.1145/3564746.3587001},
abstract = {This paper aims to provide a radical rundown on Conversational Search (ConvSearch), an approach to enhance the information retrieval (IR) method where users engage in a dialogue for the information-seeking tasks. In this survey, we predominantly focused on the human interactive characteristics of the ConvSearch systems, highlighting the operations of the action modules, likely the retrieval system, question-answering, and recommender system. We labeled various ConvSearch research problems in knowledge bases, natural language processing, and dialogue management systems with action modules. We further categorized the framework to ConvSearch, and the application is directed toward biomedical and healthcare fields for the utilization of clinical social technology. Finally, we conclude by talking through the challenges and issues of ConvSearch, particularly in Bio-Medicine. Our main aim is to provide an integrated and unified vision of the ConvSearch components from different fields, which benefit the information-seeking process in healthcare systems.},
booktitle = {Proceedings of the 2023 ACM Southeast Conference},
pages = {78–88},
numpages = {11},
keywords = {information retrieval, conversational search, question answering, knowledge base, dialogue management systems, recommender systems, generative language models, biomedical convsearch, privacy concerns},
location = {Virtual Event, USA},
series = {ACMSE '23}
}

@proceedings{10.1145/3611380,
title = {MMAsia '23 Workshops: Proceedings of the 5th ACM International Conference on Multimedia in Asia Workshops},
year = {2023},
isbn = {9798400703263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tainan, Taiwan}
}

@proceedings{10.1145/3600211,
title = {AIES '23: Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Montr\'{e}al, QC, Canada}
}

@proceedings{10.1145/3579051,
title = {IJCKG '22: Proceedings of the 11th International Joint Conference on Knowledge Graphs},
year = {2022},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@article{10.1145/3439816,
author = {Guo, Bin and Wang, Hao and Ding, Yasan and Wu, Wei and Hao, Shaoyang and Sun, Yueqi and Yu, Zhiwen},
title = {Conditional Text Generation for Harmonious Human-Machine Interaction},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3439816},
doi = {10.1145/3439816},
abstract = {In recent years, with the development of deep learning, text-generation technology has undergone great changes and provided many kinds of services for human beings, such as restaurant reservation and daily communication. The automatically generated text is becoming more and more fluent so researchers begin to consider more anthropomorphic text-generation technology, that is, the conditional text generation, including emotional text generation, personalized text generation, and so on. Conditional Text Generation (CTG) has thus become a research hotspot. As a promising research field, we find that much attention has been paid to exploring it. Therefore, we aim to give a comprehensive review of the new research trends of CTG. We first summarize several key techniques and illustrate the technical evolution route in the field of neural text generation, based on the concept model of CTG. We further make an investigation of existing CTG fields and propose several general learning models for CTG. Finally, we discuss the open issues and promising research directions of CTG.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {14},
numpages = {50},
keywords = {Human-computer interaction, conditional text generation, deep learning, dialog systems, personalization}
}

@inproceedings{10.1145/3534678.3539443,
author = {Huang, Jiaxin and Meng, Yu and Han, Jiawei},
title = {Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539443},
doi = {10.1145/3534678.3539443},
abstract = {We study the problem of few-shot Fine-grained Entity Typing (FET), where only a few annotated entity mentions with contexts are given for each entity type. Recently, prompt-based tuning has demonstrated superior performance to standard fine-tuning in few-shot scenarios by formulating the entity type classification task as a ''fill-in-the-blank'' problem. This allows effective utilization of the strong language modeling capability of Pre-trained Language Models (PLMs). Despite the success of current prompt-based tuning approaches, two major challenges remain: (1) the verbalizer in prompts is either manually designed or constructed from external knowledge bases, without considering the target corpus and label hierarchy information, and (2) current approaches mainly utilize the representation power of PLMs, but have not explored their generation power acquired through extensive general-domain pre-training. In this work, we propose a novel framework for few-shot FET consisting of two modules: (1) an entity type label interpretation module automatically learns to relate type labels to the vocabulary by jointly leveraging few-shot instances and the label hierarchy, and (2) a type-based contextualized instance generator produces new instances based on given instances to enlarge the training set for better generalization. On three benchmark datasets, our model outperforms existing methods by significant margins.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {605–614},
numpages = {10},
keywords = {entity typing, few-shot learning, prompt-based learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3583780.3614961,
author = {Li, Qi},
title = {Harnessing the Power of Pre-trained Vision-Language Models for Efficient Medical Report Generation},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614961},
doi = {10.1145/3583780.3614961},
abstract = {Medical images are commonly used in clinical practice. But the need for diagnosis and reporting from image-based examinations far excels the current medical capacity. Automatic Medical Report Generation (MRG) can help to ease the burden of radiologists. Vision-Language Pre-training (VLP) has received tremendous success on various tasks, therefore it is naturally expected that MRG can harvest from this rapid advancement. However, directly applying existing VLP models in the medical domain is impracticable due to their data-hungry nature, the need for aligning different modalities, prohibitive training time, exorbitant hardware barrier, and the challenge of open-ended text generation. To address these problems, we propose MedEPT, a parameter-efficient approach for MRG that can utilize ever-ignored image-only datasets. It employs parameter-efficient tuning (PET) for VLP adaption to mitigate inefficiency in fine-tuning time and hardware. MedEPT also employs MRGPID to augment and expand adaption datasets by synthesizing meaningful text for image-only datasets. We perform a systematic evaluation of our method. Empirical results show that we obtain a better performance than the state-of-the-art method while using less than 10% trainable parameters and not more than 30% training time than ever before.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {1308–1317},
numpages = {10},
keywords = {large language models, medical report generation, pre-trained vision-language models},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3539597.3572720,
author = {Zhu, Chenguang and Xu, Yichong and Ren, Xiang and Lin, Bill Yuchen and Jiang, Meng and Yu, Wenhao},
title = {Knowledge-Augmented Methods for Natural Language Processing},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3572720},
doi = {10.1145/3539597.3572720},
abstract = {Knowledge in NLP has been a rising trend especially after the advent of large-scale pre-trained models. Knowledge is critical to equip statistics-based models with common sense, logic and other external information. In this tutorial, we will introduce recent state-of-the-art works in applying knowledge in language understanding, language generation and commonsense reasoning.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1228–1231},
numpages = {4},
keywords = {commonsense reasoning, knowledge-augmented methods, language generation, natural language understanding},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@article{10.1145/3626763,
author = {Fan, Wenfei and Han, Ziyan and Ren, Weilong and Wang, Ding and Wang, Yaoshu and Xie, Min and Yan, Mengyi},
title = {Splitting Tuples of Mismatched Entities},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626763},
doi = {10.1145/3626763},
abstract = {There has been a host of work on entity resolution (ER), to identify tuples that refer to the same entity. This paper studies the inverse of ER, to identify tuples to which distinct real-world entities are matched by mistake, and split such tuples into a set of tuples, one for each entity. We formulate the tuple splitting problem. We propose a scheme to decide what tuples to split and what tuples to correct without splitting, fix errors/assign attribute values to the split tuples, and impute missing values. The scheme introduces a class of rules, which embed predicates for aligning entities across relations and knowledge graphs G, assessing correlation between attributes, and extracting data from G. It unifies logic deduction, correlation models, and data extraction by chasing the data with the rules. We train machine learning models to assess attribute correlation and predict missing values. We develop algorithms for the tuple splitting scheme. Using real-life data, we empirically verify that the scheme is efficient and accurate, with F-measure 0.92 on average.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {269},
numpages = {29},
keywords = {data quality, entity resolution, tuple splitting}
}

@proceedings{10.1145/3628454,
title = {IAIT '23: Proceedings of the 13th International Conference on Advances in Information Technology},
year = {2023},
isbn = {9798400708497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangkok, Thailand}
}

@proceedings{10.1145/3627915,
title = {CSAE '23: Proceedings of the 7th International Conference on Computer Science and Application Engineering},
year = {2023},
isbn = {9798400700590},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

@article{10.14778/3626292.3626294,
author = {Arora, Simran and Yang, Brandon and Eyuboglu, Sabri and Narayan, Avanika and Hojel, Andrew and Trummer, Immanuel and R\'{e}, Christopher},
title = {Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes},
year = {2023},
issue_date = {October 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3626292.3626294},
doi = {10.14778/3626292.3626294},
abstract = {A long standing goal in the data management community is developing systems that input documents and output queryable tables without user effort. Given the sheer variety of potential documents, state-of-the art systems make simplifying assumptions and use domain specific training. In this work, we ask whether we can maintain generality by using the in-context learning abilities of large language models (LLMs). We propose and evaluate Evaporate, a prototype system powered by LLMs. We identify two strategies for implementing this system: prompt the LLM to directly extract values from documents or prompt the LLM to synthesize code that performs the extraction. Our evaluations show a cost-quality tradeoff between these two approaches. Code synthesis is cheap, but far less accurate than directly processing each document with the LLM. To improve quality while maintaining low cost, we propose an extended implementation, Evaporate-Code+, which achieves better quality than direct extraction. Our insight is to generate many candidate functions and ensemble their extractions using weak supervision. Evaporate-Code+ outperforms the state-of-the art systems using a sublinear pass over the documents with the LLM. This equates to a 110X reduction in the number of documents the LLM needs to process across our 16 real-world evaluation settings.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {92–105},
numpages = {14}
}

@article{10.1145/3590773,
author = {Becattini, Federico and Bongini, Pietro and Bulla, Luana and Bimbo, Alberto Del and Marinucci, Ludovica and Mongiov\`{\i}, Misael and Presutti, Valentina},
title = {VISCOUNTH: A Large-scale Multilingual Visual Question Answering Dataset for Cultural Heritage},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1551-6857},
url = {https://doi.org/10.1145/3590773},
doi = {10.1145/3590773},
abstract = {Visual question answering has recently been settled as a fundamental multi-modal reasoning task of artificial intelligence that allows users to get information about visual content by asking questions in natural language. In the cultural heritage domain, this task can contribute to assisting visitors in museums and cultural sites, thus increasing engagement. However, the development of visual question answering models for cultural heritage is prevented by the lack of suitable large-scale datasets. To meet this demand, we built a large-scale heterogeneous and multilingual (Italian and English) dataset for cultural heritage that comprises approximately 500K Italian cultural assets and 6.5M question-answer pairs. We propose a novel formulation of the task that requires reasoning over both the visual content and an associated natural language description, and present baselines for this task. Results show that the current state of the art is reasonably effective but still far from satisfactory; therefore, further research in this area is recommended. Nonetheless, we also present a holistic baseline to address visual and contextual questions and foster future research on the topic.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {193},
numpages = {20},
keywords = {Visual question answering, cultural heritage}
}

@inproceedings{10.1145/3597638.3615650,
author = {McDonnell, Emma J. and Mack, Kelly Avery and Gerling, Kathrin and Spiel, Katta and Bennett, Cynthia L. and Brewer, Robin N. and Williams, Rua Mae and Tigwell, Garreth W.},
title = {Tackling the Lack of a Practical Guide in Disability-Centered Research},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597638.3615650},
doi = {10.1145/3597638.3615650},
abstract = {Accessibility research strives to develop technology that is useful for disabled people, but the research processes that we engage in do not always center disabled people in a way that allows us to shape artifacts so that they benefit disabled communities. In this workshop, we want to address core questions that are relevant in this context: How can research questions be defined in a way that shares power between research teams and technology users? How should research processes be designed to be broadly accessible for disabled people? And what are equitable ways of summarizing and sharing research findings in a way that allows disabled communities to critically appraise findings with us? Through discussion among all attendees, we want to develop a practical guide in disability-centered research that will be made available and further developed as a community resource when engaging in accessibility research.},
booktitle = {Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {106},
numpages = {5},
keywords = {Access, Disability Justice, Research Methods},
location = {New York, NY, USA},
series = {ASSETS '23}
}

@article{10.1145/3588911,
author = {Omar, Reham and Dhall, Ishika and Kalnis, Panos and Mansour, Essam},
title = {A Universal Question-Answering Platform for Knowledge Graphs},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588911},
doi = {10.1145/3588911},
abstract = {Knowledge from diverse application domains is organized as knowledge graphs (KGs) that are stored in RDF engines accessible in the web via SPARQL endpoints. Expressing a well-formed SPARQL query requires information about the graph structure and the exact URIs of its components, which is impractical for the average user. Question answering (QA) systems assist by translating natural language questions to SPARQL. Existing QA systems are typically based on application-specific human-curated rules, or require prior information, expensive pre-processing and model adaptation for each targeted KG. Therefore, they are hard to generalize to a broad set of applications and KGs. In this paper, we propose KGQAn, a universal QA system that does not need to be tailored to each target KG. Instead of curated rules, KGQAn introduces a novel formalization of question understanding as a text generation problem to convert a question into an intermediate abstract representation via a neural sequence-to-sequence model. We also develop a just-in-time linker that maps at query time the abstract representation to a SPARQL query for a specific KG, using only the publicly accessible APIs and the existing indices of the RDF store, without requiring any pre-processing. Our experiments with several real KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin the state-of-the-art in terms of quality of answers and processing time, especially for arbitrary KGs, unseen during the training.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {57},
numpages = {25},
keywords = {RDF, just-in-time entity and relation linking, knowledge graphs, natural language question answering, seq2seq models}
}

@inproceedings{10.1145/3583780.3614758,
author = {Yang, Kailai and Zhang, Tianlin and Ji, Shaoxiong and Ananiadou, Sophia},
title = {A Bipartite Graph is All We Need for Enhancing Emotional Reasoning with Commonsense Knowledge},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614758},
doi = {10.1145/3583780.3614758},
abstract = {The context-aware emotional reasoning ability of AI systems, especially in conversations, is of vital importance in applications such as online opinion mining from social media and empathetic dialogue systems. Due to the implicit nature of conveying emotions in many scenarios, commonsense knowledge is widely utilized to enrich utterance semantics and enhance conversation modeling. However, most previous knowledge infusion methods perform empirical knowledge filtering and design highly customized architectures for knowledge interaction with the utterances, which can discard useful knowledge aspects and limit their generalizability to different knowledge sources. Based on these observations, we propose a Bipartite Heterogeneous Graph (BHG) method for enhancing emotional reasoning with commonsense knowledge. In BHG, the extracted context-aware utterance representations and knowledge representations are modeled as heterogeneous nodes. Two more knowledge aggregation node types are proposed to perform automatic knowledge filtering and interaction. BHG-based knowledge infusion can be directly generalized to multi-type and multi-grained knowledge sources. In addition, we propose a Multi-dimensional Heterogeneous Graph Transformer (MHGT) to perform graph reasoning, which can retain unchanged feature spaces and unequal dimensions for heterogeneous node types during inference to prevent unnecessary loss of information. Experiments show that BHG-based methods significantly outperform state-of-the-art knowledge infusion methods and show generalized knowledge infusion ability with higher efficiency. Further analysis proves that previous empirical knowledge filtering methods do not guarantee to provide the most useful knowledge information. Our code is available at: https://github.com/SteveKGYang/BHG.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2917–2927},
numpages = {11},
keywords = {bipartite heterogeneous graph, casual emotion entailment, emotion recognition in conversations, knowledge infusion},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3610591.3616429,
author = {Bhardwaj, Purav and Sra, Misha},
title = {Ghost in the Machine : Discourses with AI},
year = {2023},
isbn = {9798400703201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610591.3616429},
doi = {10.1145/3610591.3616429},
abstract = {AI systems analyze vast amounts of data, uncover patterns, and make decisions - emulating a semblance of intelligence despite lacking qualia and embodiment that form the basis of the human condition. In this paper, we expound on "Ghost in the Machine", an interactive installation that delves into our pervasive tendency to anthropomorphize AI, ascribing human-like qualities, intentions, and even consciousness. Participants engage in dialogue with the AI as it collaboratively materializes the AI's thoughts in moving image and generative sound. The installation attempts to forge embodiment for an amorphous AI, revealing errors in its comprehension, represented by the metaphor of hallucinations.},
booktitle = {SIGGRAPH Asia 2023 Art Papers},
articleno = {6},
numpages = {6},
keywords = {Anthropomorphism, Artificial Intelligence, Simulation},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@article{10.1145/3505245,
author = {Gruetzemacher, Ross and Paradice, David},
title = {Deep Transfer Learning &amp; Beyond: Transformer Language Models in Information Systems Research},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3505245},
doi = {10.1145/3505245},
abstract = {AI is widely thought to be poised to transform business, yet current perceptions of the scope of this transformation may be myopic. Recent progress in natural language processing involving transformer language models (TLMs) offers a potential avenue for AI-driven business and societal transformation that is beyond the scope of what most currently foresee. We review this recent progress as well as recent literature utilizing text mining in top IS journals to develop an outline for how future IS research can benefit from these new techniques. Our review of existing IS literature reveals that suboptimal text mining techniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involving text data, and to enable new IS research topics, thus creating more value for the research community. This is possible because these techniques make it easier to develop very powerful custom systems and their performance is superior to existing methods for a wide range of tasks and applications. Further, multilingual language models make possible higher quality text analytics for research in multiple languages. We also identify new avenues for IS research, like language user interfaces, that may offer even greater potential for future IS research.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {204},
numpages = {35},
keywords = {Natural language processing, text mining, artificial intelligence, deep learning, transfer learning, language models}
}

@inproceedings{10.1145/3568294.3580129,
author = {Pramanick, Pradip and Sarkar, Chayan},
title = {Utilizing Prior Knowledge to Improve Automatic Speech Recognition in Human-Robot Interactive Scenarios},
year = {2023},
isbn = {9781450399708},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568294.3580129},
doi = {10.1145/3568294.3580129},
abstract = {The prolificacy of human-robot interaction not only depends on a robot's ability to understand the intent and content of the human utterance but also gets impacted by the automatic speech recognition (ASR) system. Modern ASR can provide highly accurate (grammatically and syntactically) translation. Yet, the general purpose ASR often misses out on the semantics of the translation by incorrect word prediction due to open-vocabulary modeling. ASR inaccuracy can have significant repercussions as this can lead to a completely different action by the robot in the real world. Can any prior knowledge be helpful in such a scenario? In this work, we explore how prior knowledge can be utilized in ASR decoding. Using our experiments, we demonstrate how our system can significantly improve ASR translation for robotic task instruction.},
booktitle = {Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {471–475},
numpages = {5},
keywords = {asr, cognitive robot, embodied agent, hri, robotics knowledge},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@article{10.1145/3575666,
author = {Greengard, Samuel},
title = {Computational Linguistics Finds its Voice},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3575666},
doi = {10.1145/3575666},
abstract = {Advances in artificial intelligence permit computers to converse with humans in seemingly realistic ways.},
journal = {Commun. ACM},
month = jan,
pages = {18–20},
numpages = {3}
}

@article{10.1145/3527546.3527568,
author = {Ghosal, Tirthankar and Al-Khatib, Khalid and Hou, Yufang and de Waard, Anita and Freitag, Dayne},
title = {Report on the 1st workshop on argumentation knowledge graphs (ArgKG 2021) at AKBC 2021},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3527546.3527568},
doi = {10.1145/3527546.3527568},
abstract = {The first workshop on Argumentation Knowledge Graphs (ArgKG) was held virtually at the Automated Knowledge Base Construction (AKBC 2021) conference on October 7, 2021. ArgKG @ AKBC 2021 brought together the Computational Argumentation and Knowledge Graphs communities, aiming to promote cross-pollination of ideas and encourage discussions and collaborations between the two communities. This paper describes the workshop and compiles several of its findings and insights.Date: 7 October, 2021.Website: https://argkg21.argmining.org.},
journal = {SIGIR Forum},
month = mar,
articleno = {19},
numpages = {12}
}

@inproceedings{10.1145/3501409.3501595,
author = {Zhao, Di and Jiang, Shuai and Wang, Wei and Zhang, Jing and Luan, Rui-Peng},
title = {Domain Named Entity Recognition and Applications in Test and Evaluation},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501595},
doi = {10.1145/3501409.3501595},
abstract = {A great amount of information in Test and Evaluation (T&amp;E) is presented in the form of multi-source heterogeneous data such as performance test, combat trial and during-service assessment. Despite the existence of numerous and well-versed Domain Named Entity Recognition (DNER) methods in the general field, it still remains scarcely resourced. In this paper we survey novel methods that have recently been introduced for such DNER tasks. In addition, we construct the dataset for further NER tasks in the field of Test and Evaluation. Finally, our work lays the cornerstone for the development of subsequent NER in this field.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1043–1049},
numpages = {7},
keywords = {DNER, NER, Test and Evaluation},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1145/3591106.3592227,
author = {Adjali, Omar and Grimal, Paul and Ferret, Olivier and Ghannay, Sahar and Le Borgne, Herv\'{e}},
title = {Explicit Knowledge Integration for Knowledge-Aware Visual Question Answering about Named Entities},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592227},
doi = {10.1145/3591106.3592227},
abstract = {Recent years have shown unprecedented growth of interest in Vision-Language related tasks, with the need to address the inherent challenges of integrating linguistic and visual information to solve real-world applications. Such a typical task is Visual Question Answering (VQA), which aims to answer questions about visual content. The limitations of the VQA task in terms of question redundancy and poor linguistic variability encouraged researchers to propose Knowledge-aware Visual Question Answering tasks as a natural extension of VQA. In this paper, we tackle the KVQAE (Knowledge-based Visual Question Answering about named Entities) task, which proposes to answer questions about named entities defined in a knowledge base and grounded in visual content. In particular, besides the textual and visual information, we propose to leverage the structural information extracted from syntactic dependency trees and external knowledge graphs to help answer questions about a large spectrum of entities of various types. Thus, by combining contextual and graph-based representations using Graph Convolutional Networks (GCNs), we are able to learn meaningful embeddings for Information Retrieval tasks. Experiments on the ViQuAE public dataset show how our approach improves the state-of-the-art baselines while demonstrating the interest of injecting external knowledge to enhance multimodal information retrieval.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {29–38},
numpages = {10},
keywords = {Knowledge injection, Multimedia retrieval},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3581641.3584057,
author = {Jung, Jeesu and Seo, Hyein and Jung, Sangkeun and Chung, Riwoo and Ryu, Hwijung and Chang, Du-Seong},
title = {Interactive User Interface for Dialogue Summarization},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584057},
doi = {10.1145/3581641.3584057},
abstract = {Summarization is one of the important tasks of natural language processing used to distill information. Recently, the sequence-to-sequence method was applied, in a general manner, to summarization tasks. The problem is that a large amount of information must be pre-trained for a specific domain, and information other than input statements cannot be utilized. To compensate for this shortcoming, controllable summarization has recently been in the spotlight. We introduced three properties into controllable summarization: 1) a new human-machine communication input format, 2) a robust constraint-sensitive summarization method for these formats, and 3) a practical interactive summarization interface available to the user. Experiments on the Wizard-of-Wikipedia dataset show that applying this input format and the constraint-sensitive method enhances summarization performance compared to the typical method. A user study shows that the interactive summarization interface is practical and that participants are evaluating it positively.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {934–957},
numpages = {24},
keywords = {Dialogue summarization, constraint-sensitive generation, neural networks, text tagging},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3583780.3614860,
author = {Tang, Zee Hen and Yeh, Mi-Yen},
title = {EAGLE: Enhance Target-Oriented Dialogs by Global Planning and Topic Flow Integration},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614860},
doi = {10.1145/3583780.3614860},
abstract = {In this study, we propose a novel model EAGLE for target-oriented dialogue generation. Without relying on any knowledge graphs, our method integrates the global planning strategy in both topic path generation and response generation given the initial and target topics. EAGLE comprises three components: a topic path sampling strategy, a topic flow generator, and a global planner. Our approach confers a number of advantages: EAGLE is robust to the target that has never appeared in the training data set and able to plan the topic flow globally. The topic path sampling strategy samples topic paths based on two predefined rules and use the sampled paths to train the topic path generator. The topic flow generator then applies a non-autoregressive method to generate intermediate topics that link the initial and target topics smoothly. In addition, the global planner is a response generator that generates a response based on the future topic sequence and conversation history, enabling it to plan how to transition to future topics smoothly. Our experimental results demonstrate that EAGLE produces more coherent responses and smoother transitions than state-of-the-art baselines, with an overall success rate improvement of approximately 25% and an average smoothness score improvement of 10% in both offline and human evaluations.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2402–2411},
numpages = {10},
keywords = {conversation generation, global planning, target-oriented, topic transition},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3582768.3582786,
author = {Dao, An Tuan and Aizawa, Akiko and Matsumoto, Yuji},
title = {Named Entity Recognition on COVID-19 Scientific Papers},
year = {2023},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582768.3582786},
doi = {10.1145/3582768.3582786},
abstract = {Text mining techniques, especially named entity recognition (NER), play a vital role in supporting researchers for keeping track of hundred thousand of papers on COVID-19 related literature. Although a few research has been performed NER on COVID-19 scientific papers, very little is currently known concerning the behaviors of current entity recognition models in this new domain. Therefore, this ongoing study attempts to analyze current NER models’ performance and limitations on the CORD-19 dataset. By examining three NER models, this study showed that NER performance is improved with the similarity between the testing and pretraining data. When there are little manually annotated resources for COVID-19 NER exist, our analysis suggested that for training purposes, enhancing the dictionary for seed annotation is effective (not necessarily requiring costly human annotation).},
booktitle = {Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
pages = {26–30},
numpages = {5},
keywords = {COVID-19, coronavirus, named entity recognition},
location = {Bangkok, Thailand},
series = {NLPIR '22}
}

@article{10.1145/3564275,
author = {van der Linden, Sanne and Sevastjanova, Rita and Funk, Mathias and El-Assady, Mennatallah},
title = {MediCoSpace: Visual Decision-Support for Doctor-Patient Consultations using Medical Concept Spaces from EHRs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3564275},
doi = {10.1145/3564275},
abstract = {Healthcare systems are under pressure from an aging population, rising costs, and increasingly complex conditions and treatments. Although data are determined to play a bigger role in how doctors diagnose and prescribe treatments, they struggle due to a lack of time and an abundance of structured and unstructured information. To address this challenge, we introduce MediCoSpace, a visual decision-support tool for more efficient doctor-patient consultations. The tool links patient reports to past and present diagnoses, diseases, drugs, and treatments, both for the current patient and other patients in comparable situations. MediCoSpace uses textual medical data, deep-learning supported text analysis and concept spaces to facilitate a visual discovery process. The tool is evaluated by five medical doctors. The results show that MediCoSpace facilitates a promising, yet complex way to discover unlikely relations and thus suggests a path toward the development of interactive visual tools to provide physicians with more holistic diagnoses and personalized, dynamic treatments for patients.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {15},
numpages = {20},
keywords = {Visual analytics, natural language processing, interaction design, electronic health records}
}

@article{10.1145/3606699,
author = {Pich\'{e}, Dominique and Font, Ludovic and Zouaq, Amal and Gagnon, Michel},
title = {Comparing Heuristic Rules and Masked Language Models for Entity Alignment in the Literature Domain},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3606699},
doi = {10.1145/3606699},
abstract = {The cultural world offers a staggering amount of rich and varied metadata on cultural heritage, accumulated by governmental, academic, and commercial players. However, the variety of involved institutions means that the data are stored in as many complex and often incompatible models and standards, which limits its availability and explorability by the greater public. The adoption of Linked Open Data technologies allows a strong interlinking of these various databases as well as external connections with existing knowledge bases. However, as they often contain references to the same entities, the delicate issue of entity alignment becomes the central challenge, especially in the absence or scarcity of unique global identifiers. To tackle this issue, we explored two approaches, one based on a set of heuristic rules and one based on masked language models, or masked language models (MLMs). We compare these two approaches, as well as different variations of MLMs, including some models trained on a different language, and various levels of data cleaning and labeling. Our results show that heuristics are a solid approach but also that MLM-based entity alignment obtains better performance coupled with the fact that it is robust to the data format and does not require any form of data preprocessing, which was not the case of the heuristic approach in our experiments.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {62},
numpages = {18},
keywords = {Linked open data, entity matching, masked language models, cultural heritage, literature}
}

@article{10.1145/3556538,
author = {Benedetto, Luca and Cremonesi, Paolo and Caines, Andrew and Buttery, Paula and Cappelli, Andrea and Giussani, Andrea and Turrin, Roberto},
title = {A Survey on Recent Approaches to Question Difficulty Estimation from Text},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3556538},
doi = {10.1145/3556538},
abstract = {Question Difficulty Estimation from Text (QDET) is the application of Natural Language Processing techniques to the estimation of a value, either numerical or categorical, which represents the difficulty of questions in educational settings. We give an introduction to the field, build a taxonomy based on question characteristics, and present the various approaches that have been proposed in recent years, outlining opportunities for further research. This survey provides an introduction for researchers and practitioners into the domain of question difficulty estimation from text and acts as a point of reference about recent research in this topic to date.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {178},
numpages = {37},
keywords = {Question difficulty estimation, question calibration, student assessment}
}

@inproceedings{10.1145/3604237.3626862,
author = {Chung, Andy and Tanaka-Ishii, Kumiko},
title = {Modeling Momentum Spillover with Economic Links Discovered from Financial Documents},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626862},
doi = {10.1145/3604237.3626862},
abstract = {Momentum spillover is a market anomaly well-acknowledged in finance literature. This paper proposes using novel economic links discovered from financial documents as a momentum spillover channel, followed by modeling with graph attention networks. These text-based economic links are constructed using contextual embeddings extracted with pre-trained language models from various sections of company annual reports and earnings call transcripts. We examine the effectiveness of our proposed methods based on point-in-time S&amp;P500 constituents from 2010/01/01 to 2022/12/31 in the US stock market. We compare our proposed model against the mean aggregator of peer firms’ momentum baseline and Monte Carlo experiments based on randomized nodes or edges. Our results show that our proposed graph neural network model significantly outperforms the peer firm’s momentum aggregation baseline. Furthermore, economic links discovered in some sections of company annual reports and earnings call transcripts are useful for modeling momentum spillover. In particular, the economic link constructed from management discussion and analysis from earnings call transcripts outperforms the industry link, which represents the well-acknowledged industry momentum.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {490–497},
numpages = {8},
keywords = {Economic links, graph neural networks, inattention, large language models, momentum},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3543507.3583387,
author = {Zhang, Zhenyu and Yu, Bowen and Liu, Tingwen and Liu, Tianyun and Wang, Yubin and Guo, Li},
title = {Learning Structural Co-occurrences for Structured Web Data Extraction in Low-Resource Settings},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583387},
doi = {10.1145/3543507.3583387},
abstract = {Extracting structured information from all manner of webpages is an important problem with the potential to automate many real-world applications. Recent work has shown the effectiveness of leveraging DOM trees and pre-trained language models to describe and encode webpages. However, they typically optimize the model to learn the semantic co-occurrence of elements and labels in the same webpage, thus their effectiveness depends on sufficient labeled data, which is labor-intensive. In this paper, we further observe structural co-occurrences in different webpages of the same website: the same position in the DOM tree usually plays the same semantic role, and the DOM nodes in this position also share similar surface forms. Motivated by this, we propose a novel method, Structor, to effectively incorporate the structural co-occurrences over DOM tree and surface form into pre-trained language models. Such structural co-occurrences help the model learn the task better under low-resource settings, and we study two challenging experimental scenarios: website-level low-resource setting and webpage-level low-resource setting, to evaluate our approach. Extensive experiments on the public SWDE dataset show that Structor significantly outperforms the state-of-the-art models in both settings, and even achieves three times the performance of the strong baseline model in the case of extreme lack of training data.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1683–1692},
numpages = {10},
keywords = {low-resource setting, regular expression, structural co-occurrence, web information extraction},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3503161.3551610,
author = {Ramesh, Raksha and Anand, Vishal and Chen, Zifan and Dong, Yifei and Chen, Yun and Lin, Ching-Yung},
title = {Leveraging Text Representation and Face-head Tracking for Long-form Multimodal Semantic Relation Understanding},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3551610},
doi = {10.1145/3503161.3551610},
abstract = {In the intricate problem of understanding long-form multi-modal inputs, few key-aspects in scene-understanding and dialogue-and-discourse are often overlooked. In this paper, we investigate two such key-aspects for better semantic and relational understanding - (i). head-object-tracking in addition to usual face-tracking, and (ii). fusing scene-to-text representation with external common-sense knowledge-base for effective mapping to sub-tasks of interest. The usage of head-tracking especially helps with enriching sparse entity mapping to inter-entity conversation interactions. These methods are guided by natural language supervision on visual models, and perform well for interaction and sentiment understanding tasks.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {7215–7219},
numpages = {5},
keywords = {dialogue and discourse, intent detection, knowledge graphs, language models, natural language processing, object tracking, slot filling, speaker diarization},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2023.3275028,
author = {Deng, Shumin and Yang, Jiacheng and Ye, Hongbin and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Huang, Fei and Chen, Huajun and Zhang, Ningyu},
title = {LOGEN: Few-Shot Logical Knowledge-Conditioned Text Generation With Self-Training},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3275028},
doi = {10.1109/TASLP.2023.3275028},
abstract = {Natural language generation from structured data mainly focuses on surface-level descriptions, suffering from uncontrollable content selection and low fidelity. Previous works leverage logical forms to facilitate logical knowledge-conditioned text generation. Though achieving remarkable progress, they are data-hungry, which makes the adoption for real-world applications challenging with limited data. To this end, this paper proposes a unified framework for logical knowledge-conditioned text generation in the few-shot setting. With only a few seeds logical forms (e.g., 20/100 shot), our approach leverages self-training and samples pseudo logical forms based on content and structure consistency. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {2124–2133},
numpages = {10}
}

@inproceedings{10.1145/3511047.3537659,
author = {Bolioli, Andrea and Bosca, Alessio and Damiano, Rossana and Lieto, Antonio and Striani, Manuel},
title = {A complementary account to emotion extraction and classification in cultural heritage based on the Plutchik’s theory},
year = {2022},
isbn = {9781450392327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511047.3537659},
doi = {10.1145/3511047.3537659},
abstract = {The paper presents a combined approach to knowledge-based emotion attribution and classification of cultural items employed in the H2020 project SPICE. In particular, we show a preliminary experimentation conducted on a selection of items contributed by the GAM Museum in Turin (Galleria di Arte Moderna), pointing out how different language-based approaches to emotion categorization (used in the systems Sophia and DEGARI respectively) can be powerfully combined to cope with both coverage and extended affective attributions. Interestingly, both approaches are based on an ontology of the Plutchik’s theory of emotions.},
booktitle = {Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {374–382},
numpages = {9},
keywords = {Affective Content Aggregation, Commonsense Reasoning, Description Logics},
location = {Barcelona, Spain},
series = {UMAP '22 Adjunct}
}

@article{10.1145/3617892,
author = {Niu, Yanrui and Liang, Chao and Lu, Ankang and Huang, Baojin and Wang, Zhongyuan and Guo, Jiahao},
title = {Person-action Instance Search in Story Videos: An Experimental Study},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3617892},
doi = {10.1145/3617892},
abstract = {Person-Action instance search (P-A INS) aims to retrieve the instances of a specific person doing a specific action, which appears in the 2019–2021 INS tasks of the world-famous TREC Video Retrieval Evaluation (TRECVID). Most of the top-ranking solutions can be summarized with a Division-Fusion-Optimization (DFO) framework, in which person and action recognition scores are obtained separately, then fused, and, optionally, further optimized to generate the final ranking. However, TRECVID only evaluates the final ranking results, ignoring the effects of intermediate steps and their implementation methods. We argue that conducting the fine-grained evaluations of intermediate steps of DFO framework will (1) provide a quantitative analysis of the different methods’ performance in intermediate steps; (2) find out better design choices that contribute to improving retrieval performance; and (3) inspire new ideas for future research from the limitation analysis of current techniques. Particularly, we propose an indirect evaluation method motivated by the leave-one-out strategy, which finds an optimal solution surpassing the champion teams in 2020–2021 INS tasks. Moreover, to validate the generalizability and robustness of the proposed solution under various scenarios, we specifically construct a new large-scale P-A INS dataset and conduct comparative experiments with both the leading NIST TRECVID INS solution and the state-of-the-art P-A INS method. Finally, we discuss the limitations of our evaluation work and suggest future research directions.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {46},
numpages = {34},
keywords = {Movie video, composite concepts, person-action instance search}
}

@proceedings{10.1145/3610591,
title = {SA '23: SIGGRAPH Asia 2023 Art Papers},
year = {2023},
isbn = {9798400703201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3615886,
title = {GeoAI '23: Proceedings of the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
year = {2023},
isbn = {9798400703485},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Emerging advances from artificial intelligence, hardware accelerators, and data processing architectures continue to reach the geospatial information sciences, with a transformative impact in many societal challenges. Recent breakthroughs in deep learning have brought forward an automated capability to learn hierarchical representational features from massive and complex data, including text, images, and videos. In tandem, rapid innovations in sensing technologies are supporting the collection of geospatial data in even higher resolution and throughput, supporting the observation, mapping, and analysis of different events/phenomena over the earth's surface with unprecedented detail. Combined, these developments are offering potential for breakthroughs in geographic knowledge discovery, impacting decision making in areas such as humanitarian mapping, intelligent transport systems, urban expansion analysis, health data analysis and epidemiology, the study of climate change, handling natural disasters, and the general monitoring of the Earth's surface.},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/3623462.3623465,
author = {Wehmeier, Colter and Artopoulos, Georgios},
title = {MetaFraming: A Methodology for Democratizing Heritage Interpretation Through Wiki Surveys},
year = {2023},
isbn = {9798400708367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623462.3623465},
doi = {10.1145/3623462.3623465},
abstract = {Recent developments in the Digital Humanities reveal how traditional survey methods, when applied to the study of cultural heritage, often struggle to encapsulate the intricate social dynamics of our interactions with built environments and artefacts. Despite the allure of digital tools promising scalability, nonlinearity, and increased engagement, their fit for heritage interpretation remains an open question. To address this gap, we introduce MetaFraming—a contribution to participatory methodology designed to leverage computational social science tools such as artificial intelligence and wiki surveys, towards inclusive and democratic approaches in heritage interpretation. MetaFraming enables researchers to transform extensive preliminary research notes into a metadata-rich, semantically structured dataset using an AI processing pipeline, thereby modelling diverse perspectives on heritage artefacts. Following manual refinement, this dataset serves as the initial ’seed’ state for a wiki survey (a user-editable, collaborative survey). Such a survey enables the crowd to rank propositions, comment, and contribute new ideas. Notably, participant input itself contains metadata, allowing for a subsequent automated pipeline to reconstruct the context of actions such as comments. This secondary process provides rich insights into recommendations, specific user/actor experiences, group interests, and the complex relationships between them. Through a design-research framework, we apply MetaFraming to a case study in architectural heritage: our artefact of study is Le Corbusier’s renowned Unit\'{e} d’habitation (1952), a seminal prototype for social housing and urbanism in post-war France. This exploration enables us to contrast our novel web-based survey method with traditional approaches, thereby highlighting new opportunities for computer-aided collaboration in heritage interpretation. By fostering reflective exploration of built environments and societal legacies, our work contributes to the growing discourse on digital technologies in cultural heritage, advocating for interdisciplinary research and dialogue.},
booktitle = {Proceedings of the 20th International Conference on Culture and Computer Science: Code and Materiality},
articleno = {4},
numpages = {9},
keywords = {MetaFraming, Modern Architectural Heritage, Participatory Heritage, Wiki Surveys},
location = {Lisbon, Portugal},
series = {KUI '23}
}

@inproceedings{10.1145/3573428.3573743,
author = {Xu, Liangbin and Ji, Baiyang},
title = {Industry Classification Algorithm Based on Improved BERT Model},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573743},
doi = {10.1145/3573428.3573743},
abstract = {With the rapid development of the economy, many enterprises are derived, and different enterprises have different economic activities, and the description of economic activities is usually in the form of short texts. The latest National Standard of the People's Republic of China - Classification of National Economic Industries is divided into 1,381 categories according to categories, major categories, medium categories and minor categories, and industry classification often relies on human experience, which takes a long time and the work is more mechanical. This paper aims to reduce the time and labor costs consumed by traditional industry classification methods. This paper draws on short text classification algorithm, convolutional neural network algorithm, and considers the particularity of different industries. An industry classification algorithm based on the improved BERT model is proposed. Based on the BERT model, the algorithm combines the convolutional neural network and the three-channel model to analyze the main business description of the listed company from the three levels of words, words and concepts, so as to determine the industry attribution. Taking the main products of listed companies screened in Shanghai Stock Exchange and Oriental Fortune Net in the past three years as a data set, the experimental results show that this method is excellent for industry classification.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1790–1794},
numpages = {5},
keywords = {BERT, Industry classification, convolutional neural network, short text classification, three-channel model},
location = {Xiamen, China},
series = {EITCE '22}
}

@proceedings{10.1145/3615887,
title = {GeoHumanities '23: Proceedings of the 7th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
year = {2023},
isbn = {9798400703492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 7th International Workshop on Geospatial Humanities (GeoHumanities 2023) was held, together with the 31st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, in Hamburg, Germany. This workshop has been a regular venue for computational research at the cutting edge of spatial data creation, curation, analysis, visualization, and interpretation in the humanities. It brings together researchers and practitioners from computer science, the geographical information sciences, and the humanities, whose work combines humanistic questions with computational spatial methods. The GeoHumanities series of workshops supports interdisciplinary, and often collaborative, research that makes novel contributions to both the humanities and the sciences.},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3603163,
title = {HT '23: Proceedings of the 34th ACM Conference on Hypertext and Social Media},
year = {2023},
isbn = {9798400702327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@inproceedings{10.1145/3583780.3614936,
author = {Guo, Hao and Zeng, Weixin and Tang, Jiuyang and Zhao, Xiang},
title = {Interpretable Fake News Detection with Graph Evidence},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614936},
doi = {10.1145/3583780.3614936},
abstract = {Automatic detection of fake news has received widespread attentions over recent years. A pile of efforts has been put forward to address the problem with high accuracy, while most of them lack convincing explanations, making it difficult to curb the continued spread of false news in real-life cases. Although some models leverage external resources to provide preliminary interpretability, such external signals are not always available. To fill in this gap, in this work, we put forward an interpretable fake news detection model IKA by making use of the historical evidence in the form of graphs. Specifically, we establish both positive and negative evidence graphs by collecting the signals from the historical news, i.e., training data. Then, given a piece of news to be detected, in addition to the common features used for detecting false news, we compare the news and evidence graphs to generate both the matching vector and the related graph evidence for explaining the prediction. We conduct extensive experiments on both Chinese and English datasets. The experiment results show that the detection accuracy of IKA exceeds the state-of-the-art approaches and IKA can provide useful explanations for the prediction results. Besides, IKA is general and can be applied on other models to improve their interpretability.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {659–668},
numpages = {10},
keywords = {explainable machine learning, fake news detection, social media},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3511808.3557506,
author = {Huang, Chao and Xia, Lianghao and Wang, Xiang and He, Xiangnan and Yin, Dawei},
title = {Self-Supervised Learning for Recommendation},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557506},
doi = {10.1145/3511808.3557506},
abstract = {Recommender systems are playing an increasingly critical role to alleviate information overload and satisfy users' information seeking requirements in a wide spectrum of online platforms. However, the ubiquity of data sparsity and noise notably limits the representation capacity of existing recommender systems to learn high-quality user (item) embeddings. Inspired by recent advances of self-supervised learning (SSL) techniques, SSL-based representation learning models benefit a variety of recommendation domains. Such methods have achieved new levels of performance while reducing the dependence on observed supervision labels in diverse recommendation tasks. In this tutorial, we aim to provide a systemic review of state-of-the-art SSL-based recommender systems. To be specific, we summarize and categorize existing work of SSL-based recommender systems in terms of recommendation scenarios. For each type of recommendation task, the corresponding challenges and methods will be presented in a comprehensive way. Finally, some future directions and open questions will be raised to inspire more investigation on this important research line.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {5136–5139},
numpages = {4},
keywords = {collaborative filtering, contrastive learning, graph neural networks, recommender system, self-supervised learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inbook{10.1145/3581906.3581922,
author = {Punjani, Dharmen and Tsalapati, Eleni},
title = {Question Answering Engines for Geospatial Knowledge Graphs},
year = {2023},
isbn = {9798400707407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3581906.3581922},
booktitle = {Geospatial Data Science: A Hands-on Approach for Building Geospatial Applications Using Linked Data Technologies},
pages = {257–282},
numpages = {26}
}

@inproceedings{10.1145/3599957.3606249,
author = {Ahn, Sung-Yoon and Lee, Sang-Woong},
title = {BERT-based classification of fungi protein sequences with multiple GO labels},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599957.3606249},
doi = {10.1145/3599957.3606249},
abstract = {Due to the increase of reported fungi-related diseases, it has come to many health organizations concern that there may be possible highly contagious fungi that may cause yet another pandemic. Though the likelihood of such is low, research is needed to grasp the understanding of unknown fungi. Identifying and figuring out the traits of unknown fungi through in vitro and in vivo experiments take time and resources. In silico methods yield faster results with a slight drop in accuracy. Modern in silico approaches utilizing deep learning, allow for faster and more accurate classifications. In this study, we perform the classification of one or more gene ontologies of fungi protein sequences. We collected open-source protein sequences from UniProt and applied an algorithm to label the sequences with their gene ontologies. We use ProtBERT with additional layers to give classification results to all the different gene ontologies. Experimental results reveal that when classifying with the top 5 most frequent gene ontologies, the model was able to yield 0.7915 for F1-score, 0.7073 for MCC, and 0.8865 for AuROC. With the top 10 most frequent gene ontologies it yielded 0.6490 for F1-score, 0.6836 for MCC, and 0.7653 for AuROC.},
booktitle = {Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
articleno = {28},
numpages = {4},
keywords = {BERT, Fungi, Gene Ontology},
location = {Gdansk, Poland},
series = {RACS '23}
}

@article{10.1145/3571726,
author = {Rahman, Md Rayhanur and Hezaveh, Rezvan Mahdavi and Williams, Laurie},
title = {What Are the Attackers Doing Now? Automating Cyberthreat Intelligence Extraction from Text on Pace with the Changing Threat Landscape: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571726},
doi = {10.1145/3571726},
abstract = {Cybersecurity researchers have contributed to the automated extraction of CTI from textual sources, such as threat reports and online articles describing cyberattack strategies, procedures, and tools. The goal of this article is to aid cybersecurity researchers in understanding the current techniques used for cyberthreat intelligence extraction from text through a survey of relevant studies in the literature. Our work finds 11 types of extraction purposes and 7 types of textual sources for CTI extraction. We observe the technical challenges associated with obtaining available clean and labeled data for replication, validation, and further extension of the studies. We advocate for building upon the current CTI extraction work to help cybersecurity practitioners with proactive decision-making such as in threat prioritization and mitigation strategy formulation to utilize knowledge from past cybersecurity incidents.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {241},
numpages = {36},
keywords = {Cyberthreat intelligence, CTI extraction, CTI mining, IoC extraction, TTPs extraction, attack pattern extraction, threat reports, tactical threat intelligence, technical threat intelligence}
}

@inproceedings{10.1145/3551349.3556929,
author = {Liu, Zixi and Feng, Yang and Yin, Yining and Sun, Jingyu and Chen, Zhenyu and Xu, Baowen},
title = {QATest: A Uniform Fuzzing Framework for Question Answering Systems},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556929},
doi = {10.1145/3551349.3556929},
abstract = {The tremendous advancements in deep learning techniques have empowered question answering(QA) systems with the capability of dealing with various tasks. Many commercial QA systems, such as Siri, Google Home, and Alexa, have been deployed to assist people in different daily activities. However, modern QA systems are often designed to deal with different topics and task formats, which makes both the test collection and labeling tasks difficult and thus threats their quality. To alleviate this challenge, in this paper, we design and implement a fuzzing framework for QA systems, namely QATest, based on the metamorphic testing theory. It provides the first uniform solution to generate tests with oracle information automatically for various QA systems, such as machine reading comprehension, open-domain QA, and QA on knowledge bases. To further improve testing efficiency and generate more tests detecting erroneous behaviors, we design N-Gram coverage and perplexity priority based on the features of the question data to guide the generation process. To evaluate the performance of QATest, we experiment with it on four QA systems that are designed for different tasks. The experiment results show that the tests generated by QATest detect hundreds of erroneous behaviors of QA systems efficiently. Also, the results confirm that the testing criteria can improve test diversity and fuzzing efficiency.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {81},
numpages = {12},
keywords = {automated testing, fuzz testing, natural language processing, question answering systems},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3629168,
author = {Ao, Xiang and Luo, Ling and Wang, Xiting and Yang, Zhao and Chen, Jiun-Hung and Qiao, Ying and He, Qing and Xie, Xing},
title = {Put Your Voice on Stage: Personalized Headline Generation for News Articles},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3629168},
doi = {10.1145/3629168},
abstract = {In this article, we study the problem of personalized news headline generation, which aims to produce not only concise and fact-consistent titles for news articles but also decorate these titles as personalized irresistible reading invitations by incorporating readers’ preferences. We propose an approach named PNG&nbsp;(Personalized News headline Generator) by utilizing distant supervision in readers’ past click behaviors to resolve. First, user preference representations are learned through a knowledge-aware user encoder that comprehensively captures the genuine, sequential, and flash interests of users reflected in their historical clicked news. Then, a user-perturbed pointer-generator network is devised to accomplish the headline generation in which the learned user representations implicitly affect the word prediction. The proposed model is optimized by reinforcement learning solvers where indicators on factual, personalized, and linguistic aspects of the generated headline are regarded as rewards. Extensive experiments are conducted on the real-world dataset PENS,1 which is a large-scale benchmark collected from Microsoft News. Both the quantitative and qualitative results validate the effectiveness of our approach.},
journal = {ACM Trans. Knowl. Discov. Data},
month = dec,
articleno = {54},
numpages = {20},
keywords = {News headline generation, user modeling, personalization}
}

@inproceedings{10.1145/3524458.3547242,
author = {Colagrossi, Marco and Consoli, Sergio and Panella, Francesco and Barbaglia, Luca},
title = {Tracking socio-economic activities in European countries with unconventional data},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524458.3547242},
doi = {10.1145/3524458.3547242},
abstract = {This contribution shows our ongoing work aimed at monitoring societal issues and economic activities (e.g., industrial production, unemployment, loneliness, cultural participation) across EU member states mining unconventional data sources to complement official statistics. Considered unconventional data sources include the Global Dataset of Events, Language and Tone (GDELT), Google Search data, and Dow Jones Data, News and Analytics (DNA). We show an early experiment aiming at nowcasting unemployment in Germany, Spain, France, and Italy, demonstrating the added value of these data both for scholars and policymakers.},
booktitle = {Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
pages = {323–330},
numpages = {8},
keywords = {alternative (big) datasets, social media, text analysis},
location = {Limassol, Cyprus},
series = {GoodIT '22}
}

@inproceedings{10.1145/3459637.3482222,
author = {Zhou, Yichao and Jiang, Jyun-Yu and Chen, Xiusi and Wang, Wei},
title = {#StayHome or #Marathon? Social Media Enhanced Pandemic Surveillance on Spatial-temporal Dynamic Graphs},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482222},
doi = {10.1145/3459637.3482222},
abstract = {COVID-19 has caused lasting damage to almost every domain in public health, society, and economy. To monitor the pandemic trend, existing studies rely on the aggregation of traditional statistical models and epidemic spread theory. In other words, historical statistics of COVID-19, as well as the population mobility data, become the essential knowledge for monitoring the pandemic trend. However, these solutions can barely provide precise prediction and satisfactory explanations on the long-term disease surveillance while the ubiquitous social media resources can be the key enabler for solving this problem. For example, serious discussions may occur on social media before and after some breaking events take place. To take advantage of the social media data, we propose a novel framework, Social Media enhAnced pandemic suRveillance Technique (SMART), which is composed of two modules: (i) information extraction module to construct heterogeneous knowledge graphs based on the extracted events and relationships among them; (ii) time series prediction module to provide both short-term and long-term forecasts of the confirmed cases and fatality at the state-level in the United States and to discover risk factors for COVID-19 interventions. Extensive experiments show that our method largely outperforms the state-of-the-art baselines by 7.3% and 7.4% in confirmed case/fatality prediction, respectively.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2738–2748},
numpages = {11},
keywords = {information extraction, social media mining, time series prediction},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1145/3580480,
author = {Du, Kelvin and Xing, Frank and Cambria, Erik},
title = {Incorporating Multiple Knowledge Sources for Targeted Aspect-based Financial Sentiment Analysis},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3580480},
doi = {10.1145/3580480},
abstract = {Combining symbolic and subsymbolic methods has become a promising strategy as research tasks in AI grow increasingly complicated and require higher levels of understanding. Targeted Aspect-based Financial Sentiment Analysis (TABFSA) is an example of such complicated tasks, as it involves processes like information extraction, information specification, and domain adaptation. However, little is known about the design principles of such hybrid models leveraging external lexical knowledge. To fill this gap, we define anterior, parallel, and posterior knowledge integration and propose incorporating multiple lexical knowledge sources strategically into the fine-tuning process of pre-trained transformer models for TABFSA. Experiments on the Financial Opinion mining and Question Answering challenge (FiQA) Task 1 and SemEval 2017 Task 5 datasets show that the knowledge-enabled models systematically improve upon their plain deep learning counterparts, and some outperform state-of-the-art results reported in terms of aspect sentiment analysis error. We discover that parallel knowledge integration is the most effective and domain-specific lexical knowledge is more important according to our ablation analysis.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {23},
numpages = {24},
keywords = {Financial sentiment analysis, neural networks, knowledge enabled system, deep learning, transformer models}
}

@inproceedings{10.1145/3583780.3614870,
author = {Tiwari, Abhisek and Saha, Anisha and Saha, Sriparna and Bhattacharyya, Pushpak and Dhar, Minakshi},
title = {Experience and Evidence are the eyes of an excellent summarizer! Towards Knowledge Infused Multi-modal Clinical Conversation Summarization},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614870},
doi = {10.1145/3583780.3614870},
abstract = {With the advancement of telemedicine, both researchers and medical practitioners are working hand-in-hand to develop various techniques to automate various medical operations, such as diagnosis report generation. In this paper, we first present a multi-modal clinical conversation summary generation task that takes a clinician-patient interaction (both textual and visual information) and generates a succinct synopsis of the conversation. We propose a knowledge-infused, multi-modal, multi-tasking medical domain identification and clinical conversation summary generation (MM-CliConSummation) framework. It leverages an adapter to infuse knowledge and visual features and unify the fused feature vector using a gated mechanism. Furthermore, we developed a multi-modal, multi-intent clinical conversation summarization corpus annotated with intent, symptom, and summary. The extensive set of experiments, both quantitatively and qualitatively, led to the following findings: (a) critical significance of visuals, (b) more precise and medical entity preserving summary with additional knowledge infusion, and (c) a correlation between medical department identification and clinical synopsis generation. Furthermore, the dataset and source code are available at https://github.com/NLP-RL/MM-CliConSummation},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2452–2461},
numpages = {10},
keywords = {multimodal infusion, multimodal medical dialogue summarization, online counselling, text generation},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3543507.3583330,
author = {Zhang, Rongjunchen and Wu, Tingmin and Chen, Xiao and Wen, Sheng and Nepal, Surya and Paris, Cecile and Xiang, Yang},
title = {Dynalogue: A Transformer-Based Dialogue System with Dynamic Attention},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583330},
doi = {10.1145/3543507.3583330},
abstract = {Businesses face a range of cyber risks, both external threats and internal vulnerabilities that continue to evolve over time. As cyber attacks continue to increase in complexity and sophistication, more organisations will experience them. For this reason, it is important that organisations seek timely consultancy from cyber professionals so that they can respond to and recover from cyber attacks as quickly as possible. However, huge surges in cyber attacks have long left cyber professionals short of what is required to cover the security needs. This problem is getting worse when an increasing number of people choose to work from home during the pandemic because this situation usually yields extra communication cost. In this paper, we propose to develop a cybersecurity-oriented dialogue system, called Dynalogue1, which can provide consultancy online as a cyber professional. For the first time, Dynalogue provides a promising solution to mitigate the need for cyber professionals via automatically generating problem-targeted conversions to victims of cyber attacks. In spite of many dialogue systems developed in the past, Dynalogue provides a distinct capability of handling long and complicated sentences that are common in cybersecurity-related conversations. It is challenging to have this capability because limited memory in dialogue systems can be hard to accommodate sufficient key information of long sentences. To overcome this challenge, Dynalogue utilises an attention mechanism that dynamically captures key semantics within a sentence instead of using fix window to cut off the sentence. To evaluate Dynalogue, we collect 67K real-world conversations (0.6M utterances) from Bleeping Computer2, which is one of the most popular cybersecurity consultancy websites in the world. The results suggest that Dynalogue outperforms all the existing dialogue systems with 1% ∼ 9% improvements on all different metrics. We further run Dynalogue on the public dataset WikiHow to validate its compatibility in other domains where conversations are also long and complicated. Dynalogue also outperforms all the other methods with at most 2.4% improvement.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1604–1615},
numpages = {12},
keywords = {Cybersecurity conversation dataset, Dynamic attention, Generation-based dialogue system, Retrieval-based dialogue system, Transformer},
location = {Austin, TX, USA},
series = {WWW '23}
}

@proceedings{10.1145/3586183,
title = {UIST '23: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Francisco, CA, USA}
}

@article{10.1109/TASLP.2023.3254166,
author = {Su, Xinxin and Huang, Zhen and Zhao, Yunxiang and Chen, Yifan and Dou, Yong and Pan, Hengyue},
title = {Recent Trends in Deep Learning Based Textual Emotion Cause Extraction},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3254166},
doi = {10.1109/TASLP.2023.3254166},
abstract = {Emotion Cause Extraction Field (ECEF) focuses on the cause that triggers an emotion in a document. Traditional ECEF aims to extract the cause based on a given emotion while recent ECEF focuses more on extracting both the emotion and its corresponding cause. ECEF has attracted a lot of attention due to the significant developments in deep learning techniques, especially machine reading comprehension and neural network-based information retrieval. However, a comprehensive review of existing approaches and recent trends in ECEF is lacking. In this paper, we present a thorough survey to summarise existing methods for ECEF, including those for Emotion Cause Extraction (ECE), Emotion Cause Pair Extraction (ECPE), and Conversational Emotion Cause Extraction Field (CECEF). We also detail the widely used public datasets and discuss the limitations and prospects of existing methods in ECEF.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2765–2786},
numpages = {22}
}

@article{10.1145/3473939,
author = {Ding, Haoran and Luo, Xiao},
title = {Attention-based Unsupervised Keyphrase Extraction and Phrase Graph for COVID-19 Medical Literature Retrieval},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3473939},
doi = {10.1145/3473939},
abstract = {Searching, reading, and finding information from the massive medical text collections are challenging. A typical biomedical search engine is not feasible to navigate each article to find critical information or keyphrases. Moreover, few tools provide a visualization of the relevant phrases to the query. However, there is a need to extract the keyphrases from each document for indexing and efficient search. The transformer-based neural networks—BERT has been used for various natural language processing tasks. The built-in self-attention mechanism can capture the associations between words and phrases in a sentence. This research investigates whether the self-attentions can be utilized to extract keyphrases from a document in an unsupervised manner and identify relevancy between phrases to construct a query relevancy phrase graph to visualize the search corpus phrases on their relevancy and importance. The comparison with six baseline methods shows that the self-attention-based unsupervised keyphrase extraction works well on a medical literature dataset. This unsupervised keyphrase extraction model can also be applied to other text data. The query relevancy graph model is applied to the COVID-19 literature dataset and to demonstrate that the attention-based phrase graph can successfully identify the medical phrases relevant to the query terms.},
journal = {ACM Trans. Comput. Healthcare},
month = oct,
articleno = {12},
numpages = {16},
keywords = {Keyphrase extraction, deep learning, medical information retrieval, COVID-19}
}

@article{10.1145/3520082,
author = {Shang, Yu-Ming and Huang, Heyan and Sun, Xin and Wei, Wei and Mao, Xian-Ling},
title = {Learning Relation Ties with a Force-Directed Graph in Distant Supervised Relation Extraction},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3520082},
doi = {10.1145/3520082},
abstract = {Relation ties, defined as the correlation and mutual exclusion between different relations, are critical for distant supervised relation extraction. Previous studies usually obtain this property by greedily learning the local connections between relations. However, they are essentially limited because of failing to capture the global topology structure of relation ties and may easily fall into a locally optimal solution. To address this issue, we propose a novel force-directed graph to comprehensively learn relation ties. Specifically, we first construct a graph according to the global co-occurrence of all relations. Then, we borrow the idea of Coulomb’s law from physics and introduce the concept of attractive force and repulsive force into this graph to learn correlation and mutual exclusion between relations. Finally, the obtained relation representations are applied as an inter-dependent relation classifier. Extensive experimental results demonstrate that our method is capable of modeling global correlation and mutual exclusion between relations, and outperforms the state-of-the-art baselines. In addition, the proposed force-directed graph can be used as a module to augment existing relation extraction systems and improve their performance.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {10},
numpages = {23},
keywords = {Distant supervision, relation extraction, relation ties, force-directed graph}
}

@inproceedings{10.1145/3485447.3511945,
author = {Liu, Xiao and Hong, Haoyun and Wang, Xinghao and Chen, Zeyi and Kharlamov, Evgeny and Dong, Yuxiao and Tang, Jie},
title = {SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511945},
doi = {10.1145/3485447.3511945},
abstract = {Entity alignment, aiming to identify equivalent entities across different knowledge graphs (KGs), is a fundamental problem for constructing Web-scale KGs. Over the course of its development, the label supervision has been considered necessary for accurate alignments. Inspired by the recent progress of self-supervised learning, we explore the extent to which we can get rid of supervision for entity alignment. Commonly, the label information (positive entity pairs) is used to supervise the process of pulling the aligned entities in each positive pair closer. However, our theoretical analysis suggests that the learning of entity alignment can actually benefit more from pushing unlabeled negative pairs far away from each other than pulling labeled positive pairs close. By leveraging this discovery, we develop the self-supervised learning objective for entity alignment. We present SelfKG with efficient strategies to optimize this objective for aligning entities without label supervision. Extensive experiments on benchmark datasets demonstrate that SelfKG &nbsp;without supervision can match or achieve comparable results with state-of-the-art supervised baselines. The performance of SelfKG suggests that self-supervised learning offers great potential for entity alignment in KGs. The code and data are available at https://github.com/THUDM/SelfKG.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {860–870},
numpages = {11},
keywords = {Entity Alignment, Knowledge Graphs, Self-Supervised Learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3578741.3578752,
author = {Yang, Wenchuan and Gu, Tianyu and Sui, Runqi},
title = {A Faster Method For Generating Chinese Text Summaries-Combining Extractive Summarization And Abstractive Summarization},
year = {2023},
isbn = {9781450399067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578741.3578752},
doi = {10.1145/3578741.3578752},
abstract = {Extractive summarization and generative summarization are the two main ways to generate summarization.However,previous work treats both of them as two independent subtasks.In this paper,we obtain new summarization by combining extractive summarization and generative summarization.This method extracts the key information of the article firstly,and then generates the summarization of the extracted information.The experimental result shows that this method can significantly improve the quality of the generative text compared with extractive summarization,and can significantly improve the generative speed compared with generative summarization.},
booktitle = {Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing},
pages = {54–58},
numpages = {5},
keywords = {Abstractive Summarization,Generative Summarization,T5 Model,Nezha Model,Recall-Oriented Understudy for Gisting Evaluation},
location = {Sanya, China},
series = {MLNLP '22}
}

@inproceedings{10.1145/3587828.3587838,
author = {He, Jun and Chen, Jing and Peng, Li and Sun, Bo and Zhang, Huiying},
title = {Question Difficulty Prediction with External Knowledge},
year = {2023},
isbn = {9781450398589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587828.3587838},
doi = {10.1145/3587828.3587838},
abstract = {The difficulty of test questions is an important indicator for educational examination and recommendation of personalized learning resources. Its evaluation mainly depends on the experience of experts, which is subjective. In recent years, question difficulty prediction (QDP) using neural networks has attracted more and more attention. Although these methods improve the QDP efficiency, it works ill for questions involving abstract concepts, such as numerical calculation, date, and questions whose answers require background knowledge. Therefore, we propose a difficulty prediction model based on rich knowledge fusion (RKF+), which solves the problem that the difficulty prediction models cannot obtain conceptual knowledge and background knowledge. The key is to introduce the attentional mechanism with a sentry vector, which can dynamically obtain the text representation and external knowledge representation of test questions. To further fusion the acquired external knowledge, our model added a bi-interaction layer. Finally, the validity of this model is verified on three different datasets. Besides, the importance of attentional mechanism and external knowledge representation is further analyzed by ablation experiment. In addition, based on a real English reading comprehension test dataset, we explore the influence of two kinds of external knowledge on the question difficulty prediction model.},
booktitle = {Proceedings of the 2023 12th International Conference on Software and Computer Applications},
pages = {59–64},
numpages = {6},
keywords = {External Knowledge, Natural Language Processing, Problem Difficulty Prediction},
location = {Kuantan, Malaysia},
series = {ICSCA '23}
}

@inproceedings{10.1145/3572549.3572629,
author = {Zhao, Gang and Yi, Jiarong and Chu, Jie and Zhang, Yinan and Yin, Jianghua},
title = {Design and Implementation of the Teacher-student Dialogue Automatic Speech Recognition Tool Based on Classroom Verbal Characteristics},
year = {2023},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572549.3572629},
doi = {10.1145/3572549.3572629},
abstract = {Speech behavior is a tool for teachers and students to express and communicate in the classroom, and analyzing the content of classroom teacher-student dialogue is one of the key basic technologies to break through the automatic analysis of teaching videos. However, the current discourse research tools rely on the universal speech recognition cloud platform, and there are problems such as confusion between teacher-student role and unclear discourse boundaries for classroom teacher-student dialogue recognition. Therefore, this paper designs and develops teacher-student dialogue recognition tools after classifing and encoding the auditory information data and summarizing the general rules of speech characteristics in classroom teaching videos. To achieve the purpose of analyzing classroom teaching videos, education researchers can use the tool to intercept teaching videos, obtain boundary points of teacher-student dialogue, and recognize their speech content. Experimental results show that the tool has stable performance, which can quickly and accurately process audio signals, segment and cluster teacher and student voiceprint features. Finally, it produce structured text containing the time point of the discourse boundary, the teacher-student role label and the discourse content, which will provide data support for analyzing further classroom teaching video behavior.},
booktitle = {Proceedings of the 14th International Conference on Education Technology and Computers},
pages = {503–508},
numpages = {6},
keywords = {Classroom verbal characteristics, Speaker diarization, Speech recognition, Teacher-student dialogue, Teaching video},
location = {Barcelona, Spain},
series = {ICETC '22}
}

@article{10.1145/3597307,
author = {Navigli, Roberto and Conia, Simone and Ross, Bj\"{o}rn},
title = {Biases in Large Language Models: Origins, Inventory, and Discussion},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3597307},
doi = {10.1145/3597307},
abstract = {In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.},
journal = {J. Data and Information Quality},
month = jun,
articleno = {10},
numpages = {21},
keywords = {Bias in NLP, language models}
}

@inproceedings{10.1145/3543873.3587657,
author = {Sahijwani, Harshita and Dhole, Kaustubh and Purwar, Ankur and Vasudevan, Venugopal and Agichtein, Eugene},
title = {Contextual Response Interpretation for Automated Structured Interviews: A Case Study in Market Research},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587657},
doi = {10.1145/3543873.3587657},
abstract = {Structured interviews are used in many settings, importantly in market research on topics such as brand perception, customer habits, or preferences, which are critical to product development, marketing, and e-commerce at large. Such interviews generally consist of a series of questions that are asked to a participant. These interviews are typically conducted by skilled interviewers, who interpret the responses from the participants and can adapt the interview accordingly. Using automated conversational agents to conduct such interviews would enable reaching a much larger and potentially more diverse group of participants than currently possible. However, the technical challenges involved in building such a conversational system are relatively unexplored. To learn more about these challenges, we convert a market research multiple-choice questionnaire to a conversational format and conduct a user study. We address the key task of conducting structured interviews, namely interpreting the participant’s response, for example, by matching it to one or more predefined options. Our findings can be applied to improve response interpretation for the information elicitation phase of conversational recommender systems.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {886–891},
numpages = {6},
keywords = {conversational preference elicitation, conversational recommender systems, intent prediction},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3503162.3503166,
author = {Ramnani, Roshni and Sengupta, Shubhashis},
title = {From Opinion Mining to Improvement Mining : Understanding Product Improvements from User Reviews},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503166},
doi = {10.1145/3503162.3503166},
abstract = {A valuable trove of information exists for product(s) or services online via user opinions like detailed reviews provided by customers on popular e-commerce websites. Users express their individual opinions in the form of overall product/service experiences, which may include explicit positive/negative feedback, preferences, concerns, and suggestions for the future. Such information can be valuable to product/service owners in helping them understand the improvement(s) that must be made to a particular product or service. The primary focus of opinion mining has been on understanding positive and negative aspects within the review effectively. Limited emphasis has been placed on finer topics like user suggestions or conflicting information from users. In this work, we describe a method to extract possible product / service improvements from opinionated text in the form of non-conflicting negative feedback, user tips, recommendations, product usage details, feature suggestions, and specific complaints.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {52–57},
numpages = {6},
keywords = {Information Extraction, Opinion Mining, Suggestion Mining},
location = {Virtual Event, India},
series = {FIRE '21}
}

@inproceedings{10.1145/3583780.3615128,
author = {Lee, Eric W. and Ho, Joyce C.},
title = {PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615128},
doi = {10.1145/3583780.3615128},
abstract = {There has been rapid growth in biomedical literature, yet capturing the heterogeneity of the bibliographic information of these articles remains relatively understudied. Graph neural networks have gained popularity, however, they may not fully capture the information available in the PubMed database, a biomedical literature repository containing over 33 million articles. We introduce PubMed Graph Benchmark (PGB), a new benchmark dataset for evaluating heterogeneous graph representations. PGB is one of the largest heterogeneous networks to date and aggregates the rich metadata into a unified source including abstract, authors, citations, keywords, and the associated keyword hierarchy. The benchmark contains an evaluation task of 21 systematic review topics, an essential knowledge translation tool.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5331–5335},
numpages = {5},
keywords = {heterogeneous information network, network embedding, pubmed benchmark, systematic review},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3583780.3615258,
author = {Ghosh, Madhusudan and Ganguly, Debasis and Basuchowdhuri, Partha and Naskar, Sudip Kumar},
title = {Extracting Methodology Components from AI Research Papers: A Data-driven Factored Sequence Labeling Approach},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615258},
doi = {10.1145/3583780.3615258},
abstract = {Extraction of methodology component names from scientific articles is a challenging task due to the diversified contexts around the occurrences of these entities, and the different levels of granularity and containment relationships exhibited by these entities. We hypothesize that standard sequence labeling approaches may not adequately model the dependence of methodology name mentions with their contexts, due to the problems of their large, fast evolving, and domain-specific vocabulary. As a solution, we propose a factored approach, where the mention-context dependencies are represented in a more fine-grained manner, thus allowing the model parameters to better adjust to the different characteristic patterns inherent within the data. In particular, we experiment with two variants of this factored approach - one that uses the per-entity category information derived from an ontology, and the other that makes use of the topology of the sentence embedding space to infer a category for each entity constituting that sentence. We demonstrate that both these factored variants of SciBERT outperform their non-factored counterpart, a state-of-the-art model for scientific concept extraction.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {3897–3901},
numpages = {5},
keywords = {clustering, factored modelling, information extraction, scientific literature},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@proceedings{10.1145/3565387,
title = {CSAE '22: Proceedings of the 6th International Conference on Computer Science and Application Engineering},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

@article{10.1109/TASLP.2023.3265205,
author = {Li, Jijie and Shuang, Kai and Guo, Jinyu and Shi, Zengyi and Wang, Hongman},
title = {Enhancing Semantic Relation Classification With Shortest Dependency Path Reasoning},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3265205},
doi = {10.1109/TASLP.2023.3265205},
abstract = {Relation Classification (RC) is a basic and essential task of Natural Language Processing. Existing RC methods can be classified into two categories: sequence-based methods and dependency-based methods. Sequence-based methods identify the target relation based on the overall semantics of the whole sentence, which will inevitably introduce noisy features. Dependency-based methods extract indicative word-level features from the Shortest Dependency Path (SDP) between given entities and attempt to establish a statistical association between the words and the target relations. This pattern relatively eliminates the influence of noisy features and achieves a robust performance on long sentences. Nevertheless, we observe that majority of relation classification processes involve complex semantic reasoning which is hard to be achieved based on the word-level statistical association. To solve this problem, we categorize all relations into atomic relations and composed-relations. The atomic relations are the basic relations that can be identified based on the word-level features, while the composed-relation requires to be deducted from multiple atomic relations. Correspondingly, we propose the &lt;bold&gt;At&lt;/bold&gt;omic Relation &lt;bold&gt;E&lt;/bold&gt;ncoding and &lt;bold&gt;R&lt;/bold&gt;easoning &lt;bold&gt;M&lt;/bold&gt;odel (ATERM). In the atomic relation encoding stage, ATERM groups the word-level features and encodes multiple atomic relations in parallel. In the atomic relation reasoning stage, ATERM establishes the atomic relation chain where relation-level features are extracted to identify composed-relations. Experiments show that our method achieves state-of-the-art results on the three most popular relation classification datasets – TACRED, TACRED-Revisit, and SemEval 2010 task 8 with significant improvements.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1550–1560},
numpages = {11}
}

@inproceedings{10.1145/3459637.3482377,
author = {Weller, Tobias and Acosta, Maribel},
title = {Predicting Instance Type Assertions in Knowledge Graphs Using Stochastic Neural Networks},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482377},
doi = {10.1145/3459637.3482377},
abstract = {Instance type information is particularly relevant to perform reasoning and obtain further information about entities in knowledge graphs (KGs). However, during automated or pay-as-you-go KG construction processes, instance types might be incomplete or missing in some entities. Previous work focused mostly on representing entities and relations as embeddings based on the statements in the KG. While the computed embeddings encode semantic descriptions and preserve the relationship between the entities, the focus of these methods is often not on predicting schema knowledge, but on predicting missing statements between instances for completing the KG. To fill this gap, we propose an approach that first learns a KG representation suitable for predicting instance type assertions. Then, our solution implements a neural network architecture to predict instance types based on the learned representation. Results show that our representations of entities are much more separable with respect to their associations with classes in the KG, compared to existing methods. For this reason, the performance of predicting instance types on a large number of KGs, in particular on cross-domain KGs with a high variety of classes, is significantly better in terms of F1-score than previous work.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2111–2118},
numpages = {8},
keywords = {entity classification, entity type prediction, knowledge graphs, stochastic networks},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1109/TASLP.2023.3325973,
author = {Wang, Siyuan and Wei, Zhongyu and Xu, Jiarong and Li, Taishan and Fan, Zhihao},
title = {Unifying Structure Reasoning and Language Pre-Training for Complex Reasoning Tasks},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3325973},
doi = {10.1109/TASLP.2023.3325973},
abstract = {Recent pre-trained language models (PLMs) equipped with foundation reasoning skills have shown remarkable performance on downstream complex tasks. However, the significant structure reasoning skill has been rarely studied, which involves modeling implicit structure information within the text and performing explicit logical reasoning over them to deduce the conclusion. This paper proposes a unified learning framework that combines explicit structure reasoning and language pre-training to endow PLMs with the structure reasoning skill. It first identifies several elementary structures within contexts to construct structured queries and performs step-by-step reasoning along the queries to identify the answer entity. The fusion of textual semantics and structure reasoning is achieved by using contextual representations learned by PLMs to initialize the representation space of structures, and performing stepwise reasoning on this semantic representation space. Experimental results on four datasets demonstrate that the proposed model achieves significant improvements in complex reasoning tasks involving diverse structures, and shows transferability to downstream tasks with limited training data and effectiveness for complex reasoning of KGs modality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {1586–1595},
numpages = {10}
}

@proceedings{10.1145/3487553,
title = {WWW '22: Companion Proceedings of the Web Conference 2022},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Lyon, France}
}

@proceedings{10.1145/3582515,
title = {GoodIT '23: Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
year = {2023},
isbn = {9798400701160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3604571,
title = {Asian CHI '23: Proceedings of the Asian HCI Symposium 2023},
year = {2023},
isbn = {9798400707612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Online, Indonesia}
}

@inproceedings{10.1145/3539618.3591957,
author = {Ghosh, Sreyan and Tyagi, Utkarsh and Kumar, Sonal and Manocha, Dinesh},
title = {BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591957},
doi = {10.1145/3539618.3591957},
abstract = {Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a significant margin (1.5%-21.5% absolute improvement) and is able to generate augmentations that are both more factual and diverse. Code: https://github.com/Sreyan88/BioAug.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1853–1858},
numpages = {6},
keywords = {biomedical, information extraction, named entity recognition},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3580219.3580250,
author = {Sun, Zhi and Zhang, Zhiyong and Zhang, Ling and Chen, Jianfeng},
title = {A Novel Approach for Associating IP Addresses with Organizations using Deep Learning},
year = {2023},
isbn = {9781450397513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580219.3580250},
doi = {10.1145/3580219.3580250},
abstract = {The cyberspace is a philosophy and computer in the field of an abstract concept, refers to the computer and computer networks in virtual reality. Cyber threats are becoming more sophisticated, and new attack techniques have the high potential to damage computers or networks. Hence, it is necessary to analyze the correlation between cyberspace entities. It is good for discovering potential unknown threats or even APT attacks. Due to cyberspace has many elements, such as IP addresses, domain names, organizations, and personal accounts, it is difficult to analyze the association relationship of entities automatically, especially to quickly discover the organization to which the IP address belongs. To find out the relationship between the IP address and organization, this paper proposes a novel approach by using deep learning. The method is designed based on an improved TF-IDF algorithm and the pre-trained deep learning mode. The experiment results show that the proposed approach has promising results with less complexity, which is beneficial to using two weight factors of entity ranking. Meanwhile, t the results show that the method has strong robustness and parallelism on the dataset.},
booktitle = {Proceedings of the 7th International Conference on Control Engineering and Artificial Intelligence},
pages = {173–177},
numpages = {5},
keywords = {cyberspace security, cyber threat intelligence, entity identification, deep learning},
location = {Sanya, China},
series = {CCEAI '23}
}

@inproceedings{10.1145/3511808.3557275,
author = {Howard, Phillip and Ma, Arden and Lal, Vasudev and Simoes, Ana Paula and Korat, Daniel and Pereg, Oren and Wasserblat, Moshe and Singer, Gadi},
title = {Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557275},
doi = {10.1145/3511808.3557275},
abstract = {The extraction of aspect terms is a critical step in fine-grained sentiment analysis of text. Existing approaches for this task have yielded impressive results when the training and testing data are from the same domain. However, these methods show a drastic decrease in performance when applied to cross-domain settings where the domain of the testing data differs from that of the training data. To address this lack of extensibility and robustness, we propose a novel approach for automatically constructing domain-specific knowledge graphs that contain information relevant to the identification of aspect terms. We introduce a methodology for injecting information from these knowledge graphs into Transformer models, including two alternative mechanisms for knowledge insertion: via query enrichment and via manipulation of attention patterns. We demonstrate state-of-the-art performance on benchmark datasets for cross-domain aspect term extraction using our approach and investigate how the amount of external knowledge available to the Transformer impacts model performance.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {780–790},
numpages = {11},
keywords = {aspect extraction, aspect-based sentiment analysis, knowledge graphs, knowledge injection, transformers},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3447548.3467438,
author = {Zhang, Jiawen and Zhu, Jiaqi and Yang, Yi and Shi, Wandong and Zhang, Congcong and Wang, Hongan},
title = {Knowledge-Enhanced Domain Adaptation in Few-Shot Relation Classification},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467438},
doi = {10.1145/3447548.3467438},
abstract = {Relation classification (RC) is an important task in knowledge extraction from texts, while data-driven approaches, although achieving high performance, heavily rely on a large amount of annotated training data. Recently, many few-shot RC models have been proposed and yielded promising results in general domain datasets, but when adapting to a specific domain, such as medicine, the performance drops dramatically. In this paper, we propose a Knowledge-Enhanced Few-shot RC model for the Domain Adaptation task (KEFDA), which incorporates general and domain-specific knowledge graphs (KGs) to the RC model to improve its domain adaptability. With the help of concept-level KGs, the model can better understand the semantics of texts and easily summarize the global semantics of relation types from only a few instances. To be more important, as a kind of meta-information, the manner of utilizing KGs can be transferred from existing tasks to new tasks, even across domains. Specifically, we design a knowledge-enhanced prototypical network to conduct instance matching, and a relation-meta learning network for implicit relation matching. The two scoring functions are combined to infer the relation type of a new instance. Experimental results on the Domain Adaptation Challenge in the FewRel 2.0 benchmark demonstrate that our approach significantly outperforms the state-of-the-art models (by 6.63% on average).},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2183–2191},
numpages = {9},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@proceedings{10.1145/3562007,
title = {CCRIS '22: Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System},
year = {2022},
isbn = {9781450396851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

@inproceedings{10.1145/3543507.3583449,
author = {Yadav, Shweta and Cobeli, undefinedtefan and Caragea, Cornelia},
title = {Towards Understanding Consumer Healthcare Questions on the Web with Semantically Enhanced Contrastive Learning},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583449},
doi = {10.1145/3543507.3583449},
abstract = {In recent years, seeking health information on the web has become a preferred way for healthcare consumers to support their information needs. Generally, healthcare consumers use long and detailed questions with several peripheral details to express their healthcare concerns, contributing to natural language understanding challenges. One way to address this challenge is by summarizing the questions. However, most of the existing abstractive summarization systems generate impeccably fluent yet factually incorrect summaries. In this paper, we present a semantically-enhanced contrastive learning-based framework for generating abstractive question summaries that are faithful and factually correct. We devised multiple strategies based on question semantics to generate the erroneous (negative) summaries, such that the model has the understanding of plausible and incorrect perturbations of the original summary. Our extensive experimental results on two benchmark consumer health question summarization datasets confirm the effectiveness of our proposed method by achieving state-of-the-art performance and generating factually correct and fluent summaries, as measured by human evaluation.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1773–1783},
numpages = {11},
keywords = {abstractive summarization, consumer healthcare question understanding, contrastive learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3594315.3594359,
author = {Dai, Chenquan and Zhuang, Xiaobin and Cai, Jiaxin},
title = {A Survey on Deep Learning for Chinese Medical Named Entity Recognition},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594359},
doi = {10.1145/3594315.3594359},
abstract = {At present, how to make full use of medical and health data for exploration and analysis to better support clinical decision-making faces many challenges. This paper aims to summarize and analyze the methods and research status of Chinese medical named entity recognition, and understand the research progress of named entity recognition technology in Chinese electronic medical record text. This paper conducts literature research from multiple perspectives, such as the basic concepts of electronic medical records and named entity recognition, the acquisition of corpus datasets and the named entity recognition algorithm. The research progress of Chinese electronic medical record named entity recognition in recent years is reviewed, and the development trend of electronic medical record named entity recognition in the future Chinese is analyzed.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {472–476},
numpages = {5},
location = {Tianjin, China},
series = {ICCAI '23}
}

@inproceedings{10.1145/3503161.3547948,
author = {Chen, Zhihong and Li, Guanbin and Wan, Xiang},
title = {Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547948},
doi = {10.1145/3503161.3547948},
abstract = {Medical vision-and-language pre-training (Med-VLP) has received considerable attention owing to its applicability to extracting generic vision-and-language representations from medical images and texts. Most existing methods mainly contain three elements: uni-modal encoders (i.e., a vision encoder and a language encoder), a multi-modal fusion module, and pretext tasks, with few studies considering the importance of medical domain expert knowledge and explicitly exploiting such knowledge to facilitate Med-VLP. Although there exist knowledge-enhanced vision-and-language pre-training (VLP) methods in the general domain, most require off-the-shelf toolkits (e.g., object detectors and scene graph parsers), which are unavailable in the medical domain. In this paper, we propose a systematic and effective approach to enhance Med-VLP by structured medical knowledge from three perspectives. First, considering knowledge can be regarded as the intermediate medium between vision and language, we align the representations of the vision encoder and the language encoder through knowledge. Second, we inject knowledge into the multi-modal fusion model to enable the model to perform reasoning using knowledge as the supplementation of the input image and text. Third, we guide the model to put emphasis on the most critical information in images and texts by designing knowledge-induced pretext tasks. To perform a comprehensive evaluation and facilitate further research, we construct a medical vision-and-language benchmark including three tasks. Experimental results illustrate the effectiveness of our approach, where state-of-the-art performance is achieved on all downstream tasks. Further analyses explore the effects of different components of our approach and various settings of pre-training.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5152–5161},
numpages = {10},
keywords = {knowledge-enhanced learning, medical analysis, multi-modal pre-training, vision-and-language},
location = {Lisboa, Portugal},
series = {MM '22}
}

@proceedings{10.1145/3589132,
title = {SIGSPATIAL '23: Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The conference started as a series of workshops and symposia back in 1993 with the aim of promoting interdisciplinary discussions among researchers, developers, users, and practitioners and fostering research in all aspects of geographic information systems, especially in relation to novel systems based on geospatial data and knowledge. It continues to provide a forum for original research contributions covering all conceptual, design and implementation aspects of geospatial data ranging from applications, user interfaces and visualization, to data storage, query processing, indexing, machine learning and data mining. The conference is the premier annual event of the ACM Special Interest Group on Spatial Information (ACM SIGSPATIAL).},
location = {Hamburg, Germany}
}

@inproceedings{10.1145/3501409.3501580,
author = {Hu, Hongwei and Yin, Meijuan and Liu, Xiaonan},
title = {A Relation-Oriented Method for Joint Entity and Relation Extraction Based on Neural Network},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501580},
doi = {10.1145/3501409.3501580},
abstract = {Entity and relation extraction is a basic task of information extraction in natural language processing. At present, Entity and relation extraction based on artificial intelligence has been widely studied, but most methods adopt the idea of identifying the entity first and then extracting the relation, which suffers from the problems of overlapping triple and entity redundancy. We propose a Relation-Oriented method for Joint Entity and Relation Extraction (ROJER), which first extracts the relation types contained in the text through the relation extraction module, then the pre-extracted relation types are integrated into the entity recognition module, reduce the focus on irrelevant entities and avoid extracting redundant entities. Then the entity pairs corresponding to the extracted relation types are identified to tackle the overlapping triple problem and finally extract all the relation triples. Experiments on the DuIE dataset show that the F1 score of ROJER reaches 78.4%, which is 1.2% higher than the CasRel model, confirming the validity of our method.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {951–955},
numpages = {5},
keywords = {Artificial intelligence, Entity redundancy, Information extraction, Overlapping triple, Relation extraction},
location = {Xiamen, China},
series = {EITCE '21}
}

@proceedings{10.1145/3584931,
title = {CSCW '23 Companion: Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Minneapolis, MN, USA}
}

@article{10.14778/3611540.3611636,
author = {Dong, Xin Luna},
title = {Generations of Knowledge Graphs: The Crazy Ideas and the Business Impact},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611636},
doi = {10.14778/3611540.3611636},
abstract = {Knowledge Graphs (KGs) have been used to support a wide range of applications, from web search to personal assistant. In this paper, we describe three generations of knowledge graphs: entity-based KGs, which have been supporting general search and question answering (e.g., at Google and Bing); text-rich KGs, which have been supporting search and recommendations for products, bio-informatics, etc. (e.g., at Amazon and Alibaba); and the emerging integration of KGs and LLMs, which we call dual neural KGs. We describe the characteristics of each generation of KGs, the crazy ideas behind the scenes in constructing such KGs, and the techniques developed over time to enable industry impact. In addition, we use KGs as examples to demonstrate a recipe to evolve research ideas from innovations to production practice, and then to the next level of innovations, to advance both science and business.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4130–4137},
numpages = {8}
}

@proceedings{10.1145/3590003,
title = {CACML '23: Proceedings of the 2023 2nd Asia Conference on Algorithms, Computing and Machine Learning},
year = {2023},
isbn = {9781450399449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@inproceedings{10.1145/3477495.3531957,
author = {Xu, Chen and Li, Piji and Wang, Wei and Yang, Haoran and Wang, Siyun and Xiao, Chuangbai},
title = {COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531957},
doi = {10.1145/3477495.3531957},
abstract = {Maintaining a consistent persona is essential for building a human-like conversational model. However, the lack of attention to the partner makes the model more egocentric: they tend to show their persona by all means such as twisting the topic stiffly, pulling the conversation to their own interests regardless, and rambling their persona with little curiosity to the partner. In this work, we propose COSPLAY(COncept Set guided PersonaLized dialogue generation Across both partY personas) that considers both parties as a "team": expressing self-persona while keeping curiosity toward the partner, leading responses around mutual personas, and finding the common ground. Specifically, we first represent self-persona, partner persona and mutual dialogue all in the concept sets. Then, we propose the Concept Set framework with a suite of knowledge-enhanced operations to process them such as set algebras, set expansion, and set distance. Based on these operations as medium, we train the model by utilizing 1) concepts of both party personas, 2) concept relationship between them, and 3) their relationship to the future dialogue. Extensive experiments on a large public dataset, Persona-Chat, demonstrate that our model outperforms state-of-the-art baselines for generating less egocentric, more human-like, and higher quality responses in both automatic and human evaluations.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {201–211},
numpages = {11},
keywords = {common ground modeling, knowledge concept set, mutual benefit, personalized dialogue generation, reinforcement learning.},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3544548.3581026,
author = {Deng, Wesley Hanwen and Guo, Boyuan and Devrio, Alicia and Shen, Hong and Eslami, Motahhare and Holstein, Kenneth},
title = {Understanding Practices, Challenges, and Opportunities for User-Engaged Algorithm Auditing in Industry Practice},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581026},
doi = {10.1145/3544548.3581026},
abstract = {Recent years have seen growing interest among both researchers and practitioners in user-engaged approaches to algorithm auditing, which directly engage users in detecting problematic behaviors in algorithmic systems. However, we know little about industry practitioners’ current practices and challenges around user-engaged auditing, nor what opportunities exist for them to better leverage such approaches in practice. To investigate, we conducted a series of interviews and iterative co-design activities with practitioners who employ user-engaged auditing approaches in their work. Our findings reveal several challenges practitioners face in appropriately recruiting and incentivizing user auditors, scaffolding user audits, and deriving actionable insights from user-engaged audit reports. Furthermore, practitioners shared organizational obstacles to user-engaged auditing, surfacing a complex relationship between practitioners and user auditors. Based on these findings, we discuss opportunities for future HCI research to help realize the potential (and mitigate risks) of user-engaged auditing in industry practice.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {377},
numpages = {18},
keywords = {bias, fairness, industry practitioners, responsible AI, user-engaged algorithm auditing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3573428.3573599,
author = {Zhang, Luyi and Li, Ren and Xiao, Qiao},
title = {A Prompt-based Few-shot Machine Reading Comprehension Model for Intelligent Bridge Management},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573599},
doi = {10.1145/3573428.3573599},
abstract = {Bridge inspection reports are an important data source in the bridge management process, and they contain a large amount of fine-grained information. However, the research on machine reading comprehension (MRC) methods for this field is insufficient, and annotating large scale domain-specific corpus is time-consuming. This paper presented a novel prompt-based few-shot MRC approach for intelligent bridge management. The proposed model uses the pretrained model MacBERT as backbone. The prompt templates are designed based on some domain-specific heuristic rules. The experimental results show that our model outperforms the baseline models in different few-shot settings. The proposed model can provide technical support for the construction of automatic question answering system in the field of bridge management.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {946–950},
numpages = {5},
keywords = {Bridge inspection, Few-shot, Machine reading comprehension, Prompt},
location = {Xiamen, China},
series = {EITCE '22}
}

@article{10.1145/3597299,
author = {Roy, Prasenjeet and Kundu, Suman},
title = {Review on Query-focused Multi-document Summarization (QMDS) with Comparative Analysis},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3597299},
doi = {10.1145/3597299},
abstract = {The problem of query-focused multi-document summarization (QMDS) is to generate a summary from multiple source documents on identical/similar topics based on the query submitted by the users. This article provides a systematic review of the literature of QMDS. The research works are classified into six major categories based on the summarization methodologies used. Different techniques used for finding query-relevant summaries for different algorithms under each of the six major groups are reported. Further, 17 evaluation metrics used for evaluating algorithms for text summaries against the human-curated summaries are compiled here in this article. Extensive experiments are performed on eight different datasets. Comparative results of nine methodologies, each representing one of the six different groups, are presented. Seven different evaluation metrics are used in the comparative study. It is observed that DL- and ML-based QMDS methods perform. better in comparison to the other methods.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {5},
numpages = {38},
keywords = {Query focused multi-document summarization, query relevance}
}

@inproceedings{10.1145/3539618.3591849,
author = {Van Gysel, Christophe},
title = {Modeling Spoken Information Queries for Virtual Assistants: Open Problems, Challenges and Opportunities},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591849},
doi = {10.1145/3539618.3591849},
abstract = {Virtual assistants are becoming increasingly important speech-driven Information Retrieval platforms that assist users with various tasks. We discuss open problems and challenges with respect to modeling spoken information queries for virtual assistants, and list opportunities where Information Retrieval methods and research can be applied to improve the quality of virtual assistant speech recognition. We discuss how query domain classification, knowledge graphs and user interaction data, and query personalization can be helpful to improve the accurate recognition of spoken information domain queries. Finally, we also provide a brief overview of current problems and challenges in speech recognition.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3335–3338},
numpages = {4},
keywords = {automated speech recognition, query log analysis, virtual assistants},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3544549.3585699,
author = {Xu, Erqian and Wang, Hecong and Bai, Zhen},
title = {Engage AI and Child in Explanatory Dialogue on Commonsense Reasoning},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585699},
doi = {10.1145/3544549.3585699},
abstract = {Human-level commonsense reasoning capability is vital for human-AI interaction, enabling AI to understand, anticipate, and respond to human’s thoughts, feelings, and behaviors. Despite the recent advancements in AI commonsense reasoning due to generative language models, a young child is often more rational than state-of-the-art AIs in terms of commonsense reasoning. The field of cognitive science, child development, and explainable AI have long recognized the importance of explanations for sharing knowledge and resolving contradictions. We, therefore, raise the question: can AIs leverage the power of explanations to learn human-level commonsense reasoning? More specifically, can explanatory dialogue with children help AIs to develop commonsense reasoning capabilities? As a first step in this line of research, we aim to engage children in explanatory dialogue with AIs during story reading. We present our novel explanatory dialogue interface based on a state-of-the-art multi-step commonsense reasoning engine and discuss our upcoming pilot study.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {99},
numpages = {8},
keywords = {Commonsense Reasoning, Explainable AI, Human-AI Collaboration},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3582768.3582772,
author = {Losing, Viktor and Eggert, Julian},
title = {Extraction of Common Physical Properties of Everyday Objects from Structured Sources},
year = {2023},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582768.3582772},
doi = {10.1145/3582768.3582772},
abstract = {Commonsense knowledge is essential for the reasoning of AI systems, particularly in the context of action planning for robots. The focus of this paper is on common-sense object properties, which are especially useful to restrict the search space of planning algorithms. Popular sources for such knowledge are commonsense knowledge bases that provide the information in a structured form. However, the utility of the provided object-property pairs is limited as they can be simply incorrect, subjective, unspecific, or relate only to a narrow context. In this paper, we suggest a methodology to create a highly accurate dataset of object properties that are related to common physical attributes. The approach is based on filtering non-physical properties within commonsense knowledge bases and improving the accuracy of the remaining object-property pairs based on supervised machine learning using annotated data. Thereby, we evaluate different types of features and models and significantly increase the correctness of object-property pairs compared to the original sources.},
booktitle = {Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
pages = {164–168},
numpages = {5},
keywords = {knowledge bases, neural networks, transformer model},
location = {Bangkok, Thailand},
series = {NLPIR '22}
}

@inproceedings{10.1145/3539597.3570478,
author = {Su, Xing and Yang, Jian and Wu, Jia and Zhang, Yuchen},
title = {Mining User-aware Multi-relations for Fake News Detection in Large Scale Online Social Networks},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3570478},
doi = {10.1145/3539597.3570478},
abstract = {Users' involvement in creating and propagating news is a vital aspect of fake news detection in online social networks. Intuitively, credible users are more likely to share trustworthy news, while untrusted users have a higher probability of spreading untrustworthy news. In this paper, we construct a dual-layer graph (i.e., news layer and user layer) to extract multi-relations of news and users in social networks to derive rich information for detecting fake news. Based on the dual-layer graph, we propose a fake news detection model Us-DeFake. It learns the propagation features of news in the news layer and the interaction features of users in the user layer. Through the inter-layer in the graph, Us-DeFake fuses the user signals that contain credibility information into the news features, to provide distinctive user-aware embeddings of news for fake news detection. The training process conducts on multiple dual-layer subgraphs obtained by a graph sampler to scale Us-DeFake in large scale social networks. Extensive experiments on real-world datasets illustrate the superiority of Us-DeFake which outperforms all baselines, and the users' credibility signals learned by interaction relation can notably improve the performance of our model.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {51–59},
numpages = {9},
keywords = {fake news detection, large scale social networks, multi-relations},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3550356.3561570,
author = {Ibrahimi, Ilirian and Moudilos, Dimitris},
title = {Towards model reuse in low-code development platforms based on knowledge graphs},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561570},
doi = {10.1145/3550356.3561570},
abstract = {Low-code Development Platforms (LCDP) are applications to provide fast full-stack application development. These platforms rely on models as their core artifacts in order to reduce the complexity of software development. Despite the growing need and importance of using models in low-code platforms, there is still limited support for model discovery and reuse, which leads to unnecessary repetitive work. Therefore, facilities for automated discovery and recommendation of relevant models and model fragments are desired. A prerequisite for producing relevant recommendations for adding new features to a model is a model repository equipped with an efficient query mechanism in combination with algorithms for processing the retrieved data. In this paper, we present an approach that enables model recommendations by initially converting and merging heterogeneous models into a homogeneous graph, which serves as the repository of our approach. Afterwards, the approach queries the repository for relevant matches and uses N-grams to produce recommendations for models under development. The current approach is demonstrated on the zAppDev LCDP, but conceptually it can be integrated on any LCDP and can be applied for the reuse of any graph-based model. We evaluated our approach by reconstructing four different models that do not exist in our repository with the support of our approach.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {826–836},
numpages = {11},
keywords = {MDE, N-grams, RDF, graph-repository, low-code platform, model reuse, recommendation system},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@proceedings{10.1145/3604237,
title = {ICAIF '23: Proceedings of the Fourth ACM International Conference on AI in Finance},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Brooklyn, NY, USA}
}

@inproceedings{10.1145/3477495.3532003,
author = {Alghanmi, Israa and Espinosa-Anke, Luis and Schockaert, Steven},
title = {Interpreting Patient Descriptions using Distantly Supervised Similar Case Retrieval},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532003},
doi = {10.1145/3477495.3532003},
abstract = {Biomedical natural language processing often involves the interpretation of patient descriptions, for instance for diagnosis or for recommending treatments. Current methods, based on biomedical language models, have been found to struggle with such tasks. Moreover, retrieval augmented strategies have only had limited success, as it is rare to find sentences which express the exact type of knowledge that is needed for interpreting a given patient description. For this reason, rather than attempting to retrieve explicit medical knowledge, we instead propose to rely on a nearest neighbour strategy. First, we retrieve text passages that are similar to the given patient description, and are thus likely to describe patients in similar situations, while also mentioning some hypothesis (e.g. a possible diagnosis of the patient). We then judge the likelihood of the hypothesis based on the similarity of the retrieved passages. Identifying similar cases is challenging, however, as descriptions of similar patients may superficially look rather different, among others because they often contain an abundance of irrelevant details. To address this challenge, we propose a strategy that relies on a distantly supervised cross-encoder. Despite its conceptual simplicity, we find this strategy to be effective in practice.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {460–470},
numpages = {11},
keywords = {biomedical nlp, distant supervision, similar case retrieval},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3477495.3531944,
author = {Chatterjee, Shubham and Dietz, Laura},
title = {BERT-ER: Query-specific BERT Entity Representations for Entity Ranking},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531944},
doi = {10.1145/3477495.3531944},
abstract = {Entity-oriented search systems often learn vector representations of entities via the introductory paragraph from the Wikipedia page of the entity. As such representations are the same for every query, our hypothesis is that the representations are not ideal for IR tasks. In this work, we present BERT Entity Representations (BERT-ER) which are query-specific vector representations of entities obtained from text that describes how an entity is relevant for a query. Using BERT-ER in a downstream entity ranking system, we achieve a performance improvement of 13-42% (Mean Average Precision) over a system that uses the BERT embedding of the introductory paragraph from Wikipedia on two large-scale test collections. Our approach also outperforms entity ranking systems using entity embeddings from Wikipedia2Vec, ERNIE, and E-BERT. We show that our entity ranking system using BERT-ER can increase precision at the top of the ranking by promoting relevant entities to the top. With this work, we release our BERT models and query-specific entity embeddings fine-tuned for the entity ranking task.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1466–1477},
numpages = {12},
keywords = {bert, entity ranking, query-specific entity representations},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3524618,
author = {Song, Yaguang and Yang, Xiaoshan and Xu, Changsheng},
title = {Self-supervised Calorie-aware Heterogeneous Graph Networks for Food Recommendation},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3524618},
doi = {10.1145/3524618},
abstract = {With the rapid development of online recipe sharing platforms, food recommendation is emerging as an important application. Although recent studies have made great progress on food recommendation, they have two shortcomings that are likely to affect the recommendation performance. (1) The relations between ingredients are not considered, which may lead to sub-optimal representations of recipes and further result in the neglect of the user’s personalized ingredient combination preference. (2) Existing methods do not consider the impact of users’ preferences on calories in users’ food decision-making process. In this article, we propose a Self-supervised Calorie-aware Heterogeneous Graph Network (SCHGN) to model the relations between ingredients and incorporate calories of food simultaneously. Specifically, we first incorporate users, recipes, ingredients, and calories into a heterogeneous graph and explicitly present the complex relations among them with directed edges. Then, we explore the co-occurrence relation of ingredients in different recipes via self-supervised ingredient prediction. To capture users’ dynamic preferences on calories of food, we learn calorie-aware user representations by hierarchical message passing and compute a comprehensive user-guided recipe representation by attention mechanism. The final food recommendation is accomplished based on the similarity between a user’s calorie-aware representation and the user-guided representation of a recipe. Extensive experiment results on benchmark datasets demonstrate the effectiveness of the proposed method.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {27},
numpages = {23},
keywords = {Food recommendation, recipe calories, heterogeneous graph, self-supervised learning}
}

@article{10.1145/3588940,
author = {Fan, Wenfei and Fu, Wenzhi and Jin, Ruochun and Liu, Muyang and Lu, Ping and Tian, Chao},
title = {Making It Tractable to Catch Duplicates and Conflicts in Graphs},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588940},
doi = {10.1145/3588940},
abstract = {This paper proposes an approach for entity resolution (ER) and conflict resolution (CR) in large-scale graphs. It is based on a class of Graph Cleaning Rules (GCRs), which support the primitives of relational data cleaning rules, and may embed machine learning classifiers as predicates. As opposed to previous graph rules, GCRs are defined with a dual graph pattern to accommodate irregular structures of schemaless graphs, and adopt patterns of a star form to reduce the complexity. We show that the satisfiability, implication and validation problems are all in polynomial time (PTIME) for GCRs, as opposed to the intractability of these classical problems for previous graph dependencies. We develop a parallel algorithm to discover GCRs by combining the generations of patterns and predicates, and a parallel PTIME algorithm for "deep" ER and CR by recursively applying the mined GCRs. We show that these algorithms guarantee to reduce runtime when more processors are used. Using real-life and synthetic graphs, we experimentally verify that rule discovery and error detection with GCRs are substantially faster than with previous graph dependencies, with improved accuracy.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {86},
numpages = {28},
keywords = {conflict resolution, entity resolution, graph cleaning rules}
}

@inproceedings{10.1145/3614008.3614053,
author = {Gong, Jibing and Fang, Xiaohan and Wang, Chenglong and Ju, Jingxin and Bao, Yanghao and Zhang, Jin and Xu, Jianjun},
title = {Author Name Disambiguation based on Capsule Network via Semantic and Structural Features},
year = {2023},
isbn = {9798400707575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3614008.3614053},
doi = {10.1145/3614008.3614053},
abstract = {Author Name Disambiguation (AND) is a crucial task in the knowledge engineering of the bibliography. In academic search systems, author name ambiguity is a common phenomenon caused by different authors with the same name and leads to that author name can not be used to reliably identify all scholar authors. In recent researches, one papers’ attributes are often used to learn its representation as feature. However, most existing methods ignore to extract deep features and potential relationship among papers. To address the problem, we propose a novel model named Author Name Disambiguation based on Capsule Network via Semantic and Structural Features (ADSSF). ADSSF uses both supervised and unsupervised methods to learn the representation of papers. First, we present a new Capsule-Networks-based feature extraction model which can mine deep features and potential relationship. And then, in the representation learning of papers, ADFFS fuses the semantic and structural features of papers by multi-task learning. Finally, a clustering method is leveraged to correctly cluster authors. Experimental results on the AMiner datasets demonstrate that the ADSSF outperforms the state-of-the-art baselines.},
booktitle = {Proceedings of the 2023 6th International Conference on Signal Processing and Machine Learning},
pages = {293–300},
numpages = {8},
keywords = {Author Name Disambiguation, Capsule Network, Knowledge Engineering, Semantic and Structural Feature Fusion},
location = {Tianjin, China},
series = {SPML '23}
}

@inproceedings{10.1145/3459637.3482273,
author = {Ye, Muchao and Cui, Suhan and Wang, Yaqing and Luo, Junyu and Xiao, Cao and Ma, Fenglong},
title = {MedRetriever: Target-Driven Interpretable Health Risk Prediction via Retrieving Unstructured Medical Text},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482273},
doi = {10.1145/3459637.3482273},
abstract = {The broad adoption of electronic health record (EHR) systems and the advances of deep learning technology have motivated the development of health risk prediction models, which mainly depend on the expressiveness and temporal modeling capacity of deep neural networks (DNNs) to improve prediction performance. Some further augment the prediction by using external knowledge, however, a great deal of EHR information inevitably loses during the knowledge mapping. In addition, prediction made by existing models usually lacks reliable interpretation, which undermines their reliability in guiding clinical decision-making. To solve these challenges, we propose MedRetriever, an effective and flexible framework that leverages unstructured medical text collected from authoritative websites to augment health risk prediction as well as to provide understandable interpretation. Besides, MedRetriever explicitly takes the target disease documents into consideration, which provide key guidance for the model to learn in a target-driven direction, i.e., from the target disease to the input EHR. To specify, MedRetriever can flexibly choose its backbone from major predictive models to learn the EHR embedding for each visit. After that, the EHR embedding and features of target disease documents are aggregated into a query by self-attention to retrieve highly relevant text segments from the medical text pool, which is stored in the dynamically updated text memory. Finally, the comprehensive EHR embedding and the text memory are used for prediction and interpretation. We evaluate MedRetriever against nine state-of-the-art approaches across three real-world EHR datasets, which consistently achieves the best performance in AUC and recall metrics and outperforms the best baseline by at least 4.8% in recall on three test datasets. Furthermore, we conduct case studies to show the easy-to-understand interpretation by MedRetriever.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2414–2423},
numpages = {10},
keywords = {data mining, electronic health records, external knowledge, health risk prediction},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3459637.3481991,
author = {Jie, Fei and Huang, Yanxiang and Bai, Qiangwei and Wu, Xindong},
title = {HAO Unity: A Graph-based System for Unifying Heterogeneous Data},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481991},
doi = {10.1145/3459637.3481991},
abstract = {Many real-world applications have to face the problem of diversity in data formats and semantics. Currently, how to deal with heterogeneous data effectively is still a big challenge. With the rise of knowledge graphs, more and more applications are built upon graph-like data models, which benefit from flexible schemas and convenient support for relationship queries. We propose a graph-based unifying system for heterogeneous data unification, which helps to (1) transform data in many other formats into graphs, or conversely, from graph to other formats, (2) integrate graph data based on HAO intelligence, which achieves schema integration and entity consolidation, and (3) explore data at different levels via querying the integrated graphs. In this paper, we introduce the overall system architecture, explain in detail the implementation, and display the usage in two practical scenarios.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4725–4729},
numpages = {5},
keywords = {data exploration, data integration, data unification, graph database},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3510003.3510129,
author = {Liu, Yalin and Lin, Jinfeng and Anuyah, Oghenemaro and Metoyer, Ronald and Cleland-Huang, Jane},
title = {Generating and visualizing trace link explanations},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510129},
doi = {10.1145/3510003.3510129},
abstract = {Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic healthcare systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1033–1044},
numpages = {12},
keywords = {concept mining, explanation interface, software traceability},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3464377,
author = {Ma, Longxuan and Li, Mingda and Zhang, Wei-Nan and Li, Jiapeng and Liu, Ting},
title = {Unstructured Text Enhanced Open-Domain Dialogue System: A Systematic Survey},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3464377},
doi = {10.1145/3464377},
abstract = {Incorporating external knowledge into dialogue generation has been proven to benefit the performance of an open-domain Dialogue System (DS), such as generating informative or stylized responses, controlling conversation topics. In this article, we study the open-domain DS that uses unstructured text as external knowledge sources (Unstructured Text Enhanced Dialogue System (UTEDS)). The existence of unstructured text entails distinctions between UTEDS and traditional data-driven DS and we aim at analyzing these differences. We first give the definition of the UTEDS related concepts, then summarize the recently released datasets and models. We categorize UTEDS into Retrieval and Generative models and introduce them from the perspective of model components. The retrieval models consist of Fusion, Matching, and Ranking modules, while the generative models comprise Dialogue and Knowledge Encoding, Knowledge Selection (KS), and Response Generation modules. We further summarize the evaluation methods utilized in UTEDS and analyze the current models’ performance. At last, we discuss the future development trends of UTEDS, hoping to inspire new research in this field.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {9},
numpages = {44},
keywords = {Unstructured text, knowledge grounded, knowledge selection, open-domain dialogue}
}

@inproceedings{10.1145/3544548.3580948,
author = {Wang, Sitong and Petridis, Savvas and Kwon, Taeahn and Ma, Xiaojuan and Chilton, Lydia B},
title = {PopBlends: Strategies for Conceptual Blending with Large Language Models},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580948},
doi = {10.1145/3544548.3580948},
abstract = {Pop culture is an important aspect of communication. On social media people often post pop culture reference images that connect an event, product or other entity to a pop culture domain. Creating these images is a creative challenge that requires finding a conceptual connection between the users’ topic and a pop culture domain. In cognitive theory, this task is called conceptual blending. We present a system called PopBlends that automatically suggests conceptual blends. The system explores three approaches that involve both traditional knowledge extraction methods and large language models. Our annotation study shows that all three methods provide connections with similar accuracy, but with very different characteristics. Our user study shows that people found twice as many blend suggestions as they did without the system, and with half the mental demand. We discuss the advantages of combining large language models with knowledge bases for supporting divergent and convergent thinking.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {435},
numpages = {19},
keywords = {applications of large language models, creativity support tools, natural language processing},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3622896,
title = {CCRIS '23: Proceedings of the 2023 4th International Conference on Control, Robotics and Intelligent System},
year = {2023},
isbn = {9798400708190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@article{10.1145/3476464,
author = {Manogaran, Gunasekaran and Qudrat-Ullah, Hassan and Xin, Qin},
title = {Introduction to the Special Issue on Deep Structured Learning for Natural Language Processing, Part 3},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3476464},
doi = {10.1145/3476464},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {72e},
numpages = {3}
}

@article{10.1145/3611641,
author = {Demartini, Gianluca and Roitero, Kevin and Mizzaro, Stefano},
title = {Data Bias Management},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3611641},
doi = {10.1145/3611641},
abstract = {Envisioning a unique approach toward bias and fairness research.},
journal = {Commun. ACM},
month = dec,
pages = {28–32},
numpages = {5}
}

@inproceedings{10.1145/3511808.3557607,
author = {Jing, Zhiwen and Zhao, Ziliang and Feng, Yang and Ma, Xaochen and Wu, Nan and Kang, Shengqiao and Yang, Cheng and Zhang, Yujia and Guo, Hao},
title = {GReS: Graphical Cross-domain Recommendation for Supply Chain Platform},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557607},
doi = {10.1145/3511808.3557607},
abstract = {Supply Chain Platforms (SCPs) provide downstream industries with raw materials. Compared with traditional e-commerce platforms, data in SCPs is more sparse due to limited user interests. To tackle the data sparsity problem, one can apply Cross-Domain Recommendation (CDR) to improve the recommendation performance of the target domain with the source domain information. However, applying CDR to SCPs directly ignores hierarchical structures of commodities in SCPs, which reduce recommendation performance. In this paper, we take the catering platform as an example and propose GReS, a graphical CDR model. The model first constructs a tree-shaped graph to represent the hierarchy of different nodes of dishes and ingredients, and then applies our proposed Tree2vec method combining GCN and BERT models to embed the graph for recommendations. Experimental results show that GReS significantly outperforms state-of-the-art methods in CDR for SCPs.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4094–4098},
numpages = {5},
keywords = {cross-domain recommendation, supply chain platform},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1109/TASLP.2022.3221045,
author = {Xie, Jiayuan and Fang, Wenhao and Huang, Qingbao and Cai, Yi and Wang, Tao},
title = {Enhancing Paraphrase Question Generation With Prior Knowledge},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3221045},
doi = {10.1109/TASLP.2022.3221045},
abstract = {Paraphrase question generation (PQG) aims to rewrite a given original question to a new paraphrase question, where the paraphrase question needs to have the same expressed meaning as the original question, but have a difference in expression form. Existing methods on PQG mainly focus on synonym substitution or word order adjustment based on the original question. However, rewriting based on the word-level may not guarantee the difference between paraphrase questions and original questions. In this paper, we propose a knowledge-aware paraphrase question generation model. Our model first employs a knowledge extractor to extract the prior knowledge related to the original question from the knowledge base. Then an attention mechanism and a gate mechanism are introduced in our model to selectively utilize the extracted prior knowledge for rewriting, which helps to expand the content of the generated question to maximize the difference. Additionally, we use a discriminator module to promote the generated paraphrase to be semantically close to the original question and the ground truth. Specifically, the loss function of the discriminator penalizes the excessive distance between the representation of the paraphrase question and the ground truth. Extensive experiments on the Quora dataset show that the proposed model outperforms the baselines. Further, our model is applied to the SQuAD dataset, which proves the generalization ability of our model in the existing QA dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {1464–1475},
numpages = {12}
}

@book{10.1145/3581906,
editor = {Koubarakis, Manolis},
title = {Geospatial Data Science: A Hands-on Approach for Building Geospatial Applications Using Linked Data Technologies},
year = {2023},
isbn = {9798400707407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {51},
abstract = {This introductory textbook teaches the simple development of geospatial applications based on the principles and software tools of geospatial data science. It introduces a new generation of geospatial technologies that have emerged from the development of the Semantic Web and the Linked Data paradigm, and shows how data scientists can use them to build environmental applications easily.Geospatial data science is the science of collecting, organizing, analyzing, and visualizing geospatial data. Since around 2010, there has been extensive work in the area of geospatial data science using semantic technologies and linked data, from researchers in the areas of the Semantic Web, Geospatial Databases and Geoinformatics. The main results of this research have been the publication of the OGC standard GeoSPARQL and the implementation of a number of linked data tools supporting this standard. Up to now, there has been no textbook that enables someone to teach this material to undergraduate or graduate students.The material of the book is developed in a tutorial style and it is appropriate for an introductory course on the subject. This can be an advanced undergraduate course or a graduate course offered by Computer Science or GIS faculty. It is a hands-on approach and every chapter contains exercises that help students master the material.The book is accompanied by a Web site:  where solutions to some of the exercises are given together with supplementary material such as datasets and code. Most of the material in the book has been tried in the “Knowledge Technologies” course taught by the editor in the Department of Informatics and Telecommunications of the National and Kapodistrian University of Athens since 2012.}
}

@inproceedings{10.1145/3560071.3560086,
author = {Zhang, Kunli and Zhang, Chenghao and Ye, Yajuan and Zan, Hongying and Liu, Xiaomei},
title = {Named Entity Recognition in Electronic Medical Records Based on Transfer Learning},
year = {2022},
isbn = {9781450397087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560071.3560086},
doi = {10.1145/3560071.3560086},
abstract = {Named entity recognition is the first step in clinical electronic medical record text mining, which is significant for clinical decision support and personalized medicine. However, the lack of annotated electronic medical record datasets limits the application of pre-trained language models and deep neural networks in this field. To alleviate the problem of data scarcity, we propose T-RoBERTa-BiLSTM-CRF, a transfer learning-based electronic medical record entity recognition model, which aggregates the characteristics of medical data from different sources and uses a small amount of electronic medical record data as target data for further training. Compared with existing models, our approach can model medical entities more effectively, and the extensive comparative experiments on the CCKS 2019 and DEMRC datasets show the effectiveness of our approach.},
booktitle = {Proceedings of the 2022 International Conference on Intelligent Medicine and Health},
pages = {91–98},
numpages = {8},
keywords = {Electronic Medical Record, Named Entity Recognition, Transfer Learning},
location = {Xiamen, China},
series = {ICIMH '22}
}

@proceedings{10.1145/3615335,
title = {SIGDOC '23: Proceedings of the 41st ACM International Conference on Design of Communication},
year = {2023},
isbn = {9798400703362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@inproceedings{10.1145/3404835.3464922,
author = {Li, Feng-Lin and Zhao, Zhongzhou and Lu, Qin and Lin, Xuming and Chen, Hehong and Chen, Bo and Pu, Liming and Zhang, Jiashuo and Sun, Fu and Liu, Xikai and Xie, Liqun and Huang, Qi and Zhang, Ji and Chen, Haiqing},
title = {AliMe Avatar: Multi-modal Content Production and Presentation for Live-streaming E-commerce},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3464922},
doi = {10.1145/3404835.3464922},
abstract = {We present AliMe Avatar, a Vtuber designed for live-streaming sales in the E-commerce field. To support the emerging live shopping mode, the core of our digitial avatar is to enable customers to understand products and encourage customers to purchase in a virtual broadcasting room. Based on computer graphics &amp; vision, natural language processing, and speech recognition &amp; synthesis, our AI avatar is able to offer three kinds of key capabilities: custom appearance, product broadcasting, and multi-modal interaction. Currently, it has been launched online in the Taobao app, broadcasts 700+ hours and serves hundreds of thousands of customers per day. In this paper, we mainly focus on the product broadcasting part, demonstrate the system, present the underlying techniques, and share our experience in dealing with live-streaming E-commerce.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2635–2636},
numpages = {2},
keywords = {Vtuber, content production, multi-modality, visual presentation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{10.1109/TASLP.2022.3161157,
author = {Mao, Qianren and Li, Jianxin and Peng, Hao and He, Shizhu and Wang, Lihong and Yu, Philip S. and Wang, Zheng},
title = {Fact-Driven Abstractive Summarization by Utilizing Multi-Granular Multi-Relational Knowledge},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3161157},
doi = {10.1109/TASLP.2022.3161157},
abstract = {Abstractive summarization generates a concise summary to capture the key ideas of the source text. This task underpins important applications like information retrieval, document comprehension, and event tracking. While much progress has been achieved, state-of-the-art summarization approaches often fail to generate high-quality summaries to reproduce factual details accurately. One of the key limitations of existing solutions is that they are primarily concerned about extracting facts from the source text but overlook other crucial factual information, such as the related time, locations, reasons, consequences, purposes, participants and involved parties. Furthermore, the current summarization frameworks are inadequate in modeling the complex semantic relations among facts and the corresponding factual information, leaving much room for improvement. This paper presents &lt;sc&gt;FFSum&lt;/sc&gt;, a novel summarization framework for exploiting multi-grained factual information to improve text summarization. To this end, &lt;sc&gt;FFSum&lt;/sc&gt; constructs an individual fine-grained factual graph with multiple relations among facts and the corresponding factual information. It employs a fact-driven graph attention network to integrate multi-granular factual representations at the encoding stage. It then uses a hybrid pointer network to retrieve factual pieces from the graph for the summary generation. We evaluate the &lt;sc&gt;FFSum&lt;/sc&gt; by applying it to two real-world datasets. Experimental results show that the &lt;sc&gt;FFSum&lt;/sc&gt; consistently outperforms a state-of-the-art approach across evaluation datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1665–1678},
numpages = {14}
}

@article{10.1145/3627824,
author = {Xu, Ronghui and Huang, Weiming and Zhao, Jun and Chen, Meng and Nie, Liqiang},
title = {A Spatial and Adversarial Representation Learning Approach for Land Use Classification with POIs},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3627824},
doi = {10.1145/3627824},
abstract = {Points-of-interests (POIs) have been proven to be indicative for sensing urban land use in numerous studies. However, recent progress mainly relies on spatial co-occurrence patterns among POI categories, which falls short in utilizing the rich semantic information embodied in POI hierarchical categories and in sensing the spatial distribution patterns of POIs at an individual zonal scale. In this context, we present a spatial and adversarial representation learning approach (SARL) for predicting land use of urban zones with POIs. SARL deeply mines the information from POIs from both spatial and categorical perspectives. Specifically, we first utilize a convolutional neural network to sense the spatial distribution patterns of POIs in each urban zone. We then leverage an autoencoder and an adversarial learning strategy to mine the POI categorical information in all hierarchical levels, which emphasizes the prominent and definitive POIs while preserves the overall POI hierarchical structures in each zone. Finally, we fuse these information from the two perspectives via a Wide &amp; Deep network and carry out land use prediction with the fused embeddings. We conduct comprehensive experiments to validate the effectiveness of SARL in four European cities with real-world data. The results demonstrate that SARL substantially outperforms several competitive baselines.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {114},
numpages = {25},
keywords = {Land use classification, urban zone embedding, POI spatial distribution, POI categorical hierarchy, adversarial learning}
}

@inproceedings{10.1145/3611450.3611477,
author = {Liu, Haitao and Song, Jihua and Peng, Weiming},
title = {An Evidential Classifier with Multiple Pre-trained Language Models for Nested Named Entity Recognition},
year = {2023},
isbn = {9798400707605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611450.3611477},
doi = {10.1145/3611450.3611477},
abstract = {Nested named entity recognition (NER) is an important and challenging task in information extraction. One effective approach is to detect regions in sentences that are later classified by neural networks. Since pre-trained language models (PLMs) were proposed, nested NER models have benefited a lot from them. However, it is common that only one PLM is utilized for a given model, and the performance varies with different PLMs. We note that there exist some conflicting predictions which lead to the final variation. Thus, there is still room for investigation as to whether a model could achieve even better performance by conducting a comprehensive analysis of results from various PLMs. In this paper, we propose an evidential classifier with multiple PLMs for nested NER. First, the well-known deep exhaustive model is trained separately with different PLMs, whose predictions are then treated as pieces of evidence that can be represented in the framework of Dempster-Shafer theory. Finally, the pooled evidence is obtained using a combination rule, based on which the inference is performed. Experiments are conducted on the GENIA dataset, and detailed analysis demonstrates the merits of our model.},
booktitle = {Proceedings of the 2023 3rd International Conference on Artificial Intelligence, Automation and Algorithms},
pages = {181–185},
numpages = {5},
keywords = {Dempster-Shafer theory, information fusion, nested named entity recognition, pre-trained language models},
location = {Beijing, China},
series = {AI2A '23}
}

@article{10.1109/TCBB.2021.3135844,
author = {Sun, Yi and Wang, Jian and Lin, Hongfei and Zhang, Yijia and Yang, Zhihao},
title = {Knowledge Guided Attention and Graph Convolutional Networks for Chemical-Disease Relation Extraction},
year = {2022},
issue_date = {Jan.-Feb. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3135844},
doi = {10.1109/TCBB.2021.3135844},
abstract = {The automatic extraction of the chemical-disease relation (CDR) from the text becomes critical because it takes a lot of time and effort to extract valuable CDR manually. Studies have shown that prior knowledge from the biomedical knowledge base is important for relation extraction. The method of combining deep learning models with prior knowledge is worthy of our study. In this paper, we propose a new model called Knowledge Guided Attention and Graph Convolutional Networks (KGAGN) for CDR extraction. First, to make full advantage of domain knowledge, we train entity embedding as a feature representation of input sequence, and relation embedding to capture weighted contextual information further through the attention mechanism. Then, to make full advantage of syntactic dependency information in cross-sentence CDR extraction, we construct document-level syntactic dependency graphs and encode them using a graph convolution network (GCN). Finally, the chemical-induced disease (CID) relation is extracted by using weighted context features and long-range dependency features both of which contain additional knowledge information We evaluated our model on the CDR dataset published by the BioCreative-V community and achieves an F1-score of 73.3%, surpassing other state-of-the-art methods. the code implemented by PyTorch 1.7.0 deep learning library can be downloaded from Github: &lt;uri&gt;https://github.com/sunyi123/cdr&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {489–499},
numpages = {11}
}

@inproceedings{10.1145/3487553.3524674,
author = {Alam, Mehwish and Iana, Andreea and Grote, Alexander and Ludwig, Katharina and M\"{u}ller, Philipp and Paulheim, Heiko},
title = {Towards Analyzing the Bias of News Recommender Systems Using Sentiment and Stance Detection},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524674},
doi = {10.1145/3487553.3524674},
abstract = {News recommender systems are used by online news providers to alleviate information overload and to provide personalized content to users. However, algorithmic news curation has been hypothesized to create filter bubbles and to intensify users’ selective exposure, potentially increasing their vulnerability to polarized opinions and fake news. In this paper, we show how information on news items’ stance and sentiment can be utilized to analyze and quantify the extent to which recommender systems suffer from biases. To that end, we have annotated a German news corpus on the topic of migration using stance detection and sentiment analysis. In an experimental evaluation with four different recommender systems, our results show a slight tendency of all four models for recommending articles with negative sentiments and stances against the topic of refugees and migration. Moreover, we observed a positive correlation between the sentiment and stance bias of the text-based recommenders and the preexisting user bias, which indicates that these systems amplify users’ opinions and decrease the diversity of recommended news. The knowledge-aware model appears to be the least prone to such biases, at the cost of predictive accuracy.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {448–457},
numpages = {10},
keywords = {German news articles, echo chambers, filter bubbles, news recommendation, polarization, sentiment analysis, stance detection},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1613/jair.1.14318,
author = {Aksoy, Meltem and Yanik, Seda and Amasyali, Mehmet Fatih},
title = {Reviewer Assignment Problem: A Systematic Review of the Literature},
year = {2023},
issue_date = {May 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {76},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14318},
doi = {10.1613/jair.1.14318},
abstract = {Appropriate reviewer assignment significantly impacts the quality of proposal evaluation, as accurate and fair reviews are contingent on their assignment to relevant reviewers. The crucial task of assigning reviewers to submitted proposals is the starting point of the review process and is also known as the reviewer assignment problem (RAP). Due to the obvious restrictions of manual assignment, journal editors, conference organizers, and grant managers demand automatic reviewer assignment approaches. Many studies have proposed assignment solutions in response to the demand for automated procedures since 1992. The primary objective of this survey paper is to provide scholars and practitioners with a comprehensive overview of available research on the RAP. To achieve this goal, this article presents an in-depth systematic review of 103 publications in the field of reviewer assignment published in the past three decades and available in the Web of Science, Scopus, ScienceDirect, Google Scholar, and Semantic Scholar databases. This review paper classified and discussed the RAP approaches into two broad categories and numerous subcategories based on their underlying techniques. Furthermore, potential future research directions for each category are presented. This survey shows that the research on the RAP is becoming more significant and that more effort is required to develop new approaches and a framework.},
journal = {J. Artif. Int. Res.},
month = may,
numpages = {67}
}

@article{10.1145/3571730,
author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
title = {Survey of Hallucination in Natural Language Generation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3571730},
doi = {10.1145/3571730},
abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {248},
numpages = {38},
keywords = {Hallucination, intrinsic hallucination, extrinsic hallucination, faithfulness in NLG, factuality in NLG, consistency in NLG}
}

@inproceedings{10.1145/3526113.3545631,
author = {Kaur, Harmanpreet and Downey, Doug and Singh, Amanpreet and Cheng, Evie Yu-Yen and Weld, Daniel and Bragg, Jonathan},
title = {FeedLens: Polymorphic Lenses for Personalizing Exploratory Search over Knowledge Graphs},
year = {2022},
isbn = {9781450393201},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526113.3545631},
doi = {10.1145/3526113.3545631},
abstract = {The vast scale and open-ended nature of knowledge graphs (KGs) make exploratory search over them cognitively demanding for users. We introduce a new technique, polymorphic lenses, that improves exploratory search over a KG by obtaining new leverage from the existing preference models that KG-based systems maintain for recommending content. The approach is based on a simple but powerful observation: in a KG, preference models can be re-targeted to recommend not only entities of a single base entity type (e.g., papers in the scientific literature KG, products in an e-commerce KG), but also all other types (e.g., authors, conferences, institutions; sellers, buyers). We implement our technique in a novel system,&nbsp;FeedLens, which is built over&nbsp;Semantic Scholar, a production system for navigating the scientific literature KG.&nbsp;FeedLens reuses the existing preference models on&nbsp;Semantic Scholar—people’s curated research feeds—as lenses for exploratory search. Semantic Scholar users can curate multiple feeds/lenses for different topics of interest, e.g., one for human-centered AI and another for document embeddings. Although these lenses are defined in terms of papers, FeedLens re-purposes them to also guide search over authors, institutions, venues, etc. Our system design is based on feedback from intended users via two pilot surveys (n = 17 and n = 13, respectively). We compare&nbsp;FeedLens and&nbsp;Semantic Scholar via a third (within-subjects) user study (n = 15) and find that&nbsp;FeedLens increases user engagement while reducing the cognitive effort required to complete a short literature review task. Our qualitative results also highlight people’s preference for this more effective exploratory search experience enabled by&nbsp;FeedLens.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
articleno = {97},
numpages = {15},
keywords = {Exploratory search, Interaction techniques, Knowledge graphs, Recommender systems, System design, User study},
location = {Bend, OR, USA},
series = {UIST '22}
}

@inproceedings{10.1145/3581783.3611784,
author = {Lu, Jinda and Wang, Shuo and Zhang, Xinyu and Hao, Yanbin and He, Xiangnan},
title = {Semantic-based Selection, Synthesis, and Supervision for Few-shot Learning},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611784},
doi = {10.1145/3581783.3611784},
abstract = {Few-shot learning (FSL) is designed to explore the distribution of novel categories from a few samples. It is a challenging task since the classifier is usually susceptible to over-fitting when learning from limited training samples. To alleviate this phenomenon, a common solution is to achieve more training samples using a generic generation strategy in visual space. However, there are some limitations to this solution. It is because a feature extractor trained on base samples (known knowledge) tends to focus on the textures and structures of the objects it learns, which is inadequate for describing novel samples. To solve these issues, we introduce semantics and propose a Semantic-based Selection, Synthesis, and S upervision (4S) method, where semantics provide more diverse and informative supervision for recognizing novel objects. Specifically, we first utilize semantic knowledge to explore the correlation of categories in the textual space and select base categories related to the given novel category. This process can improve the efficiency of subsequent operations (synthesis and supervision). Then, we analyze the semantic knowledge to hallucinate the training samples by selectively synthesizing the contents from base and support samples. This operation not only increases the number of training samples but also takes advantage of the contents of the base categories to enhance the description of support samples. Finally, we also employ semantic knowledge as both soft and hard supervision to enrich the supervision for the fine-tuning procedure. Empirical studies on four FSL benchmarks demonstrate the effectiveness of 4S.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {3569–3578},
numpages = {10},
keywords = {data synthesis, few-shot learning, semantic supervision},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3539618.3591843,
author = {Hu, Sen and Yang, Changlin and Wang, Junjie and Liu, Siye and Xu, Teng and Zhang, Wangshu and Zheng, Jing},
title = {A Data-centric Solution to Improve Online Performance of Customer Service Bots},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591843},
doi = {10.1145/3539618.3591843},
abstract = {The online performance of customer service bots is often less than satisfactory because of the gap between limited training data and real-world user questions. As a straightforward way to improve online performance, model iteration and re-deployment are time consuming and labor-intensive, and therefore difficult to sustain. To fix badcases and improve online performance of chatbots in a timely and continuous manner, we propose a data-centric solution consisting of three main modules: badcase detection, bad case correction, and answer extraction. By making full use of online model signals, implicit user feedback and artificial customer service log, the proposed solution can fix online badcases automatically. Our solution has been deployed and bringing consistently positive impacts for hundreds of customer service bots used by Alipay app.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3305–3309},
numpages = {5},
keywords = {continuous improvement, customer service bots, data-centric AI},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3534678.3539344,
author = {Datta, Debanjan and Chen, Feng and Ramakrishnan, Naren},
title = {Framing Algorithmic Recourse for Anomaly Detection},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539344},
doi = {10.1145/3534678.3539344},
abstract = {The problem of algorithmic recourse has been explored for supervised machine learning models, to provide more interpretable, transparent and robust outcomes from decision support systems. An unexplored area is that of algorithmic recourse for anomaly detection, specifically for tabular data with only discrete feature values. Here the problem is to present a set of counterfactuals that are deemed normal by the underlying anomaly detection model so that applications can utilize this information for explanation purposes or to recommend countermeasures. We present an approach-Context preserving Algorithmic Recourse for Anomalies in Tabular data(CARAT), that is effective, scalable, and agnostic to the underlying anomaly detection model. CARAT uses a transformer based encoder-decoder model to explain an anomaly by finding features with low likelihood. Subsequently semantically coherent counterfactuals are generated by modifying the highlighted features, using the overall context of features in the anomalous instance(s). Extensive experiments help demonstrate the efficacy of CARAT.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {283–293},
numpages = {11},
keywords = {algorithmic recourse, anomaly detection, deep learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.14778/3523210.3523224,
author = {Fan, Wenfei and Fu, Wenzhi and Jin, Ruochun and Lu, Ping and Tian, Chao},
title = {Discovering association rules from big graphs},
year = {2022},
issue_date = {March 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {7},
issn = {2150-8097},
url = {https://doi.org/10.14778/3523210.3523224},
doi = {10.14778/3523210.3523224},
abstract = {This paper tackles two challenges to discovery of graph rules. Existing discovery methods often (a) return an excessive number of rules, and (b) do not scale with large graphs given the intractability of the discovery problem. We propose an application-driven strategy to cut back rules and data that are irrelevant to users' interests, by training a machine learning (ML) model to identify data pertaining to a given application. Moreover, we introduce a sampling method to reduce a big graph G to a set H of small sample graphs. Given expected support and recall bounds, the method is able to deduce samples in H and mine rules from H to satisfy the bounds in the entire G. As proof of concept, we develop an algorithm to discover Graph Association Rules (GARs), which are a combination of graph patterns and attribute dependencies, and may embed ML classifiers as predicates. We show that the algorithm is parallelly scalable, i.e., it guarantees to reduce runtime when more machines are used. We experimentally verify that the method is able to discover rules with recall above 91% when using sample ratio 10%, with speedup of 61 times.},
journal = {Proc. VLDB Endow.},
month = mar,
pages = {1479–1492},
numpages = {14}
}

@article{10.1145/3481608,
author = {Han, Xu and Zhang, Zhengyan and Liu, Zhiyuan},
title = {Knowledgeable machine learning for natural language processing},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3481608},
doi = {10.1145/3481608},
journal = {Commun. ACM},
month = oct,
pages = {50–51},
numpages = {2}
}

@inproceedings{10.1145/3488560.3498393,
author = {Wu, Sixing and Wang, Minghui and Li, Ying and Zhang, Dawei and Wu, Zhonghai},
title = {Improving the Applicability of Knowledge-Enhanced Dialogue Generation Systems by Using Heterogeneous Knowledge from Multiple Sources},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498393},
doi = {10.1145/3488560.3498393},
abstract = {Traditional conversational systems can only access the given query during the response generation, leading to meaningless responses. To this end, researchers proposed to enhance dialogue generation by integrating external knowledge. Although such methods have achieved remarkable gains, the use of only single-source knowledge often makes existing knowledge-enhanced methods degenerate into traditional models in real scenarios because of the insufficient knowledge coverage of single-source knowledge. To improve the applicability of knowledge-enhanced methods, we propose two novel frameworks to use heterogeneous knowledge from multiple sources. We first propose an MHKD-Seq2Seq framework, which can use different heterogeneous knowledge by identifying abstract-level knowledge behaviors; meanwhile, a Diffuse-Aggregate scheme is used to process multiple knowledge simultaneously and produce a unified result. The next framework MHKD-ARPLM can leverage the advantages of pretrained language models with Knowledge Linearization techniques. In experiments, we collected dialogues from previously open-released datasets and built a multi-source knowledge-aligned dataset TriKE-Weibo, which involves three knowledge sources: commonsense, texts, and infobox tables. Extensive evaluations demonstrate the performance leadership of our approaches against competitive baseline models.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {1149–1157},
numpages = {9},
keywords = {knowledge-enhanced dialogue generation, multi-source knowledge},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.5555/3535850.3536091,
author = {Singh, Ishika and Singh, Gargi and Modi, Ashutosh},
title = {Pre-trained Language Models as Prior Knowledge for Playing Text-based Games},
year = {2022},
isbn = {9781450392136},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recently, text world games have been proposed to enable artificial agents to understand and reason about real-world scenarios. These text-based games are challenging for artificial agents, as it requires an understanding of and interaction using natural language in a partially observable environment. Past approaches have paid less attention to the language understanding capability of the proposed agents. In this paper, we improve the semantic understanding of the agent by proposing a simple RL with LM framework where we use transformer-based language models with Deep RL models. Overall, our proposed approach outperforms on 4 games out of the 14 text-based games, while performing comparable to the state-of-the-art models on the remaining games.},
booktitle = {Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems},
pages = {1729–1731},
numpages = {3},
keywords = {NLP, interactive fiction games, reinforcement learning},
location = {Virtual Event, New Zealand},
series = {AAMAS '22}
}

@inproceedings{10.1145/3477495.3531668,
author = {La Cava, Lucio and Simeri, Andrea and Tagarelli, Andrea},
title = {LawNet-Viz: A Web-based System to Visually Explore Networks of Law Article References},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531668},
doi = {10.1145/3477495.3531668},
abstract = {We present LawNet-Viz, a web-based tool for the modeling, analysis and visualization of law reference networks extracted from a statute law corpus. LawNet-Viz is designed to support legal research tasks and help legal professionals as well as laymen visually exploring the article connections built upon the explicit law references detected in the article contents. To demonstrate LawNet-Viz, we show its application to the Italian Civil Code (ICC), which exploits a recent BERT-based model fine-tuned on the ICC. LawNet-Viz is a system prototype that is planned for product development.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3300–3305},
numpages = {6},
keywords = {artificial intelligence and law, deep language models, law article citation networks, network analysis and visualization},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3604915.3608801,
author = {Yang, Boming and Liu, Dairui and Suzumura, Toyotaro and Dong, Ruihai and Li, Irene},
title = {✨ Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608801},
doi = {10.1145/3604915.3608801},
abstract = {Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users’ hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator. Similarly, we extend this approach to a Global Candidate News Encoder, utilizing a global entity graph and a candidate news aggregator to enhance candidate news representation. Evaluation results on two public news datasets demonstrate that our method outperforms existing approaches. Furthermore, our model offers more diverse recommendations1.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {24–34},
numpages = {11},
keywords = {Graph Neural Network, News Modeling, News Recommendation},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@proceedings{10.1145/3587828,
title = {ICSCA '23: Proceedings of the 2023 12th International Conference on Software and Computer Applications},
year = {2023},
isbn = {9781450398589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kuantan, Malaysia}
}

@proceedings{10.1145/3544549,
title = {CHI EA '23: Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@article{10.1145/3588314,
author = {Yan, Jinghui and Zong, Chengqing and Xu, Jinan},
title = {Combination of Loss-based Active Learning and Semi-supervised Learning for Recognizing Entities in Chinese Electronic Medical Records},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3588314},
doi = {10.1145/3588314},
abstract = {The recognition of entities in an electronic medical record (EMR) is especially important to downstream tasks, such as clinical entity normalization and medical dialogue understanding. However, in the medical professional field, training a high-quality named entity recognition system always requires large-scale annotated datasets, which are highly expensive to obtain. In this article, to lower the cost of data annotation and maximizing the use of unlabeled data, we propose a hybrid approach to recognizing the entities in Chinese electronic medical record, which is in combination of loss-based active learning and semi-supervised learning. Specifically, we adopted a dynamic balance strategy to dynamically balance the minimum loss predicted by a named entity recognition decoder and a loss prediction module at different stages in the process. Experimental results demonstrated our proposed framework’s effectiveness and efficiency, achieving higher performances than existing approaches on Chinese EMR entity recognition datasets under limited labeling resources.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {123},
numpages = {19},
keywords = {Electronic medical record, loss-based active learning, dynamic balance strategy, semi-supervised learning}
}

@inproceedings{10.1145/3584371.3613008,
author = {Zhang, Wenlong and Zeng, Kangping and Yang, Xinming and Shi, Tian and Wang, Ping},
title = {Text-to-ESQ: A Two-Stage Controllable Approach for Efficient Retrieval of Vaccine Adverse Events from NoSQL Database},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3613008},
doi = {10.1145/3584371.3613008},
abstract = {The Vaccine Adverse Event Reporting System (VAERS) contains detailed reports of adverse events following vaccine administration. However, efficiently and accurately searching for specific information from VAERS poses significant challenges, especially for medical experts. Natural language querying (NLQ) methods tackle the challenge by translating the input questions into executable queries, allowing for the exploration of complex databases with large amounts of information. Most existing studies focus on the relational database and solve the Text-to-SQL task. However, the capability of full-text for Text-to-SQL is greatly limited by the data structures and functionality of the SQL databases. In addition, the potential of natural language querying has not been comprehensively explored in the healthcare domain. To overcome these limitations, we investigate the potential of NoSQL databases, specifically Elasticsearch, and forge a new research direction for NLQ, which we refer to as Text-to-ESQ generation. This exploration requires us to re-design various aspects of NLQ, such as the target application and the advantages of NoSQL database. In our approach, we develop a two-stage controllable (TSC) framework consisting of a question-to-question (Q2Q) translation module and an ESQ condition extraction (ECE) module. These modules are carefully designed to efficiently retrieve information from the VEARS data stored in a NoSQL database. Additionally, we construct a dedicated question-ESQ pair dataset called VAERSESQ, to support the task in the healthcare domain. Extensive experiments were conducted on the VAERSESQ dataset to evaluate the proposed methods. The results, both quantitative and qualitative, demonstrate the accuracy and efficiency of our approach in generating queries for NoSQL databases, thus enabling efficient retrieval of VEARS data.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {54},
numpages = {10},
keywords = {natural language querying, question translation, text-to-ESQ, VAERS, elasticsearch query},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3485447.3511998,
author = {Chen, Xiang and Zhang, Ningyu and Xie, Xin and Deng, Shumin and Yao, Yunzhi and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
title = {KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511998},
doi = {10.1145/3485447.3511998},
abstract = {Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in GitHub1 for reproducibility.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2778–2788},
numpages = {11},
keywords = {Knowledge-aware, Prompt-tuning, Relation Extraction},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3459637.3482111,
author = {Lin, Xuming and Cui, Shaobo and Zhao, Zhongzhou and Zhou, Wei and Zhang, Ji and Chen, Haiqing},
title = {GGP: A Graph-based Grouping Planner for Explicit Control of Long Text Generation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482111},
doi = {10.1145/3459637.3482111},
abstract = {Existing data-driven methods can well handle short text generation. However, when applied to the long-text generation scenarios such as story generation or advertising text generation in the commercial scenario, these methods may generate illogical and uncontrollable texts. To address these aforementioned issues, we propose a graph-based grouping planner~(GGP) following the idea of first-plan-then-generate. Specifically, given a collection of key phrases, GGP firstly encodes these phrases into a instance-level sequential representation and a corpus-level graph-based representation separately. With these two synergic representations, we then regroup these phrases into a fine-grained plan, based on which we generate the final long text. We conduct our experiments on three long text generation datasets and the experimental results reveal that GGP significantly outperforms baselines, which proves that GGP can control the long text generation with knowing how to say and in what order.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3253–3257},
numpages = {5},
keywords = {copy mechanism, graph neural networks, long text generation, planning based data-to-text},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.14778/3611479.3611533,
author = {Eltabakh, Mohamed Y. and Kunjir, Mayuresh and Elmagarmid, Ahmed K. and Ahmad, Mohammad Shahmeer},
title = {Cross Modal Data Discovery over Structured and Unstructured Data Lakes},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611533},
doi = {10.14778/3611479.3611533},
abstract = {Organizations are collecting increasingly large amounts of data for data-driven decision making. These data are often dumped into a centralized repository, e.g., a data lake, consisting of thousands of structured and unstructured datasets. Perversely, such mixture makes the problem of discovering tables or documents that are relevant to a user's query very challenging. Despite the recent efforts in data discovery, the problem remains widely open especially in the two fronts of (1) discovering relationships and relatedness across structured and unstructured datasets-where existing techniques suffer from either scalability, being customized for a specific problem type (e.g., entity matching or data integration), or demolishing the structural properties on its way, and (2) developing a holistic system for integrating various similarity measurements and sketches in an effective way to boost the discovery accuracy.In this paper, we propose a new data discovery system, named CMDL, for addressing these two limitations. CMDL supports the data discovery process over both structured and unstructured data while retaining the structural properties of tables. As a result, CMDL is the only system to date that empowers end-users to seamlessly pipeline the discovery tasks across the two modalities. We propose a novel multi-modal embedding representation that captures the similarities between text documents and tabular columns. The model training relies on labeled datasets generated though weak supervision, and thus the system is domain agnostic and easily generalizable. We evaluate CMDL on three real-world data lakes with diverse applications and show that our system is significantly more effective for cross-modality discovery compared to the search-based baseline techniques. Moreover, CMDL is more accurate and robust to different data types and distributions compared to the state-of-the-art systems that are limited to only the structured datasets.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3377–3390},
numpages = {14}
}

@inproceedings{10.1145/3539618.3591884,
author = {Guo, Shuyu and Zhang, Shuo and Sun, Weiwei and Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun},
title = {Towards Explainable Conversational Recommender Systems},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591884},
doi = {10.1145/3539618.3591884},
abstract = {Explanations in conventional recommender systems have demonstrated benefits in helping the user understand the rationality of the recommendations and improving the system's efficiency, transparency, and trustworthiness. In the conversational environment, multiple contextualized explanations need to be generated, which poses further challenges for explanations. To better measure explainability in CRS, we propose ten evaluation perspectives based on the concepts from conventional recommender systems together with the characteristics of CRS. We assess five existing CRS benchmark datasets using these metrics and observe the necessity of improving the explanation quality of CRS. To achieve this, we conduct manual and automatic approaches to extend these dialogues and construct a new CRS dataset, namely Explainable Recommendation Dialogues (E-ReDial). It includes 756 dialogues with over 2,000 high-quality rewritten explanations. We compare two baseline approaches to perform explanation generation based on E-ReDial. Experimental results suggest that models trained on E-ReDial can significantly improve explainability while introducing knowledge into the models can further improve the performance. GPT-3 in the in-context learning setting can generate more realistic and diverse movie descriptions. In contrast, T5 training on E-Redial can better generate clear reasons for recommendations based on user preferences. E-ReDial is available at https://github.com/Superbooming/E-ReDial.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2786–2795},
numpages = {10},
keywords = {conversational information access, conversational recommendation, explainable recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3486635.3491068,
author = {Gurav, Rutuja and De, Debraj and Thakur, Gautam and Fan, Junchuan},
title = {Conflation of Geospatial POI Data and Ground-level Imagery via Link Prediction on Joint Semantic Graph},
year = {2021},
isbn = {9781450391207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486635.3491068},
doi = {10.1145/3486635.3491068},
abstract = {With the proliferation of smartphone cameras and social networks, we have rich, multi-modal data about points of interest (POIs) - like cultural landmarks, institutions, businesses, etc. - within a given areas of interest (AOI) (e.g., a county, city or a neighborhood) available to us. Data conflation across multiple modalities of data sources is one of the key challenges in maintaining a geographical information system (GIS) which accumulate data about POIs. Given POI data from nine different sources, and ground-level geo-tagged and scene-captioned images from two different image hosting platforms, in this work we explore the application of graph neural networks (GNNs) to perform data conflation, while leveraging a natural graph structure evident in geospatial data. The preliminary results demonstrate the capacity of a GNN operation to learn distributions of entity (POIs and images) features, coupled with topological structure of entity's local neighborhood in a semantic nearest neighbor graph, in order to predict links between a pair of entities.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {5–8},
numpages = {4},
keywords = {Areas of Interest (AOI), POI configuration, Points of Interest (POI), data conflation, graph neural network, ground-level imagery, semantic space, word embedding},
location = {Beijing, China},
series = {GEOAI '21}
}

@proceedings{10.1145/3603765,
title = {ICISDM '23: Proceedings of the 2023 7th International Conference on Information System and Data Mining},
year = {2023},
isbn = {9798400700637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Atlanta, USA}
}

@proceedings{10.1145/3582768,
title = {NLPIR '22: Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
year = {2022},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bangkok, Thailand}
}

@inproceedings{10.1145/3539618.3591740,
author = {Zamani, Hamed and Bendersky, Michael},
title = {Multivariate Representation Learning for Information Retrieval},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591740},
doi = {10.1145/3539618.3591740},
abstract = {Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range of datasets, and demonstrate significant improvements compared to competitive dense retrieval models.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {163–173},
numpages = {11},
keywords = {approximate nearest neighbor search, dense retrieval, learning to rank, neural information retrieval},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3543873.3587540,
author = {Naik, Riya},
title = {Multi-turn mediated solutions for Conversational Artificial Intelligent systems leveraging graph-based techniques},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587540},
doi = {10.1145/3543873.3587540},
abstract = {The current era is dominated by intelligent Question Answering (QA) systems that can instantly answer almost all their questions, saving users search time and increasing the throughput and precision in the applied domain. A vast amount of work is being carried out in QA systems to deliver better content satisfying users’ information needs [2]. Since QA systems are ascending the cycle of emerging technologies, there are potential research gaps that can be explored. QA systems form a significant part of Conversational Artificial Intelligent systems giving rise to a new research pathway, i.e., Conversational Question Answering (CQA) systems [32]. We propose to design and develop a CQA system leveraging Hypergraph-based techniques. The approach focuses on the multi-turn conversation and multi-context to gauge users’ exact information needs and deliver better answers. We further aim to address "supporting evidence-based retrieval" for fact-based responsible answer generation. Since the QA system requires a large amount of data and processing, we also intend to investigate hardware performance for effective system utilization.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {586–590},
numpages = {5},
keywords = {Contextual Embeddings, Conversational Artificial Intelligence, Evidence-based retrieval, Graph-based models, Question Answering},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1109/TCBB.2022.3161032,
author = {Bai, Jun and Yin, Chuantao and Zhang, Jianfei and Wang, Yanmeng and Dong, Yi and Rong, Wenge and Xiong, Zhang},
title = {Adversarial Knowledge Distillation Based Biomedical Factoid Question Answering},
year = {2022},
issue_date = {Jan.-Feb. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3161032},
doi = {10.1109/TCBB.2022.3161032},
abstract = {Biomedical factoid question answering is an essential application for biomedical information sharing. Recently, neural network based approaches have shown remarkable performance for this task. However, due to the scarcity of annotated data which requires intensive knowledge of expertise, training a robust model on limited-scale biomedical datasets remains a challenge. Previous works solve this problem by introducing useful knowledge. It is found that the interaction between question and answer (QA-interaction) is also a kind of knowledge which could help extract answer accurately. This research develops a knowledge distillation framework for biomedical factoid question answering, in which a teacher model as the knowledge source of QA-interaction is designed to enhance the student model. In addition, to further alleviate the problem of limited-scale dataset, a novel adversarial knowledge distillation technique is proposed to robustly distill the knowledge from teacher model to student model by constructing perturbed examples as additional training data. By forcing the student model to mimic the predicted distributions of teacher model on both original examples and perturbed examples, the knowledge of QA-interaction can be learned by student model. We evaluate the proposed framework on the widely used BioASQ datasets, and experimental results have shown the proposed method’s promising potential.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {106–118},
numpages = {13}
}

@inproceedings{10.1145/3512527.3531401,
author = {Geng, Minghao and Zhao, Qingjie},
title = {Improve Image Captioning by Modeling Dynamic Scene Graph Extension},
year = {2022},
isbn = {9781450392389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512527.3531401},
doi = {10.1145/3512527.3531401},
abstract = {Recently, scene graph generation methods have been used in image captioning to encode the objects and their relationships in the encoder-decoder framework, where the decoder selects part of the graph nodes as input for word inference. However, current methods attend to scene graph relying on ambiguous language information, neglecting the strong connections between scene graph nodes. In this paper, we propose a Scene Graph Extension (SGE) architecture to model the dynamic scene graph extension using the partly generated sentence. Our model first uses the generated words and previous attention results of scene graph nodes to make up a partial scene graph. Then we choose objects or relationships that has close connection with the generated graph to infer the next word. Our SGE is appealing in view that it is pluggable to any scene graph based image captioning method. We conduct the extensive experiments on MSCOCO dataset. The results shows that the proposed SGE significantly outperforms the baselines, resulting in a state-of-the-art performance under most metrics.},
booktitle = {Proceedings of the 2022 International Conference on Multimedia Retrieval},
pages = {398–406},
numpages = {9},
keywords = {image captioning, image representation, language generation, scene graph},
location = {Newark, NJ, USA},
series = {ICMR '22}
}

@inproceedings{10.1145/3477495.3531742,
author = {Plum, Alistair and Ranasinghe, Tharindu and Jones, Spencer and Orasan, Constantin and Mitkov, Ruslan},
title = {Biographical Semi-Supervised Relation Extraction Dataset},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531742},
doi = {10.1145/3477495.3531742},
abstract = {Extracting biographical information from online documents is a popular research topic among the information extraction (IE) community. Various natural language processing (NLP) techniques such as text classification, text summarisation and relation extraction are commonly used to achieve this. Among these techniques, RE is the most common since it can be directly used to build biographical knowledge graphs. RE is usually framed as a supervised machine learning (ML) problem, where ML models are trained on annotated datasets. However, there are few annotated datasets for RE since the annotation process can be costly and time-consuming. To address this, we developedBiographical, the first semi-supervised dataset for RE. The dataset, which is aimed towards digital humanities (DH) and historical research, is automatically compiled by aligning sentences from Wikipedia articles with matching structured data from sources including Pantheon and Wikidata. By exploiting the structure of Wikipedia articles and robust named entity recognition (NER), we match information with relatively high precision in order to compile annotated relation pairs for ten different relations that are important in the DH domain. Furthermore, we demonstrate the effectiveness of the dataset by training a state-of-the-art neural model to classify relation pairs, and evaluate it on a manually annotated gold standard set.Biographical is primarily aimed at training neural models for RE within the domain of digital humanities and history, but as we discuss at the end of this paper, it can be useful for other purposes as well.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3121–3130},
numpages = {10},
keywords = {biographical information extraction, relation extraction, transformers},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3604915.3608866,
author = {Spillo, Giuseppe},
title = {Knowledge-Aware Recommender Systems based on Multi-Modal Information Sources},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608866},
doi = {10.1145/3604915.3608866},
abstract = {The last few years showed a growing interest in the design and development of Knowledge-Aware Recommender Systems (KARSs). This is mainly due to their capability in encoding and exploiting several data sources, both structured (such as knowledge graphs) and unstructured (such as plain text). Nowadays, a lot of models at the state-of-the-art in KARSs use deep learning, enabling them to exploit large amounts of information, including knowledge graphs (KGs), user reviews, plain text, and multimedia content (pictures, audio, videos). In my Ph.D. I will follow this research trend and I will explore and study techniques for designing KARSs leveraging representations learnt from multi-modal information sources, in order to provide users with fair, accurate, and explainable recommendations.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1312–1317},
numpages = {6},
keywords = {graph neural networks, knowledge aware recommender systems, knowledge graphs, multimedia content embedding, word embedding},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3442381.3449991,
author = {Yan, Rui and Liao, Weiheng and Cui, Jianwei and Zhang, Hailei and Hu, Yichuan and Zhao, Dongyan},
title = {Multilingual COVID-QA: Learning towards Global Information Sharing via Web Question Answering in Multiple Languages},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449991},
doi = {10.1145/3442381.3449991},
abstract = {Since late December 2019, it has been reported an outbreak of atypical pneumonia, now known as COVID-19 caused by the novel coronavirus. Cases have spread to more than 200 countries and regions internationally. World Health Organization (WHO) officially declares the coronavirus outbreak a pandemic and the public health emergency has caused world-wide impact to daily lives: people are advised to keep social distance, in-person events have been moved online, and some function facilitates have been locked-down. Alternatively, the Web becomes an active venue for people to share information. With respect to the on-going topic, people continuously post questions online and seek for answers. Yet, sharing global information conveyed in different languages is challenging because the language barrier is intrinsically unfriendly to monolingual speakers. In this paper, we propose a multilingual COVID-QA model to answer people’s questions in their own languages while the model is able to absorb knowledge from other languages. Another challenge is that in most cases, the information to share does not have parallel data in multiple languages. To this end, we propose a novel framework which incorporates (unsupervised) translation alignment to learn as pseudo-parallel data. Then we train multilingual question-answering mapping and generation. We demonstrate the effectiveness of our proposed approach compared against a series of competitive baselines. In this way, we make it easier to share global information across the language barriers, and hopefully we contribute to the battle against COVID-19.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2590–2600},
numpages = {11},
keywords = {Web question and answering (Web QA), multilingual text generation, response to COVID-19 pandemic},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@proceedings{10.1145/3568199,
title = {MLMI '22: Proceedings of the 2022 5th International Conference on Machine Learning and Machine Intelligence},
year = {2022},
isbn = {9781450397551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3632314,
title = {ISIA '23: Proceedings of the 2023 International Conference on Intelligent Sensing and Industrial Automation},
year = {2023},
isbn = {9798400709401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, China}
}

@article{10.1109/TASLP.2023.3310879,
author = {Chang, Hongyang and Xu, Hongfei and van Genabith, Josef and Xiong, Deyi and Zan, Hongying},
title = {JoinER-BART: Joint Entity and Relation Extraction With Constrained Decoding, Representation Reuse and Fusion},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3310879},
doi = {10.1109/TASLP.2023.3310879},
abstract = {Joint Entity and Relation Extraction (JERE) is an important research direction in Information Extraction (IE). Given the surprising performance with fine-tuning of pre-trained BERT in a wide range of NLP tasks, nowadays most studies for JERE are based on the BERT model. Rather than predicting a simple tag for each word, these approaches are usually forced to design complex tagging schemes, as they may have to extract entity-relation pairs which may overlap with others from the same sequence of word representations in a sentence. Recently, sequence-to-sequence (seq2seq) pre-trained BART models show better performance than BERT models in many NLP tasks. Importantly, a seq2seq BART model can simply generate sequences of (many) entity-relation triplets with its decoder, rather than just tag input words. In this article, we present a new generative JERE framework based on pre-trained BART. Different from the basic seq2seq BART architecture: 1) our framework employs a constrained classifier which only predicts either a token of the input sentence or a relation in each decoding step, and 2) we reuse representations from the pre-trained BART encoder in the classifier instead of a newly trained weight matrix, as this better utilizes the knowledge of the pre-trained model and context-aware representations for classification, and empirically leads to better performance. In our experiments on the widely studied NYT and WebNLG datasets, we show that our approach outperforms previous studies and establishes a new state-of-the-art (92.91 and 91.37 F1 respectively in exact match evaluation).},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3603–3616},
numpages = {14}
}

@inproceedings{10.1145/3582935.3583030,
author = {Tang, Xilang and Xie, Xiaoyue and Cui, Lijie and Xu, Xiao and Wang, Jianhao},
title = {Fault Knowledge Acquisition of Aircraft Based on Event Extraction Technology},
year = {2023},
isbn = {9781450396806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582935.3583030},
doi = {10.1145/3582935.3583030},
abstract = {In order to make full use of aircraft fault information of total life cycle and construct a knowledge base for intelligent fault diagnosis, this paper preliminarily explored extracting fault knowledge from massive unstructured texts by using the event extraction technology. According to the business requirements of fault diagnosis, we defined the elements of fault events, including trigger words, and fault time, argument roles such fault time, fault occasion, fault unit, signal indicate. To extract above elements, three models, including fault event identification model, trigger extraction model and Argument extraction model is developed in this paper. The results show that this method is effective.},
booktitle = {Proceedings of the 5th International Conference on Information Technologies and Electrical Engineering},
pages = {566–571},
numpages = {6},
location = {Changsha, China},
series = {ICITEE '22}
}

@proceedings{10.1145/3604915,
title = {RecSys '23: Proceedings of the 17th ACM Conference on Recommender Systems},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3459637.3482276,
author = {Ai, Qingyao and Narayanan.R, Lakshmi},
title = {Model-agnostic vs. Model-intrinsic Interpretability for Explainable Product Search},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482276},
doi = {10.1145/3459637.3482276},
abstract = {Product retrieval systems have served as the main entry for customers to discover and purchase products online. With increasing concerns on the transparency and accountability of AI systems, studies on explainable information retrieval has received more and more attention in the research community. Interestingly, in the domain of e-commerce, despite the extensive studies on explainable product recommendation, the studies of explainable product search is still in an early stage. In this paper, we study how to construct effective explainable product search by comparing model-agnostic explanation paradigms with model-intrinsic paradigms and analyzing the important factors that determine the performance of product search explanations. We propose an explainable product search model with model-intrinsic interpretability and conduct crowdsourcing to compare it with the state-of-the-art explainable product search model with model-agnostic interpretability. We observe that both paradigms have their own advantages and the effectiveness of search explanations on different properties are affected by different factors. For example, explanation fidelity is more important for user's overall satisfaction on the system while explanation novelty may be more useful in attracting user purchases. These findings could have important implications for the future studies and design of explainable product search engines.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {5–15},
numpages = {11},
keywords = {attention mechanism, product search, search explanation},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1109/TASLP.2023.3278185,
author = {Liu, Yuanzhi and He, Min and Yang, Qingqing and Jeon, Gwanggil},
title = {An Unsupervised Framework With Attention Mechanism and Embedding Perturbed Encoder for Non-Parallel Text Sentiment Style Transfer},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3278185},
doi = {10.1109/TASLP.2023.3278185},
abstract = {Text sentiment style transfer aims to extract the sentiment words from a sentence and transfer them into another expected sentiment style while retaining the original sentence's content. However, previous works have not achieved satisfactory performance on the text sentiment style transfer task, especially for non-parallel text. In this article, a novel framework with the attention mechanism and embedding perturbed encoder is proposed to improve the performance of non-parallel text sentiment style transfer. Firstly, the reverse attention mechanism is adopted to disentangle the sentiment style information from the latent representation. And then an embedding perturbed encoder is designed to append an adjustable noise to the embedding space to make the latent representation more semantic. Finally, the attention mechanism is introduced to give different weights for generated words during the decoding process, so that the model can focus on those high-weight words to enhance the quality of sentiment style transfer. Experiments on the corpora of Yelp and IMDB demonstrate that the suggested framework outperforms previous works on the aspects of sentiment style transfer accuracy, content preservation and language fluency.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {2134–2144},
numpages = {11}
}

@proceedings{10.1145/3579895,
title = {ICNCC '22: Proceedings of the 2022 11th International Conference on Networks, Communication and Computing},
year = {2022},
isbn = {9781450398039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3442381.3450126,
author = {Zhang, Zhihan and Geng, Xiubo and Qin, Tao and Wu, Yunfang and Jiang, Daxin},
title = {Knowledge-Aware Procedural Text Understanding with Multi-Stage Training},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450126},
doi = {10.1145/3442381.3450126},
abstract = {Procedural text describes dynamic state changes during a step-by-step natural process (e.g., photosynthesis). In this work, we focus on the task of procedural text understanding, which aims to comprehend such documents and track entities’ states and locations during a process. Although recent approaches have achieved substantial progress, their results are far behind human performance. Two challenges, the difficulty of commonsense reasoning and data insufficiency, still remain unsolved, which require the incorporation of external knowledge bases. Previous works on external knowledge injection usually rely on noisy web mining tools and heuristic rules with limited applicable scenarios. In this paper, we propose a novel KnOwledge-Aware proceduraL text understAnding (KoaLa) model, which effectively leverages multiple forms of external knowledge in this task. Specifically, we retrieve informative knowledge triples from ConceptNet and perform knowledge-aware reasoning while tracking the entities. Besides, we employ a multi-stage training schema which fine-tunes the BERT model over unlabeled data collected from Wikipedia before further fine-tuning it on the final model. Experimental results on two procedural text datasets, ProPara and Recipes, verify the effectiveness of the proposed methods, in which our model achieves state-of-the-art performance in comparison to various baselines.1},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3512–3523},
numpages = {12},
keywords = {Entity Tracking, Knowledge-Aware Reasoning, Multi-Stage Training, Procedural Text Understanding},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3539618.3591673,
author = {Chen, Xiaolin and Song, Xuemeng and Wei, Yinwei and Nie, Liqiang and Chua, Tat-Seng},
title = {Dual Semantic Knowledge Composed Multimodal Dialog Systems},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591673},
doi = {10.1145/3539618.3591673},
abstract = {Textual response generation is an essential task for multimodal task-oriented dialog systems. Although existing studies have achieved fruitful progress, they still suffer from two critical limitations: 1) focusing on the attribute knowledge but ignoring the relation knowledge that can reveal the correlations between different entities and hence promote the response generation, and 2)only conducting the cross-entropy loss based output-level supervision but lacking the representation-level regularization. To address these limitations, we devise a novel multimodal task-oriented dialog system (named MDS-S2). Specifically, MDS-S2 first simultaneously acquires the context related attribute and relation knowledge from the knowledge base, whereby the non-intuitive relation knowledge is extracted by the n-hop graph walk. Thereafter, considering that the attribute knowledge and relation knowledge can benefit the responding to different levels of questions, we design a multi-level knowledge composition module in MDS-S^2 to obtain the latent composed response representation. Moreover, we devise a set of latent query variables to distill the semantic information from the composed response representation and the ground truth response representation, respectively, and thus conduct the representation-level semantic regularization. Extensive experiments on a public dataset have verified the superiority of our proposed MDS-S2. We have released the codes and parameters to facilitate the research community.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1518–1527},
numpages = {10},
keywords = {dual semantic knowledge, multimodal task-oriented dialog systems, representation-level regularization},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3584871.3584872,
author = {Patel, Manali and Jariwala, Krupa and Chattopadhyay, Chiranjoy},
title = {Deep Learning techniques for stock market forecasting: Recent trends and challenges},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584871.3584872},
doi = {10.1145/3584871.3584872},
abstract = {Stock market forecasting has been a very intensive area of research in recent years due to the highly uncertain and volatile nature of stock data which makes this task challenging. By accurately predicting a particular stock's price investors can gain maximum profit out of their investment. With the great success of Deep Learning methods in various domains, it has attracted the research community to apply these models for financial domain also. These DL methods have been proven to achieve better accuracy and predictions compared to econometric and traditional ML methods. This work reviews recent papers according to various Deep Learning models which included: Artificial Neural Networks, Convolution Neural Networks, Sequence to Sequence models, Generative Adversarial Networks, Graph Neural Networks and Transformers applied for stock market forecasting. Furthermore this work also reviews datasets, features, evaluation parameters and results of various methods. From the analysis done on various DL models we found that Graph Neural Networks and Transformer models have potential to interpret dynamic and non-linear patterns of financial time series data with greater accuracy. In addition to this, correlation among various stock indices and investors sentiment along with historical data has great influence on the prediction accuracy. We also identified the benchmark datasets for stock market forecasting based on market capitalization value of an economy. The aim of this paper is to provide insight into most recent work done in the finance domain and identify future directions for more accurate predictions.},
booktitle = {Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
pages = {1–11},
numpages = {11},
keywords = {Corporate relationship, Deep Learning, Graph Neural Networks, Sentiment analysis, Stock market forecasting, Transformers},
location = {Palmerston North, New Zealand},
series = {ICSIM '23}
}

@inproceedings{10.1145/3487553.3524701,
author = {Cuffy, Clint and French, Evan and Fehrmann, Sophia and McInnes, Bridget T.},
title = {Exploring Representations for Singular and Multi-Concept Relations for Biomedical Named Entity Normalization},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524701},
doi = {10.1145/3487553.3524701},
abstract = {Since the rise of the COVID-19 pandemic, peer-reviewed biomedical repositories have experienced a surge in chemical and disease related queries. These queries have a wide variety of naming conventions and nomenclatures from trademark and generic, to chemical composition mentions. Normalizing or disambiguating these mentions within texts provides researchers and data-curators with more relevant articles returned by their search query. Named entity normalization aims to automate this disambiguation process by linking entity mentions onto their appropriate candidate concepts within a biomedical knowledge base or ontology. We explore several term embedding aggregation techniques in addition to how the term’s context affects evaluation performance. We also evaluate our embedding approaches for normalizing term instances containing one or many relations within unstructured texts.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {823–832},
numpages = {10},
keywords = {MeSH identifier, concept linking, concept mapping, concept normalization, concept unique identifier, datasets, entity linking, entity normalization, named entity disambiguation, named entity linking, named entity normalization, neural networks, transformer, word embeddings},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3511808.3557422,
author = {Genest, Pierre-Yves and Portier, Pierre-Edouard and Egyed-Zsigmond, El\"{o}d and Goix, Laurent-Walter},
title = {PromptORE - A Novel Approach Towards Fully Unsupervised Relation Extraction},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557422},
doi = {10.1145/3511808.3557422},
abstract = {Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a "Prompt-based Open Relation Extraction" model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Results on three general and specific domain datasets show that PromptORE consistently outperforms state-of-the-art models with a relative gain of more than 40% in B3, V-measure and ARI. Qualitative analysis also indicates PromptORE's ability to identify semantically coherent clusters that are very close to true relations.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {561–571},
numpages = {11},
keywords = {natural language processing, open relation extraction, prompt-tuning, unsupervised relation extraction},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3523227.3546769,
author = {Ning, Lin and Chien, Steve and Song, Shuang and Chen, Mei and Xue, Yunqi and Berlowitz, Devora},
title = {EANA: Reducing Privacy Risk on Large-scale Recommendation Models},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3546769},
doi = {10.1145/3523227.3546769},
abstract = {Embedding-based deep neural networks (DNNs) are widely used in large-scale recommendation systems. Differentially-private stochastic gradient descent (DP-SGD) provides a way to enable personalized experiences while preserving user privacy by injecting noise into every model parameter during the training process. However, it is challenging to apply DP-SGD to large-scale embedding-based DNNs due to its effect on training speed. This happens because the noise added by DP-SGD causes normally sparse gradients to become dense, introducing a large communication overhead between workers and parameter servers in a typical distributed training framework. This paper proposes embedding-aware noise addition (EANA) to mitigate the communication overhead, making training a large-scale embedding-based DNN possible. We examine the privacy benefit of EANA both analytically and empirically using secret sharer techniques. We demonstrate that training with EANA can achieve reasonable model precision while providing good practical privacy protection as measured by the secret sharer tests. Experiments on a real-world, large-scale dataset and model show that EANA is much faster than standard DP-SGD, improving the training speed by 54X and unblocking the training of a large-scale embedding-based DNN with reduced privacy risk.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {399–407},
numpages = {9},
keywords = {embedding-based deep neural networks, large-scale, privacy, recommendation system, secret sharer},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{10.1145/3487664.3487701,
author = {Deka, Pritam and Jurek-Loughrey, Anna and Deepak},
title = {Unsupervised Keyword Combination Query Generation from Online Health Related Content for Evidence-Based Fact Checking},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487701},
doi = {10.1145/3487664.3487701},
abstract = {False information in the domain of online health related articles is of great concern, which can be witnessed in the current pandemic situation of Covid-19. It is markedly different from fake news in the political context as health information should be evaluated against the most recent and reliable medical resources such as scholarly repositories. However, one of the challenges with such an approach is the retrieval of the pertinent resources. In this work, we formulate a new unsupervised task of generating queries using keywords extracted from a health-related article which can be further applied to retrieve relevant authoritative and reliable medical content from scholarly repositories to assess the article’s veracity. We propose a three-step approach for it and illustrate that our method is able to generate effective queries. We also curate a new dataset to aid the evaluation for this task which will be made available upon request.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {267–277},
numpages = {11},
keywords = {health misinformation, information retrieval, keyword extraction, query generation},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3459637.3482166,
author = {F\"{a}rber, Michael and Leisinger, Ann-Kathrin},
title = {Recommending Datasets for Scientific Problem Descriptions},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482166},
doi = {10.1145/3459637.3482166},
abstract = {The steadily rising number of datasets is making it increasingly difficult for researchers and practitioners to be aware of all datasets, particularly of the most relevant datasets for a given research problem. To this end, dataset search engines have been proposed. However, they are based on user's keywords and, thus, have difficulty determining precisely fitting datasets for complex research problems. In this paper, we propose a system that recommends suitable datasets based on a given research problem description. The recommendation task is designed as a domain-specific text classification task. As shown in a comprehensive offline evaluation using various state-of-the-art models, as well as 88,000 paper abstracts and 265,000 citation contexts as research problem descriptions, we obtain an F1-score of 0.75. In an additional user study, we show that users in real-world settings are 88% satisfied in all test cases. We therefore see promising future directions for dataset recommendation.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3014–3018},
numpages = {5},
keywords = {datasets, machine learning, recommendation, text classification},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3460210.3493547,
author = {Cadorel, Lucie and Blanchi, Alicia and Tettamanzi, Andrea G. B.},
title = {Geospatial Knowledge in Housing Advertisements: Capturing and Extracting Spatial Information from Text},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493547},
doi = {10.1145/3460210.3493547},
abstract = {Information of the geographical and spatial type is found in numerous text documents and constitutes a very challenging target for extraction. Geoparsing applications have been developed to extract geographic terms. However, off-the-shelf Named Entity Recognition (NER) models are mainly designed for Toponym recognition and are very sensitive to language specificity. In this paper, we propose a workflow to first extract geographic and spatial entities based on a BiLSTM-CRF architecture with a concatenation of several text representations. We also propose a Relation Extraction module, particularly aimed at spatial relationships extraction, to build a structured Geospatial knowledge base. We demonstrate our pipeline by applying it to the case of French housing advertisements, which generally provide information about a property's location and neighbourhood. Our results show that the workflow tackles French language and the variability and irregularity of housing advertisements, generalizes Geoparsing to all geographic and spatial terms, and successfully retrieves most of the relationships between entities from the text.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {41–48},
numpages = {8},
keywords = {geographical knowledge, information extraction, named entity recognition, relation extraction, text mining},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@article{10.1145/3476415.3476428,
author = {Metzler, Donald and Tay, Yi and Bahri, Dara and Najork, Marc},
title = {Rethinking search: making domain experts out of dilettantes},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3476415.3476428},
doi = {10.1145/3476415.3476428},
abstract = {When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts - they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice.},
journal = {SIGIR Forum},
month = jul,
articleno = {13},
numpages = {27}
}

@inproceedings{10.1145/3544549.3585755,
author = {Kernan Freire, Samuel and Wang, Chaofan and Ruiz-Arenas, Santiago and Niforatos, Evangelos},
title = {Tacit Knowledge Elicitation for Shop-floor Workers with an Intelligent Assistant},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585755},
doi = {10.1145/3544549.3585755},
abstract = {Many industries face the challenge of capturing workers’ knowledge to share it, particularly tacit knowledge. The operation of complex systems such as a manufacturing line is knowledge-intensive. Considering this knowledge’s breadth and dynamic nature, existing knowledge-sharing solutions are inefficient and resource intensive. Conversational user interfaces are an efficient way to convey information that mimics how humans share knowledge; however, we know little about how to design them specifically for knowledge sharing, especially regarding tacit knowledge. In this work, we present an intelligent assistant that we have developed to support the elicitation of tacit knowledge from workers through systematic reflection. The system can interact with workers by voice or text and generate visualizations of shop floor data to support reflective prompts.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {266},
numpages = {7},
keywords = {chatbots, human-centered AI, industry 5.0, intelligent assistant, knowledge sharing, systematic reflection, tacit knowledge},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@article{10.1145/3582900.3582915,
author = {Piscopo, Alessandro and Inel, Oana and Vrijenhoek, Sanne and Millecamp, Martijn and Balog, Krisztian},
title = {Report on the 1st Workshop on Measuring the Quality of Explanations in Recommender Systems (QUARE 2022) at SIGIR 2022},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3582900.3582915},
doi = {10.1145/3582900.3582915},
abstract = {Explainable recommenders are systems that explain why an item is recommended, in addition to suggesting relevant items to the users of the system. Although explanations are known to be able to significantly affect a user's decision-making process, significant gaps remain concerning methodologies to evaluate them. This hinders cross-comparison between explainable recommendation approaches and is one of the issues hampering the widespread adoption of explanations in industry settings. The goal of QUARE '22 was to promote discussion upon future research and practice directions around evaluation methodologies for explanations in recommender systems. To that end, we brought together researchers and practitioners from academia and industry in a half-day event, co-located with SIGIR 2022. The workshop's program included two keynote talks, three sessions of technical paper presentations in the form of lightning talks followed by panel discussions, and a final plenary discussion session. Although the area of explanations for recommender systems is still in its early stages, QUARE saw the participation of researchers and practitioners from several fields, laying the groundwork for the creation of a community around this topic and indicating promising directions for future research and development.Date: 15 July, 2022.Website: https://sites.google.com/view/quare-2022/home.},
journal = {SIGIR Forum},
month = jan,
articleno = {11},
numpages = {16}
}

@article{10.1109/TASLP.2022.3202123,
author = {Hong, Ruixin and Zhang, Hongming and Yu, Xintong and Zhang, Changshui},
title = {Learning Event Extraction From a Few Guideline Examples},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3202123},
doi = {10.1109/TASLP.2022.3202123},
abstract = {Existing fully supervised event extraction models achieve advanced performance with large-scale labeled data. However, when new event types emerge and annotations are scarce, it is hard for the supervised models to master the new types with limited annotations. In contrast, humans can learn to understand new event types with only a few examples in the event extraction guideline. In this paper, we work on a challenging yet more realistic setting, the few-example event extraction. It requires models to learn event extraction with only a few sentences in guidelines as training data, so that we do not need to collect large-scale annotations each time when new event types emerge. As models tend to overfit when trained with only a few examples, we propose knowledge-guided data augmentation to generate valid and diverse sentences from the guideline examples. To help models better leverage the augmented data, we add a consistency regularization to guarantee consistent representations between the augmented sentences and the original ones. Experiments on the standard benchmark ACE-2005 indicate that our method can extract event triggers and arguments effectively with only a few guideline examples.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2955–2967},
numpages = {13}
}

@inproceedings{10.1145/3437963.3441781,
author = {Su, Lixin and Zhang, Ruqing and Guo, Jiafeng and Fan, Yixing and Chen, Jiangui and Lan, Yanyan and Cheng, Xueqi},
title = {Beyond Relevance: Trustworthy Answer Selection via Consensus Verification},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441781},
doi = {10.1145/3437963.3441781},
abstract = {Community Question Answering (CQA) sites such as Yahoo! Answers and Baidu Knows have emerged as rich knowledge resources for information seekers. However, answers posted to CQA sites often vary a lot in their qualities. User votes from the community may partially reflect the overall quality of the answer, but they are often missing. Hence, automatic selection of "good'' answers becomes a practical research problem that will help us manage the quality of accumulated knowledge. Without loss of generality, a good answer should deliver not only relevant but also trustworthy information that can help resolve the information needs of the posted question, but the latter has received less investigation in the past. In this paper, we propose a novel matching-verification framework for automatic answer selection. The matching component assesses the relevance of a candidate answer to a given question as conventional QA methods. The major enhancement is the verification component, which aims to leverage the wisdom of crowds, e.g., some big information repository, for trustworthiness measurement. Given a question, we take the top retrieved results from the information repository as the supporting evidences to distill the consensus representation. A major challenge is that there is no guarantee that one can always obtain reliable consensus from the wisdom of crowds for a question due to the noisy nature and the limitation of the existing search technology.Therefore, we decompose the trustworthiness measurement into two parts, i.e., a verification score which measures the consistency between a candidate answer and the consensus representation, and a confidence score which measures the reliability of the consensus itself. Empirical studies on three real-world CQA data collections, i.e. YahooQA, QuoraQA and AmazonQA, show that our approach can significantly outperform the state-of-the-art methods on the answer selection task.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {562–570},
numpages = {9},
keywords = {answer selection, answer verification, trustworthy},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@proceedings{10.1145/3625704,
title = {ICEMT '23: Proceedings of the 7th International Conference on Education and Multimedia Technology},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3459637.3481930,
author = {Sun, Fu and Li, Feng-Lin and Wang, Ruize and Chen, Qianglong and Cheng, Xingyi and Zhang, Ji},
title = {K-AID: Enhancing Pre-trained Language Models with Domain Knowledge for Question Answering},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481930},
doi = {10.1145/3459637.3481930},
abstract = {Knowledge enhanced pre-trained language models (K-PLMs) are shown to be effective for many public tasks in the literature, but few of them have been successfully applied in practice. To address this problem, we propose K-AID, a systematic approach that includes a low-cost knowledge acquisition process for acquiring domain knowledge, an effective knowledge infusion module for improving model performance, and a knowledge distillation component for reducing the model size and deploying K-PLMs on resource-restricted devices (e.g., CPU) for real-world application. Importantly, instead of capturing entity knowledge like the majority of existing K-PLMs, our approach captures relational knowledge, which contributes to better improving sentence-level text classification and text matching tasks that play a key role in question answering (QA). We conducted a set of experiments on five text classification tasks and three text matching tasks from three domains, namely E-commerce, Government, and Film&amp;TV, and performed online A/B tests in E-commerce. Experimental results show that our approach is able to achieve substantial improvement on sentence-level question answering tasks and bring beneficial business value in industrial settings.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4125–4134},
numpages = {10},
keywords = {domain knowledge, knowledge infusion, pre-trained language models, question answering},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@proceedings{10.1145/3579654,
title = {ACAI '22: Proceedings of the 2022 5th International Conference on Algorithms, Computing and Artificial Intelligence},
year = {2022},
isbn = {9781450398336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@inproceedings{10.1145/3531146.3533099,
author = {Wang, Angelina and Barocas, Solon and Laird, Kristen and Wallach, Hanna},
title = {Measuring Representational Harms in Image Captioning},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533099},
doi = {10.1145/3531146.3533099},
abstract = {Previous work has largely considered the fairness of image captioning systems through the underspecified lens of “bias.” In contrast, we present a set of techniques for measuring five types of representational harms, as well as the resulting measurements obtained for two of the most popular image captioning datasets using a state-of-the-art image captioning system. Our goal was not to audit this image captioning system, but rather to develop normatively grounded measurement techniques, in turn providing an opportunity to reflect on the many challenges involved. We propose multiple measurement techniques for each type of harm. We argue that by doing so, we are better able to capture the multi-faceted nature of each type of harm, in turn improving the (collective) validity of the resulting measurements. Throughout, we discuss the assumptions underlying our measurement approach and point out when they do not hold.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {324–335},
numpages = {12},
keywords = {fairness measurement, harm propagation, image captioning},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3534678.3539249,
author = {Sun, Mingchen and Zhou, Kaixiong and He, Xin and Wang, Ying and Wang, Xin},
title = {GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539249},
doi = {10.1145/3534678.3539249},
abstract = {Despite the promising representation learning of graph neural networks (GNNs), the supervised training of GNNs notoriously requires large amounts of labeled data from each application. An effective solution is to apply the transfer learning in graph: using easily accessible information to pre-train GNNs, and fine-tuning them to optimize the downstream task with only a few labels. Recently, many efforts have been paid to design the self-supervised pretext tasks, and encode the universal graph knowledge among the various applications. However, they rarely notice the inherent training objective gap between the pretext and downstream tasks. This significant gap often requires costly fine-tuning for adapting the pre-trained model to downstream problem, which prevents the efficient elicitation of pre-trained knowledge and then results in poor results. Even worse, the naive pre-training strategy usually deteriorates the downstream task, and damages the reliability of transfer learning in graph data. To bridge the task gap, we propose a novel transfer learning paradigm to generalize GNNs, namely graph pre-training and prompt tuning (GPPT). Specifically, we first adopt the masked edge prediction, the most simplest and popular pretext task, to pre-train GNNs. Based on the pre-trained model, we propose the graph prompting function to modify the standalone node into a token pair, and reformulate the downstream node classification looking the same as edge prediction. The token pair is consisted of candidate label class and node entity. Therefore, the pre-trained GNNs could be applied without tedious fine-tuning to evaluate the linking probability of token pair, and produce the node classification decision. The extensive experiments on eight benchmark datasets demonstrate the superiority of GPPT, delivering an average improvement of 4.29% in few-shot graph analysis and accelerating the model convergence up to 4.32X. The code is available in: https://github.com/MingChen-Sun/GPPT.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1717–1727},
numpages = {11},
keywords = {graph neural networks, pre-training, prompt tuning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3565387.3565396,
author = {Chen, Yang and Xu, Chunyan and Zhang, Tong and Li, Guangyu},
title = {Complementary Random Walk: A New Perspective on Graph Embedding},
year = {2022},
isbn = {9781450396004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565387.3565396},
doi = {10.1145/3565387.3565396},
abstract = {Random-walk based graph embedding algorithms like DeepWalk and Node2Vec are widely used to learn distinguishable representations of the nodes in a network. These methods treat different walks starting from every node as sentences in language to learn latent representations. However, nodes in a unique walking sequence often appear repeatedly. This situation results in the latent representations obtained by the aforementioned algorithms cannot capture the relationship between unconnected nodes, which have similar node features and graph topology structures. In this paper, we propose Complementary Random Walk (CRW) to solve this problem and embed the nodes in a network to obtain more robust low-dimensional vectors. By conducting a K-means clustering algorithm to cluster different features extracted from the graph, we can supply the original random walk with many other walking sequences, which consist of different unconnected nodes. And those nodes are sampled from the same cluster based on graph features, such as node degree, motif features, and so on. Our experiments achieve comparable or superior performance compared with other methods, validating the effectiveness of CRW.},
booktitle = {Proceedings of the 6th International Conference on Computer Science and Application Engineering},
articleno = {9},
numpages = {5},
keywords = {Clustering, Graph Representation Learning, Random Walk},
location = {Virtual Event, China},
series = {CSAE '22}
}

@inproceedings{10.1145/3584371.3613001,
author = {Wang, Zifeng and Xiao, Cao and Sun, Jimeng},
title = {SPOT: Sequential Predictive Modeling of Clinical Trial Outcome with Meta-Learning},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3613001},
doi = {10.1145/3584371.3613001},
abstract = {Clinical trials are essential to drug development but time-consuming, costly, and prone to failure. Accurate trial outcome prediction based on historical trial data promises better trial investment decisions and more trial success. Existing trial outcome prediction models were not designed to model the relations among similar trials, capture the progression of features and designs of similar trials, or address the skewness of trial data which causes inferior performance for less common trials.To fill the gap and provide accurate trial outcome prediction, we propose Sequential Predictive mOdeling of clinical Trial outcome (SPOT) that first identifies trial topics to cluster the multisourced trial data into relevant trial topics. It then generates trial embeddings and organizes them by topic and time to create clinical trial sequences. With the consideration of each trial sequence as a task, it uses a meta-learning strategy to achieve a point where the model can rapidly adapt to new tasks with minimal updates. In particular, the topic discovery module enables a deeper understanding of the underlying structure of the data, while sequential learning captures the evolution of trial designs and outcomes. This results in predictions that are not only more accurate but also more interpretable, taking into account the temporal patterns and unique characteristics of each trial topic. We demonstrate that SPOT wins over the prior methods by a significant margin on trial outcome benchmark data: with a 21.5% lift on phase I, an 8.9% lift on phase II, and a 5.5% lift on phase III trials in the metric of the area under precision-recall curve (PR-AUC). Code is available at https://github.com/RyanWangZf/PyTrial.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {53},
numpages = {11},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3584376.3584478,
author = {Ge, Junwei and Qin, Zhixiang and Fang, Yiqiu},
title = {A Document-level Event Extraction Method Based on Mogrifier LSTM},
year = {2023},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584376.3584478},
doi = {10.1145/3584376.3584478},
abstract = {Element dispersion is a difficulty for document-level event extraction. While classic LSTM lacks the capability to interact between input and context while collecting long sequence features, previous document-level event extraction utilizes the entire document as input, leaving the sequence features of the document devoid of deeper contextual information. This paper suggests a document-level event extraction strategy based on Mogrifier LSTM to solve this issue. We divide the text into multiple paragraphs and then input each paragraph individually into the Mogrifier LSTM. To increase the context modeling capability of lengthy sequence text, the upgraded LSTM will allow the input of the present moment and the output of the preceding moment to be computed several times initially. Then an attention mechanism is introduced to capture the internal correlation of each paragraph and integrate the contextual semantics of each paragraph. Finally, sequence annotation is used to extract dispersed event elements and match event types. According to the experimental results on Chinese financial dataset, the method in this paper can effectively solve the problems of loss of depth information and scattered theoretical elements of long sequence features of documents, and improve the effectiveness of document-level event extraction.},
booktitle = {Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {571–576},
numpages = {6},
location = {Dongguan, China},
series = {RICAI '22}
}

@inproceedings{10.1145/3511808.3557198,
author = {Balalau, Oana and Ebel, Simon and Galizzi, Th\'{e}o and Manolescu, Ioana and Massonnat, Quentin and Deiana, Antoine and Gautreau, Emilie and Krempf, Antoine and Pontillon, Thomas and Roux, G\'{e}rald and Yakin, Joanna},
title = {Statistical Claim Checking: StatCheck in Action},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557198},
doi = {10.1145/3511808.3557198},
abstract = {To strengthen public trust and counter disinformation, computational fact-checking, leveraging digital data sources, attracts interest from the journalists and the computer science community. A particular class of interesting data sources is statistics, that is, numerical data compiled mostly by governments, administrations, and international organizations. Statistics typically are multidimensional datasets, where multiple dimensions characterize one value, and the dimensions may be organized in a hierarchy.We developed StatCheck, a fact-checking system specialized in French. The technical novelty of StatCheck is twofold: (i) we focus on multidimensional, complex-structure statistics, which have received little attention so far, despite their practical importance; and (ii) novel statistical claim extraction modules for French, an area where few resources exist. We will demonstrate our system on large statistic datasets (hundreds of millions of facts), including the complete INSEE (French) and Eurostat (European Union) datasets.More information about StatCheckis available online at: https://team.inria.fr/cedar/projects/statcheck/.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {4798–4802},
numpages = {5},
keywords = {data warehouses, fact-checking, multidimensional data, natural language processing},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3583780.3615101,
author = {Zhang, Xiyuan and Chowdhury, Ranak Roy and Zhang, Jiayun and Hong, Dezhi and Gupta, Rajesh K. and Shang, Jingbo},
title = {Unleashing the Power of Shared Label Structures for Human Activity Recognition},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615101},
doi = {10.1145/3583780.3615101},
abstract = {Current human activity recognition (HAR) techniques regard activity labels as integer class IDs without explicitly modeling the semantics of class labels. We observe that different activity names often have shared structures. For example, "open door" and "open fridge" both have "open" as the action; "kicking soccer ball" and "playing tennis ball" both have "ball" as the object. Such shared structures in label names can be translated to the similarity in sensory data and modeling common structures would help uncover knowledge across different activities, especially for activities with limited samples. In this paper, we propose SHARE, a HAR framework that takes into account shared structures of label names for different activities. To exploit the shared structures, SHARE comprises an encoder for extracting features from input sensory time series and a decoder for generating label names as a token sequence. We also propose three label augmentation techniques to help the model more effectively capture semantic structures across activities, including a basic token-level augmentation, and two enhanced embedding-level and sequence-level augmentations utilizing the capabilities of pre-trained models. SHARE outperforms state-of-the-art HAR models in extensive experiments on seven HAR benchmark datasets. We also evaluate in few-shot learning and label imbalance settings and observe even more significant performance gap.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {3340–3350},
numpages = {11},
keywords = {human activity recognition, label name semantics, natural language processing, time series classification},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3603607.3613481,
author = {Atzenbeck, Claus and Brooker, Sam and Ro\ss{}ner, Daniel},
title = {Storytelling Machines},
year = {2023},
isbn = {9798400702396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603607.3613481},
doi = {10.1145/3603607.3613481},
abstract = {We are entering a period of unprecedented collaboration between authors and computers, where artificial intelligence in particular seems likely to act increasingly in a co-authoring capacity. Automated or procedural storytelling represents one exciting avenue of research. By entering prompts and parameters into an AI text generator like ChatGPT, authors could leverage an enormous textual corpus to generate a “new” work that appears to have been authored by a human. This paper proposes an alternative platform, one more reflective of the collaborative and organic creative process. Approached as a tool for augmentation, Mother showcases the potential for spatial hypertext to work alongside the author.},
booktitle = {Proceedings of the 6th Workshop on Human Factors in Hypertext},
articleno = {4},
numpages = {9},
keywords = {Mother, education, hypertext, linguistics, recommender system, spatial hypertext, storytelling, tropes},
location = {Rome, Italy},
series = {HUMAN '23}
}

@article{10.1145/3543826,
author = {Demir, Seniz},
title = {Turkish Data-to-Text Generation Using Sequence-to-Sequence Neural Networks},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3543826},
doi = {10.1145/3543826},
abstract = {End-to-end data-driven approaches lead to rapid development of language generation and dialogue systems. Despite the need for large amounts of well-organized data, these approaches jointly learn multiple components of the traditional generation pipeline without requiring costly human intervention. End-to-end approaches also enable the use of loosely aligned parallel datasets in system development by relaxing the degree of semantic correspondences between training data representations and text spans. However, their potential in Turkish language generation has not yet been fully exploited. In this work, we apply sequence-to-sequence (Seq2Seq) neural models to Turkish data-to-text generation where the input data given in the form of a meaning representation is verbalized. We explore encoder-decoder architectures with attention mechanism in unidirectional, bidirectional, and stacked recurrent neural network (RNN) models. Our models generate one-sentence biographies and dining venue descriptions using a crowdsourced dataset where all field value pairs that appear in meaning representations are fully captured in reference sentences. To support this work, we also explore the performances of our models on a more challenging dataset, where the content of a meaning representation is too large to fit into a single sentence, and hence content selection and surface realization need to be learned jointly. This dataset is retrieved by coupling introductory sentences of person-related Turkish Wikipedia articles with their contained infobox tables. Our empirical experiments on both datasets demonstrate that Seq2Seq models are capable of generating coherent and fluent biographies and venue descriptions from field value pairs. We argue that the wealth of knowledge residing in our datasets and the insights obtained from this study hold the potential to give rise to the development of new end-to-end generation approaches for Turkish and other morphologically rich languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {37},
numpages = {27},
keywords = {Data-to-text generation, sequence-to-sequence model, Turkish, Wikipedia}
}

@inproceedings{10.1145/3485447.3511949,
author = {Fan, Lu and Li, Qimai and Liu, Bo and Wu, Xiao-Ming and Zhang, Xiaotong and Lv, Fuyu and Lin, Guli and Li, Sen and Jin, Taiwei and Yang, Keping},
title = {Modeling User Behavior with Graph Convolution for Personalized Product Search},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511949},
doi = {10.1145/3485447.3511949},
abstract = {User preference modeling is a vital yet challenging problem in personalized product search. In recent years, latent space based methods have achieved state-of-the-art performance by jointly learning semantic representations of products, users, and text tokens. However, existing methods are limited in their ability to model user preferences. They typically represent users by the products they visited in a short span of time using attentive models and lack the ability to exploit relational information such as user-product interactions or item co-occurrence relations. In this work, we propose to address the limitations of prior arts by exploring local and global user behavior patterns on a user successive behavior graph, which is constructed by utilizing short-term actions of all users. To capture implicit user preference signals and collaborative patterns, we use an efficient jumping graph convolution to explore high-order relations to enrich product representations for user preference modeling. Our approach can be seamlessly integrated with existing latent space based methods and be potentially applied in any product retrieval method that uses purchase history to model user preferences. Extensive experiments on eight Amazon benchmarks demonstrate the effectiveness and potential of our approach. The source code is available at https://github.com/floatSDSDS/SBG .},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {203–212},
numpages = {10},
keywords = {Graph Convolution, Personalized Product Search, User Preference Modeling},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3586183.3606715,
author = {Zaidi, Ali and Turbeville, Kelsey and Ivan\v{c}i\'{c}, Kristijan and Moss, Jason and Gutierrez Villalobos, Jenny and Sagar, Aravind and Li, Huiying and Mehra, Charu and Li, Sixuan and Hutchins, Scott and Kumar, Ranjitha},
title = {Learning Custom Experience Ontologies via Embedding-based Feedback Loops},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606715},
doi = {10.1145/3586183.3606715},
abstract = {Organizations increasingly rely on behavioral analytics tools like Google Analytics to monitor their digital experiences. Making sense of the data these tools capture, however, requires manual event tagging and filtering — often a tedious process. Prior approaches have trained machine learning models to automatically tag interaction data, but draw from fixed digital experience vocabularies which cannot be easily augmented or customized. This paper introduces a novel machine learning interaction pattern that generates customized tag predictions for organizations. The approach employs a general user experience word embedding to bootstrap an initial set of predictions, which can then be refined and customized by users to adapt the underlying vector space, iteratively improving the quality of future predictions. The paper presents a needfinding study that grounds the design choices of the system, and describes a real-world deployment as part of UserTesting.com that demonstrates the efficacy of the approach.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {111},
numpages = {13},
keywords = {Sankey diagrams, UX research, clickstream analytics, sequence alignment, usability testing},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3477495.3531678,
author = {Dong, Yuyang and Oyamada, Masafumi},
title = {Table Enrichment System for Machine Learning},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531678},
doi = {10.1145/3477495.3531678},
abstract = {Data scientists are constantly facing the problem of how to improve prediction accuracy with insufficient tabular data. We propose a table enrichment system that enriches a query table by adding external attributes (columns) from data lakes and improves the accuracy of machine learning predictive models. Our system has four stages, join row search, task-related table selection, row and column alignment, and feature selection and evaluation, to efficiently create an enriched table for a given query table and a specified machine learning task. We demonstrate our system with a web UI to show the use cases of table enrichment.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3267–3271},
numpages = {5},
keywords = {machine learning, table augmentation, table discovery, table enrichment},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3511808.3557313,
author = {Xu, Tianyu and Hua, Wen and Qu, Jianfeng and Li, Zhixu and Xu, Jiajie and Liu, An and Zhao, Lei},
title = {Evidence-aware Document-level Relation Extraction},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557313},
doi = {10.1145/3511808.3557313},
abstract = {Document-level Relation Extraction (RE) is a promising task aiming at identifying relations of multiple entity pairs in a document. However, in most cases, a relational fact can be expressed enough via a small subset of sentences from the document, namely evidence sentence. Moreover, there often exist strong semantic correlations between evidence sentences that collaborate together to describe a specific relation. To address these challenges, we propose a novel evidence-aware model for document-level RE. Particularly, we formulate evidence sentence selection as a sequential decision problem through a crafted reinforcement learning mechanism. Considering the explosive search space of our agent, an efficient path searching strategy is executed on the converted document graph to heuristically obtain hopeful sentences and feed them to reinforcement learning. Finally, each entity pair owns a customized-filtered document for further inferring the relation between them. We conduct various experiments on two document-level RE benchmarks and achieve a remarkable improvement over previous competitive baselines, verifying the effectiveness of our method.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {2311–2320},
numpages = {10},
keywords = {document-level relation extraction, evidence extraction, reinforcement learning},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3411764.3445368,
author = {Pinhanez, Claudio Santos and Candello, Heloisa and Cavalin, Paulo and Pichiliani, Mauro Carlos and Appel, Ana Paula and Alves Ribeiro, Victor Henrique and Nogima, Julio and de Bayser, Maira and Guerra, Melina and Ferreira, Henrique and Malfatti, Gabriel},
title = {Integrating Machine Learning Data with Symbolic Knowledge from Collaboration Practices of Curators to Improve Conversational Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445368},
doi = {10.1145/3411764.3445368},
abstract = {This paper describes how machine learning training data and symbolic knowledge from curators of conversational systems can be used together to improve the accuracy of those systems and to enable better curatorial tools. This is done in the context of a real-world practice of curators of conversational systems who often embed taxonomically-structured meta-knowledge into their documentation. The paper provides evidence that the practice is quite common among curators, that is used as part of their collaborative practices, and that the embedded knowledge can be mined by algorithms. Further, this meta-knowledge can be integrated, using neuro-symbolic algorithms, to the machine learning-based conversational system, to improve its run-time accuracy and to enable tools to support curatorial tasks. Those results point towards new ways of designing development tools which explore an integrated use of code and documentation by machines.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {104},
numpages = {13},
keywords = {Conversational Systems, Curatorial Practices., Documentation, Neuro-symbolic Systems},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3580305.3599375,
author = {Liu, Jiayu and Huang, Zhenya and Ma, Zhiyuan and Liu, Qi and Chen, Enhong and Su, Tianhuang and Liu, Haifeng},
title = {Guiding Mathematical Reasoning via Mastering Commonsense Formula Knowledge},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599375},
doi = {10.1145/3580305.3599375},
abstract = {Math formulas (e.g., "distance = speed X time'') serve as one of the fundamental commonsense knowledge in human cognition, where humans naturally acquire and manipulate them in logical thinking for mathematical reasoning problems. However, existing reasoning models mainly focus on learning heuristic linguistics or patterns to generate answers, but do not pay enough attention on learning with such formula knowledge. Thus, they are not transparent (thus uninterpretable) in terms of understanding and grasping basic mathematical logic. In this paper, to promote a step forward in the domain, we first construct two datasets (Math23K-F and MAWPS-F) with precise annotations of formula usage in each reasoning step for math word problems. Especially, our datasets are refined on the benchmark datasets, and thus ensure the generality and comparability for relevant research. Then, we propose a novel Formula-mastered Solver (FOMAS) with the guidance of mastering formula knowledge to solve the problems. Specifically, we establish FOMAS with two systems drawing insight from the dual process theory, including a Knowledge System and a Reasoning System, to learn and apply formula knowledge, respectively. The Knowledge System accumulates the math formulas, where we propose a novel pretraining manner to mimic how humans grasp the mathematical logic behind them. Then, in the Reasoning System, we develop elaborate formula-guided symbol prediction and goal generation methods that retrieve the necessary formula knowledge from Knowledge System to improve both reasoning accuracy and interpretability. It organically simulates how humans conduct complex reasoning under the explicit instruction of math formulas. Experimental results prove that FOMAS has a stronger reasoning ability and achieves a more interpretable reasoning process, which verifies the necessity of introducing formula knowledge transparently.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1477–1488},
numpages = {12},
keywords = {knowledge representation and reasoning, math word problem},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1109/TASLP.2023.3331149,
author = {Zhu, Tiantian and Qin, Yang and Feng, Ming and Chen, Qingcai and Hu, Baotian and Xiang, Yang},
title = {BioPRO: Context-Infused Prompt Learning for Biomedical Entity Linking},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3331149},
doi = {10.1109/TASLP.2023.3331149},
abstract = {Recent research tends to address the biomedical entity linking problem in a unified framework solely based on surface form matching between mentions and entities. Specifically, these methods focus on addressing the &lt;italic&gt;variety&lt;/italic&gt; challenge of the heterogeneous naming of biomedical concepts. Yet, the &lt;italic&gt;ambiguity&lt;/italic&gt; challenge that the same word under different contexts can be used to refer to distinct concepts is usually ignored. To address this challenge, we propose BioPRO, a two-stage entity linking algorithm to enhance the biomedical entity representations based on context-infused prompt learning. The first stage includes a coarse-grained retrieval from a representation space defined by a bi-encoder that independently embeds the mention and entity's surface forms. Unlike previous one-model-fits-all systems, each candidate is then re-ranked with a fine-grained encoder based on prompt-tuning that sufficiently stimulates knowledge in contextual information of mentions and entities. Furthermore, the trained fine-grained encoder can be utilized to generate deep representations of bio-entities and boost candidate retrieval in the first stage. Extensive experiments show that our model achieves promising performance improvements compared with several state-of-the-art (SOTA) techniques on 4 biomedical corpora. We also observe by cases that the proposed context-infused prompt-tuning strategy is effective in solving both the &lt;italic&gt;variety&lt;/italic&gt; and &lt;italic&gt;ambiguity&lt;/italic&gt; challenges in the linking task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {374–385},
numpages = {12}
}

@article{10.1109/TCBB.2022.3157630,
author = {Chai, Zhaoying and Jin, Han and Shi, Shenghui and Zhan, Siyan and Zhuo, Lin and Yang, Yu and Lian, Qi},
title = {Noise Reduction Learning Based on XLNet-CRF for Biomedical Named Entity Recognition},
year = {2022},
issue_date = {Jan.-Feb. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3157630},
doi = {10.1109/TCBB.2022.3157630},
abstract = {In recent years, Biomedical Named Entity Recognition (BioNER) systems have mainly been based on deep neural networks, which are used to extract information from the rapidly expanding biomedical literature. Long-distance context autoencoding language models based on transformers have recently been employed for BioNER with great success. However, noise interference exists in the process of pre-training and fine-tuning, and there is no effective decoder for label dependency. Current models have many aspects in need of improvement for better performance. We propose two kinds of noise reduction models, Shared Labels and Dynamic Splicing, based on XLNet encoding which is a permutation language pre-training model and decoding by Conditional Random Field (CRF). By testing 15 biomedical named entity recognition datasets, the two models improved the average F1-score by 1.504 and 1.48, respectively, and state-of-the-art performance was achieved on 7 of them. Further analysis proves the effectiveness of the two models and the improvement of the recognition effect of CRF, and suggests the applicable scope of the models according to different data characteristics.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {595–605},
numpages = {11}
}

@inproceedings{10.1145/3534678.3539215,
author = {Chen, Changyu and Wang, Xiting and Yi, Xiaoyuan and Wu, Fangzhao and Xie, Xing and Yan, Rui},
title = {Personalized Chit-Chat Generation for Recommendation Using External Chat Corpora},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539215},
doi = {10.1145/3534678.3539215},
abstract = {Chit-chat has been shown effective in engaging users in human-computer interaction. We find with a user study that generating appropriate chit-chat for news articles can help expand user interest and increase the probability that a user reads a recommended news article. Based on this observation, we propose a method to generate personalized chit-chat for news recommendation. Different from existing methods for personalized text generation, our method only requires an external chat corpus obtained from an online forum, which can be disconnected from the recommendation dataset from both the user and item (news) perspectives. This is achieved by designing a weak supervision method for estimating users' personalized interest in a chit-chat post by transferring knowledge learned by a news recommendation model. Based on the method for estimating user interest, a reinforcement learning framework is proposed to generate personalized chit-chat. Extensive experiments, including the automatic offline evaluation and user studies, demonstrate the effectiveness of our method.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2721–2731},
numpages = {11},
keywords = {chit-chat, news recommendation, personalized text generation, reinforcement learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3523286.3524517,
author = {Zhang, Sizhou and Chen, Zhihong and Liu, Dejian and Lv, Qing},
title = {Building Structured Patient Follow-up Records from Chinese Medical Records via Deep Learning},
year = {2022},
isbn = {9781450395755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523286.3524517},
doi = {10.1145/3523286.3524517},
abstract = {Employing deep learning (DL) method to process and analyze Chinese medical records to build patient follow-up records (PFRs) has been a very valuable task. In recent years, the identification and classification of clinical terms in electronic medical records has received increased attention. However, electronic medical records are difficult to access because of their exceedingly high privacy, so it has become more feasible to extract information from paper medical records. This study proposed a DL approach that extract text information from the pre-processed images of Chinese medical records by optical character recognition (OCR) model base on CRNN first, and then identify the clinical entities using named entity recognition (NER) model based on BERT-CRF. The experimental results of this study demonstrate that the proposed method achieves precision over 75%, which is more than 90% for some specific entities. In addition, the proposed method can be extended as a universal approach to other diseases that require the establishment of the structured patient follow-up records (PFRs).},
booktitle = {Proceedings of the 2022 2nd International Conference on Bioinformatics and Intelligent Computing},
pages = {65–71},
numpages = {7},
keywords = {Named Entity Recognition, Optical Character Recognition, Patient Follow-up Records},
location = {Harbin, China},
series = {BIC '22}
}

@inproceedings{10.1145/3594536.3595168,
author = {Habba, Eliya and Keydar, Renana and Bareket, Dan and Stanovsky, Gabriel},
title = {The Perfect Victim: Computational Analysis of Judicial Attitudes towards Victims of Sexual Violence},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594536.3595168},
doi = {10.1145/3594536.3595168},
abstract = {We develop computational models to analyze court statements in order to assess judicial attitudes toward victims of sexual violence in the Israeli court system. The study examines the resonance of "rape myths" in the criminal justice system's response to sex crimes, in particular in judicial assessment of victim's credibility. We begin by formulating an ontology for evaluating judicial attitudes toward victim's credibility, with eight ordinal labels and binary categorizations. Second, we curate a manually annotated dataset for judicial assessments of victim's credibility in the Hebrew language, as well as a model that can extract credibility labels from court cases. The dataset consists of 855 verdict decision documents in sexual assault cases from 1990-2021, annotated with the help of legal experts and trained law students. The model uses a combined approach of syntactic and latent structures to find sentences that convey the judge's attitude towards the victim and classify them according to the credibility label set. Our ontology, data, and models will be made available upon request, in the hope they spur future progress in this judicial important task.},
booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
pages = {111–120},
numpages = {10},
keywords = {Judicial decision making, Rape myths, Sexual violence, Witness credibility},
location = {Braga, Portugal},
series = {ICAIL '23}
}

@inproceedings{10.1145/3459637.3482401,
author = {Shi, Shaoyun and Ma, Weizhi and Wang, Zhen and Zhang, Min and Fang, Kun and Xu, Jingfang and Liu, Yiqun and Ma, Shaoping},
title = {WG4Rec: Modeling Textual Content with Word Graph for News Recommendation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482401},
doi = {10.1145/3459637.3482401},
abstract = {News recommendation plays an indispensable role in acquiring daily news for users. Previous studies make great efforts to model high-order feature interactions between users and items, where various neural models are applied (e.g., RNN, GNN). However, we find that seldom efforts are made to get better representations for news. Most previous methods simply adopt pre-trained word embeddings to represent news and also suffer from cold-start users.  In this work, we propose a new textual content representation method by building a word graph for recommendation, which is named WG4Rec. Three types of word associations are adopted in WG4Rec for content representation and user preference modeling, namely: 1)semantically-similar according to pre-trained word vectors, 2)co-occurrence in documents, and 3)co-click by users across documents. As extra information can be unified by adding nodes/edges to the word graph easily, WG4Rec is flexible to make use of cross-platform and cross-domain context for recommendation to alleviate the cold-start issue. To the best of our knowledge, it is the first attempt that using these relationships for news recommendation to better model textual content and adopt cross-platform information. Experimental results on two large-scale real-world datasets show that WG4Rec significantly outperforms state-of-the-art algorithms, especially for cold users in the online environment. Besides, WG4Rec achieves better performances when cross-platform information is utilized.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1651–1660},
numpages = {10},
keywords = {neural networks, news recommendation, word graph},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3477495.3531746,
author = {Chen, Xiang and Li, Lei and Zhang, Ningyu and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
title = {Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531746},
doi = {10.1145/3477495.3531746},
abstract = {Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In this way, our model not only infers relation through knowledge stored in the weights during training but also assists decision-making by unwinding and querying examples in the open-book datastore. Extensive experiments on benchmark datasets show that our method can achieve state-of-the-art in both standard supervised and few-shot settings},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2443–2448},
numpages = {6},
keywords = {few-shot learning, prompt tuning, relation extraction},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1109/TCBB.2023.3292883,
author = {Zhu, Xinyu and Lu, Weiming},
title = {Multi-Label Classification With Dual Tail-Node Augmentation for Drug Repositioning},
year = {2023},
issue_date = {Sept.-Oct. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2023.3292883},
doi = {10.1109/TCBB.2023.3292883},
abstract = {Due to the lengthy and costly process of new drug discovery, increasing attention has been paid to drug repositioning, i.e., identifying new drug-disease associations. Current machine learning methods for drug repositioning mainly leverage matrix factorization or graph neural networks, and have achieved impressive performance. However, they often suffer from insufficient training labels of inter-domain associations, while ignore the intra-domain associations. Moreover, they often neglect the importance of tail nodes that have few known associations, which limits their effectiveness in drug repositioning. In this paper, we propose a novel multi-label classification model with dual &lt;bold&gt;T&lt;/bold&gt;ail-&lt;bold&gt;N&lt;/bold&gt;ode Augmentation for &lt;bold&gt;D&lt;/bold&gt;rug &lt;bold&gt;R&lt;/bold&gt;epositioning (TNA-DR). We incorporate disease-disease similarity and drug-drug similarity information into &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="lu-ieq1-3292883.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-nearest neighbor (&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="lu-ieq2-3292883.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;NN) augmentation module and contrastive augmentation module, respectively, which effectively complements the weak supervision of drug-disease associations. Furthermore, before employing the two augmentation modules, we filter the nodes by their degrees, so that the two modules are only applied to tail nodes. We conduct 10-fold cross validation experiments on four different real-world datasets, and our model achieves the state-of-the-art performance on all the four datasets. We also demonstrate our model's capability of identifying drug candidates for new diseases and discovering potential new links between existing drugs and diseases.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {3068–3079},
numpages = {12}
}

@article{10.14778/3447689.3447703,
author = {Tata, Sandeep and Potti, Navneet and Wendt, James B. and Costa, Lauro Beltr\~{a}o and Najork, Marc and Gunel, Beliz},
title = {Glean: structured extractions from templatic documents},
year = {2021},
issue_date = {February 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3447689.3447703},
doi = {10.14778/3447689.3447703},
abstract = {Extracting structured information from templatic documents is an important problem with the potential to automate many real-world business workflows such as payment, procurement, and payroll. The core challenge is that such documents can be laid out in virtually infinitely different ways. A good solution to this problem is one that generalizes well not only to known templates such as invoices from a known vendor, but also to unseen ones.We developed a system called Glean to tackle this problem. Given a target schema for a document type and some labeled documents of that type, Glean uses machine learning to automatically extract structured information from other documents of that type. In this paper, we describe the overall architecture of Glean, and discuss three key data management challenges : 1) managing the quality of ground truth data, 2) generating training data for the machine learning model using labeled documents, and 3) building tools that help a developer rapidly build and improve a model for a given document type. Through empirical studies on a real-world dataset, we show that these data management techniques allow us to train a model that is over 5 F1 points better than the exact same model architecture without the techniques we describe. We argue that for such information-extraction problems, designing abstractions that carefully manage the training data is at least as important as choosing a good model architecture.},
journal = {Proc. VLDB Endow.},
month = feb,
pages = {997–1005},
numpages = {9}
}

@inproceedings{10.1145/3397481.3450655,
author = {Sovrano, Francesco and Vitali, Fabio},
title = {From Philosophy to Interfaces: an Explanatory Method and a Tool Inspired by Achinstein’s Theory of Explanation},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450655},
doi = {10.1145/3397481.3450655},
abstract = {We propose a new method for explanations in Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. Among the mainstream philosophical theories of explanation we identified one that in our view is more easily applicable as a practical model for user-centric tools: Achinstein’s Theory of Explanation. With this work we aim to prove that the theory proposed by Achinstein can be actually adapted for being implemented into a concrete software application, as an interactive process answering questions. To this end we found a way to handle the generic (archetypal) questions that implicitly characterise an explanatory processes as preliminary overviews rather than as answers to explicit questions, as commonly understood. To show the expressive power of this approach we designed and implemented a pipeline of AI algorithms for the generation of interactive explanations under the form of overviews, focusing on this aspect of explanations rather than on existing interfaces and presentation logic layers for question answering. Accordingly, through the identification of a minimal set of archetypal questions it is possible to create a generator of explanatory overviews that is generic enough to significantly ease the acquisition of knowledge by humans, regardless of the specificities of the users outside of a minimum set of very broad requirements (e.g. people able to read and understand English and capable of performing basic common-sense reasoning). We tested our hypothesis on a well-known XAI-powered credit approval system by IBM, comparing CEM, a static explanatory tool for post-hoc explanations, with an extension we developed adding interactive explanations based on our model. The results of the user study, involving more than 100 participants, showed that our proposed solution produced a statistically relevant improvement on effectiveness (U=931.0, p=0.036) over the baseline, thus giving evidence in favour of our theory.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {81–91},
numpages = {11},
keywords = {Education and learning-related technologies, ExplanatorY Artificial Intelligence (YAI), Methods for explanations},
location = {College Station, TX, USA},
series = {IUI '21}
}

@inproceedings{10.1145/3477495.3532080,
author = {Lin, Li and Cao, Yixin and Huang, Lifu and Li, Shu'Ang and Hu, Xuming and Wen, Lijie and Wang, Jianmin},
title = {What Makes the Story Forward? Inferring Commonsense Explanations as Prompts for Future Event Generation},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532080},
doi = {10.1145/3477495.3532080},
abstract = {Prediction over event sequences is critical for many real-world applications in Information Retrieval and Natural Language Processing. Future Event Generation (FEG) is a challenging task in event sequence prediction because it requires not only fluent text generation but also commonsense reasoning to maintain the logical coherence of the entire event story. In this paper, we propose a novel explainable FEG framework, Coep. It highlights and integrates two types of event knowledge, sequential knowledge of direct event-event relations and inferential knowledge that reflects the intermediate character psychology between events, such as intents, causes, reactions, which intrinsically pushes the story forward. To alleviate the knowledge forgetting issue, we design two modules, IM and GM, for each type of knowledge, which are combined via prompt tuning. First, IM focuses on understanding inferential knowledge to generate commonsense explanations and provide a soft prompt vector for GM. We also design a contrastive discriminator for better generalization ability. Second, GM generates future events by modeling direct sequential knowledge with the guidance of IM. Automatic and human evaluation demonstrate that our approach can generate more coherent, specific, and logical future events.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1098–1109},
numpages = {12},
keywords = {commonsense reasoning, contrastive training, textual event generation},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1109/TASLP.2021.3074014,
author = {Xu, Kun and Wu, Han and Song, Linfeng and Zhang, Haisong and Song, Linqi and Yu, Dong},
title = {Conversational Semantic Role Labeling},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3074014},
doi = {10.1109/TASLP.2021.3074014},
abstract = {Semantic role labeling (SRL) aims to extract the arguments for each predicate in an input sentence. Traditional SRL can fail to analyze dialogues because it only works on every single sentence, while ellipsis and anaphora frequently occur in dialogues. To address this problem, we propose the conversational SRL task, where an argument can be the dialogue participants, a phrase in the dialogue history or the current sentence. As the existing SRL datasets are in the sentence level, we manually annotate semantic roles for 3000 chit-chat dialogues (27198 sentences) to boost the research in this direction. Experiments show that while traditional SRL systems (even with the help of coreference resolution or rewriting) perform poorly for analyzing dialogues, modeling dialogue histories and participants greatly helps the performance, indicating that adapting SRL to conversations is very promising for universal dialogue understanding. Our initial study by applying CSRL to two mainstream conversational tasks, dialogue response generation and dialogue context rewriting, also confirms the usefulness of CSRL.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {2465–2475},
numpages = {11}
}

@inproceedings{10.1145/3611643.3616258,
author = {Cao, Jialun and Lu, Yaojie and Wen, Ming and Cheung, Shing-Chi},
title = {Testing Coreference Resolution Systems without Labeled Test Sets},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616258},
doi = {10.1145/3611643.3616258},
abstract = {Coreference resolution (CR) is a task to resolve different expressions  
(e.g., named entities, pronouns) that refer to the same real-world en-  
tity/event. It is a core natural language processing (NLP) component  
that underlies and empowers major downstream NLP applications  
such as machine translation, chatbots, and question-answering. De-  
spite its broad impact, the problem of testing CR systems has rarely  
been studied. A major difficulty is the shortage of a labeled dataset  
for testing. While it is possible to feed arbitrary sentences as test  
inputs to a CR system, a test oracle that captures their expected  
test outputs (coreference relations) is hard to define automatically.  
To address the challenge, we propose Crest, an automated testing  
methodology for CR systems. Crest uses constituency and depen-  
dency relations to construct pairs of test inputs subject to the same  
coreference. These relations can be leveraged to define the meta-  
morphic relation for metamorphic testing. We compare Crest with  
five state-of-the-art test generation baselines on two popular CR  
systems, and apply them to generate tests from 1,000 sentences  
randomly sampled from CoNLL-2012, a popular dataset for corefer-  
ence resolution. Experimental results show that Crest outperforms  
baselines significantly. The issues reported by Crest are all true  
positives (i.e., 100% precision), compared with 63% to 75% achieved  
by the baselines.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {107–119},
numpages = {13},
keywords = {Coreference resolution testing, Metamorphic testing, SE4AI},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1145/3519265,
author = {Sovrano, Francesco and Vitali, Fabio},
title = {Generating User-Centred Explanations via Illocutionary Question Answering: From Philosophy to Interfaces},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3519265},
doi = {10.1145/3519265},
abstract = {We propose a new method for generating explanations with Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. With this work, we aim to prove that the philosophical theory of explanations presented by Achinstein can be actually adapted for being implemented into a concrete software application, as an interactive and illocutionary process of answering questions. Specifically, our contribution is an approach to frame illocution in a computer-friendly way, to achieve user-centrality with statistical question answering. Indeed, we frame the illocution of an explanatory process as that mechanism responsible for anticipating the needs of the explainee in the form of unposed, implicit, archetypal questions, hence improving the user-centrality of the underlying explanatory process. Therefore, we hypothesise that if an explanatory process is an illocutionary act of providing content-giving answers to questions, and illocution is as we defined it, the more explicit and implicit questions can be answered by an explanatory tool, the more usable (as per ISO 9241-210) its explanations. We tested our hypothesis with a user-study involving more than 60 participants, on two XAI-based systems, one for credit approval (finance) and one for heart disease prediction (healthcare). The results showed that increasing the illocutionary power of an explanatory tool can produce statistically significant improvements (hence with a P value lower than .05) on effectiveness. This, combined with a visible alignment between the increments in effectiveness and satisfaction, suggests that our understanding of illocution can be correct, giving evidence in favour of our theory.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = nov,
articleno = {26},
numpages = {32},
keywords = {Methods for explanations, education and learning-related technologies, explanatory artificial intelligence (YAI)}
}

@inproceedings{10.1145/3539618.3591880,
author = {Zhang, Haochen and Korikov, Anton and Farinneya, Parsa and Abdollah Pour, Mohammad Mahdi and Bharadwaj, Manasa and Pesaranghader, Ali and Huang, Xi Yu and Lok, Yi Xin and Wang, Zhaoqi and Jones, Nathan and Sanner, Scott},
title = {Recipe-MPR: A Test Collection for Evaluating Multi-aspect Preference-based Natural Language Retrieval},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591880},
doi = {10.1145/3539618.3591880},
abstract = {The rise of interactive recommendation assistants has led to a novel domain of natural language (NL) recommendation that would benefit from improved multi-aspect reasoning to retrieve relevant items based on NL statements of preference. Such preference statements often involve multiple aspects, e.g., "I would like meat lasagna but I'm watching my weight". Unfortunately, progress in this domain is slowed by the lack of annotated data. To address this gap, we curate a novel dataset which captures logical reasoning over multi-aspect, NL preference-based queries and a set of multiple-choice, multi-aspect item descriptions. We focus on the recipe domain in which multi-aspect preferences are often encountered due to the complexity of the human diet. The goal of publishing our dataset is to provide a benchmark for joint progress in three key areas: 1) structured, multi-aspect NL reasoning with a variety of properties (e.g., level of specificity, presence of negation, and the need for commonsense, analogical, and/or temporal inference), 2) the ability of recommender systems to respond to NL preference utterances, and 3) explainable NL recommendation facilitated by aspect extraction and reasoning. We perform experiments using a variety of methods (sparse and dense retrieval, zero- and few-shot reasoning with large language models) in two settings: a monolithic setting which uses the full query and an aspect-based setting which isolates individual query aspects and aggregates the results. GPT-3 results in much stronger performance than other methods with 73% zero-shot accuracy and 83% few-shot accuracy in the monolithic setting. Aspect-based GPT-3, which facilitates structured explanations, also shows promise with 68% zero-shot accuracy. These results establish baselines for future research into explainable recommendations via multi-aspect preference-based NL reasoning.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2744–2753},
numpages = {10},
keywords = {benchmark dataset, multi-aspect preference retrieval, natural language reasoning, recipe retrieval},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@proceedings{10.1145/3611643,
title = {ESEC/FSE 2023: Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome all delegates to ESEC/FSE 2023, the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. ESEC/FSE brings together experts from academia and industry to exchange the latest research results and trends as well as their practical application in all areas of software engineering.},
location = {San Francisco, CA, USA}
}

@inproceedings{10.1145/3510003.3510159,
author = {Wei, Moshi and Harzevili, Nima Shiri and Huang, Yuchao and Wang, Junjie and Wang, Song},
title = {CLEAR: &lt;u&gt;c&lt;/u&gt;ontrastive &lt;u&gt;le&lt;/u&gt;arning for &lt;u&gt;A&lt;/u&gt;PI &lt;u&gt;r&lt;/u&gt;ecommendation},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510159},
doi = {10.1145/3510003.3510159},
abstract = {Automatic API recommendation has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information-retrieval-based (IR-based) and neural-based methods. Although these approaches were reported having remarkable performance, our observation shows that existing approaches can fail due to the following two reasons: 1) most IR-based approaches treat task queries as bag-of-words and use word embedding to represent queries, which cannot capture the sequential semantic information. 2) both the IR-based and the neural-based approaches are weak at distinguishing the semantic difference among lexically similar queries.In this paper, we propose CLEAR, which leverages BERT sentence embedding and contrastive learning to tackle the above two issues. Specifically, CLEAR embeds the whole sentence of queries and Stack Overflow (SO) posts with a BERT-based model rather than the bag-of-word-based word embedding model, which can preserve the semantic-related sequential information. In addition, CLEAR uses contrastive learning to train the BERT-based embedding model for learning precise semantic representation of programming terminologies regardless of their lexical information. CLEAR also builds a BERT-based re-ranking model to optimize its recommendation results. Given a query, CLEAR first selects a set of candidate SO posts via the BERT sentence embedding-based similarity to reduce search space. CLEAR further leverages a BERT-based re-ranking model to rank candidate SO posts and recommends the APIs from the ranked top SO posts for the query.Our experiment results on three different test datasets confirm the effectiveness of CLEAR for both method-level and class-level API recommendation. Compared to the state-of-the-art API recommendation approaches, CLEAR improves the MAP by 25%-187% at method-level and 10%-100% at class-level.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {376–387},
numpages = {12},
keywords = {API recommendation, contrastive learning, semantic difference},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@proceedings{10.1145/3609437,
title = {Internetware '23: Proceedings of the 14th Asia-Pacific Symposium on Internetware},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@article{10.1145/3591280,
author = {Li, Ziyang and Huang, Jiani and Naik, Mayur},
title = {Scallop: A Language for Neurosymbolic Programming},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591280},
doi = {10.1145/3591280},
abstract = {We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data- and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-the-art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability.},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {166},
numpages = {25},
keywords = {Differentiable reasoning, Neurosymbolic methods}
}

@inproceedings{10.1145/3580305.3599873,
author = {Liao, Hao and Peng, Jiahao and Huang, Zhanyi and Zhang, Wei and Li, Guanghua and Shu, Kai and Xie, Xing},
title = {MUSER: A MUlti-Step Evidence Retrieval Enhancement Framework for Fake News Detection},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599873},
doi = {10.1145/3580305.3599873},
abstract = {The ease of spreading false information online enables individuals with malicious intent to manipulate public opinion and destabilize social stability. Recently, fake news detection based on evidence retrieval has gained popularity in an effort to identify fake news reliably and reduce its impact. Evidence retrieval-based methods can improve the reliability of fake news detection by computing the textual consistency between the evidence and the claim in the news. In this paper, we propose a framework for fake news detection based on MUlti- Step Evidence Retrieval enhancement (MUSER), which simulates the steps of human beings in the process of reading news, summarizing, consulting materials, and inferring whether the news is true or fake. Our model can explicitly model dependencies among multiple pieces of evidence, and perform multi-step associations for the evidence required for news verification through multi-step retrieval. In addition, our model is able to automatically collect existing evidence through paragraph retrieval and key evidence selection, which can save the tedious process of manual evidence collection. We conducted extensive experiments on real-world datasets in different languages, and the results demonstrate that our proposed model outperforms state-of-the-art baseline methods for detecting fake news by at least 3% in F1-Macro and 4% in F1-Micro. Furthermore, it provides interpretable evidence for end users.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4461–4472},
numpages = {12},
keywords = {evidence-based fake news detection, explainability, multi-step retrieval},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1109/TASLP.2023.3270771,
author = {Li, Shu'ang and Hu, Xuming and Lin, Li and Liu, Aiwei and Wen, Lijie and Yu, Philip S.},
title = {A Multi-Level Supervised Contrastive Learning Framework for Low-Resource Natural Language Inference},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3270771},
doi = {10.1109/TASLP.2023.3270771},
abstract = {Natural Language Inference (NLI) is a growingly essential task in natural language understanding, which requires inferring the relationship between the sentence pairs (&lt;bold&gt;premise&lt;/bold&gt; and &lt;bold&gt;hypothesis&lt;/bold&gt;). Recently, low-resource natural language inference has gained increasing attention, due to significant savings in manual annotation costs and a better fit with real-world scenarios. Existing works fail to characterize discriminative representations between different classes with limited training data, which may cause faults in label prediction. Here we propose a multi-level supervised contrastive learning framework named MultiSCL for low-resource natural language inference. MultiSCL leverages a sentence-level and pair-level contrastive learning objective to discriminate between different classes of sentence pairs by bringing those in one class together and pushing away those in different classes. MultiSCL adopts a data augmentation module that generates different views for input samples to better learn the latent representation. The pair-level representation is obtained from a cross attention module. We conduct extensive experiments on two public NLI datasets in low-resource settings, and the accuracy of MultiSCL exceeds other models by 1.8%, 3.1% and 4.1% on SNLI, MNLI and Sick with 5 instances per label respectively. Moreover, our method outperforms the previous state-of-the-art method on cross-domain tasks of text classification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1771–1783},
numpages = {13}
}

@proceedings{10.1145/3581641,
title = {IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3581807,
title = {ICCPR '22: Proceedings of the 2022 11th International Conference on Computing and Pattern Recognition},
year = {2022},
isbn = {9781450397056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3459930.3469533,
author = {Noh, Jiho and Kavuluru, Ramakanth},
title = {Joint learning for biomedical NER and entity normalization: encoding schemes, counterfactual examples, and zero-shot evaluation},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469533},
doi = {10.1145/3459930.3469533},
abstract = {Named entity recognition (NER) and normalization (EN) form an indispensable first step to many biomedical natural language processing applications. In biomedical information science, recognizing entities (e.g., genes, diseases, or drugs) and normalizing them to concepts in standard terminologies or thesauri (e.g., Entrez, ICD-10, or RxNorm) is crucial for identifying more informative relations among them that drive disease etiology, progression, and treatment. In this effort we pursue two high level strategies to improve biomedical ER and EN. The first is to decouple standard entity encoding tags (e.g., "B-Drug" for the beginning of a drug) into type tags (e.g., "Drug") and positional tags (e.g., "B"). A second strategy is to use additional counterfactual training examples to handle the issue of models learning spurious correlations between surrounding context and normalized concepts in training data. We conduct elaborate experiments using the MedMentions dataset, the largest dataset of its kind for ER and EN in biomedicine. We find that our first strategy performs better in entity normalization when compared with the standard coding scheme. The second data augmentation strategy uniformly improves performance in span detection, typing, and normalization. The gains from counterfactual examples are more prominent when evaluating in zero-shot settings, for concepts that have never been encountered during training.},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {55},
numpages = {10},
keywords = {biomedical natural language processing, deep neural networks, entity normalization, information extraction, named entity recognition},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inproceedings{10.1145/3477495.3532039,
author = {Cai, Zefeng and Cai, Zerui},
title = {PEVAE: A Hierarchical VAE for Personalized Explainable Recommendation.},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3532039},
doi = {10.1145/3477495.3532039},
abstract = {Variational autoencoders (VAEs) have been widely applied in recommendations. One reason is that their amortized inferences are beneficial for overcoming the data sparsity. However, in explainable recommendation that generates natural language explanations, they are still rarely explored. Thus, we aim to extend VAE to explainable recommendation. In this task, we find that VAE can generate acceptable explanations for users with few relevant training samples, however, it tends to generate less personalized explanations for users with relatively sufficient samples than autoencoders (AEs). We conjecture that information shared by different users in VAE disturbs the information for a specific user. To deal with this problem, we present PErsonalized VAE (PEVAE) that generates personalized natural language explanations for explainable recommendation. Moreover, we propose two novel mechanisms to aid our model in generating more personalized explanations, including 1) Self-Adaption Fusion (SAF) manipulates the latent space in a self-adaption manner for controlling the influence of shared information. In this way, our model can enjoy the advantage of overcoming the sparsity of data while generating more personalized explanations for a user with relatively sufficient training samples. 2) DEpendence Maximization (DEM) strengthens dependence between recommendations and explanations by maximizing the mutual information. It makes the explanation more specific to the input user-item pair and thus improves the personalization of the generated explanations. Extensive experiments show PEVAE can generate more personalized explanations and further analyses demonstrate the practical effect of our proposed methods.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {692–702},
numpages = {11},
keywords = {natural language generation, recommender systems, variational inference},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@proceedings{10.1145/3593013,
title = {FAccT '23: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/3607827.3616842,
author = {Rossetto, Federico and Dalton, Jeffrey and Murray-Smith, Roderick},
title = {Generating Multimodal Augmentations with LLMs from Song Metadata for Music Information Retrieval},
year = {2023},
isbn = {9798400702839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607827.3616842},
doi = {10.1145/3607827.3616842},
abstract = {In this work we propose a set of new automatic text augmentations that leverage Large Language Models from song metadata to improve on music information retrieval tasks. Compared to recent works, our proposed methods leverage large language models and copyright-free corpora from web sources, enabling us to release the knowledge sources collected. We show how combining these representations with the audio signal provides a 21% relative improvement on five of six datasets on genre classification, emotion recognition and music tagging, achieving state-of-the-art in three (GTZAN, FMA-Small and Deezer). We demonstrate the benefit of injecting external knowledge sources by comparing them withintrinsic text representation methods that rely only on the sample's information.},
booktitle = {Proceedings of the 1st Workshop on Large Generative Models Meet Multimodal Applications},
pages = {51–59},
numpages = {9},
keywords = {large language models application, multimodal learning, music information retrieval},
location = {Ottawa ON, Canada},
series = {LGM3A '23}
}

@proceedings{10.1145/3625156,
title = {ICISS '23: Proceedings of the 2023 6th International Conference on Information Science and Systems},
year = {2023},
isbn = {9798400708206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@article{10.1145/3503917,
author = {Li, Rui and Yang, Cheng and Li, Tingwei and Su, Sen},
title = {MiDTD: A Simple and Effective Distillation Framework for Distantly Supervised Relation Extraction},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3503917},
doi = {10.1145/3503917},
abstract = {Relation extraction (RE), an important information extraction task, faced the great challenge brought by limited annotation data. To this end, distant supervision was proposed to automatically label RE data, and thus largely increased the number of annotated instances. Unfortunately, lots of noise relation annotations brought by automatic labeling become a new obstacle. Some recent studies have shown that the teacher-student framework of knowledge distillation can alleviate the interference of noise relation annotations via label softening. Nevertheless, we find that they still suffer from two problems: propagation of inaccurate dark knowledge and constraint of a unified distillation temperature. In this article, we propose a simple and effective Multi-instance Dynamic Temperature Distillation (MiDTD) framework, which is model-agnostic and mainly involves two modules: multi-instance target fusion (MiTF) and dynamic temperature regulation (DTR). MiTF combines the teacher’s predictions for multiple sentences with the same entity pair to amend the inaccurate dark knowledge in each student’s target. DTR allocates alterable distillation temperatures to different training instances to enable the softness of most student’s targets to be regulated to a moderate range. In experiments, we construct three concrete MiDTD instantiations with BERT, PCNN, and BiLSTM-based RE models, and the distilled students significantly outperform their teachers and the state-of-the-art (SOTA) methods.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {83},
numpages = {32},
keywords = {Natural language processing, NLP, knowledge distillation, distant supervision, neural network, multi-instance learning, label softening}
}

@inproceedings{10.1145/3477495.3531803,
author = {Liu, Han and Zhao, Siyang and Zhang, Xiaotong and Zhang, Feng and Sun, Junjie and Yu, Hong and Zhang, Xianchao},
title = {A Simple Meta-learning Paradigm for Zero-shot Intent Classification with Mixture Attention Mechanism},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531803},
doi = {10.1145/3477495.3531803},
abstract = {Zero-shot intent classification is a vital and challenging task in dialogue systems, which aims to deal with numerous fast-emerging unacquainted intents without annotated training data. To obtain more satisfactory performance, the crucial points lie in two aspects: extracting better utterance features and strengthening the model generalization ability. In this paper, we propose a simple yet effective meta-learning paradigm for zero-shot intent classification. To learn better semantic representations for utterances, we introduce a new mixture attention mechanism, which encodes the pertinent word occurrence patterns by leveraging the distributional signature attention and multi-layer perceptron attention simultaneously. To strengthen the transfer ability of the model from seen classes to unseen classes, we reformulate zero-shot intent classification with a meta-learning strategy, which trains the model by simulating multiple zero-shot classification tasks on seen categories, and promotes the model generalization ability with a meta-adapting procedure on mimic unseen categories. Extensive experiments on two real-world dialogue datasets in different languages show that our model outperforms other strong baselines on both standard and generalized zero-shot intent classification tasks.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2047–2052},
numpages = {6},
keywords = {meta-learning, mixture attention mechanism, zero-shot intent classification},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3511808.3557271,
author = {Chen, Jiangui and Zhang, Ruqing and Guo, Jiafeng and Liu, Yiqun and Fan, Yixing and Cheng, Xueqi},
title = {CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557271},
doi = {10.1145/3511808.3557271},
abstract = {Knowledge-intensive language tasks (KILT) usually require a large body of information to provide correct answers. A popular paradigm to solve this problem is to combine a search system with a machine reader, where the former retrieves supporting evidences and the latter examines them to produce answers. Recently, the reader component has witnessed significant advances with the help of large-scale pre-trained generative models. Meanwhile most existing solutions in the search component rely on the traditional "index-retrieve-then-rank'' pipeline, which suffers from large memory footprint and difficulty in end-to-end optimization. Inspired by recent efforts in constructing model-based IR models, we propose to replace the traditional multi-step search pipeline with a novel single-step generative model, which can dramatically simplify the search process and be optimized in an end-to-end manner. We show that a strong generative retrieval model can be learned with a set of adequately designed pre-training tasks, and be adopted to improve a variety of downstream KILT tasks with further fine-tuning. We name the pre-trained generative retrieval model as CorpusBrain as all information about the corpus is encoded in its parameters without the need of constructing additional index. Empirical results show that CorpusBrain can significantly outperform strong baselines for the retrieval task on the KILT benchmark and establish new state-of-the-art downstream performances. We also show that CorpusBrain works well under zero- and low-resource settings.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {191–200},
numpages = {10},
keywords = {generative retrieval, model-based ir, pre-training},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3485447.3512168,
author = {Wang, Peng and Cai, Renqin and Wang, Hongning},
title = {Graph-based Extractive Explainer for Recommendations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512168},
doi = {10.1145/3485447.3512168},
abstract = {Explanations in a recommender system assist users make informed decisions among a set of recommended items. Extensive research attention has been devoted to generate natural language explanations to depict how the recommendations are generated and why the users should pay attention to them. However, due to different limitations of those solutions, e.g., template-based or generation-based, it is hard to make the explanations easily perceivable, reliable, and personalized at the same time. In this work, we develop a graph attentive neural network model that seamlessly integrates user, item, attributes and sentences for extraction-based explanation. The attributes of items are selected as the intermediary to facilitate message passing for user-item specific evaluation of sentence relevance. And to balance individual sentence relevance, overall attribute coverage and content redundancy, we solve an integer linear programming problem to make the final selection of sentences. Extensive empirical evaluations against a set of state-of-the-art baseline methods on two benchmark review datasets demonstrated the generation quality of proposed solution.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2163–2171},
numpages = {9},
keywords = {Extraction-based explanation, graph neural networks},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3629264,
title = {ICCDA '23: Proceedings of the 2023 7th International Conference on Computing and Data Analysis},
year = {2023},
isbn = {9798400700576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guiyang, China}
}

@article{10.1145/3430937,
author = {Reis, Eduardo Souza Dos and Costa, Cristiano Andr\'{e} Da and Silveira, Di\'{o}rgenes Eug\^{e}nio Da and Bavaresco, Rodrigo Simon and Righi, Rodrigo Da Rosa and Barbosa, Jorge Luis Vict\'{o}ria and Antunes, Rodolfo Stoffel and Gomes, M\'{a}rcio Miguel and Federizzi, Gustavo},
title = {Transformers aftermath: current research and rising trends},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3430937},
doi = {10.1145/3430937},
abstract = {Attention, particularly self-attention, is a standard in current NLP literature, but to achieve meaningful models, attention is not enough.},
journal = {Commun. ACM},
month = mar,
pages = {154–163},
numpages = {10}
}

@inproceedings{10.1145/3589132.3625618,
author = {Wang, Renzhong and Najafabadi, Maryam and Zhang, Chiqun and Chen, Long-Qi and Olenina, Tanya and Yankov, Dragomir},
title = {GPT Applications in Relevance Model Training in Map Search},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625618},
doi = {10.1145/3589132.3625618},
abstract = {Understanding map queries and retrieving correct entity results are the two main relevance tasks in Map search. They are usually performed by a set of task specific machine learning models. Collecting large amount of high quality labelled data for training such models is a time-consuming and labor-intensive process. Although various methods have been studied for producing pseudo data labels, they are limited in their effectiveness when applied across different languages or tasks. The recently released Large Language models (LLMs), including ChatGPT and GPT-4 (GPT for short), have demonstrated state-of-the-art performance in text understanding by using simple prompt instructions with only a handful of examples for in-context learning. In this paper, we explore GPT as a cost-effective alternative for both data labeling and synthetic data generation, where we subsequently use data obtained from this approach to train various task specific models such as maps intent detection, address detection, address parsing, geo-entity ranking, and rank scores calibration. GPT demonstrates strong potential in generating otherwise hard-to-synthesize data. We observe significant accuracy and relevance improvement across all task specific models when trained or fine-tuned on data generated by GPT. Lastly, we propose a general framework combining labeled data from GPT with other sources and a prompt fine-tune structure to guide GPT model in completing a given task.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {68},
numpages = {4},
keywords = {GPT, query processing, maps service, information retrieval},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@article{10.1145/3495162,
author = {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang},
title = {A Survey on Text Classification: From Traditional to Deep Learning},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3495162},
doi = {10.1145/3495162},
abstract = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = apr,
articleno = {31},
numpages = {41},
keywords = {Deep learning, traditional models, text classification, evaluation metrics, challenges}
}

@inproceedings{10.1145/3459637.3482490,
author = {Guan, Renchu and Liu, Yonghao and Feng, Xiaoyue and Li, Ximing},
title = {VPALG: Paper-publication Prediction with Graph Neural Networks},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482490},
doi = {10.1145/3459637.3482490},
abstract = {Paper-publication venue prediction aims to predict candidate publication venues that effectively suit given submissions. This technology is developing rapidly with the popularity of machine learning models. However, most previous methods ignore the structure information of papers, while modeling them with graphs can naturally solve this drawback. Meanwhile, they either use hand-crafted or bag-of-word features to represent the papers, ignoring the ones that involve high-level semantics. Moreover, existing methods assume that the venue where a paper is published as a correct venue for the data annotation, which is unrealistic. One paper can be relevant to many venues. In this paper, we attempt to address these problems above and develop a novel prediction model, namelyVenue Prediction with Abstract-Level Graph (Vpalg xspace), which can serve as an effective decision-making tool for venue selections. Specifically, to achieve more discriminative paper abstract representations, we construct each abstract as a semantic graph and perform a dual attention message passing neural network for representation learning. Then, the proposed model can be trained over the learned abstract representations with their labels and generalized via self-training. Empirically, we employ the PubMed dataset and further collect two new datasets from the top journals and conferences in computer science. Experimental results indicate the superior performance of Vpalg xspace, consistently outperforming the existing baseline methods.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {617–626},
numpages = {10},
keywords = {graph neural networks, paper-publication prediction, text mining},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1145/3627989,
author = {Jin, Weiqiang and Zhao, Biao and Zhang, Yu and Sun, Gege and Yu, Hang},
title = {Fintech Key-Phrase: A New Chinese Financial High-Tech Dataset Accelerating Expression-Level Information Retrieval},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {11},
issn = {2375-4699},
url = {https://doi.org/10.1145/3627989},
doi = {10.1145/3627989},
abstract = {Expression-level information extraction is a challenging task in natural language processing (NLP), which aims to retrieve crucial semantic information from linguistic documents. However, there is a lack of up-to-date data resources for accelerating expression-level information extraction, particularly in the Chinese financial high technology field. To address this gap, we introduce Fintech Key-Phrase: a human-annotated key-phrase dataset for the Chinese financial high technology domain. This dataset comprises over 12K paragraphs along with annotated domain-specific key-phrases. We extract the publicly released reports, Chinese management’s discussion and analysis (CMD&amp;A), from the renowned Chinese research data services platform (CNRDS) and then filter the reports related to high technology. The high technology key-phrases are annotated following pre-defined philosophy guidelines to ensure annotation quality. In order to better understand the limitations and challenges in the purposed dataset, we conducted comprehensive noise evaluation experiments for the Fintech Key-Phrase, including annotation consistency assessment and absolute annotation quality evaluation. To demonstrate the usefulness of our released Fintech Key-Phrase in retrieving valuable information in the Chinese financial high technology field, we evaluate its significance using several superior information retrieval systems as representative baselines and report corresponding performance statistics. Additionally, we further applied ChatGPT to the text augmentation approach of the Fintech Key-Phrase dataset. Extensive comparative experiments demonstrate that the augmented Fintech Key-Phrase dataset significantly improved the coverage and accuracy of extracting key phrases in the finance and high-tech domains. We believe that this dataset can facilitate scientific research and exploration in the Chinese financial high technology field. We have made the Fintech Key-Phrase dataset and the experimental code of the adopted baselines accessible on Github: https://github.com/albert-jin/Fintech-Key-Phrase. To encourage newcomers to participate in the financial high-tech domain information retrieval research, we have developed a series of tools, including an open website1 and corresponding real-time information retrieval APIs.2},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {253},
numpages = {37},
keywords = {Information retrieval, expression-level information extraction, financial high technology field, Chinese management’s discussion and analysis, ChatGPT-based data augment}
}

@proceedings{10.1145/3606043,
title = {HP3C '23: Proceedings of the 2023 7th International Conference on High Performance Compilation, Computing and Communications},
year = {2023},
isbn = {9781450399883},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jinan, China}
}

@inproceedings{10.1145/3498366.3505819,
author = {Smith, Catherine and Urgo, Kelsey and Arguello, Jaime and Capra, Robert},
title = {Learner, Assignment, and Domain: Contextualizing Search for Comprehension},
year = {2022},
isbn = {9781450391863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498366.3505819},
doi = {10.1145/3498366.3505819},
abstract = {Modern search systems are largely designed and optimized for simple navigational or fact-finding tasks, with little support for complex tasks involving comprehension and learning. In response, the search-as-learning research community has undertaken a wide range of research questions focused on understanding how various types of learning outcomes are affected by searcher characteristics, the search task, and the search system. Typically, these views embed learning within a search system. In this paper we take a different view, embedding search within a framework for an end-to-end learning system designed to support learning in a formal educational context. Our central goal is to motivate research questions aligned to advance progress on techniques for active support of comprehension and formal learning. Thus we intentionally set aside goals for informal and surface learning. We argue that to be effective, such a search-centric learning system must model four key components: individual students (searcher factors), the educational domain (topic factors), academic assignments (task factors), and progress toward learning goals (the objective function of the end-to-end system). In modeling these components, our hypothetical system makes inferences about students’ learning histories, knowledge states, comprehension, and the utilities of different types of information resources. We present examples of possible techniques and data sources for each model. We also introduce the novel concept of leveraging school assignments as rich task context. Our intention is not to propose a functional system, but to frame search-as-learning in the context of comprehension and to inspire research questions arising from an end-to-end view of this important research domain.},
booktitle = {Proceedings of the 2022 Conference on Human Information Interaction and Retrieval},
pages = {191–201},
numpages = {11},
keywords = {interfaces for learning, retrieval models, search-as-learning, self-regulated learning, user models},
location = {Regensburg, Germany},
series = {CHIIR '22}
}

@inproceedings{10.1145/3604915.3608775,
author = {Wu, Yaxiong and Macdonald, Craig and Ounis, Iadh},
title = {Goal-Oriented Multi-Modal Interactive Recommendation with Verbal and Non-Verbal Relevance Feedback},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608775},
doi = {10.1145/3604915.3608775},
abstract = {Interactive recommendation enables users to provide verbal and non-verbal relevance feedback (such as natural-language critiques and likes/dislikes) when viewing a ranked list of recommendations (such as images of fashion products), in order to guide the recommender system towards their desired items (i.e. goals) across multiple interaction turns. Such a multi-modal interactive recommendation (MMIR) task has been successfully formulated with deep reinforcement learning (DRL) algorithms by simulating the interactions between an environment (i.e. a user) and an agent (i.e. a recommender system). However, it is typically challenging and unstable to optimise the agent to improve the recommendation quality associated with implicit learning of multi-modal representations in an end-to-end fashion in DRL. This is known as the coupling of policy optimisation and representation learning. To address this coupling issue, we propose a novel goal-oriented multi-modal interactive recommendation model (GOMMIR) that uses both verbal and non-verbal relevance feedback to effectively incorporate the users’ preferences over time. Specifically, our GOMMIR model employs a multi-task learning approach to explicitly learn the multi-modal representations using a multi-modal composition network when optimising the recommendation agent. Moreover, we formulate the MMIR task using goal-oriented reinforcement learning and enhance the optimisation objective by leveraging non-verbal relevance feedback for hard negative sampling and providing extra goal-oriented rewards to effectively optimise the recommendation agent. Following previous work, we train and evaluate our GOMMIR model by using user simulators that can generate natural-language feedback about the recommendations as a surrogate for real human users. Experiments conducted on four well-known fashion datasets demonstrate that our proposed GOMMIR model yields significant improvements in comparison to the existing state-of-the-art baseline models.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {362–373},
numpages = {12},
keywords = {interactive recommendation, multi-modal, reinforcement learning, relevance feedback},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@inproceedings{10.1145/3608298.3608324,
author = {Schl\"{o}r, Daniel and Pfister, Jan and Hotho, Andreas},
title = {Optimizing Medical Service Request Processes through Language Modeling and Semantic Search},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608298.3608324},
doi = {10.1145/3608298.3608324},
abstract = {Medical service requests are a crucial part of the workflow in hospitals and healthcare organizations. However, the process of requesting medical services can be time consuming and can require physicians and medical personnel to navigate complex interfaces and enter detailed information about the requested service. In this paper, we propose a system that uses machine learning techniques such as large language models and semantic search to optimize the process of requesting medical services. Our approach enables physicians to request medical services using natural language rather than navigating complex interfaces, allowing for more efficient and flexible interactions with hospital information systems. We evaluate our approach on real-world data and discuss the implications of our work for the future of digital health care. Our results suggest that our approach has the potential to streamline the process of requesting medical services and reduce the time and manual effort required in the daily hospital routine.},
booktitle = {Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
pages = {136–141},
numpages = {6},
keywords = {language modeling, medical service optimization, semantic search},
location = {Kyoto, Japan},
series = {ICMHI '23}
}

@article{10.1145/3552311,
author = {Wang, Yashen and Wang, Zhaoyu and Zhang, Huanhuan and Liu, Zhirun},
title = {Microblog Retrieval Based on Concept-Enhanced Pre-Training Model},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3552311},
doi = {10.1145/3552311},
abstract = {Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have mostly been applied to conventional ad-hoc retrieval tasks over web pages and newswire articles. This article proposes a concept-enhanced pre-training model for microblog retrieval task, leveraging Semantic Matching Model (SMM) objective and Concept Correlation Model (CCM) objective. The proposed model is a novel neural ranking model specifically designed for ranking short-text microblog, which could merge the advantage of pre-training methodology for generating valid contextualized embedding with the superiority of the prior lexical knowledge (e.g., concept knowledge) for understanding short-text language semantic. We conduct experiments on widely used real-world datasets, and the experimental results demonstrate the efficiency of the proposed model, even compared with latest state-of-the-art neural-based models and pre-training based models.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {41},
numpages = {32},
keywords = {Microblog retrieval, pre-training mechanism, concept semantic knowledge, representation learning}
}

@inproceedings{10.1145/3589132.3625629,
author = {Wang, Zhaonan and Jin, Bowen and Hu, Wei and Jiang, Minhao and Kang, Seungyeon and Li, Zhiyuan and Zhou, Sizhe and Han, Jiawei and Wang, Shaowen},
title = {Geospatial Knowledge Hypercube},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625629},
doi = {10.1145/3589132.3625629},
abstract = {Today a tremendous amount of geospatial knowledge is hidden in massive volumes of text data. To facilitate flexible and powerful geospatial analysis and applications, we introduce a new architecture: geospatial knowledge hypercube, a multi-scale, multidimensional knowledge structure that integrates information from geospatial dimensions, thematic themes and diverse application semantics, extracted and computed from spatial-related text data. To construct such a knowledge hypercube, weakly supervised language models are leveraged for automatic, dynamic and incremental extraction of heterogeneous geospatial data, thematic themes, latent connections and relationships, and application semantics, through combining a variety of information from unstructured text, structured tables, and maps. The hypercube lays a foundation for many knowledge discovery and in-depth spatial analysis, and other advanced applications. We have deployed a prototype web application of proposed geospatial knowledge hypercube for public access at: https://hcwebapp.cigi.illinois.edu/.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {79},
numpages = {4},
keywords = {knowledge hypercube, geographic information retrieval, weakly-supervised text classification},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@inproceedings{10.1145/3534678.3539021,
author = {Huang, Jizhou and Wang, Haifeng and Sun, Yibo and Shi, Yunsheng and Huang, Zhengjie and Zhuo, An and Feng, Shikun},
title = {ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539021},
doi = {10.1145/3534678.3539021},
abstract = {Pre-trained models (PTMs) have become a fundamental backbone for downstream tasks in natural language processing and computer vision. Despite initial gains that were obtained by applying generic PTMs to geo-related tasks at Baidu Maps, a clear performance plateau over time was observed. One of the main reasons for this plateau is the lack of readily available geographic knowledge in generic PTMs. To address this problem, in this paper, we present ERNIE-GeoL, which is a geography-and-language pre-trained model designed and developed for improving the geo-related tasks at Baidu Maps. ERNIE-GeoL is elaborately designed to learn a universal representation of geography-language by pre-training on large-scale data generated from a heterogeneous graph that contains abundant geographic knowledge. Extensive quantitative and qualitative experiments conducted on large-scale real-world datasets demonstrate the superiority and effectiveness of ERNIE-GeoL. ERNIE-GeoL has already been deployed in production at Baidu Maps since April 2021, which significantly benefits the performance of various downstream tasks. This demonstrates that ERNIE-GeoL can serve as a fundamental backbone for a wide range of geo-related tasks.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3029–3039},
numpages = {11},
keywords = {graph neural network, heterogeneous graph, pre-training},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3488560.3498378,
author = {Gao, Shen and Zhang, Yuchi and Wang, Yongliang and Dong, Yang and Chen, Xiuying and Zhao, Dongyan and Yan, Rui},
title = {HeteroQA: Learning towards Question-and-Answering through Multiple Information Sources via Heterogeneous Graph Modeling},
year = {2022},
isbn = {9781450391320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488560.3498378},
doi = {10.1145/3488560.3498378},
abstract = {Community Question Answering (CQA) is a well-defined task that can be used in many scenarios, such as E-Commerce and online user community for special interests. In these communities, users can post articles, give comment, raise a question and answer it. These data form the heterogeneous information sources where each information source have their own special structure and context (comments attached to an article or related question with answers). Most of the CQA methods only incorporate articles or Wikipedia to extract knowledge and answer the user's question. However, various types of information sources in the community are not fully explored by these CQA methods and these multiple information sources (MIS) can provide more related knowledge to user's questions. Thus, we propose a question-aware heterogeneous graph transformer to incorporate the MIS in the user community to automatically generate the answer. To evaluate our proposed method, we conduct the experiments on two datasets: $textMSM ^textplus $ the modified version of benchmark dataset MS-MARCO and the AntQA dataset which is the first large-scale CQA dataset with four types of MIS. Extensive experiments on two datasets show that our model outperforms all the baselines in terms of all the metrics.},
booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
pages = {307–315},
numpages = {9},
keywords = {heterogeneous graph, question answering system, text generation},
location = {Virtual Event, AZ, USA},
series = {WSDM '22}
}

@inproceedings{10.1145/3539618.3591997,
author = {Lu, Jiaying and Shen, Jiaming and Xiong, Bo and Ma, Wenjing and Staab, Steffen and Yang, Carl},
title = {HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591997},
doi = {10.1145/3539618.3591997},
abstract = {Medical decision-making processes can be enhanced by comprehensive biomedical knowledge bases, which require fusing knowledge graphs constructed from different sources via a uniform index system. The index system often organizes biomedical terms in a hierarchy to provide the aligned entities with fine-grained granularity. To address the challenge of scarce supervision in the biomedical knowledge fusion (BKF) task, researchers have proposed various unsupervised methods. However, these methods heavily rely on ad-hoc lexical and structural matching algorithms, which fail to capture the rich semantics conveyed by biomedical entities and terms. Recently, neural embedding models have proved effective in semantic-rich tasks, but they rely on sufficient labeled data to be adequately trained. To bridge the gap between the scarce-labeled BKF and neural embedding models, we propose HiPrompt, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large language models through hierarchy-oriented prompts. Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the effectiveness of HiPrompt.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2052–2056},
numpages = {5},
keywords = {biomedical knowledge fusion, few-shot prompting, large language models for resource-constrained field, retrieve &amp; re-rank},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.14778/3476311.3476402,
author = {Orr, Laurel and Sanyal, Atindriyo and Ling, Xiao and Goel, Karan and Leszczynski, Megan},
title = {Managing ML pipelines: feature stores and the coming wave of embedding ecosystems},
year = {2021},
issue_date = {July 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3476311.3476402},
doi = {10.14778/3476311.3476402},
abstract = {The industrial machine learning pipeline requires iterating on model features, training and deploying models, and monitoring deployed models at scale. Feature stores were developed to manage and standardize the engineer's workflow in this end-to-end pipeline, focusing on traditional tabular feature data. In recent years, however, model development has shifted towards using self-supervised pretrained embeddings as model features. Managing these embeddings and the downstream systems that use them introduces new challenges with respect to managing embedding training data, measuring embedding quality, and monitoring downstream models that use embeddings. These challenges are largely unaddressed in standard feature stores. Our goal in this tutorial is to introduce the feature store system and discuss the challenges and current solutions to managing these new embedding-centric pipelines.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3178–3181},
numpages = {4}
}

@article{10.1145/3589131,
author = {Van Thin, Dang and Hao, Duong Ngoc and Nguyen, Ngan Luu-Thuy},
title = {Vietnamese Sentiment Analysis: An Overview and Comparative Study of Fine-tuning Pretrained Language Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3589131},
doi = {10.1145/3589131},
abstract = {Sentiment Analysis (SA) is one of the most active research areas in the Natural Language Processing (NLP) field due to its potential for business and society. With the development of language representation models, numerous methods have shown promising efficiency in fine-tuning pre-trained language models in NLP downstream tasks. For Vietnamese, many available pre-trained language models were also released, including the monolingual and multilingual language models. Unfortunately, all of these models were trained on different architectures, pre-trained data, and pre-processing steps; consequently, fine-tuning these models can be expected to yield different effectiveness. In addition, there is no study focusing on evaluating the performance of these models on the same datasets for the SA task up to now. This article presents a fine-tuning approach to investigate the performance of different pre-trained language models for the Vietnamese SA task. The experimental results show the superior performance of the monolingual PhoBERT model and ViT5 model in comparison with previous studies and provide new state-of-the-art performances on five benchmark Vietnamese SA datasets. To the best of our knowledge, our study is the first attempt to investigate the performance of fine-tuning Transformer-based models on five datasets with different domains and sizes for the Vietnamese SA task.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {166},
numpages = {27},
keywords = {Vietnamese Sentiment Analysis, fine-tuning language models, monolingual BERT model, multilingual BERT model, T5 architecture}
}

@inproceedings{10.1145/3539618.3592010,
author = {Tavares, Diogo and Semedo, David and Rudnicky, Alexander and Magalhaes, Joao},
title = {Learning to Ask Questions for Zero-shot Dialogue State Tracking},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592010},
doi = {10.1145/3539618.3592010},
abstract = {We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2118–2122},
numpages = {5},
keywords = {dialogue state tracking, question answering, zero-shot},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3448016.3457542,
author = {Li, Guoliang and Zhou, Xuanhe and Cao, Lei},
title = {AI Meets Database: AI4DB and DB4AI},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457542},
doi = {10.1145/3448016.3457542},
abstract = {Database and Artificial Intelligence (AI) can benefit from each other. On one hand, AI can make database more intelligent (AI4DB). For example, traditional empirical database optimization techniques (e.g., cost estimation, join order selection, knob tuning, index and view advisor) cannot meet the high-performance requirement for large-scale database instances, various applications and diversified users, especially on the cloud. Fortunately, learning-based techniques can alleviate this problem. On the other hand, database techniques can optimize AI models (DB4AI). For example, AI is hard to deploy, because it requires developers to write complex codes and train complicated models. Database techniques can be used to reduce the complexity of using AI models, accelerate AI algorithms and provide AI capability inside databases. DB4AI and AI4DB have been extensively studied recently. In this tutorial, we review existing studies on AI4DB and DB4AI. For AI4DB, we review the techniques on learning-based database configuration, optimization, design, monitoring, and security. For DB4AI, we review AI-oriented declarative language, data governance, training acceleration, and inference acceleration. Finally, we provide research challenges and future directions in AI4DB and DB4AI.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2859–2866},
numpages = {8},
keywords = {AI, database, machine learning, models},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1109/TASLP.2023.3304481,
author = {Zhang, Ying and Meng, Fandong and Chen, Yufeng and Xu, Jinan and Zhou, Jie},
title = {Complex Question Enhanced Transfer Learning for Zero-Shot Joint Information Extraction},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3304481},
doi = {10.1109/TASLP.2023.3304481},
abstract = {Zero-shot information extraction (IE) tasks have attracted great attention recently. However, how to jointly model multiple IE tasks in the zero-shot scenario is still an open question. In this article, we focus on zero-shot joint IE tasks and highlight how to transfer the knowledge of cross-task relations from the source domain to the target domain. To solve this problem, we first unify all IE tasks with a machine reading comprehension (MRC) framework, which can make the most of training data and enhance its ability on span extraction. Then, we generate &lt;italic&gt;complex questions&lt;/italic&gt; to explicitly model cross-task relations with natural language descriptions, thereby providing prior knowledge for pre-defined types and building more general linkages among different entities and triggers as well. Specifically, we define three operations for generating templates for complex questions, i.e., &lt;italic&gt;intersecting&lt;/italic&gt;, &lt;italic&gt;connecting&lt;/italic&gt;, and &lt;italic&gt;composing&lt;/italic&gt;. Besides, we design an efficient training strategy to exploit the synthetic data with complex questions. We evaluate our approach on four datasets from different domains for various IE tasks. Experimental results show the effectiveness of our approach in improving the performance of zero-shot joint IE tasks in multiple domains.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {261–275},
numpages = {15}
}

@inproceedings{10.1145/3529372.3530922,
author = {Brack, Arthur and Hoppe, Anett and Buscherm\"{o}hle, Pascal and Ewerth, Ralph},
title = {Cross-domain multi-task learning for sequential sentence classification in research papers},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3530922},
doi = {10.1145/3529372.3530922},
abstract = {Sequential sentence classification deals with the categorisation of sentences based on their content and context. Applied to scientific texts, it enables the automatic structuring of research papers and the improvement of academic search engines. However, previous work has not investigated the potential of transfer learning for sentence classification across different scientific domains and the issue of different text structure of full papers and abstracts. In this paper, we derive seven related research questions and present several contributions to address them: First, we suggest a novel uniform deep learning architecture and multi-task learning for cross-domain sequential sentence classification in scientific texts. Second, we tailor two common transfer learning methods, sequential transfer learning and multi-task learning, to deal with the challenges of the given task. Semantic relatedness of tasks is a prerequisite for successful transfer learning of neural models. Consequently, our third contribution is an approach to semi-automatically identify semantically related classes from different annotation schemes and we present an analysis of four annotation schemes. Comprehensive experimental results indicate that models, which are trained on datasets from different scientific domains, benefit from one another when using the proposed multi-task learning architecture. We also report comparisons with several state-of-the-art approaches. Our approach outperforms the state of the art on full paper datasets significantly while being on par for datasets consisting of abstracts.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {34},
numpages = {13},
keywords = {multi-task learning, scholarly communication, sequential sentence classification, transfer learning, zone identification},
location = {Cologne, Germany},
series = {JCDL '22}
}

@article{10.1145/3539014,
author = {Chen, Jiusheng and Xu, Xingkai and Zhang, Xiaoyu},
title = {Radial Basis Function Attention for Named Entity Recognition},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3539014},
doi = {10.1145/3539014},
abstract = {Attention mechanism is an increasingly important approach in the field of natural language processing (NLP). In the attention-based named entity recognition (NER) model, most attention mechanisms can calculate attention coefficient to express the importance of sentence semantic information but cannot adjust the position distribution of contextual feature vectors in the semantic space. To address this issue, a radial basis function attention (RBF-attention) layer is proposed to adaptively regulate the position distribution of sequence contextual feature vectors, which can minimize the relative distance of within-category named entities and maximize the relative distance of between-category named entities in the semantic space. The experimental results on CoNLL2003 English and MSRA Chinese NER datasets indicate that the proposed model performs better than other baseline approaches without relying on any external feature engineering.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {30},
numpages = {18},
keywords = {RBF-attention, NER, self-attention, BiLSTM}
}

@article{10.1109/TASLP.2020.3042006,
author = {Duan, Chaoqun and Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro and Zhu, Conghui and Zhao, Tiejun},
title = {Modeling Future Cost for Neural Machine Translation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.3042006},
doi = {10.1109/TASLP.2020.3042006},
abstract = {Existing neural machine translation (NMT) systems utilize sequence-to-sequence neural networks to generate target translation word by word, and then make the generated word at each time-step and the counterpart in the references as consistent as possible. However, the trained translation model tends to focus on ensuring the accuracy of the generated target word at the current time-step and does not consider its future cost which means the expected cost of generating the subsequent target translation (i.e., the next target word). To respond to this issue, in this article, we propose a simple and effective method to model the future cost of each target word for NMT systems. In detail, a future cost representation is learned based on the current generated target word and its contextual information to compute an additional loss to guide the training of the NMT model. Furthermore, the learned future cost representation at the current time-step is used to help the generation of the next target word in the decoding. Experimental results on three widely-used translation datasets, including the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English, show that the proposed approach achieves significant improvements over strong Transformer-based NMT baseline.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {770–781},
numpages = {12}
}

@article{10.14778/3494124.3494129,
author = {Yuan, Ye and Ma, Delong and Wen, Zhenyu and Zhang, Zhiwei and Wang, Guoren},
title = {Subgraph matching over graph federation},
year = {2021},
issue_date = {November 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3494124.3494129},
doi = {10.14778/3494124.3494129},
abstract = {Many real-life applications require processing graph data across heterogeneous sources. In this paper, we define the graph federation that indicates that the graph data sources are temporarily federated and offer their data for users. Next, we propose a new framework FedGraph to efficiently and effectively perform subgraph matching, which is a crucial application in graph federation. FedGraph consists of three phases, including query decomposition, distributed matching, and distributed joining. We also develop new efficient approximation algorithms and apply them in each phase to attack the NP-hard problem. The evaluations are conducted in a real test bed using both real-life and synthetic graph datasets. FedGraph outperforms the state-of-the-art methods, reducing the execution time and communication cost by 37.3 \texttimes{} and 61.8 \texttimes{}, respectively.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {437–450},
numpages = {14}
}

@proceedings{10.1145/3617695,
title = {BDIOT '23: Proceedings of the 2023 7th International Conference on Big Data and Internet of Things},
year = {2023},
isbn = {9798400708015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3557915.3560999,
author = {Hong, Zhiqing and Yang, Heng and Wang, Haotian and Lyu, Wenjun and Yang, Yu and Wang, Guang and Liu, Yunhuai and Wang, Yang and Zhang, Desheng},
title = {FastAddr: real-time abnormal address detection via contrastive augmentation for location-based services},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557915.3560999},
doi = {10.1145/3557915.3560999},
abstract = {An address, a textual description of a physical location, plays an important role in location-based services such as on-demand delivery and e-commerce. However, abnormal addresses (i.e., an address without detailed information representing a spatial location) have led to significant costs. In real-world settings like e-commerce, abnormal address detection is not trivial because it needs to be completed in real-time to support massive online queries. In this study, we design FastAddr, a fast abnormal address detection framework, which detects abnormal addresses among millions of addresses in a short time. By investigating and modeling the hierarchical structure of address data, we first design a novel contrastive address augmentation approach to generate training data via learning the entity transition probability matrix. We further design a lightweight multi-head attention model for learning compact address representation by modeling the address characteristics. We conduct a comprehensive three-phase evaluation. (i) We evaluate FastAddr on a real-world dataset and it yields the average F1 of 85.7% in 0.058 milliseconds, which outperforms the state-of-the-art models by 47.4% with similar detection time. (ii) An offline A/B test shows that FastAddr outperforms the previous deployed model significantly. (iii) We also conduct an online A/B test to compare FastAddr with the deployed model, which shows an improvement of F1 by more than 20%. Moreover, a real-world case study demonstrates both the efficiency and effectiveness of FastAddr.},
booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
articleno = {64},
numpages = {10},
keywords = {contrastive augmentation, geocoding, multi-head attention, real-time abnormal address detection},
location = {Seattle, Washington},
series = {SIGSPATIAL '22}
}

@inproceedings{10.1145/3457682.3457751,
author = {Deng, Fei and Zhang, Dongdong and Peng, Jing},
title = {Biological Named Entity Recognition and Role Labeling via Deep Multi-task Learning},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457751},
doi = {10.1145/3457682.3457751},
abstract = {Bioscience is an experimental science. The qualitative and quantitative findings of the biological experiments are often exclusively available in the form of figures in published papers. In this paper, we introduce the SourceData model, which captures a key aspect of the biological experimental design by categorizing biological entity involved in the experiment into one of the six roles. Our work aims at determining whether a given entity is subjected to a perturbation or is the object of a measurement (entity role labeling) through automatic natural language algorithms. We use state-of-the-art transformer models (e.g., Bert and its variants) as a strong baseline, find that after jointly trained with biological named entity recognition task by deep multi-task learning (MTL), the F1 score gets improved by 2% compared to previous single-task architecture. Also, for named entity recognition task, the MTL method achieves comparable performance in five public datasets. Further analysis reveals the importance of fusing entity information at the input layer of entity role labeling task and incorporating global context.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {450–455},
numpages = {6},
keywords = {entity role labeling, figure caption, multi-task learning, named entity recognition},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@inproceedings{10.1145/3543507.3583250,
author = {Li, Yingjie and Zhao, Chenye and Caragea, Cornelia},
title = {TTS: A Target-based Teacher-Student Framework for Zero-Shot Stance Detection},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583250},
doi = {10.1145/3543507.3583250},
abstract = {The goal of zero-shot stance detection (ZSSD) is to identify the stance (in favor of, against, or neutral) of a text towards an unseen target in the inference stage. In this paper, we explore this problem from a novel angle by proposing a Target-based Teacher-Student learning (TTS) framework. Specifically, we first augment the training set by extracting diversified targets that are unseen during training with a keyphrase generation model. Then, we develop a teacher-student framework which effectively utilizes the augmented data. Extensive experiments show that our model significantly outperforms state-of-the-art ZSSD baselines on the available benchmark dataset for this task by 8.9% in macro-averaged F1. In addition, previous ZSSD requires human-annotated targets and labels during training, which may not be available in real-world applications. Therefore, we go one step further by proposing a more challenging open-world ZSSD task: identifying the stance of a text towards an unseen target without human-annotated targets and stance labels. We show that our TTS can be easily adapted to the new task. Remarkably, TTS without human-annotated targets and stance labels even significantly outperforms previous state-of-the-art ZSSD baselines trained with human-annotated data. We publicly release our code 1 to facilitate future research.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1500–1509},
numpages = {10},
keywords = {data augmentation, stance detection, zero-shot learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3581783.3612053,
author = {Li, Bobo and Fei, Hao and Liao, Lizi and Zhao, Yu and Teng, Chong and Chua, Tat-Seng and Ji, Donghong and Li, Fei},
title = {Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612053},
doi = {10.1145/3581783.3612053},
abstract = {It has been a hot research topic to enable machines to understand human emotions in multimodal contexts under dialogue scenarios, which is tasked with multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received consistent attention in recent years, where a diverse range of methods has been proposed for securing better task performance. Most existing works treat MM-ERC as a standard multimodal classification problem and perform multimodal feature disentanglement and fusion for maximizing feature utility. Yet after revisiting the characteristic of MM-ERC, we argue that both the feature multimodality and conversational contextualization should be properly modeled simultaneously during the feature disentanglement and fusion steps. In this work, we target further pushing the task performance by taking full consideration of the above insights. On the one hand, during feature disentanglement, based on the contrastive learning technique, we devise a Dual-level Disentanglement Mechanism (DDM) to decouple the features into both the modality space and utterance space. On the other hand, during the feature fusion stage, we propose a Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism (CRM) for multimodal and context integration, respectively. They together schedule the proper integrations of multimodal and context features. Specifically, CFM explicitly manages the multimodal feature contributions dynamically, while CRM flexibly coordinates the introduction of dialogue contexts. On two public MM-ERC datasets, our system achieves new state-of-the-art performance consistently. Further analyses demonstrate that all our proposed mechanisms greatly facilitate the MM-ERC task by making full use of the multimodal and context features adaptively. Note that our proposed methods have the great potential to facilitate a broader range of other conversational multimodal tasks.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {5923–5934},
numpages = {12},
keywords = {emotion recognition, multimodal learning},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3581641.3584034,
author = {Fok, Raymond and Kambhamettu, Hita and Soldaini, Luca and Bragg, Jonathan and Lo, Kyle and Hearst, Marti and Head, Andrew and Weld, Daniel S},
title = {Scim: Intelligent Skimming Support for Scientific Papers},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584034},
doi = {10.1145/3581641.3584034},
abstract = {Scholars need to keep up with an exponentially increasing flood of scientific papers. To aid this challenge, we introduce Scim, a novel intelligent interface that helps experienced researchers skim – or rapidly review – a paper to attain a cursory understanding of its contents. Scim supports the skimming process by highlighting salient paper contents in order to direct a reader’s attention. The system’s highlights are faceted by content type, evenly distributed across a paper, and have a density configurable by readers at both the global and local level. We evaluate Scim with both an in-lab usability study and a longitudinal diary study, revealing how its highlights facilitate the more efficient construction of a conceptualization of a paper. We conclude by discussing design considerations and tensions for the design of future intelligent skimming tools.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {476–490},
numpages = {15},
keywords = {Intelligent reading interfaces, highlights, scientific papers, skimming},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3491102.3517551,
author = {Wang, Yunlong and Venkatesh, Priyadarshini and Lim, Brian Y},
title = {Interpretable Directed Diversity: Leveraging Model Explanations for Iterative Crowd Ideation},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517551},
doi = {10.1145/3491102.3517551},
abstract = {Feedback in creativity support tools can help crowdworkers to improve their ideations. However, current feedback methods require human assessment from facilitators or peers. This is not scalable to large crowds. We propose Interpretable Directed Diversity to automatically predict ideation quality and diversity scores, and provide AI explanations — Attribution, Contrastive Attribution, and Counterfactual Suggestions — to feedback on why ideations were scored (low), and how to get higher scores. These explanations provide multi-faceted feedback as users iteratively improve their ideations. We conducted formative and controlled user studies to understand the usage and usefulness of explanations to improve ideation diversity and quality. Users appreciated that explanation feedback helped focus their efforts and provided directions for improvement. This resulted in explanations improving diversity compared to no feedback or feedback with scores only. Hence, our approach opens opportunities for explainable AI towards scalable and rich feedback for iterative crowd ideation and creativity support tools.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {183},
numpages = {28},
keywords = {Collective Creativity, Crowdsourcing, Diversity, Explainable AI},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3583780.3614923,
author = {Dong, Qian and Liu, Yiding and Ai, Qingyao and Li, Haitao and Wang, Shuaiqiang and Liu, Yiqun and Yin, Dawei and Ma, Shaoping},
title = {I3 Retriever: Incorporating Implicit Interaction in Pre-trained Language Models for Passage Retrieval},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614923},
doi = {10.1145/3583780.3614923},
abstract = {Passage retrieval is a fundamental task in many information systems, such as web search and question answering, where both efficiency and effectiveness are critical concerns. In recent years, neural retrievers based on pre-trained language models (PLM), such as dual-encoders, have achieved huge success. Yet, studies have found that the performance of dual-encoders are often limited due to the neglecting of the interaction information between queries and candidate passages. Therefore, various interaction paradigms have been proposed to improve the performance of vanilla dual-encoders. Particularly, recent state-of-the-art methods often introduce late-interaction during the model inference process. However, such late-interaction based methods usually bring extensive computation and storage cost on large corpus. Despite their effectiveness, the concern of efficiency and space footprint is still an important factor that limits the application of interaction-based neural retrieval models. To tackle this issue, we Incorporate Implicit Interaction into dual-encoders, and propose I3 retriever. In particular, our implicit interaction paradigm leverages generated pseudo-queries to simulate query-passage interaction, which jointly optimizes with query and passage encoders in an end-to-end manner. It can be fully pre-computed and cached, and its inference process only involves simple dot product operation of the query vector and passage vector, which makes it as efficient as the vanilla dual encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep Learning Datasets, demonstrating the I3 retriever's superiority in terms of both effectiveness and efficiency. Moreover, the proposed implicit interaction is compatible with special pre-training and knowledge distillation for passage retrieval, which brings a new state-of-the-art performance. The codes are available at https://github.com/Deriq-Qian-Dong/III-Retriever.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {441–451},
numpages = {11},
keywords = {language models, learning to rank, semantic matching},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3469213.3471370,
author = {Qiu, Jing},
title = {Design and Implementation of English Online Translation Software Based on Intelligent Proofreading System},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3471370},
doi = {10.1145/3469213.3471370},
abstract = {With the continuous development of modern information technology, the ways and means of learning English have become more diversified. Especially, the emergence of English electronic dictionary makes English learning more and more simple. This paper puts forward the design idea of English online translation software based on intelligent proofreading system. Firstly, the semantic ontology model is created by creating semantic ontology model and phrase translation combination translation algorithm. In addition, the overall system is designed. The hardware and software of the system are designed through the overall architecture of the system, and the hardware design is mainly the search module, so as to improve the search accuracy. In the process of software module, it mainly includes information transmission model, system network topology design and system function design, and can support translation and proofreading of multiple languages. Finally, the designed system is tested. The test results show that this method is accurate and intelligent in English automatic translation.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {339},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@inproceedings{10.1145/3485447.3512031,
author = {Yang, Aobo and Wang, Nan and Cai, Renqin and Deng, Hongbo and Wang, Hongning},
title = {Comparative Explanations of Recommendations},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512031},
doi = {10.1145/3485447.3512031},
abstract = {As recommendation is essentially a comparative (or ranking) process, a good explanation should illustrate to users why an item is believed to be better than another, i.e., comparative explanations about the recommended items. Ideally, after reading the explanations, a user should reach the same ranking of items as the system’s. Unfortunately, little research attention has yet been paid on such comparative explanations. In this work, we develop an extract-and-refine architecture to explain the relative comparisons among a set of ranked items from a recommender system. For each recommended item, we first extract one sentence from its associated reviews that best suits the desired comparison against a set of reference items. Then this extracted sentence is further articulated with respect to the target user through a generative model to better explain why the item is recommended. We design a new explanation quality metric based on BLEU to guide the end-to-end training of the extraction and refinement components, which avoids generation of generic content. Extensive offline evaluations on two large recommendation benchmark datasets and serious user studies against an array of state-of-the-art explainable recommendation algorithms demonstrate the necessity of comparative explanations and the effectiveness of our solution.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3113–3123},
numpages = {11},
keywords = {comparative explanation, explainable recommendation, extract-and-refine, text generation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3575882,
title = {IC3INA '22: Proceedings of the 2022 International Conference on Computer, Control, Informatics and Its Applications},
year = {2022},
isbn = {9781450397902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Indonesia}
}

@article{10.1145/3582495,
author = {Zhou, Kun and Wang, Hui and Wen, Ji-rong and Zhao, Wayne Xin},
title = {Enhancing Multi-View Smoothness for Sequential Recommendation Models},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3582495},
doi = {10.1145/3582495},
abstract = {Sequential recommendation models aim to predict the interested items to a user based on his historical behaviors. To train sequential recommenders, implicit feedback data is widely adopted since it is easier to obtain than explicit feedback data. In the setting of implicit feedback, a user’s historical behaviors can be characterized as a chronologically ordered sequence of interacted items. From a perspective of machine learning, the historical interaction sequence and the recommended items can be considered as context and label, respectively, which are usually in one-hot representations in the recommendation models.However, due to the discrete nature, one-hot representations are hard to sufficiently reflect the underlying user preference, and might also contain noise from implicit feedback that will mislead the model training. To solve these issues, we propose a general optimization framework, Multi-View Smoothness (MVS), to enhance the smoothness of sequential recommendation models in both data representations and model learning. Specifically, with the help of a complementary model, we smooth and enrich the one-hot representations of contexts and labels to better depict the underlying user preference (i.e., context smoothness and label smoothness), and devise a model regularization strategy to enforce the neighborhood smoothness of the model itself (i.e., model smoothness). Based on these strategies, we design three regularizers to constrain and improve the training of sequential recommendation models. Extensive experiments on five datasets show that our approach is able to improve the performance of various base models consistently and outperform other regularization training methods.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {107},
numpages = {27},
keywords = {Sequential recommendation, data smoothness, model smoothness}
}

@inproceedings{10.1145/3510003.3510068,
author = {Wan, Chengcheng and Liu, Shicheng and Xie, Sophie and Liu, Yifan and Hoffmann, Henry and Maire, Michael and Lu, Shan},
title = {Automated testing of software that uses machine learning APIs},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510068},
doi = {10.1145/3510003.3510068},
abstract = {An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API.This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically generate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evaluation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many previously unknown bugs.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {212–224},
numpages = {13},
keywords = {machine learning, machine learning API, software testing},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@proceedings{10.1145/3603781,
title = {CNIOT '23: Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@inproceedings{10.1145/3404835.3463234,
author = {Jia, Qinglin and Li, Jingjie and Zhang, Qi and He, Xiuqiang and Zhu, Jieming},
title = {RMBERT: News Recommendation via Recurrent Reasoning Memory Network over BERT},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463234},
doi = {10.1145/3404835.3463234},
abstract = {Personalized news recommendation aims to alleviate information overload and help users find news of their interests. Accurately matching candidate news and users' interests is the key to news recommendation. Most existing methods separately encode each user and news into vectors by news contents and then match the two vectors. However, a user's interest may differ in each news or each topic of one news. It's necessary to dynamically learn user and news vector and model their interaction. In this work, we present Recurrent Reasoning Memory Network over BERT (RMBERT) for news recommendation. Compared with other methods, our approach can leverage the ability of content modeling from BERT. Moreover, the recurrent reasoning memory network which performs a series of attention based reasoning steps can dynamically learn user and news vector and model their interaction in each step. As a result, our approach can better model user's interests. We conduct extensive experiments on a real-world news recommendation dataset and the results show that our approach significantly outperforms existing state-of-the-art methods.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1773–1777},
numpages = {5},
keywords = {attention model, deep neural network, news recommendation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{10.1145/3622863,
author = {Chen, Qiaochu and Banerjee, Arko and Demiralp, \c{C}a\u{g}atay and Durrett, Greg and Dillig, I\c{s}\i{}l},
title = {Data Extraction via Semantic Regular Expression Synthesis},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622863},
doi = {10.1145/3622863},
abstract = {Many data extraction tasks of practical relevance require not only syntactic pattern matching but also semantic reasoning about the content of the underlying text. While regular expressions are very well suited for tasks that require only syntactic pattern matching, they fall short for data extraction tasks that involve both a syntactic and semantic component. To address this issue, we introduce semantic regexes, a generalization of regular expressions that facilitates combined syntactic and semantic reasoning about textual data. We also propose a novel learning algorithm that can synthesize semantic regexes from a small number of positive and negative examples. Our proposed learning algorithm uses a combination of neural sketch generation and compositional type-directed synthesis for fast and effective generalization from a small number of examples.  We have implemented these ideas in a new tool called Smore and evaluated it on representative data extraction tasks involving several textual datasets. Our evaluation shows that semantic regexes can better support complex data extraction tasks than standard regular expressions and that our learning algorithm significantly outperforms existing tools, including state-of-the-art neural networks and program synthesis tools.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {287},
numpages = {30},
keywords = {Program Synthesis, Regular Expression}
}

@proceedings{10.1145/3625469,
title = {IMMS '23: Proceedings of the 2023 6th International Conference on Information Management and Management Science},
year = {2023},
isbn = {9798400707681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chengdu, China}
}

@inproceedings{10.1145/3447548.3467227,
author = {Xiong, Hao and Yan, Junchi and Pan, Li},
title = {Contrastive Multi-View Multiplex Network Embedding with Applications to Robust Network Alignment},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467227},
doi = {10.1145/3447548.3467227},
abstract = {Despite its success in learning network node representations, network embedding is still relatively new for multiplex networks (MNs) with multiple types of edges. In such networks, the inter-layer anchor links are usually missing, which represent the alignment relations between nodes on different layers and are a crucial prerequisite for many cross-network applications like network alignment. For mining such anchor links between layers for MNs, multiplex network embedding (MNE) has become one of the most promising techniques. In this paper, we consider two problems for MNs: 1) edges can be missing to different extent, and data augmentation may mitigate this issue; 2) the known alignment anchor links between layers can be misleading since the behaviors of nodes on different layers are not always consistent, so the most informative ones should be emphasized compared with those misleading ones. However, most existing works neglect the two problems and simply 1) adopt one structural view for all the layers (e.g. random walk with the same window size) and 2) equally extract information from all the anchor links. We propose an end-to-end contrastive framework called cM2NE for MNE, utilizing multiple structural views for each layer and learning with several plug-in components for different scenarios. Through end-to-end optimization on three levels, the intra-view, inter-view, and inter-layer level, our framework achieves to select the fitted views for different layers and maximize the inter-layer mutual information by emphasizing those most informative anchor links. Extensive experimental results on real-world datasets for node classification and multi-network alignment show that our approach consistently outperforms peer methods.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {1913–1923},
numpages = {11},
keywords = {contrastive learning, multiple structural views, multiplex network embedding, network alignment, node classification},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@article{10.1145/3581786,
author = {Dusart, Alexis and Pinel-Sauvagnat, Karen and Hubert, Gilles},
title = {TSSuBERT: How to Sum Up Multiple Years of Reading in a Few Tweets},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3581786},
doi = {10.1145/3581786},
abstract = {The development of deep neural networks and the emergence of pre-trained language models such as BERT allow to increase performance on many NLP tasks. However, these models do not meet the same popularity for tweet stream summarization, which is probably because their computation limitation requires to drastically truncate the textual input.Our contribution in this article is threefold. First, we propose a neural model to automatically and incrementally summarize huge tweet streams. This extractive model combines in an original way pre-trained language models and vocabulary frequency based representations to predict tweet salience. An additional advantage of the model is that it automatically adapts the size of the output summary according to the input tweet stream. Second, we detail an original methodology to construct tweet stream summarization datasets requiring little human effort. Third, we release the TES 2012-2016 dataset constructed using the aforementioned methodology. Baselines, oracle summaries, gold standard, and qualitative assessments are made publicly available.To evaluate our approach, we conducted extensive quantitative experiments using three different tweet collections as well as an additional qualitative evaluation. Results show that our method outperforms state-of-the-art ones. We believe that this work opens avenues of research for incremental summarization, which has not received much attention yet.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {109},
numpages = {33},
keywords = {Text summarization, tweet stream, Twitter, BERT, dataset}
}

@article{10.1145/3592601,
author = {Gharagozlou, Hamid and Mohammadzadeh, Javad and Bastanfard, Azam and Ghidary, Saeed Shiry},
title = {Semantic Relation Extraction: A Review of Approaches, Datasets, and Evaluation Methods With Looking at the Methods and Datasets in the Persian Language},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3592601},
doi = {10.1145/3592601},
abstract = {A large volume of unstructured data, especially text data, is generated and exchanged daily. Consequently, the importance of extracting patterns and discovering knowledge from textual data is significantly increasing. As the task of automatically recognizing the relations between two or more entities, semantic relation extraction has a prominent role in the exploitation of raw text. This article surveys different approaches and types of relation extraction in English and the most prominent proposed methods in Persian. We also introduce, analyze, and compare the most important datasets available for relation extraction in Persian and English. Furthermore, traditional and emerging evaluation metrics for supervised, semi-supervised, and unsupervised methods are described, along with pointers to commonly used performance evaluation datasets. Finally, we briefly describe challenges in extracting relationships in Persian and English and dataset creation challenges.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {189},
numpages = {29},
keywords = {Semantic relations, relation extraction, Natural Language Processing (NLP), automatic extraction, Persian text processing, linguistics, dataset, evaluation methods, information extraction}
}

@inproceedings{10.1145/3442381.3450111,
author = {Pelrine, Kellin and Danovitch, Jacob and Rabbany, Reihaneh},
title = {The Surprising Performance of Simple Baselines for Misinformation Detection},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450111},
doi = {10.1145/3442381.3450111},
abstract = {As social media becomes increasingly prominent in our day to day lives, it is increasingly important to detect informative content and prevent the spread of disinformation and unverified rumours. While many sophisticated and successful models have been proposed in the literature, they are often compared with older NLP baselines such as SVMs, CNNs, and LSTMs. In this paper, we examine the performance of a broad set of modern transformer-based language models and show that with basic fine-tuning, these models are competitive with and can even significantly outperform recently proposed state-of-the-art methods. We present our framework as a baseline for creating and evaluating new methods for misinformation detection. We further study a comprehensive set of benchmark datasets, and discuss potential data leakage and the need for careful design of the experiments and understanding of datasets to account for confounding variables. As an extreme case example, we show that classifying only based on the first three digits of tweet ids, which contain information on the date, gives state-of-the-art performance on a commonly used benchmark dataset for fake news detection –Twitter16. We provide a simple tool to detect this problem and suggest steps to mitigate it in future datasets.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3432–3441},
numpages = {10},
keywords = {COVID-19, datasets, misinformation, natural language processing, social media},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@proceedings{10.1145/3584684,
title = {ApPLIED 2023: Proceedings of the 5th workshop on Advanced tools, programming languages, and PLatforms for Implementing and Evaluating algorithms for Distributed systems},
year = {2023},
isbn = {9798400701283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@inproceedings{10.1145/3583780.3614896,
author = {Liu, Meizhen and He, Jiakai and Guo, Xu and Chen, Jianye and Hui, Siu Cheung and Zhou, Fengyu},
title = {GranCATs: Cross-Lingual Enhancement through Granularity-Specific Contrastive Adapters},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614896},
doi = {10.1145/3583780.3614896},
abstract = {Multilingual language models (MLLMs) have demonstrated remarkable success in various cross-lingual downstream tasks, facilitating the transfer of knowledge across numerous languages, whereas this transfer is not universally effective. Our study reveals that while existing MLLMs like mBERT can capturephrase-level alignments across the language families, they struggle to effectively capturesentence-level andparagraph-level alignments. To address this limitation, we propose GranCATs, Granularity-specific Contrastive AdapTers. We collect a new dataset that observes each sample at three distinct levels of granularity and employ contrastive learning as a pre-training task to train GranCATs on this dataset. Our objective is to enhance MLLMs' adaptation to a broader range of cross-lingual tasks by equipping them with improved capabilities to capture global information at different levels of granularity. Extensive experiments show that MLLMs with GranCATs yield significant performance advancements across various language tasks with different text granularities, including entity alignment, relation extraction, sentence classification and retrieval, and question-answering. These results validate the effectiveness of our proposed GranCATs in enhancing cross-lingual alignments across various text granularities and effectively transferring this knowledge to downstream tasks.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {1461–1471},
numpages = {11},
keywords = {adapters, cross-lingual alignments, multilingual language models, universal patterns},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3543507.3583513,
author = {Zhang, Chi and Chen, Rui and Zhao, Xiangyu and Han, Qilong and Li, Li},
title = {Denoising and Prompt-Tuning for Multi-Behavior Recommendation},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583513},
doi = {10.1145/3543507.3583513},
abstract = {In practical recommendation scenarios, users often interact with items under multi-typed behaviors (e.g., click, add-to-cart, and purchase). Traditional collaborative filtering techniques typically assume that users only have a single type of behavior with items, making it insufficient to utilize complex collaborative signals to learn informative representations and infer actual user preferences. Consequently, some pioneer studies explore modeling multi-behavior heterogeneity to learn better representations and boost the performance of recommendations for a target behavior. However, a large number of auxiliary behaviors (i.e., click and add-to-cart) could introduce irrelevant information to recommenders, which could mislead the target behavior (i.e., purchase) recommendation, rendering two critical challenges: (i) denoising auxiliary behaviors and (ii) bridging the semantic gap between auxiliary and target behaviors. Motivated by the above observation, we propose a novel framework–Denoising and Prompt-Tuning (DPT) with a three-stage learning paradigm to solve the aforementioned challenges. In particular, DPT is equipped with a pattern-enhanced graph encoder in the first stage to learn complex patterns as prior knowledge in a data-driven manner to guide learning informative representation and pinpointing reliable noise for subsequent stages. Accordingly, we adopt different lightweight tuning approaches with effectiveness and efficiency in the following stages to further attenuate the influence of noise and alleviate the semantic gap among multi-typed behaviors. Extensive experiments on two real-world datasets demonstrate the superiority of DPT over a wide range of state-of-the-art methods. The implementation code is available online at https://github.com/zc-97/DPT.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1355–1363},
numpages = {9},
keywords = {Multi-behavior recommendation, auxiliary behavior denoising, graph neural networks, prompt tuning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@proceedings{10.1145/3570991,
title = {CODS-COMAD '23: Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Mumbai, India}
}

@proceedings{10.1145/3450569,
title = {SACMAT '21: Proceedings of the 26th ACM Symposium on Access Control Models and Technologies},
year = {2021},
isbn = {9781450383653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the ACM Symposium on Access Control Models and Technologies (SACMAT 2021). This year's symposium continues its tradition of being the premier forum for the presentation of research results and experience reports on leading-edge issues of access control, including models, systems, applications, and theory, while also embracing a renovated focus on the general area of security.The aim of the symposium is to share novel access control and security solutions that fulfill the needs of heterogeneous applications and environments, and to identify new directions for future research and development.SACMAT provides researchers and practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and security.},
location = {Virtual Event, Spain}
}

@article{10.1145/3491206,
author = {Zhou, Jingya and Liu, Ling and Wei, Wenqi and Fan, Jianxi},
title = {Network Representation Learning: From Preprocessing, Feature Extraction to Node Embedding},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3491206},
doi = {10.1145/3491206},
abstract = {Network representation learning (NRL) advances the conventional graph mining of social networks, knowledge graphs, and complex biomedical and physics information networks. Dozens of NRL algorithms have been reported in the literature. Most of them focus on learning node embeddings for homogeneous networks, but they differ in the specific encoding schemes and specific types of node semantics captured and used for learning node embedding. This article reviews the design principles and the different node embedding techniques for NRL over homogeneous networks. To facilitate the comparison of different node embedding algorithms, we introduce a unified reference framework to divide and generalize the node embedding learning process on a given network into preprocessing steps, node feature extraction steps, and node embedding model training for an NRL task such as link prediction and node clustering. With this unifying reference framework, we highlight the representative methods, models, and techniques used at different stages of the node embedding model learning process. This survey not only helps researchers and practitioners gain an in-depth understanding of different NRL techniques but also provides practical guidelines for designing and developing the next generation of NRL algorithms and systems.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {38},
numpages = {35},
keywords = {Network representation learning, data preprocessing, feature extraction, node embedding}
}

@article{10.1145/3604552,
author = {Qin, Chuan and Zhu, Hengshu and Shen, Dazhong and Sun, Ying and Yao, Kaichun and Wang, Peng and Xiong, Hui},
title = {Automatic Skill-Oriented Question Generation and Recommendation for Intelligent Job Interviews},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3604552},
doi = {10.1145/3604552},
abstract = {Job interviews are the most widely accepted method for companies to select suitable candidates, and a critical challenge is finding the right questions to ask job candidates. Moreover, there is a lack of integrated tools for automatically generating interview questions and recommending the right questions to interviewers. To this end, in this paper, we propose an intelligent system for assisting job interviews, namely, DuerQues. To build this system, we first investigate how to automatically generate skill-oriented interview questions in a scalable way by learning external knowledge from online knowledge-sharing communities. Along this line, we develop a novel distantly supervised skill entity recognition method to identify skill entities from large-scale search queries and web page titles with less need for human annotation. Additionally, we propose a neural generative model for generating skill-oriented interview questions. In particular, we introduce a data-driven solution to create high-quality training instances and design a learning algorithm to improve the performance of question generation. Furthermore, we exploit click-through data from query logs and design a recommender system for recommending suitable questions to interviewers. Specifically, we introduce a graph-enhanced algorithm to efficiently recommend suitable questions given a set of queried skills. Finally, extensive experiments on real-world datasets demonstrate the effectiveness of our DuerQues system in terms of the quality of generated skill-oriented questions and the performance of question recommendation.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {27},
numpages = {32},
keywords = {Job interview assessment, question generation, question recommendation}
}

@inproceedings{10.1145/3577190.3614158,
author = {Hensel, Laura Birka and Yongsatianchot, Nutchanon and Torshizi, Parisa and Minucci, Elena and Marsella, Stacy},
title = {Large language models in textual analysis for gesture selection},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3614158},
doi = {10.1145/3577190.3614158},
abstract = {Gestures perform a variety of communicative functions that powerfully influence human face-to-face interaction. How this communicative function is achieved varies greatly between individuals and depends on the role of the speaker and the context of the interaction. Approaches to automatic gesture generation vary not only in the degree to which they rely on data-driven techniques but also the degree to which they can produce context and speaker specific gestures. However, these approaches face two major challenges: The first is obtaining sufficient training data that is appropriate for the context and the goal of the application. The second is related to designer control to realize their specific intent for the application. Here, we approach these challenges by using large language models (LLMs) to show that these powerful models of large amounts of data can be adapted for gesture analysis and generation. Specifically, we used ChatGPT as a tool for suggesting context-specific gestures that can realize designer intent based on minimal prompts. We also find that ChatGPT can suggests novel yet appropriate gestures not present in the minimal training data. The use of LLMs is a promising avenue for gesture generation that reduce the need for laborious annotations and has the potential to flexibly and quickly adapt to different designer intents.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {378–387},
numpages = {10},
keywords = {gesture analysis, gesture selection, large language models},
location = {Paris, France},
series = {ICMI '23}
}

@article{10.1145/3609796,
author = {Ma, Yixiao and Wu, Yueyue and Ai, Qingyao and Liu, Yiqun and Shao, Yunqiu and Zhang, Min and Ma, Shaoping},
title = {Incorporating Structural Information into Legal Case Retrieval},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3609796},
doi = {10.1145/3609796},
abstract = {Legal case retrieval has received increasing attention in recent years. However, compared to ad hoc retrieval tasks, legal case retrieval has its unique challenges. First, case documents are rather lengthy and contain complex legal structures. Therefore, it is difficult for most existing dense retrieval models to encode an entire document and capture its inherent complex structure information. Most existing methods simply truncate part of the document content to meet the input length limit of PLMs, which will lead to information loss. Additionally, the definition of relevance in the legal domain differs from that in the general domain. Previous semantic-based or lexical-based methods fail to provide a comprehensive understanding of the relevance of legal cases. In this article, we propose a Structured Legal case Retrieval (SLR) framework, which incorporates internal and external structural information to address the above two challenges. Specifically, to avoid the truncation of long legal documents, the internal structural information, which is the organization pattern of legal documents, can be utilized to split a case document into segments. By dividing the document-level semantic matching task into segment-level subtasks, SLR can separately process segments using different methods based on the characteristic of each segment. In this way, the key elements of a case document can be highlighted without losing other content information. Second, toward a better understanding of relevance in the legal domain, we investigate the connections between criminal charges appearing in large-scale case corpus to generate a chargewise relation graph. Then, the similarity between criminal charges can be pre-computed as the external structural information to enhance the recognition of relevant cases. Finally, a learning-to-rank algorithm integrates the features collected from internal and external structures to output the final retrieval results. Experimental results on public legal case retrieval benchmarks demonstrate the superior effectiveness of SLR over existing state-of-the-art baselines, including traditional bag-of-words and neural-based methods. Furthermore, we conduct a case study to visualize how the proposed model focuses on key elements and improves retrieval performance.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {40},
numpages = {28},
keywords = {Legal case retrieval, structural information, relevance}
}

@article{10.1109/TCBB.2021.3079339,
author = {Peng, Keqin and Yin, Chuantao and Rong, Wenge and Lin, Chenghua and Zhou, Deyu and Xiong, Zhang},
title = {Named Entity Aware Transfer Learning for Biomedical Factoid Question Answering},
year = {2021},
issue_date = {July-Aug. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3079339},
doi = {10.1109/TCBB.2021.3079339},
abstract = {Biomedical factoid question answering is an important task in biomedical question answering applications. It has attracted much attention because of its reliability. In question answering systems, better representation of words is of great importance, and proper word embedding can significantly improve the performance of the system. With the success of pretrained models in general natural language processing tasks, pretrained models have been widely used in biomedical areas, and many pretrained model-based approaches have been proven effective in biomedical question-answering tasks. In addition to proper word embedding, name entities also provide important information for biomedical question answering. Inspired by the concept of transfer learning, in this study, we developed a mechanism to fine-tune BioBERT with a named entity dataset to improve the question answering performance. Furthermore, we applied BiLSTM to encode the question text to obtain sentence-level information. To better combine the question level and token level information, we use bagging to further improve the overall performance. The proposed framework was evaluated on BioASQ 6b and 7b datasets, and the results have shown that our proposed framework can outperform all baselines.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {2365–2376},
numpages = {12}
}

@inproceedings{10.1145/3437963.3441833,
author = {Rosin, Guy D. and Guy, Ido and Radinsky, Kira},
title = {Event-Driven Query Expansion},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441833},
doi = {10.1145/3437963.3441833},
abstract = {A significant number of event-related queries are issued in Web search. In this paper, we seek to improve retrieval performance by leveraging events and specifically target the classic task of query expansion. We propose a method to expand an event-related query by first detecting the events related to it. Then, we derive the candidates for expansion as terms semantically related to both the query and the events. To identify the candidates, we utilize a novel mechanism to simultaneously embed words and events in the same vector space. We show that our proposed method of leveraging events improves query expansion performance significantly compared with state-of-the-art methods on various newswire TREC datasets.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {391–399},
numpages = {9},
keywords = {query expansion, temporal semantics, word embeddings},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3543507.3583191,
author = {Zhang, Xuefeng and Zhang, Richong and Li, Xiaoyang and Kong, Fanshuang and Chen, Junfan and Mensah, Samuel and Mao, Yongyi},
title = {Word Sense Disambiguation by Refining Target Word Embedding},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583191},
doi = {10.1145/3543507.3583191},
abstract = {Word Sense Disambiguation (WSD) which aims to identify the correct sense of a target word appearing in a specific context is essential for web text analysis. The use of glosses has been explored as a means for WSD. However, only a few works model the correlation between the target context and gloss. We add to the body of literature by presenting a model that employs a multi-head attention mechanism on deep contextual features of the target word and candidate glosses to refine the target word embedding. Furthermore, to encourage the model to learn the relevant part of target features that align with the correct gloss, we recursively alternate attention on target word features and that of candidate glosses to gradually extract the relevant contextual features of the target word, refining its representation and strengthening the final disambiguation results. Empirical studies on the five most commonly used benchmark datasets show that our proposed model is effective and achieves state-of-the-art results.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1405–1414},
numpages = {10},
keywords = {Embedding Refinement, Recursive Attention, Word Sense Disambiguation},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3448016.3452763,
author = {Ho, Vinh Thinh and Pal, Koninika and Weikum, Gerhard},
title = {QuTE: Answering Quantity Queries from Web Tables},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3452763},
doi = {10.1145/3448016.3452763},
abstract = {Quantities are financial, technological, physical and other measures that denote relevant properties of entities, such as revenue of companies, energy efficiency of cars or distance and brightness of stars and galaxies. Queries with filter conditions on quantities are an important building block for downstream analytics, and pose challenges when the content of interest is spread across a huge number of web tables and other ad-hoc datasets. Search engines support quantity lookups, but largely fail on quantity filters. The QuTE system presented in this paper aims to overcome these problems. It comprises methods for automatically extracting entity-quantity facts from web tables, as well as methods for online query processing, with new techniques for query matching and answer ranking.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {2740–2744},
numpages = {5},
keywords = {information extraction, quantity search, web tables},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/3474085.3476968,
author = {Zheng, Changmeng and Feng, Junhao and Fu, Ze and Cai, Yi and Li, Qing and Wang, Tao},
title = {Multimodal Relation Extraction with Efficient Graph Alignment},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3476968},
doi = {10.1145/3474085.3476968},
abstract = {Relation extraction (RE) is a fundamental process in constructing knowledge graphs. However, previous methods on relation extraction suffer sharp performance decline in short and noisy social media texts due to a lack of contexts. Fortunately, the related visual contents (objects and their relations) in social media posts can supplement the missing semantics and help to extract relations precisely. We introduce the multimodal relation extraction (MRE), a task that identifies textual relations with visual clues. To tackle this problem, we present a large-scale dataset which contains 15000+ sentences with 23 pre-defined relation categories. Considering that the visual relations among objects are corresponding to textual relations, we develop a dual graph alignment method to capture this correlation for better performance. Experimental results demonstrate that visual contents help to identify relations more precisely against the text-only baselines. Besides, our alignment method can find the correlations between vision and language, resulting in better performance. Our dataset and code are available at https://github.com/thecharm/Mega.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {5298–5306},
numpages = {9},
keywords = {graph alignment, multimodal dataset, multimodal relation extraction},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3579142.3594293,
author = {Sun, Ricky and Chen, Jamie},
title = {Design of Highly Scalable Graph Database Systems without Exponential Performance Degradation},
year = {2023},
isbn = {9798400700934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579142.3594293},
doi = {10.1145/3579142.3594293},
abstract = {The main challenge faced by today's graph database systems is sacrificing performance (computation) for scalability (storage). Such systems probably can store a large amount of data across many instances but can't offer adequate graph-computing power to deeply penetrate dynamic graph dataset in real time. A seemingly simple and intuitive graph query like K-hop traversal or finding all shortest paths may lead to deep traversal of large amount of graph data, which tends to cause a typical BSP (Bulky Synchronous Processing) system to exchange heavily amongst its distributed instances, therefore causing significant latencies. This paper proposes three schools of architectural designs for distributed and horizontally scalable graph database while achieving highly performant graph data processing capabilities. The first school, coined HTAP, augments distributed consensus algorithm RAFT paired with vector-based computing acceleration to achieve fast online data ingestion and real-time deep-data traversal in a TP and AP hybrid mode. The second school, named as GRID, leverages human-intelligence for data partitioning, and preserving the HTAP data processing capabilities across all partitioned clusters. The last school incorporates SHARD and advanced GQL optimization techniques to allow data partitioning to be done fully automated yet strive to achieve lower latency via minimum I/O cost data migration model when queries spread across multiple clusters.},
booktitle = {Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},
articleno = {4},
numpages = {6},
keywords = {distributed graph database, GQL, graph query optimization, graph data modeling, graph analytics, XAI, HTAP, grid, shard, linear scalability, deep traversal, real-time data processing},
location = {Seattle, WA, USA},
series = {BiDEDE '23}
}

@proceedings{10.5555/3606013,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@article{10.1145/3465055,
author = {Chaudhari, Sneha and Mithal, Varun and Polatkan, Gungor and Ramanath, Rohan},
title = {An Attentive Survey of Attention Models},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3465055},
doi = {10.1145/3465055},
abstract = {Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy that groups existing techniques into coherent categories. We review salient neural architectures in which attention has been incorporated and discuss applications in which modeling attention has shown a significant impact. We also describe how attention has been used to improve the interpretability of neural networks. Finally, we discuss some future research directions in attention. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {53},
numpages = {32},
keywords = {Attention, attention models, neural networks}
}

@article{10.1145/3554734,
author = {Nag, Arijit and Samanta, Bidisha and Mukherjee, Animesh and Ganguly, Niloy and Chakrabarti, Soumen},
title = {Transfer Learning for Low-Resource Multilingual Relation Classification},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3554734},
doi = {10.1145/3554734},
abstract = {Relation classification (sometimes called relation extraction) requires trustworthy datasets for fine-tuning large language models, as well as for evaluation. Data collection is challenging for Indian languages, because they are syntactically and morphologically diverse, as well as different from resource-rich languages like English. Despite recent interest in deep generative models for Indian languages, relation classification is still not well served by public datasets. In response, we present IndoRE, a dataset with 21K entity- and relation-tagged gold sentences in three Indian languages (Bengali, Hindi, and Telugu), plus English. We start with a multilingual BERT (mBERT)-based system that captures entity span positions and type information, and provides competitive performance on monolingual relation classification. Using this baseline system, we explore transfer mechanisms between languages and the scope to reduce expensive data annotation while achieving reasonable relation extraction performance. Specifically, we
(a)study the accuracy-efficiency trade-off between expensive, manually labeled gold instances vs. automatically translated and aligned silver instances to train a relation extractor,(b)device a simple mechanism for budgeted gold data annotation by intelligently converting distant-supervised silver training instances to gold training instances with human annotators using active learning, and finally(c)propose an ensemble model to provide a performance boost over that achieved via limited gold training instances.
 We release the dataset for future research.1},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {50},
numpages = {24},
keywords = {Relation extraction}
}

@inproceedings{10.1145/3543507.3583238,
author = {Yang, Yuting and Lei, Wenqiang and Huang, Pei and Cao, Juan and Li, Jintao and Chua, Tat-Seng},
title = {A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583238},
doi = {10.1145/3543507.3583238},
abstract = {Dialogue State Tracking (DST) module is an essential component of task-oriented dialog systems to understand users’ goals and needs. Collecting dialogue state labels including slots and values can be costly, requiring experts to annotate all (slot, value) information for each turn in dialogues. It is also difficult to define all possible slots and values in advance, especially with the wide application of dialogue systems in more and more new-rising applications. In this paper, we focus on improving DST module to generate dialogue states in circumstances with limited annotations and knowledge about slot ontology. To this end, we design a dual prompt learning framework for few-shot DST. The dual framework aims to explore how to utilize the language understanding and generation capabilities of pre-trained language models for DST efficiently. Specifically, we consider the learning of slot generation and value generation as dual tasks, and two kinds of prompts are designed based on this dual structure to incorporate task-related knowledge of these two tasks respectively. In this way, the DST task can be formulated as a language modeling task efficiently under few-shot settings. To evaluate the proposed framework, we conduct experiments on two task-oriented dialogue datasets. The results demonstrate that the proposed method not only outperforms existing state-of-the-art few-shot methods, but also can generate unseen slots. It indicates that DST-related knowledge can be probed from pre-trained language models and utilized to address low-resource DST efficiently with the help of prompt learning.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1468–1477},
numpages = {10},
keywords = {dialogue state tracking, few-shot learning, prompt learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.1145/3579602,
author = {Liu, Yiren and Mayfield, Ryan and Huang, Yun},
title = {Discovering the Hidden Facts of User-Dispatcher Interactions via Text-based Reporting Systems for Community Safety},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579602},
doi = {10.1145/3579602},
abstract = {Recently, an increasing number of safety organizations in the U.S. have incorporated text-based risk reporting systems to respond to safety incident reports from their community members. To gain a better understanding of the interaction between community members and dispatchers using text-based risk reporting systems, this study conducts a system log analysis ofLiveSafe, a community safety reporting system, to provide empirical evidence of the conversational patterns between users and dispatchers using both quantitative and qualitative methods. We created an ontology to capture information (e.g., location, attacker, target, weapon, start-time, and end-time, etc.) that dispatchers often collected from users regarding their incident tips. Applying the proposed ontology, we found that dispatchers often asked users for different information across varied event types (e.g.,Attacker forAbuse andAttack events,Target forHarassment events). Additionally, using emotion detection and regression analysis, we found an inconsistency in dispatchers' emotional support and responsiveness to users' messages between different organizations and between incident categories. The results also showed that users had a higher response rate and responded quicker when dispatchers provided emotional support. These novel findings brought significant insights to both practitioners and system designers, e.g., AI-based solutions to augment human agents' skills for improved service quality.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {126},
numpages = {31},
keywords = {conversation analysis, emergency dispatcher, live chat, safety reporting, text-based reporting system}
}

@article{10.14778/3554821.3554899,
author = {Fan, Wenfei},
title = {Big graphs: challenges and opportunities},
year = {2022},
issue_date = {August 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3554821.3554899},
doi = {10.14778/3554821.3554899},
abstract = {Big data is typically characterized with 4V's: Volume, Velocity, Variety and Veracity. When it comes to big graphs, these challenges become even more staggering. Each and every of the 4V's raises new questions, from theory to systems and practice. Is it possible to parallelize sequential graph algorithms and guarantee the correctness of the parallelized computations? Given a computational problem, does there exist a parallel algorithm for it that guarantees to reduce parallel runtime when more machines are used? Is there a systematic method for developing incremental algorithms with effectiveness guarantees in response to frequent updates? Is it possible to write queries across relational databases and semistructured graphs in SQL? Can we unify logic rules and machine learning, to improve the quality of graph-structured data, and deduce associations between entities? This paper aims to incite interest and curiosity in these topics. It raises as many questions as it answers.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3782–3797},
numpages = {16}
}

@inproceedings{10.1145/3543507.3583379,
author = {Hou, Zhenyu and He, Yufei and Cen, Yukuo and Liu, Xiao and Dong, Yuxiao and Kharlamov, Evgeny and Tang, Jie},
title = {GraphMAE2: A Decoding-Enhanced Masked Self-Supervised Graph Learner},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583379},
doi = {10.1145/3543507.3583379},
abstract = {Graph self-supervised learning (SSL), including contrastive and generative approaches, offers great potential to address the fundamental challenge of label scarcity in real-world graph data. Among both sets of graph SSL techniques, the masked graph autoencoders (e.g., GraphMAE)—one type of generative methods—have recently produced promising results. The idea behind this is to reconstruct the node features (or structures)—that are randomly masked from the input—with the autoencoder architecture. However, the performance of masked feature reconstruction naturally relies on the discriminability of the input features and is usually vulnerable to disturbance in the features. In this paper, we present a masked self-supervised learning framework1 GraphMAE2 with the goal of overcoming this issue. The idea is to impose regularization on feature reconstruction for graph SSL. Specifically, we design the strategies of multi-view random re-mask decoding and latent representation prediction to regularize the feature reconstruction. The multi-view random re-mask decoding is to introduce randomness into reconstruction in the feature space, while the latent representation prediction is to enforce the reconstruction in the embedding space. Extensive experiments show that GraphMAE2 can consistently generate top results on various public datasets, including at least 2.45% improvements over state-of-the-art baselines on ogbn-Papers100M with 111M nodes and 1.6B edges.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {737–746},
numpages = {10},
keywords = {Graph Neural Networks, Graph Representation Learning, Pre-Training, Self-Supervised Learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.1145/3572403,
author = {Yang, Yingguang and Yang, Renyu and Li, Yangyang and Cui, Kai and Yang, Zhiqin and Wang, Yue and Xu, Jie and Xie, Haiyong},
title = {RoSGAS: Adaptive Social Bot Detection with Reinforced Self-supervised GNN Architecture Search},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/3572403},
doi = {10.1145/3572403},
abstract = {Social bots are referred to as the automated accounts on social networks that make attempts to behave like humans. While Graph Neural Networks (GNNs) have been massively applied to the field of social bot detection, a huge amount of domain expertise and prior knowledge is heavily engaged in the state-of-the-art approaches to design a dedicated neural network architecture for a specific classification task. Involving oversized nodes and network layers in the model design, however, usually causes the over-smoothing problem and the lack of embedding discrimination. In this article, we propose RoSGAS, a novel Reinforced and Self-supervised GNN Architecture Search framework to adaptively pinpoint the most suitable multi-hop neighborhood and the number of layers in the GNN architecture. More specifically, we consider the social bot detection problem as a user-centric subgraph embedding and classification task. We exploit the heterogeneous information network to present the user connectivity by leveraging account metadata, relationships, behavioral features, and content features. RoSGAS uses a multi-agent deep reinforcement learning (RL), 31 pages. mechanism for navigating the search of optimal neighborhood and network layers to learn individually the subgraph embedding for each target user. A nearest neighbor mechanism is developed for accelerating the RL training process, and RoSGAS can learn more discriminative subgraph embedding with the aid of self-supervised learning. Experiments on five Twitter datasets show that RoSGAS outperforms the state-of-the-art approaches in terms of accuracy, training efficiency, and stability and has better generalization when handling unseen samples.},
journal = {ACM Trans. Web},
month = may,
articleno = {15},
numpages = {31},
keywords = {Graph neural network, architecture search, reinforcement learning}
}

@inproceedings{10.1145/3573428.3573598,
author = {Yao, Shunyu and Hu, Jie and Sun, Chuxiong and Gao, Zhiqiao and Liu, Ning},
title = {Key Phrase Extraction based on Pre-trained Language Models},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573598},
doi = {10.1145/3573428.3573598},
abstract = {With the explosion of information and a large amount of data appearing every moment, it is a meaningful task to quickly find the information people want to know in a large amount of text and to present long texts in a streamlined form. Key phrase extraction, which aims to extract from documents a collection of key phrases that express the topic and content of the document, is important for text processing tasks such as information retrieval and document classification and can provide readers with a more comprehensive overview of the topic. We use two types of pre-trained language models for key phrase extraction, namely DeBERTa and RoBERTa, which are first pre-trained on the dataset and then fine-tuned, and the experimental results of these models proved that DeBERTa-V3-Large has reached an F1 score of 0.8925, which is the best result among these models.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {941–945},
numpages = {5},
keywords = {Artificial Intelligence, Key Phrase Extraction, Natural Language Processing, Neural Network},
location = {Xiamen, China},
series = {EITCE '22}
}

@article{10.1145/3586075,
author = {Das, Ringki and Singh, Thoudam Doren},
title = {Multimodal Sentiment Analysis: A Survey of Methods, Trends, and Challenges},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3586075},
doi = {10.1145/3586075},
abstract = {Sentiment analysis has come long way since it was introduced as a natural language processing task nearly 20 years ago. Sentiment analysis aims to extract the underlying attitudes and opinions toward an entity. It has become a powerful tool used by governments, businesses, medicine, marketing, and others. The traditional sentiment analysis model focuses mainly on text content. However, technological advances have allowed people to express their opinions and feelings through audio, image and video channels. As a result, sentiment analysis is shifting from unimodality to multimodality. Multimodal sentiment analysis brings new opportunities with the rapid increase of sentiment analysis as complementary data streams enable improved and deeper sentiment detection which goes beyond text-based analysis. Audio and video channels are included in multimodal sentiment analysis in terms of broadness. People have been working on different approaches to improve sentiment analysis system performance by employing complex deep neural architectures. Recently, sentiment analysis has achieved significant success using the transformer-based model. This paper presents a comprehensive study of different sentiment analysis approaches, applications, challenges, and resources then concludes that it holds tremendous potential. The primary motivation of this survey is to highlight changing trends in the unimodality to multimodality for solving sentiment analysis tasks.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {270},
numpages = {38},
keywords = {Multimodal sentiment analysis, text sentiment analysis, image sentiment analysis, audio sentiment analysis, transfer learning}
}

@article{10.1109/TASLP.2023.3316459,
author = {Atri, Yash Kumar and Goyal, Vikram and Chakraborty, Tanmoy},
title = {Multi-Document Summarization Using Selective Attention Span and Reinforcement Learning},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3316459},
doi = {10.1109/TASLP.2023.3316459},
abstract = {Abstractive text summarization systems using recently improved RNN-based sequence-to-sequence architecture have shown great promise for single-document summarization. However, such neural models fail to perpetuate the performance in the multi-document summarization setting owing to the long-range dependencies within the documents, overlapping/contradicting facts and extrinsic model hallucinations. These shortcomings augment the model to generate inconsistent, repetitive and non-factual summaries. In this work, we introduce &lt;monospace&gt;REISA&lt;/monospace&gt;, a sequence-to-sequence model with a novel &lt;italic&gt;reinforced selective attention span&lt;/italic&gt; that attends over the input and recalibrates the local attention weights to focus on important segments while generating output at each time step. &lt;monospace&gt;REISA&lt;/monospace&gt; utilizes a reinforcement learning-based policy gradient algorithm to reward the model and formulate attention distributions over the encoder input. We further benchmark &lt;monospace&gt;REISA&lt;/monospace&gt; on two widely-used multi-document summarization corpora – Multinews and CQASumm, and observe an improvement of &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$+2.91$&lt;/tex-math&gt;&lt;/inline-formula&gt; and &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$+6.64$&lt;/tex-math&gt;&lt;/inline-formula&gt; ROUGE-L scores, respectively. The qualitative analyses on semantic similarity by BERTScore, faithfulness by question-answer evaluation and human evaluation show significant improvement over the baseline-generated summaries.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3457–3467},
numpages = {11}
}

@inproceedings{10.1145/3555776.3578577,
author = {Sousa, Hugo and Mario Jorge, Alipio and Pasquali, Arian and Santos, Catarina and Lopes, Mario},
title = {A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578577},
doi = {10.1145/3555776.3578577},
abstract = {Textual health records of cancer patients are usually protracted and highly unstructured, making it very time-consuming for health professionals to get a complete overview of the patient's therapeutic course. As such limitations can lead to suboptimal and/or inefficient treatment procedures, healthcare providers would greatly benefit from a system that effectively summarizes the information of those records. With the advent of deep neural models, this objective has been partially attained for English clinical texts, however, the research community still lacks an effective solution for languages with limited resources. In this paper, we present the approach we developed to extract procedures, drugs, and diseases from oncology health records written in European Portuguese. This project was conducted in collaboration with the Portuguese Institute for Oncology which, besides holding over 10 years of duly protected medical records, also provided oncologist expertise throughout the development of the project. Since there is no annotated corpus for biomedical entity extraction in Portuguese, we also present the strategy we followed in annotating the corpus for the development of the models. The final models, which combined a neural architecture with entity linking, achieved F1 scores of 88.6, 95.0, and 55.8 per cent in the mention extraction of procedures, drugs, and diseases, respectively.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {950–956},
numpages = {7},
keywords = {biomedical entity recognition, data mining, oncology electronic health records},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@article{10.1145/3617330,
author = {Fan, Lihang and Fan, Wenfei and Lu, Ping and Tian, Chao and Yin, Qiang},
title = {Enriching Recommendation Models with Logic Conditions},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3617330},
doi = {10.1145/3617330},
abstract = {This paper proposes RecLogic, a framework for improving the accuracy of machine learning (ML) models for recommendation. It aims to enhance existing ML models with logic conditions to reduce false positives and false negatives, without training a new model. Underlying RecLogic are (a) a class of prediction rules on graphs, denoted by TIEs, (b) a new approach to learning TIEs, and (c) a new paradigm for recommendation with TIEs. TIEs may embed ML recommendation models as predicates; as opposed to prior graph rules, it is tractable to decide whether a graph satisfies a set of TIEs. To enrich ML models, RecLogic iteratively trains a generator with feedback from each round, to learn TIEs with a probabilistic bound. RecLogic also provides a PTIME parallel algorithm for making recommendations with the learned TIEs. Using real-life data, we empirically verify that RecLogic improves the accuracy of ML predictions by 22.89% on average in an area where the prediction strength is neither sufficiently large nor sufficiently small, up to 33.10%.},
journal = {Proc. ACM Manag. Data},
month = nov,
articleno = {210},
numpages = {28},
keywords = {prediction rules, recommender system, rule discovery}
}

@proceedings{10.5555/3590145,
title = {ASONAM '22: Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
year = {2022},
isbn = {9781665456616},
publisher = {IEEE Press},
abstract = {We were delighted to welcome each participant at ASONAM 2022 and thank you for having contributed virtually or in person in Istanbul. ASONAM 2022 was the fourteenth annual conference in the successful ASONAM conferences series and also the first hybrid version of the conference. Previous ASONAM conferences were held in Athens (2009), Odense (2010), Kaohsiung (2011), Istanbul (2012), Niagara Falls (2013), Beijing (2014), Paris (2015), San Francisco (2016), Sydney (2017), Barcelona (2018), Vancouver (2019), Virtual (2020), Virtual (2021). The pre-pandemic locations of the conferences have enabled the participants to enjoy local sights and to engage in person-to-person interactions, making new contacts and form new scientific collaborations. These possibilities were only available in a limited form during the virtual conferences. As the covid pandemic seems to be moving towards an endemic form it was decided to have the conference in the hybrid form, as a move towards normal endemic in-person conferences.For more than a century, social networks have been studied in a variety of disciplines including sociology, anthropology, psychology, and economics. The Internet, the social Web, and other large-scale, sociotechnological infrastructures have triggered a growing interest and resulted in significant methodological advancements in social network analysis and mining. Method development in graph theory, statistics, data mining, machine learning, and AI have inspired new research problems and, in turn, opens up further possibilities for application. These spiraling trends have led to a rising prominence of social network analysis and mining methods and tools in academia, politics, security, and business.},
location = {Istanbul, Turkey}
}

@article{10.1109/TASLP.2023.3277245,
author = {Liu, Zhidong and Li, Junhui and Zhu, Muhua},
title = {Alleviating Exposure Bias for Neural Machine Translation via Contextual Augmentation and Self Distillation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3277245},
doi = {10.1109/TASLP.2023.3277245},
abstract = {In neural machine translation (NMT), most sequence-to-sequence (seq2seq) models are trained only with the teacher-forcing paradigm, where the ground truth history is used to predict the next ground truth word. At the inference stage, however, the decoder predicts the next token solely based on history generated from scratch. Both using ground truth history and predicting ground truth words potentially lead to exposure bias. On the one hand, to alleviate the issue of exposure bias caused by using ground truth history, we propose contextual augmentation by allowing substitution, insertion, and deletion of words. The contextual augmentation applies to target sequence to generate non-ground truth and natural history when predicting next words. On the other hand, to alleviate the exposure bias caused by predicting ground truth words, we further apply self distillation to guide the model to carry out optimization according to smoothed prediction distribution, i.e, enable the model to predict not only ground truth words, but also other potentially correct and reasonable words. Experimental results on WMT14 English &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$leftrightarrow$&lt;/tex-math&gt;&lt;/inline-formula&gt; German and IWSLT14 German &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$rightarrow$&lt;/tex-math&gt;&lt;/inline-formula&gt; English translation tasks demonstrate that our approach achieves significant improvements over Transformer on standard benchmarks. Detailed experimental analyses further reveal the effectiveness of our proposed approach in improving translation quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {2079–2089},
numpages = {11}
}

@article{10.1145/3578362,
author = {Meng, Qing and Yan, Hui and Liu, Bo and Sun, Xiangguo and Hu, Mingrui and Cao, Jiuxin},
title = {Recognize News Transition from Collective Behavior for News Recommendation},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3578362},
doi = {10.1145/3578362},
abstract = {In the news recommendation, users are overwhelmed by thousands of news daily, which makes the users’ behavior data have high sparsity. Therefore, only considering a single user’s personalized preferences cannot support the news recommendation. How to improve the relatedness of news and users and reduce data sparsity has become a hot issue. Recent studies have attempted to use graph models to enrich the relationship between users and news, but they are still limited to modeling the historical behaviors of a single user. To fill the gap, we integrate user-news relationships and the overall user historical clicked news sequences to construct a global heterogeneous transition graph. And a refinement approach is proposed to recognize the news transition patterns in the graph. Based on the global heterogeneous transition graph, we propose a heterogeneous transition graph attention network to capture the common behavior patterns of most users to enhance the representation of user interest. Fusing the users’ personalized and common interest, we propose the GAINRec model to recommend news effectively. Extensive experiments are conducted on two public news recommendation datasets, and the results show the superiority of the proposed GAINRec model compared with the state-of-the-art news recommendation models. The implementation of our model is available at .},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {93},
numpages = {30},
keywords = {News recommendation, transition graph, graph attention network, collective behavior}
}

@inproceedings{10.1145/3442442.3452303,
author = {Berendt, Bettina and Karadeniz, \"{O}zg\"{u}r and Mertens, Stefan and d'Haenens, Leen},
title = {Fairness beyond “equal”: The Diversity Searcher as a Tool to Detect and Enhance the Representation of Socio-political Actors in News Media},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3452303},
doi = {10.1145/3442442.3452303},
abstract = {“Fairness” is a multi-faceted concept that is contested within and across disciplines. In machine learning, it usually denotes some form of equality of measurable outcomes of algorithmic decision making. In this paper, we start from a viewpoint of sociology and media studies, which highlights that to even claim fair treatment, individuals and groups first have to be visible. We draw on a notion and a quantitative measure of diversity that expresses this wider requirement. We used the measure to design and build the Diversity Searcher, a Web-based tool to detect and enhance the representation of socio-political actors in news media. We show how the tool's combination of natural language processing and a rich user interface can help news producers and consumers detect and understand diversity-relevant aspects of representation, which can ultimately contribute to enhancing diversity and fairness in media. We comment on our observation that, through interactions with target users during the construction of the tool, NLP results and interface questions became increasingly important, such that the formal measure of diversity has become a catalyst for functionality, but in itself less important.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {202–212},
numpages = {11},
keywords = {Fairness-aware recommender systems and diversity in recommendation,&nbsp;Innovative methods for studying/analyzing the fairness, accountability, transparency and ethics of web platforms,&nbsp;Usability challenges of machine learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.14778/3489496.3489504,
author = {Ge, Congcong and Liu, Xiaoze and Chen, Lu and Gao, Yunjun and Zheng, Baihua},
title = {LargeEA: aligning entities for large-scale knowledge graphs},
year = {2021},
issue_date = {October 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3489496.3489504},
doi = {10.14778/3489496.3489504},
abstract = {Entity alignment (EA) aims to find equivalent entities in different knowledge graphs (KGs). Current EA approaches suffer from scalability issues, limiting their usage in real-world EA scenarios. To tackle this challenge, we propose LargeEA to align entities between large-scale KGs. LargeEA consists of two channels, i.e., structure channel and name channel. For the structure channel, we present METIS-CPS, a memory-saving mini-batch generation strategy, to partition large KGs into smaller mini-batches. LargeEA, designed as a general tool, can adopt any existing EA approach to learn entities' structural features within each mini-batch independently. For the name channel, we first introduce NFF, a name feature fusion method, to capture rich name features of entities without involving any complex training process; we then exploit a name-based data augmentation to generate seed alignment without any human intervention. Such design fits common real-world scenarios much better, as seed alignment is not always available. Finally, LargeEA derives the EA results by fusing the structural features and name features of entities. Since no widely-acknowledged benchmark is available for large-scale EA evaluation, we also develop a large-scale EA benchmark called DBP1M extracted from real-world KGs. Extensive experiments confirm the superiority of LargeEA against state-of-the-art competitors.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {237–245},
numpages = {9}
}

@article{10.1145/3479604,
author = {Zhao, Rui and Atkinson, Malcolm and Papapanagiotou, Petros and Magnoni, Federica and Fleuriot, Jacques},
title = {Dr.Aid: Supporting Data-governance Rule Compliance for Decentralized Collaboration in an Automated Way},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479604},
doi = {10.1145/3479604},
abstract = {Collaboration across institutional boundaries is widespread and increasing today. It depends on federations sharing data that often have governance rules or external regulations restricting their use. However, the handling of data governance rules (aka. data-use policies) remains manual, time-consuming and error-prone, limiting the rate at which collaborations can form and respond to challenges and opportunities, inhibiting citizen science and reducing data providers' trust in compliance. Using an automated system to facilitate compliance handling reduces substantially the time needed for such non-mission work, thereby accelerating collaboration and improving productivity. We present a framework, Dr.Aid, that helps individuals, organisations and federations comply with data rules, using automation to track which rules are applicable as data is passed between processes and as derived data is generated. It encodes data-governance rules using a formal language and performs reasoning on multi-input-multi-output data-flow graphs in decentralised contexts. We test its power and utility by working with users performing cyclone tracking and earthquake modelling to support mitigation and emergency response. We query standard provenance traces to detach Dr.Aid from details of the tools and systems they are using, as these inevitably vary across members of a federation and through time. We evaluate the model in three aspects by encoding real-life data-use policies from diverse fields, showing its capability for real-world usage and its advantages compared with traditional frameworks. We argue that this approach will lead to more agile, more productive and more trustworthy collaborations and show that the approach can be adopted incrementally. This, in-turn, will allow more appropriate data policies to emerge opening up new forms of collaboration.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {460},
numpages = {43},
keywords = {automated reasoning, data governance, data policy, formal model, obligation policy}
}

@article{10.1145/3589784,
author = {Dhelim, Sahraoui and Chen, Liming and Das, Sajal K. and Ning, Huansheng and Nugent, Chris and Leavey, Gerard and Pesch, Dirk and Bantry-White, Eleanor and Burns, Devin},
title = {Detecting Mental Distresses Using Social Behavior Analysis in the Context of COVID-19: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3589784},
doi = {10.1145/3589784},
abstract = {Online social media provides a channel for monitoring people’s social behaviors from which to infer and detect their mental distresses. During the COVID-19 pandemic, online social networks were increasingly used to express opinions, views, and moods due to the restrictions on physical activities and in-person meetings, leading to a significant amount of diverse user-generated social media content. This offers a unique opportunity to examine how COVID-19 changed global behaviors regarding its ramifications on mental well-being. In this article, we surveyed the literature on social media analysis for the detection of mental distress, with a special emphasis on the studies published since the COVID-19 outbreak. We analyze relevant research and its characteristics and propose new approaches to organizing the large amount of studies arising from this emerging research area, thus drawing new views, insights, and knowledge for interested communities. Specifically, we first classify the studies in terms of feature extraction types, language usage patterns, aesthetic preferences, and online behaviors. We then explored various methods (including machine learning and deep learning techniques) for detecting mental health problems. Building upon the in-depth review, we present our findings and discuss future research directions and niche areas in detecting mental health problems using social media data. We also elaborate on the challenges of this fast-growing research area, such as technical issues in deploying such systems at scale as well as privacy and ethical concerns.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {318},
numpages = {30},
keywords = {Social media analysis, mental disorder detection, COVID-19, mental health}
}

@article{10.1145/3626766,
author = {Miao, Zhengjie and Wang, Jin},
title = {Watchog: A Light-weight Contrastive Learning based Framework for Column Annotation},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {4},
url = {https://doi.org/10.1145/3626766},
doi = {10.1145/3626766},
abstract = {Relational Web tables provide valuable resources for numerous downstream applications, making table understanding, especially column annotation that identifies semantic types and relations of columns, a hot topic in the field of data management. Despite recent efforts to improve different tasks in table understanding by using the power of large pre-trained language models, existing methods heavily rely on large-scale and high-quality labeled instances, while they still suffer from the data sparsity problem due to the imbalanced data distribution among different classes. In this paper, we propose the Watchog framework, which employs contrastive learning techniques to learn robust representations for tables by leveraging a large-scale unlabeled table corpus with minimal overhead. Our approach enables the learned table representations to enhance fine tuning with much fewer additional labeled instances than in prior studies for downstream column annotation tasks. Besides, we further proposed optimization techniques for semi-supervised settings. Experimental results on popular benchmarking datasets illustrate the superiority of our proposed techniques in two column annotation tasks under different settings. In particular, our Watchog framework effectively alleviates the class imbalance issue caused by a long-tailed label distribution. In the semi-supervised setting, Watchog outperforms the best-known method by up to 26% and 41% in Micro and Macro F1 scores, respectively, on the task of semantic type detection.},
journal = {Proc. ACM Manag. Data},
month = dec,
articleno = {272},
numpages = {24},
keywords = {contrastive learning, semi-supervised learning, table understanding, web tables}
}

@article{10.1145/3563041,
author = {Sun, Wei and Ji, Shaoxiong and Cambria, Erik and Marttinen, Pekka},
title = {Multitask Balanced and Recalibrated Network for Medical Code Prediction},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3563041},
doi = {10.1145/3563041},
abstract = {Human coders assign standardized medical codes to clinical documents generated during patients’ hospitalization, which is error prone and labor intensive. Automated medical coding approaches have been developed using machine learning methods, such as deep neural networks. Nevertheless, automated medical coding is still challenging because of complex code association, noise in lengthy documents, and the imbalanced class problem. We propose a novel neural network, called the Multitask Balanced and Recalibrated Neural Network, to solve these issues. Significantly, the multitask learning scheme shares the relationship knowledge between different coding branches to capture code association. A recalibrated aggregation module is developed by cascading convolutional blocks to extract high-level semantic features that mitigate the impact of noise in documents. Also, the cascaded structure of the recalibrated module can benefit learning from lengthy notes. To solve the imbalanced class problem, we deploy focal loss to redistribute the attention on low- and high-frequency medical codes. Experimental results show that our proposed model outperforms competitive baselines on a real-world clinical dataset called the Medical Information Mart for Intensive Care (MIMIC-III).},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {17},
numpages = {20},
keywords = {Medical code prediction, multitask learning, imbalanced class problem, balanced and recalibrated network}
}

@proceedings{10.1145/3592573,
title = {LSC '23: Proceedings of the 6th Annual ACM Lifelog Search Challenge},
year = {2023},
isbn = {9798400701887},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Thessaloniki, Greece}
}

@inproceedings{10.1145/3589132.3625600,
author = {Liu, Mengyi and Wang, Xieyang and Xu, Jianqiu and Lu, Hua},
title = {NALSpatial: An Effective Natural Language Transformation Framework for Queries over Spatial Data},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625600},
doi = {10.1145/3589132.3625600},
abstract = {Spatial databases play a vital role in many applications that access spatial data via appropriate queries. However, most application users lack the expertise necessary for formulating spatial queries. To fill in this gap, we propose an effective framework called NALSpatial that translates natural language queries over spatial data into executable database queries. NALSpatial consists of two core phases. The natural language understanding phase extracts key entity information, comprehends the query intent and determines the query type. The key entities and query type are passed to the subsequent natural language translation phase, which employs entity mapping rules and structured language models to construct executable database queries accordingly. We implement NALSpatial on the open-source extensible database system SECONDO to support range queries, nearest neighbor queries, spatial joins and aggregation queries. Extensive experiments show that NALSpatial on average achieves response time of about 2.5 seconds, translatability of 95% and translation precision of 92%, outperforming state-of-the-art natural language transformation methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {57},
numpages = {4},
keywords = {spatial database, natural language interface, semantic parsing, query processing},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@article{10.1109/TASLP.2021.3054309,
author = {Balaraman, Vevake and Magnini, Bernardo},
title = {Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3054309},
doi = {10.1109/TASLP.2021.3054309},
abstract = {In task-oriented dialogue systems the dialogue state tracker component (DST) is responsible for predicting the current state of the dialogue based on the dialogue history and the user utterance. Current DST approaches rely on a predefined domain ontology, a fact that limits their effective usage for large scale conversational agents, where the DST constantly needs to be interfaced with ever-increasing services and APIs. Focused towards overcoming this drawback, we propose a domain-aware dialogue state tracker, that is completely data-driven and it is modeled to predict for dynamic service schemas, including zero-shot domains. Unlike approaches that propose separate models for prediction of intents, requested slots, slot status, categorical slots and non-categorical slots, we propose a single model in an end-to-end architecture. The proposed model utilizes domain and slot information to extract both domain and slot specific representations from a given dialogue, and then uses such representations to predict the values of the corresponding slot in a given domain. Integrating this mechanism with pretrained language models, our approach can effectively learn semantic relations and effectively perform transfer learning between domains or zero-shot tracking for domains not present in training.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {866–873},
numpages = {8}
}

@inproceedings{10.1145/3539618.3591877,
author = {Zhan, Haolan and Li, Zhuang and Wang, Yufei and Luo, Linhao and Feng, Tao and Kang, Xiaoxi and Hua, Yuncheng and Qu, Lizhen and Soon, Lay-Ki and Sharma, Suraj and Zukerman, Ingrid and Semnani-Azad, Zhaleh and Haffari, Gholamreza},
title = {SocialDial: A Benchmark for Socially-Aware Dialogue Systems},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591877},
doi = {10.1145/3539618.3591877},
abstract = {Content Warning: this paper may contain content that is offensive or upsetting.Dialogue systems have been widely applied in many scenarios and are now more powerful and ubiquitous than ever before. With large neural models and massive available data, current dialogue systems have access to more knowledge than any people in their life. However, current dialogue systems still do not perform at a human level. One major gap between conversational agents and humans lies in their abilities to be aware of social norms. The development of socially-aware dialogue systems is impeded due to the lack of resources. In this paper, we present the first socially-aware dialogue corpus -- SocialDial based on Chinese social culture. SocialDial consists of two parts: 1,563 multi-turn dialogues between two human speakers with fine-grained labels, and 4,870 synthetic conversations generated by ChatGPT. The human corpus covers five categories of social norms, which have 14 sub-categories in total. Specifically, it contains social factor annotations including social relation, context, social distance, and social norms. However, collecting sufficient socially-aware dialogues is costly. Thus, we harness the power of ChatGPT and devise an ontology-based synthetic data generation framework. This framework is able to generate synthetic data at scale. To ensure the quality of synthetic dialogues, we design several mechanisms for quality control during data collection. Finally, we evaluate our dataset using several pre-trained models, such as BERT and RoBERTa. Comprehensive empirical results based on state-of-the-art neural models demonstrate that modeling of social norms for dialogue systems is a promising research direction. To the best of our knowledge, SocialDial is the first socially-aware dialogue dataset that covers multiple social factors and has fine-grained labels.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2712–2722},
numpages = {11},
keywords = {datasets, social norms, socially-aware dialogue},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3615355,
author = {Davis, Ernest},
title = {Benchmarks for Automated Commonsense Reasoning: A Survey},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3615355},
doi = {10.1145/3615355},
abstract = {More than one hundred benchmarks have been developed to test the commonsense knowledge and commonsense reasoning abilities of artificial intelligence (AI) systems. However, these benchmarks are often flawed, and many aspects of common sense remain untested. Consequently, there is currently no reliable way of measuring to what extent existing AI systems have achieved these abilities.This article surveys the development and uses of AI commonsense benchmarks. It enumerates 139 commonsense benchmarks that have been developed: 102 text-based, 18 image-based, 12 video-based, and 7 based in simulated physical environments. It gives more detailed descriptions of twelve of these, three from each category. It surveys the various methods used to construct commonsense benchmarks. It discusses the nature of common sense, the role of common sense in AI, the goals served by constructing commonsense benchmarks, desirable features of commonsense benchmarks, and flaws and gap in existing benchmarks. It concludes with a number of recommendations for future development of commonsense AI benchmarks; most importantly, that the creators of benchmarks invest the work needed to ensure that benchmark examples are consistently high quality.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {81},
numpages = {41},
keywords = {Common sense, commonsense knowledge, commonsense reasoning, benchmarks, evaluation}
}

@proceedings{10.1145/3584371,
title = {BCB '23: Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ACM-BCB is the flagship conference of the ACM SIGBio, the ACM Special Interest Group in Bioinformatics, Computational Biology, and Biomedical Informatics. Continuing the annual tradition, the conference focuses on interdisciplinary research linking computer science, mathematics, statistics, biology, bioinformatics, biomedical informatics, and health informatics.},
location = {Houston, TX, USA}
}

@inproceedings{10.1145/3447535.3462640,
author = {Woloszyn, Vinicius and Cortes, Eduardo G. and Amantea, Rafael and Schmitt, Vera and Barone, Dante A. C. and M\"{o}ller, Sebastian},
title = {Towards a Novel Benchmark for Automatic Generation of ClaimReview Markup},
year = {2021},
isbn = {9781450383301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447535.3462640},
doi = {10.1145/3447535.3462640},
abstract = {The spreading of disinformation throughout the web has become a critical problem for a democratic society. The dissemination of fake news has become a profitable business and a common practice among politicians and content producers. On the other hand, journalists and fact-checkers work unceasingly to debunk misinformation and prevent it from further spreading. In 2015, a new web markup called ClaimReview has been introduced to grant access to the fact-checking article’s meaning by search engines. It is an important initiative to fight fake news by promoting and highlighting fact-check articles among users. However, barely half of fact-checkers have adopted the ClaimReview markup so far, resulting in low findability of fact-check articles, especially in under-represented countries and languages. In this work, we investigate the viability of using Artificial Intelligence for generating ClaimReview automatically. Besides promoting fact-check articles, the automatic generating of ClaimReview is an important step towards the creation of updated multilingual knowledge base for fighting disinformation. Our experiments show noticeable results, which indicate a viable solution in a production environment. Furthermore, this work has created a benchmark that can be used in upcoming investigations in this domain.},
booktitle = {Proceedings of the 13th ACM Web Science Conference 2021},
pages = {29–35},
numpages = {7},
keywords = {ClaimReview, data sets, fact-checking, machine learning, misinformation},
location = {Virtual Event, United Kingdom},
series = {WebSci '21}
}

@inproceedings{10.1145/3534678.3539304,
author = {Wei, Ying and Li, Qi},
title = {SagDRE: Sequence-Aware Graph-Based Document-Level Relation Extraction with Adaptive Margin Loss},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539304},
doi = {10.1145/3534678.3539304},
abstract = {Relation extraction (RE) is an important task for many natural language processing applications. Document-level relation extraction task aims to extract the relations within a document and poses many challenges to the RE tasks as it requires reasoning across sentences and handling multiple relations expressed in the same document. Existing state-of-the-art document-level RE models use the graph structure to better connect long-distance correlations. In this work, we propose SagDRE model, which further considers and captures the original sequential information from the text. The proposed model learns sentence-level directional edges to capture the information flow in the document and uses the token-level sequential information to encode the shortest paths from one entity to the other. In addition, we propose an adaptive margin loss to address the long-tailed multi-label problem of document-level RE tasks, where multiple relations can be expressed in a document for an entity pair and there are a few popular relations. The loss function aims to encourage separations between positive and negative classes. The experimental results on datasets from various domains demonstrate the effectiveness of the proposed methods.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2000–2008},
numpages = {9},
keywords = {document-level re, graph, relation extraction, sequence information},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1109/TASLP.2021.3076876,
author = {Jeong, Myeongho and Choi, Seungtaek and Yeo, Jinyoung and Hwang, Seung-won},
title = {Label and Context Augmentation for Response Selection at DSTC8},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3076876},
doi = {10.1109/TASLP.2021.3076876},
abstract = {This paper studies the dialogue response selection task. As state-of-the-arts are neural models requiring a large training set, data augmentation has been considered as a means to overcome the sparsity of observational annotation, where only one observed response is annotated as gold. In this paper, we first consider label augmentation, of selecting, among unobserved utterances, that would “counterfactually” replace the labeled response, for the given context, and augmenting labels only if that is the case. The key advantage of this model is not incurring human annotation overhead, thus not increasing the training cost, i.e., for low-resource scenarios. In addition, we consider context augmentation scenarios where the given dialogue context is not sufficient for label augmentation. In this case, inspired by open-domain question answering, we “decontextualize” by retrieving missing contexts, such as related persona. We empirically show that our pipeline improves BERT-based models in two different response selection tasks without incurring annotation overheads.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {2541–2550},
numpages = {10}
}

@inproceedings{10.1145/3442381.3449988,
author = {Xia, Tingyu and Wang, Yue and Tian, Yuan and Chang, Yi},
title = {Using Prior Knowledge to Guide BERT’s Attention in Semantic Textual Matching Tasks},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449988},
doi = {10.1145/3442381.3449988},
abstract = {We study the problem of incorporating prior knowledge into a deep Transformer-based model, i.e., Bidirectional Encoder Representations from Transformers (BERT), to enhance its performance on semantic textual matching tasks. By probing and analyzing what BERT has already known when solving this task, we obtain better understanding of what task-specific knowledge BERT needs the most and where it is most needed. The analysis further motivates us to take a different approach than most existing works. Instead of using prior knowledge to create a new training task for fine-tuning BERT, we directly inject knowledge into BERT’s multi-head attention mechanism. This leads us to a simple yet effective approach that enjoys fast training stage as it saves the model from training on additional data or tasks other than the main task. Extensive experiments demonstrate that the proposed knowledge-enhanced BERT is able to consistently improve semantic textual matching performance over the original BERT model, and the performance benefit is most salient when training data is scarce.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2466–2475},
numpages = {10},
keywords = {BERT, Deep Neural Networks, Prior Knowledge, Semantic Textual Similarity},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3597610,
author = {Huang, Heyan and Yuan, Changsen and Liu, Qian and Cao, Yixin},
title = {Document-level Relation Extraction via Separate Relation Representation and Logical Reasoning},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3597610},
doi = {10.1145/3597610},
abstract = {Document-level relation extraction (RE) extends the identification of entity/mentions’ relation from the single sentence to the long document. It is more realistic and poses new challenges to relation representation and reasoning skills. In this article, we propose a novel model, SRLR, using Separate Relation Representation and Logical Reasoning considering the indirect relation representation and complex reasoning of evidence sentence problems. Specifically, we first expand the judgment of relational facts from the entity-level to the mention-level, highlighting fine-grained information to capture the relation representation for the entity pair. Second, we propose a logical reasoning module to identify evidence sentences and conduct relational reasoning. Extensive experiments on two publicly available benchmark datasets demonstrate the effectiveness of our proposed SRLR as compared to 19 baseline models. Further ablation study also verifies the effects of the key components.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {22},
numpages = {24},
keywords = {Document-level Relation Extraction, Separate Relation Representation, Mention-level, Logical Reasoning}
}

@inproceedings{10.1145/3579051.3579063,
author = {Pozzi, Riccardo and Moiraghi, Federico and Lodi, Fausto and Palmonari, Matteo},
title = {Evaluation of Incremental Entity Extraction with Background Knowledge and Entity Linking},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579063},
doi = {10.1145/3579051.3579063},
abstract = {Named entity extraction is a crucial task to support the population of Knowledge Bases (KBs) from documents written in natural language. However, in many application domains, these documents must be collected and processed incrementally to update the KB as more data are ingested. In some cases, quality concerns may even require human validation mechanisms along the process. While very recent work in the NLP community has discussed the importance of evaluating and benchmarking continuous entity extraction, it has proposed methods and datasets that avoid Named Entity Linking (NEL) as a component of the extraction process. In this paper, we advocate for batch-based incremental entity extraction methods that can exploit NEL with a background KB, detect mentions of entities that are not present in the KB yet (NIL mentions), and update the KB with the novel entities. Based on this assumption, we present a methodology to evaluate NEL-based incremental entity extraction, which can be applied to a “static” dataset for evaluating NEL into a dataset for evaluating incremental entity extraction. We apply this methodology to an existing benchmark for evaluating NEL algorithms, and evaluate an incremental extraction pipeline that orchestrates different strong state-of-the-art and baseline algorithms for the tasks involved in the extraction process, namely, NEL, NIL prediction, and NIL clustering. In presenting our experiments, we demonstrate the increased difficulty of the information extraction task in incremental settings and discuss the strengths of the available solutions as well as open challenges.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {30–38},
numpages = {9},
keywords = {Entity Extraction, Incremental Entity Extraction, Knowledge Base Population, Named Entity Linking},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@article{10.1145/3631521,
author = {Zhang, Yuting and Sun, Ying and Zhuang, Fuzhen and Zhu, Yongchun and An, Zhulin and Xu, Yongjun},
title = {Triple Dual Learning for Opinion-based Explainable Recommendation},
year = {2023},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3631521},
doi = {10.1145/3631521},
abstract = {Recently, with the aim of enhancing the trustworthiness of recommender systems, explainable recommendation has attracted much attention from the research community. Intuitively, users’ opinions toward different aspects of an item determine their ratings (i.e., users’ preferences) for the item. Therefore, rating prediction from the perspective of opinions can realize personalized explanations at the level of item aspects and user preferences. However, there are several challenges in developing an opinion-based explainable recommendation: (1) The complicated relationship between users’ opinions and ratings. (2) The difficulty of predicting the potential (i.e., unseen) user-item opinions because of the sparsity of opinion information. To tackle these challenges, we propose an overall preference-aware opinion-based explainable rating prediction model by jointly modeling the multiple observations of user-item interaction (i.e., review, opinion, rating). To alleviate the sparsity problem and raise the effectiveness of opinion prediction, we further propose a triple dual learning-based framework with a novelly designed triple dual constraint. Finally, experiments on three popular datasets show the effectiveness and great explanation performance of our framework.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {70},
numpages = {27},
keywords = {Explainable recommendation; triple dual learning; opinion-based explanation}
}

@inproceedings{10.1145/3446132.3446408,
author = {Kong, Siyin and Zhu, Ping and Yang, Qian and Wei, Zhihua},
title = {HSCKE: A Hybrid Supervised Method for Chinese Keywords Extraction},
year = {2021},
isbn = {9781450388115},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3446132.3446408},
doi = {10.1145/3446132.3446408},
abstract = {Automatic keywords extraction refers to extracting words or phrases from a single text or text collection. Supervised methods outperform unsupervised methods, but it requires a large volume of labeled corpus for training. To address the problem, extra knowledge is obtained through labels generated by other tools. Moreover, the preprocessing of Chinese text is more challenging than that in English because of the fragments caused by word segment. Hence the named entity recognition in the preprocessing is introduced to enhance the accuracy. On the other hand, text contains different separate parts, and each part conveys information to readers on different levels. Thus, we present a text weighting method based on priority that takes into consideration the importance of different texture parts. In this paper, we integrate the three ideas above and propose a novel hybrid method for Chinese keywords extraction (HSCKE). To evaluate the performance of our proposed approach, we compare HSCKE with four most commonly used methods on two typical Chinese keywords extraction datasets. The experimental results show that the proposed approach achieves the optimal performance in terms of precision, recall and F1 score.},
booktitle = {Proceedings of the 2020 3rd International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {81},
numpages = {7},
keywords = {Chinese keywords extraction, Pre-trained model, Supervised method},
location = {Sanya, China},
series = {ACAI '20}
}

@proceedings{10.1145/3571560,
title = {ICAAI '22: Proceedings of the 6th International Conference on Advances in Artificial Intelligence},
year = {2022},
isbn = {9781450396943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Birmingham, United Kingdom}
}

@inproceedings{10.1145/3543507.3583504,
author = {Zhang, Mi and Qian, Tieyun and Zhang, Ting and Miao, Xin},
title = {Towards Model Robustness: Generating Contextual Counterfactuals for Entities in Relation Extraction},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583504},
doi = {10.1145/3543507.3583504},
abstract = {The goal of relation extraction (RE) is to extract the semantic relations between/among entities in the text. As a fundamental task in information systems, it is crucial to ensure the robustness of RE models. Despite the high accuracy current deep neural models have achieved in RE tasks, they are easily affected by spurious correlations. One solution to this problem is to train the model with counterfactually augmented data (CAD) such that it can learn the causation rather than the confounding. However, no attempt has been made on generating counterfactuals for RE tasks. In this paper, we formulate the problem of automatically generating CAD for RE tasks from an entity-centric viewpoint, and develop a novel approach to derive contextual counterfactuals for entities. Specifically, we exploit two elementary topological properties, i.e., the centrality and the shortest path, in syntactic and semantic dependency graphs, to first identify and then intervene on the contextual causal features for entities. We conduct a comprehensive evaluation on four RE datasets by combining our proposed approach with a variety of RE backbones. Results prove that our approach not only improves the performance of the backbones but also makes them more robust in the out-of-domain test &nbsp;1.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1832–1842},
numpages = {11},
keywords = {counterfactual reasoning, relation extraction, semantic and syntactic graph topology},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.1145/3545572,
author = {Jabeen, Summaira and Li, Xi and Amin, Muhammad Shoib and Bourahla, Omar and Li, Songyuan and Jabbar, Abdul},
title = {A Review on Methods and Applications in Multimodal Deep Learning},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3545572},
doi = {10.1145/3545572},
abstract = {Deep Learning has implemented a wide range of applications and has become increasingly popular in recent years. The goal of multimodal deep learning (MMDL) is to create models that can process and link information using various modalities. Despite the extensive development made for unimodal learning, it still cannot cover all the aspects of human learning. Multimodal learning helps to understand and analyze better when various senses are engaged in the processing of information. This article focuses on multiple types of modalities, i.e., image, video, text, audio, body gestures, facial expressions, physiological signals, flow, RGB, pose, depth, mesh, and point cloud. Detailed analysis of the baseline approaches and an in-depth study of recent advancements during the past five years (2017 to 2021) in multimodal deep learning applications has been provided. A fine-grained taxonomy of various multimodal deep learning methods is proposed, elaborating on different applications in more depth. Last, main issues are highlighted separately for each domain, along with their possible future research directions.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {76},
numpages = {41},
keywords = {Deep learning, multimedia, multimodal learning, datasets, neural networks, survey}
}

@proceedings{10.1145/3545822,
title = {ICMSSP '22: Proceedings of the 2022 7th International Conference on Multimedia Systems and Signal Processing},
year = {2022},
isbn = {9781450396424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3579375.3579391,
author = {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla, Sandeep Kumar},
title = {TTPHunter: Automated Extraction of Actionable Intelligence as TTPs from Narrative Threat Reports},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579391},
doi = {10.1145/3579375.3579391},
abstract = {With the proliferation of attacks from various Advanced Persistent Threats (APT) groups, it is essential to comprehend the threat actor’s attack patterns to accelerate threat detection and response. The MITRE ATT&amp;CK framework’s Tactics, Techniques, and Procedures (TTPs) help to decipher attack patterns. The APT reports, published by security firms, contain rich information on tools and techniques used by threat actors. These reports are available in unstructured and natural language texts. There is a need for an automated tool to extract TTPs present in natural language text. However, there are few tools available in the literature, but their performance is not very satisfactory. In this work, we propose TTPHunter, to extract TTPs from APT reports by mapping sentence context to relevant TTPs. We fine-tune linear classifiers, which take input as BERT (Bidirectional Encoder Representations from Transformers) embeddings of sentences. We create two datasets: sentence-based (8,387 sentence samples) and document-based (50 threat reports) to validate TTPHunter. TTPHunter achieves the F1-score of 88% and 75% for both datasets, respectively. We compare the TTPHunter with rcATT and AttacKG baseline models, and it outperforms both baselines.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {126–134},
numpages = {9},
keywords = {Cybersecurity, MITRE ATT&amp;CK, Natural Language Processing, TTP Extraction, Threat Intelligence},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@proceedings{10.1145/3599957,
title = {RACS '23: Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {With the expansion of both the Internet and the advanced information technology development profession, reliable and convergent computing has attracted increasing interest in both academia and industry. To cope with this important problem, the Research in Adaptive and Convergent Systems (RACS) provides a forum for exchanging highly original ideas about an important class of computing systems. The RACS aims primarily at researchers who have experience in reliable and convergent computing systems and are engaged in the design and implementation of new computing applications. Each year RACS brings together engineers and scientists from diverse communities with interests in practical computing technologies and creates an environment for them to discuss and report experimental results, novel designs, work-in-progress, experiences, case studies, and trend-setting ideas.},
location = {Gdansk, Poland}
}

@proceedings{10.1145/3555776,
title = {SAC '23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/3511808.3557484,
author = {Arnaout, Hiba and Razniewski, Simon and Weikum, Gerhard and Pan, Jeff Z.},
title = {UnCommonSense: Informative Negative Knowledge about Everyday Concepts},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557484},
doi = {10.1145/3511808.3557484},
abstract = {Commonsense knowledge about everyday concepts is an important asset for AI applications, such as question answering and chatbots. Recently, we have seen an increasing interest in the construction of structured commonsense knowledge bases (CSKBs). An important part of human commonsense is about properties that do not apply to concepts, yet existing CSKBs only store positive statements. Moreover, since CSKBs operate under the open-world assumption, absent statements are considered to have unknown truth rather than being invalid. This paper presents the UNCOMMONSENSE framework for materializing informative negative commonsense statements. Given a target concept, comparable concepts are identified in the CSKB, for which a local closed-world assumption is postulated. This way, positive statements about comparable concepts that are absent for the target concept become seeds for negative statement candidates. The large set of candidates is then scrutinized, pruned and ranked by informativeness. Intrinsic and extrinsic evaluations show that our method significantly outperforms the state-of-the-art. A large dataset of informative negations is released as a resource for future research.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {37–46},
numpages = {10},
keywords = {commonsense, knowledge base, negation},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@proceedings{10.1145/3548608,
title = {ICCIR '22: Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nanjing, China}
}

@proceedings{10.1145/3600100,
title = {BuildSys '23: Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Istanbul, Turkey}
}

@inproceedings{10.1145/3511808.3557067,
author = {Shin, Wonyoung and Park, Jonghun and Woo, Taekang and Cho, Yongwoo and Oh, Kwangjin and Song, Hwanjun},
title = {e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557067},
doi = {10.1145/3511808.3557067},
abstract = {Understanding vision and language representations of product content is vital for search and recommendation applications in e-commerce. As a backbone for online shopping platforms and inspired by the recent success in representation learning research, we propose a contrastive learning framework that aligns language and visual models using unlabeled raw product text and images. We present techniques we used to train large-scale representation learning models and share solutions that address domain-specific challenges. We study the performance using our pre-trained model as backbones for diverse downstream tasks, including category classification, attribute extraction, product matching, product clustering, and adult product recognition. Experimental results show that our proposed method outperforms the baseline in each downstream task regarding both single modality and multiple modalities.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {3484–3494},
numpages = {11},
keywords = {large-scale pre-training, multimodal pre-training},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@proceedings{10.1145/3563359,
title = {UMAP '23 Adjunct: Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
year = {2023},
isbn = {9781450398916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@article{10.1613/jair.1.12918,
author = {Erdem, Erkut and Kuyu, Menekse and Yagcioglu, Semih and Frank, Anette and Parcalabescu, Letitia and Plank, Barbara and Babii, Andrii and Turuta, Oleksii and Erdem, Aykut and Calixto, Iacer and Lloret, Elena and Apostol, Elena-Simona and Truic\u{a}, Ciprian-Octavian and \v{S}andrih, Branislava and Martin\v{c}i\'{c}-Ip\v{s}i\'{c}, Sanda and Berend, G\'{a}bor and Gatt, Albert and Korvel, Gr\u{a}zina},
title = {Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12918},
doi = {10.1613/jair.1.12918},
abstract = {Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions.},
journal = {J. Artif. Int. Res.},
month = may,
numpages = {77},
keywords = {natural language, neural networks}
}

@proceedings{10.1145/3546607,
title = {ICVARS '22: Proceedings of the 2022 6th International Conference on Virtual and Augmented Reality Simulations},
year = {2022},
isbn = {9781450387330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Brisbane, QLD, Australia}
}

@inproceedings{10.1145/3442442.3451385,
author = {Pei, Yulong and Zhang, Qian},
title = {GOAT at the FinSim-2 task: Learning Word Representations of Financial Data with Customized Corpus},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451385},
doi = {10.1145/3442442.3451385},
abstract = {In this paper, we present our approaches for the FinSim 2021 Shared Task on Learning Semantic Similarities for the Financial Domain. The aim of the FinSim shared task is to automatically classify a given list of terms from the financial domain into the most relevant hypernym (or top-level) concept in an external ontology. Two different word representations have been compared in our study, i.e., customized word2vec provided by the shared task and FinBERT. We first create a customized corpus from the given prospectuses and relevant articles from Investopedia. Then we train the domain-specific word2vec embeddings using the customized data with customized word2vec and FinBERT as the initialized embeddings respectively. Our experimental results demonstrate that these customized word embeddings can effectively improve the classification performance and achieve better results than the direct utilization of the provided word embeddings. The class imbalance issue of the given data is also explored. We empirically study the classification performance by employing several different strategies for imbalanced classification problems. Our system ranks 2nd on both Average Accuracy and Mean Rank metrics.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {307–310},
numpages = {4},
keywords = {BERT, Word representations, imbalance classification, word2vec},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.1145/3464380,
author = {Tian, Xiuxia and Li, Can and Zhao, Bo},
title = {A Novel Classification Model SA-MPCNN for Power Equipment Defect Text},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3464380},
doi = {10.1145/3464380},
abstract = {The text classification of power equipment defect is of great significance to equipment health condition evaluation and power equipment maintenance decisions. Most of the existing classification methods do not sufficiently consider the semantic relation between words in the same sentence and cannot extract deep semantic features. To tackle those problems, this article proposes a novel classification method by combining the self-attention mechanism and multi-channel pyramid convolution neural networks. We utilize the bidirectional gated recurrent unit to model the text sequence and, on this basis, improve self-attention layer to dot multiplication on the forward and backward features to obtain the global attention score. Thereby, effective features are enhanced, invalid features are weakened, and important text representation vectors are obtained. To solve the problem that the shallow network structure cannot extract deep semantic features, we design a multi-channel pyramid convolution network, which first extracts deep text features from the channels of different windows and then fuses the text features of each channel. By comparing with the state-of-the-art methods, the model in this article has better performance in text classification of power equipment defects.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {105},
numpages = {21},
keywords = {Text classification, defect texts, self-attention mechanism, bidirectional gated recurrent unit, pyramid convolution}
}

@article{10.1109/TCBB.2022.3215257,
author = {Wu, Kaitao and Wang, Lexiang and Liu, Bo and Liu, Yang and Wang, Yadong and Li, Junyi},
title = {PSPGO: Cross-Species Heterogeneous Network Propagation for Protein Function Prediction},
year = {2022},
issue_date = {May-June 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3215257},
doi = {10.1109/TCBB.2022.3215257},
abstract = {How to use computational methods to effectively predict the function of proteins remains a challenge. Most prediction methods based on single species or single data source have some limitations: the former need to train different models for different species, the latter only to infer protein function from a single perspective, such as the method only using Protein-Protein Interaction (PPI) network just considers the protein environment but ignore the intrinsic characteristics of protein sequences. We found that in some network-based multi-species methods the networks of each species are isolated, which means there is no communication between networks of different species. To solve these problems, we propose a cross-species heterogeneous network propagation method based on graph attention mechanism, PSPGO, which can propagate feature and label information on sequence similarity (SS) network and PPI network for predicting gene ontology terms. Our model is evaluated on a large multi-species dataset split based on time and is compared with several state-of-the-art methods. The results show that our method has good performance. We also explore the predictive performance of PSPGO for a single species. The results illustrate that PSPGO also performs well in prediction for single species.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = oct,
pages = {1713–1724},
numpages = {12}
}

@inproceedings{10.1145/3543507.3583303,
author = {Du, Yuntao and Lian, Jianxun and Yao, Jing and Wang, Xiting and Wu, Mingqi and Chen, Lu and Gao, Yunjun and Xie, Xing},
title = {Towards Explainable Collaborative Filtering with Taste Clusters Learning},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583303},
doi = {10.1145/3543507.3583303},
abstract = {Collaborative Filtering (CF) is a widely used and effective technique for recommender systems. In recent decades, there have been significant advancements in latent embedding-based CF methods for improved accuracy, such as matrix factorization, neural collaborative filtering, and LightGCN. However, the explainability of these models has not been fully explored. Adding explainability to recommendation models can not only increase trust in the decision-making process, but also have multiple benefits such as providing persuasive explanations for item recommendations, creating explicit profiles for users and items, and assisting item producers in design improvements. In this paper, we propose a neat and effective Explainable Collaborative Filtering (ECF) model that leverages interpretable cluster learning to achieve the two most demanding objectives: (1) Precise - the model should not compromise accuracy in the pursuit of explainability; and (2) Self-explainable - the model’s explanations should truly reflect its decision-making process, not generated from post-hoc methods. The core of ECF is mining taste clusters from user-item interactions and item profiles. We map each user and item to a sparse set of taste clusters, and taste clusters are distinguished by a few representative tags. The user-item preference, users/items’ cluster affiliations, and the generation of taste clusters are jointly optimized in an end-to-end manner. Additionally, we introduce a forest mechanism to ensure the model’s accuracy, explainability, and diversity. To comprehensively evaluate the explainability quality of taste clusters, we design several quantitative metrics, including in-cluster item coverage, tag utilization, silhouette, and informativeness. Our model’s effectiveness is demonstrated through extensive experiments on three real-world datasets.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3712–3722},
numpages = {11},
keywords = {Clustering, Collaborative Filtering, Explanability, Recommendation},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3477495.3531731,
author = {Dietz, Laura and Chatterjee, Shubham and Lennox, Connor and Kashyapi, Sumanta and Oza, Pooja and Gamari, Ben},
title = {Wikimarks: Harvesting Relevance Benchmarks from Wikipedia},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531731},
doi = {10.1145/3477495.3531731},
abstract = {We provide a resource for automatically harvesting relevance benchmarks from Wikipedia -- which we refer to as "Wikimarks" to differentiate them from manually created benchmarks. Unlike simulated benchmarks, they are based on manual annotations of Wikipedia authors. Studies on the TREC Complex Answer Retrieval track demonstrated that leaderboards under Wikimarks and manually annotated benchmarks are very similar. Because of their availability, Wikimarks can fill an important need for Information Retrieval research.  We provide a meta-resource to harvest Wikimarks for several information retrieval tasks across different languages: paragraph retrieval, entity ranking, query-specific clustering, outline prediction, and relevant entity linking and many more. In addition, we provide example Wikimarks for English, Simple English, and Japanese derived from the 01/01/2022 Wikipedia dump.  Resource available: https://trema-unh.github.io/wikimarks/},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3003–3012},
numpages = {10},
keywords = {query-specific clustering, relevant entity linking, test collections},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@proceedings{10.1145/3603607,
title = {HUMAN '23: Proceedings of the 6th Workshop on Human Factors in Hypertext},
year = {2023},
isbn = {9798400702396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@article{10.1109/TASLP.2022.3199648,
author = {Wang, Huadong and Shen, Xin and Tu, Mei and Zhuang, Yimeng and Liu, Zhiyuan},
title = {Improved Transformer With Multi-Head Dense Collaboration},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3199648},
doi = {10.1109/TASLP.2022.3199648},
abstract = {Recently, the attention mechanism boosts the performance of many neural network models in Natural Language Processing (NLP). Among the various attention mechanisms, Multi-Head Attention (MHA) is a powerful and popular variant. MHA helps the model to attend to different feature subspaces independently which is an essential component of Transformer. Despite its success, we conjecture that the different heads of the existing MHA may not collaborate properly. To validate this assumption and further improve the performance of Transformer, we study the collaboration problem for MHA in this paper. First, we propose the Single-Layer Collaboration (SLC) mechanism to help each attention head improve its attention distribution based on the feedback of other heads. Furthermore, we extend SLC to the cross-layer Multi-Head Dense Collaboration (MHDC) mechanism. MHDC helps each MHA layer learn the attention distributions considering the knowledge from the other MHA layers. Both SLC and MHDC are implemented as lightweight modules with very few additional parameters. When equipped with these modules, our new framework, i.e., Collaborative TransFormer (&lt;italic&gt;CollFormer&lt;/italic&gt;), significantly outperforms the vanilla Transformer on a range of NLP tasks, including machine translation, sentence semantic relatedness, natural language inference, sentence classification, and reading comprehension. Besides, we also carry out extensive quantitative experiments to analyze the properties of the MHDC in different settings. The experimental results validate the effectiveness and universality of MHDC as well as &lt;italic&gt;CollFormer&lt;/italic&gt;.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2754–2767},
numpages = {14}
}

@proceedings{10.1145/3573942,
title = {AIPR '22: Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
year = {2022},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3599696,
title = {OASIS '23: Proceedings of the 3rd International Workshop on Open Challenges in Online Social Networks},
year = {2023},
isbn = {9798400702259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@proceedings{10.1145/3594536,
title = {ICAIL '23: Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is my great pleasure to present to you the proceedings of the Nineteenth International Conference on Artificial Intelligence and Law (ICAIL 2023). The conference will be held June 19-23 at the Universidade do Minho in Braga, Portugal. It has been organized by the International Association for Artificial Intelligence and Law (IAAIL) and is held in cooperation with AAAI and ACM SIGAI. IAAIL's mission is to facilitate research, collaboration, and interdisciplinary communication at the intersection of law and the technical disciplines belonging to the field of artificial intelligence. The first ICAIL conference was held in 1987 and its 2023 iteration is the first to be held in person again after the Covid-19 pandemic.},
location = {Braga, Portugal}
}

@article{10.1145/3557894,
author = {Liao, Junwei and Eskimez, Sefik and Lu, Liyang and Shi, Yu and Gong, Ming and Shou, Linjun and Qu, Hong and Zeng, Michael},
title = {Improving Readability for Automatic Speech Recognition Transcription},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3557894},
doi = {10.1145/3557894},
abstract = {Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other noises common in spoken communication. These readable issues introduced by speakers and ASR systems will impair the performance of downstream tasks and the understanding of human readers. In this work, we present a task called ASR post-processing for readability (APR) and formulate it as a sequence-to-sequence text generation problem. The APR task aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of speakers. We further study the APR task from the benchmark dataset, evaluation metrics, and baseline models: First, to address the lack of task-specific data, we propose a method to construct a dataset for the APR task by using the data collected for grammatical error correction. Second, we utilize metrics adapted or borrowed from similar tasks to evaluate model performance on the APR task. Lastly, we use several typical or adapted pre-trained models as the baseline models for the APR task. Furthermore, we fine-tune the baseline models on the constructed dataset and compare their performance with a traditional pipeline method in terms of proposed evaluation metrics. Experimental results show that all the fine-tuned baseline models perform better than the traditional pipeline method, and our adapted RoBERTa model outperforms the pipeline method by 4.95 and 6.63 BLEU points on two test sets, respectively. The human evaluation and case study further reveal the ability of the proposed model to improve the readability of ASR transcripts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {142},
numpages = {23},
keywords = {Automatic speech recognition, post-processing for readability, data synthesis, pre-trained model}
}

@inproceedings{10.1145/3544548.3581393,
author = {Yang, Qian and Hao, Yuexing and Quan, Kexin and Yang, Stephen and Zhao, Yiran and Kuleshov, Volodymyr and Wang, Fei},
title = {Harnessing Biomedical Literature to Calibrate Clinicians’ Trust in AI Decision Support Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581393},
doi = {10.1145/3544548.3581393},
abstract = {Clinical decision support tools (DSTs), powered by Artificial Intelligence (AI), promise to improve clinicians’ diagnostic and treatment decision-making. However, no AI model is always correct. DSTs must enable clinicians to validate each AI suggestion, convincing them to take the correct suggestions while rejecting its errors. While prior work often tried to do so by explaining AI’s inner workings or performance, we chose a different approach: We investigated how clinicians validated each other’s suggestions in practice (often by referencing scientific literature) and designed a new DST that embraces these naturalistic interactions. This design uses GPT-3 to draw literature evidence that shows the AI suggestions’ robustness and applicability (or the lack thereof). A prototyping study with clinicians from three disease areas proved this approach promising. Clinicians’ interactions with the prototype also revealed new design and research opportunities around (1) harnessing the complementary strengths of literature-based and predictive decision supports; (2) mitigating risks of de-skilling clinicians; and (3) offering low-data decision support with literature.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {14},
numpages = {14},
keywords = {Biomedical Literature, Clinical AI, Qualitative Method, XAI},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3617184,
title = {ICCSIE '23: Proceedings of the 8th International Conference on Cyber Security and Information Engineering},
year = {2023},
isbn = {9798400708800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Putrajaya, Malaysia}
}

@inproceedings{10.1145/3604951.3605514,
author = {Anuradha Nanomi Arachchige, Isuri and Ha, Le and Mitkov, Ruslan and Steinert, Johannes-Dieter},
title = {Enhancing Named Entity Recognition for Holocaust Testimonies through Pseudo Labelling and Transformer-based Models},
year = {2023},
isbn = {9798400708411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604951.3605514},
doi = {10.1145/3604951.3605514},
abstract = {The Holocaust was a tragic and catastrophic event in World War II (WWII) history that resulted in the loss of millions of lives. In recent years, the emergence of the field of digital humanities has made the study of Holocaust testimonies an important area of research for historians, Holocaust educators, social scientists, and linguists. One of the challenges in analysing Holocaust testimonies is the recognition and categorisation of named entities such as concentration camps, military officers, ships, and ghettos, due to the scarcity of annotated data. This paper presents a research study on a domain-specific hybrid named-entity recognition model, which focuses on developing NER models specifically tailored for the Holocaust domain. To overcome the problem of data scarcity, we employed hybrid annotation approach to training different transformer model architectures in order to recognise the named entities. Results show transformer models to have good performance compared to other approaches.},
booktitle = {Proceedings of the 7th International Workshop on Historical Document Imaging and Processing},
pages = {85–90},
numpages = {6},
keywords = {Holocaust Testimonies, NER, Pseudo Labelling, Transformers},
location = {San Jose, CA, USA},
series = {HIP '23}
}

@article{10.14778/3538598.3538608,
author = {Fan, Wenfei and Jin, Ruochun and Lu, Ping and Tian, Chao and Xu, Ruiqi},
title = {Towards event prediction in temporal graphs},
year = {2022},
issue_date = {May 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3538598.3538608},
doi = {10.14778/3538598.3538608},
abstract = {This paper proposes a class of temporal association rules, denoted by TACOs, for event prediction. As opposed to previous graph rules, TACOs monitor updates to graphs, and can be used to capture temporal interests in recommendation and catch frauds in response to behavior changes, among other things. TACOs are defined on temporal graphs in terms of change patterns and (temporal) conditions, and may carry machine learning (ML) predicates for temporal event prediction. We settle the complexity of reasoning about TACOs, including their satisfiability, implication and prediction problems. We develop a system, referred to as TASTE. TASTE discovers TACOs by iteratively training a rule creator based on generative ML models in a creator-critic framework. Moreover, it predicts events by applying the discovered TACOs. Using real-life and synthetic datasets, we experimentally verify that TASTE is on average 31.4 times faster than conventional data mining methods in TACO discovery, and it improves the accuracy of state-of-the-art event prediction models by 23.4%.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1861–1874},
numpages = {14}
}

@article{10.1145/3462476,
author = {Tamine, Lynda and Goeuriot, Lorraine},
title = {Semantic Information Retrieval on Medical Texts: Research Challenges, Survey, and Open Issues},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3462476},
doi = {10.1145/3462476},
abstract = {The explosive growth and widespread accessibility of medical information on the Internet have led to a surge of research activity in a wide range of scientific communities including health informatics and information retrieval (IR). One of the common concerns of this research, across these disciplines, is how to design either clinical decision support systems or medical search engines capable of providing adequate support for both novices (e.g., patients and their next-of-kin) and experts (e.g., physicians, clinicians) tackling complex tasks (e.g., search for diagnosis, search for a treatment). However, despite the significant multi-disciplinary research advances, current medical search systems exhibit low levels of performance. This survey provides an overview of the state of the art in the disciplines of IR and health informatics, and bridging these disciplines shows how semantic search techniques can facilitate medical IR. First,we will give a broad picture of semantic search and medical IR and then highlight the major scientific challenges. Second, focusing on the semantic gap challenge, we will discuss representative state-of-the-art work related to feature-based as well as semantic-based representation and matching models that support medical search systems. In addition to seminal works, we will present recent works that rely on research advancements in deep learning. Third, we make a thorough cross-model analysis and provide some findings and lessons learned. Finally, we discuss some open issues and possible promising directions for future research trends.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {146},
numpages = {38},
keywords = {Information retrieval, evaluation, knowledge resources, medical texts, relevance}
}

@proceedings{10.1145/3585088,
title = {IDC '23: Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@article{10.1145/3569423,
author = {Li, Lei and Zhang, Yongfeng and Chen, Li},
title = {On the Relationship between Explanation and Recommendation: Learning to Rank Explanations for Improved Performance},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3569423},
doi = {10.1145/3569423},
abstract = {Explaining to users why some items are recommended is critical, as it can help users to make better decisions, increase their satisfaction, and gain their trust in recommender systems (RS). However, existing explainable RS usually consider explanation as a side output of the recommendation model, which has two problems: (1) It is difficult to evaluate the produced explanations, because they are usually model-dependent, and (2) as a result, how the explanations impact the recommendation performance is less investigated.In this article, explaining recommendations is formulated as a ranking task and learned from data, similarly to item ranking for recommendation. This makes it possible for standard evaluation of explanations via ranking metrics (e.g., Normalized Discounted Cumulative Gain). Furthermore, this article extends traditional item ranking to an item–explanation joint-ranking formalization to study if purposely selecting explanations could reach certain learning goals, e.g., improving recommendation performance. A great challenge, however, is that the sparsity issue in the user-item-explanation data would be inevitably severer than that in traditional user–item interaction data, since not every user–item pair can be associated with all explanations. To mitigate this issue, this article proposes to perform two sets of matrix factorization by considering the ternary relationship as two groups of binary relationships. Experiments on three large datasets verify the solution’s effectiveness on both explanation ranking and item recommendation.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {21},
numpages = {24},
keywords = {Explainable recommendation, explanation ranking, learning to explain}
}

@inproceedings{10.1145/3459637.3482261,
author = {Liu, Zhihong and Li, Huiyu and Li, Ruixin and Zeng, Yong and Ma, Jianfeng},
title = {Graph Embedding Based on Euclidean Distance Matrix and its Applications},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482261},
doi = {10.1145/3459637.3482261},
abstract = {Graph embedding converts a graph into a multi-dimensional space in which the graph structural information or graph properties are maximumly preserved. It is an effective and efficient way to provide users a deeper understanding of what is behind the data and thus can benefit a lot of useful applications. However, most graph embedding methods suffer from high computation and space costs. In this paper, we present a simple graph embedding method that directly embeds the graph into its Euclidean distance space. This method does not require the learned representations to be low dimensional, but it has several good characteristics. We find that the centrality of nodes/edges can be represented by the position of nodes or the length of edges when a graph is embedded. Besides, the edge length is closely related to the density of regions in a graph. We then apply this graph embedding method into graph analytics, such as community detection, graph compression, and wormhole detection, etc. Our evaluation shows the effectiveness and efficiency of this embedding method and contends that it yields a promising approach to graph analytics.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1140–1149},
numpages = {10},
keywords = {community detection, graph compression, graph embedding, social networks, wormhole detection},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.14778/3529337.3529355,
author = {Leone, Manuel and Huber, Stefano and Arora, Akhil and Garc\'{\i}a-Dur\'{a}n, Alberto and West, Robert},
title = {A critical re-evaluation of neural methods for entity alignment},
year = {2022},
issue_date = {April 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3529337.3529355},
doi = {10.14778/3529337.3529355},
abstract = {Neural methods have become the de-facto choice for the vast majority of data analysis tasks, and entity alignment (EA) is no exception. Not surprisingly, more than 50 different neural EA methods have been published since 2017. However, surprisingly, an analysis of the differences between neural and non-neural EA methods has been lacking. We bridge this gap by performing an in-depth comparison among five carefully chosen representative state-of-the-art methods from the pre-neural and neural era. We unravel, and consequently mitigate, the inherent deficiencies in the experimental setup utilized for evaluating neural EA methods. To ensure fairness in evaluation, we homogenize the entity matching modules of neural and non-neural methods. Additionally, for the first time, we draw a parallel between EA and record linkage (RL) by empirically showcasing the ability of RL methods to perform EA. Our results indicate that Paris, the state-of-the-art non-neural method, statistically significantly outperforms all the representative state-of-the-art neural methods in terms of both efficacy and efficiency across a wide variety of dataset types and scenarios, and is second only to BERT-INT for a specific scenario of cross-lingual EA. Our findings shed light on the potential problems resulting from an impulsive application of neural methods as a panacea for all data analytics tasks. Overall, our work results in two overarching conclusions: (1) Paris should be used as a baseline in every follow-up work on EA, and (2) neural methods need to be positioned better to showcase their true potential, for which we provide multiple recommendations.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {1712–1725},
numpages = {14}
}

@proceedings{10.1145/3578503,
title = {WebSci '23: Proceedings of the 15th ACM Web Science Conference 2023},
year = {2023},
isbn = {9798400700897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@article{10.1145/3578553,
author = {Wanjawa, Barack W. and Wanzare, Lilian D. A. and Indede, Florence and Mconyango, Owen and Muchemi, Lawrence and Ombui, Edward},
title = {KenSwQuAD—A Question Answering Dataset for Swahili Low-resource Language},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3578553},
doi = {10.1145/3578553},
abstract = {The need for question-answering (QA) datasets in low-resource languages is the motivation of this research, leading to the development of the Kencorpus Swahili Question Answering Dataset (KenSwQuAD). This dataset is annotated from raw story texts of Swahili, a low-resource language that is predominantly spoken in eastern Africa and in other parts of the world. Question-answering datasets are important for machine comprehension of natural language for tasks such as internet search and dialog systems. Machine learning systems need training data such as the gold-standard question-answering set developed in this research. The research engaged annotators to formulate QA pairs from Swahili texts collected by the Kencorpus project, a Kenyan languages corpus. The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA pairs each, resulting in a final dataset of 7,526 QA pairs. A quality assurance set of 12.5% of the annotated texts confirmed that the QA pairs were all correctly annotated. A proof of concept on applying the set to the QA task confirmed that the dataset can be usable for such tasks. KenSwQuAD has also contributed to resourcing of the Swahili language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {113},
numpages = {20},
keywords = {Swahili, question answer, low-resource languages}
}

@inproceedings{10.1145/3485447.3512026,
author = {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and Hai, Zhen},
title = {Geospatial Entity Resolution},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512026},
doi = {10.1145/3485447.3512026},
abstract = {A geospatial database is today at the core of an ever increasing number of services. Building and maintaining it remains challenging due to the need to merge information from multiple providers. Entity Resolution (ER) consists of finding entity mentions from different sources that refer to the same real world entity. In geospatial ER, entities are often represented using different schemes and are subject to incomplete information and inaccurate location, making ER and deduplication daunting tasks. While tremendous advances have been made in traditional entity resolution and natural language processing, geospatial data integration approaches still heavily rely on static similarity measures and human-designed rules. In order to achieve automatic linking of geospatial data, a unified representation of entities with heterogeneous attributes and their geographical context, is needed. To this end, we propose Geo-ER1, a joint framework that combines Transformer-based language models, that have been successfully applied in ER, with a novel learning-based architecture to represent the geospatial character of the entity. Different from existing solutions, Geo-ER does not rely on pre-defined rules and is able to capture information from surrounding entities in order to make context-based, accurate predictions. Extensive experiments on eight real world datasets demonstrate the effectiveness of our solution over state-of-the-art methods. Moreover, Geo-ER proves to be robust in settings where there is no available training data for a specific city.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {3061–3070},
numpages = {10},
keywords = {Entity resolution, geospatial data, graph attention, neighbourhood embedding, neural networks},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3459637.3481937,
author = {Yu, Tan and Yang, Yi and Li, Yi and Liu, Lin and Sun, Mingming and Li, Ping},
title = {Multi-modal Dictionary BERT for Cross-modal Video Search in Baidu Advertising},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481937},
doi = {10.1145/3459637.3481937},
abstract = {Due to their attractiveness, video advertisements are adored by advertisers. Baidu, as one of the leading search advertisement platforms in China, is putting more and more effort into video advertisements for its advertisement customers. Search-based video advertisement display is, in essence, a cross-modal retrieval problem, which is normally tackled through joint embedding methods. Nevertheless, due to the lack of interactions between text features and image features, joint embedding methods cannot achieve as high accuracy as its counterpart based on attention. Inspired by the great success achieved by BERT in NLP tasks, many cross-modal BERT models emerge and achieve excellent performance in cross-modal retrieval. Last year, Baidu also launched a cross-modal BERT, CAN, in video advertisement platform, and achieved considerably better performance than the previous joint-embedding model. In this paper, we present our recent work for video advertisement retrieval, Multi-modal Dictionary BERT (MDBERT) model. Compared with CAN and other cross-modal BERT models, MDBERT integrates a joint dictionary, which is shared among video features and word features. It maps the relevant word features and video features into the same codeword and thus fosters effective cross-modal attention. To support end-to-end training, we propose to soften the codeword assignment. Meanwhile, to enhance the inference efficiency, we adopt the product quantization to achieve fine-level feature space partition at a low cost. After launching MDBERT in Baidu video advertising platform, the conversion ratio (CVR) increases by 3.34%, bringing a considerable revenue boost for advertisers in Baidu.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4341–4351},
numpages = {11},
keywords = {advertisement, computer vision, cross-modal retrieval, deep learning, natural language processing, search},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3571884.3597138,
author = {Braunschweiler, Norbert and Doddipatla, Rama Sanand and Keizer, Simon and Stoyanchev, Svetlana},
title = {Enabling Semi-Structured Knowledge Access via a Question-Answering Module in Task-oriented Dialogue Systems},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3597138},
doi = {10.1145/3571884.3597138},
abstract = {Users of task-oriented dialogue systems are often limited to ‘in-schema queries’, i.e., questions constrained by a predefined database structure. Providing access to additional semi- or unstructured knowledge could enable users to enter a wider range of queries answerable by the system. To this end, we have integrated a Question-Answering (QA)-module in an interactive restaurant search system and evaluated its impact using a crowd-sourced user evaluation. The QA-module includes knowledge selection and response generation components, both driven by fine-tuned GPT-2 language models, and a method to prevent responses unrelated to a user question (‘off-topic responses’). The results show that systems with QA-module are significantly preferred over the baseline without QA-module. Moreover, while the off-topic response prevention method was correctly triggered in 98.1% of questions not covered in the knowledge base, users showed more preference to the system that can retrieve information irrespective of whether it is relevant or not.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {36},
numpages = {11},
keywords = {dialogue, dialogue systems, human-computer interaction, information retrieval, question-answering},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}

@article{10.1145/3626095,
author = {Zamudio Padilla, Juan Diego and Wang, Liuqin},
title = {Binary Semantic Pattern Rules for Chinese-English Machine Translation Based on Machine Learning Algorithms},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3626095},
doi = {10.1145/3626095},
abstract = {With the increase of internationalization and the exponential growth of intercultural communication, the importance of interlingual translation has become increasingly prominent. Machine translation has been a booming area of research as technology has advanced. Due to the complexity of language ability and limited understanding of language laws, there are challenges for machine translation. This paper focused on how to construct and apply binary semantic pattern rules through machine learning to improve the translation effect in Chinese-English machine translation. The research results of this paper would contribute to the further development and improvement of Chinese-English machine translation technology. In order to produce high-quality translation results, research in machine translation has recognized the need to analyze and understand the semantics of natural language. To address the important issue of lexical and syntactic ambiguity, representations of binary semantic pattern rules have been developed to formally describe these rules. Based on this, this paper designed and implemented a corpus-based binary semantic rule extraction and optimization algorithm, which used machine learning algorithms to automatically detect the semantic rules of two or more than two phrases in the Chinese corpus, and then automatically optimized and converted them according to the statistical results, and realized the design of Chinese-English machine translation system. The article evaluated the quality of machine translation to test the effectiveness of machine translation binary semantic pattern rules based on machine learning algorithms. The study found that compared with the rule set A, the rule sets B and C obtained automatically by the rule mining algorithm had significantly improved accuracy, both reaching more than 90%. This showed that the binary semantic pattern rule mining algorithm and optimization algorithm proposed in this paper were reasonable.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
keywords = {Machine Learning, Machine Translation, Binary Semantic Pattern Rules, Association Rules, Chinese and English Corpora}
}

@article{10.1145/3624988,
author = {Bassani, Elias and Tonellotto, Nicola and Pasi, Gabriella},
title = {Personalized Query Expansion with Contextual Word Embeddings},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3624988},
doi = {10.1145/3624988},
abstract = {Personalized Query Expansion, the task of expanding queries with additional terms extracted from the user-related vocabulary, is a well-known solution to improve the retrieval performance of a system w.r.t. short queries. Recent approaches rely on word embeddings to select expansion terms from user-related texts. Although promising results have been delivered with former word embedding techniques, we argue that these methods are not suited for contextual word embeddings, which produce a unique vector representation for each term occurrence.In this article, we propose a Personalized Query Expansion method designed to solve the issues arising from the use of contextual word embeddings with the current Personalized Query Expansion approaches based on word embeddings. Specifically, we employ a clustering-based procedure to identify the terms that better represent the user interests and to improve the diversity of those selected for expansion, achieving improvements of up to 4% w.r.t. the best-performing baseline in terms of MAP@100. Moreover, our approach outperforms previous ones in terms of efficiency, allowing us to achieve sub-millisecond expansion times even in data-rich scenarios. Finally, we introduce a novel metric to evaluate the expansion terms’ diversity and empirically show the unsuitability of previous approaches based on word embeddings when employed along with contextual word embeddings, which cause the selection of semantically overlapping expansion terms.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {61},
numpages = {35},
keywords = {Personalization, Query Expansion, contextual word embeddings, dense retrieval}
}

@inproceedings{10.1109/ICSE48619.2023.00161,
author = {Nam, Daye and Myers, Brad and Vasilescu, Bogdan and Hellendoorn, Vincent},
title = {Improving API Knowledge Discovery with ML: A Case Study of Comparable API Methods},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00161},
doi = {10.1109/ICSE48619.2023.00161},
abstract = {Developers constantly learn new APIs, but often lack necessary information from documentation, resorting instead to popular question-and-answer platforms such as Stack Overflow. In this paper, we investigate how to use recent machine-learning-based knowledge extraction techniques to automatically identify pairs of comparable API methods and the sentences describing the comparison from Stack Overflow answers. We first built a prototype that can be stocked with a dataset of comparable API methods and provides tool-tips to users in search results and in API documentation. We conducted a user study with this tool based on a dataset of TensorFlow comparable API methods spanning 198 hand-annotated facts from Stack Overflow posts. This study confirmed that providing comparable API methods can be useful for helping developers understand the design space of APIs: developers using our tool were significantly more aware of the comparable API methods and better understood the differences between them. We then created SOREL, an comparable API methods knowledge extraction tool trained on our hand-annotated corpus, which achieves a 71% precision and 55% recall at discovering our manually extracted facts and discovers 433 pairs of comparable API methods from thousands of unseen Stack Overflow posts. This work highlights the merit of jointly studying programming assistance tools and constructing machine learning techniques to power them.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1890–1906},
numpages = {17},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@proceedings{10.1145/3545801,
title = {ICBDC '22: Proceedings of the 7th International Conference on Big Data and Computing},
year = {2022},
isbn = {9781450396097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@proceedings{10.1145/3614008,
title = {SPML '23: Proceedings of the 2023 6th International Conference on Signal Processing and Machine Learning},
year = {2023},
isbn = {9798400707575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tianjin, China}
}

@inproceedings{10.1145/3490725.3490737,
author = {Huang, Chao and Di, Hui and Wang, Lina and Ouchi, Kazushige},
title = {ECO-DST: An Efficient Cross-lingual Dialogue State Tracking Framework},
year = {2022},
isbn = {9781450384247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490725.3490737},
doi = {10.1145/3490725.3490737},
abstract = {Data efficiency is a critical challenge for cross-lingual task-oriented dialogue state tracking (DST) due to high cost of collecting large amount of task-related labeled training set for specific language. Therefore, we focus on adapting high-performance source language DST to target language by using only bilingual dictionary, without accessing labeled target data. We propose a novel data efficient cross-lingual DST framework (ECO-DST), which consists of cross-lingual encoder and language independent decoder. To support cross-lingual zero-shot adaptation, we leverage two advanced methods in encoder: 1) pre-trained cross-lingual model XLM-RoBERTa (XLM-R), 2) dynamic local phrase code-switching data augmentation for cross-lingual representation alignment. We evaluate the proposed method on The Ninth Dialogue System Technology Challenge (DSTC9) cross-lingual tasks. For target language DST, we compare our proposed framework with submitted systems in DSTC9, our model achieves state-of-the-art result on CrossWOZ dataset and promising result on MultiWOZ 2.1 dataset. Meanwhile on source language DST, the same model keeps competitive performance compared with original source DST model.},
booktitle = {Proceedings of the 2021 4th International Conference on Machine Learning and Machine Intelligence},
pages = {77–82},
numpages = {6},
keywords = {Cross-Lingual Transfer, Data Efficiency, Dialogue State Tracking, Dynamic Local Phrase Code-Switching, Task-oriented Dialogue},
location = {Hangzhou, China},
series = {MLMI '21}
}

@article{10.1109/TASLP.2022.3181350,
author = {An, Jinwon and Cho, Sungzoon and Bang, Junseong and Kim, Misuk},
title = {Domain-Slot Relationship Modeling Using a Pre-Trained Language Encoder for Multi-Domain Dialogue State Tracking},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3181350},
doi = {10.1109/TASLP.2022.3181350},
abstract = {Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. As using pre-trained language models is the de facto standard for natural language processing tasks, many recent studies use them to encode the dialogue context for predicting the dialogue states. Model architectures that have certain inductive biases for modeling the relationship among different domain-slot pairs are also emerging. Our work is based on these research approaches on multi-domain dialogue state tracking. We propose a model architecture that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$[CLS]$&lt;/tex-math&gt;&lt;/inline-formula&gt; token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results on the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our model outperforms other models with the same setting. Our ablation studies incorporate three main parts. The first component shows the effectiveness of our approach exploiting the relationship modeling. The second component compares the effect of using different pre-trained language encoders. The final component involves comparing different initialization methods that could be used for the special tokens. Qualitative analysis of the attention map of the pre-trained language encoder shows that our special tokens encode relevant information through the encoding process by attending to each other.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2091–2102},
numpages = {12}
}

@article{10.1145/3450315,
author = {Joaristi, Mikel and Serra, Edoardo},
title = {SIR-GN: A Fast Structural Iterative Representation Learning Approach For Graph Nodes},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3450315},
doi = {10.1145/3450315},
abstract = {Graph representation learning methods have attracted an increasing amount of attention in recent years. These methods focus on learning a numerical representation of the nodes in a graph. Learning these representations is a powerful instrument for tasks such as graph mining, visualization, and hashing. They are of particular interest because they facilitate the direct use of standard machine learning models on graphs. Graph representation learning methods can be divided into two main categories: methods preserving the connectivity information of the nodes and methods preserving nodes’ structural information. Connectivity-based methods focus on encoding relationships between nodes, with connected nodes being closer together in the resulting latent space. While methods preserving structure generate a latent space where nodes serving a similar structural function in the network are encoded close to each other, independently of them being connected or even close to each other in the graph. While there are a lot of works that focus on preserving node connectivity, only a few works focus on preserving nodes’ structure. Properly encoding nodes’ structural information is fundamental for many real-world applications as it has been demonstrated that this information can be leveraged to successfully solve many tasks where connectivity-based methods usually fail. A typical example is the task of node classification, i.e., the assignment or prediction of a particular label for a node. Current limitations of structural representation methods are their scalability, representation meaning, and no formal proof that guaranteed the preservation of structural properties. We propose a new graph representation learning method, called Structural Iterative Representation learning approach for Graph Nodes (SIR-GN). In this work, we propose two variations (SIR-GN:&nbsp;GMM and SIR-GN:&nbsp;K-Means) and show how our best variation SIR-GN:&nbsp;K-Means: (1) theoretically guarantees the preservation of graph structural similarities, (2) provides a clear meaning about its representation and a way to interpret it with a specifically designed attribution procedure, and (3) is scalable and fast to compute. In addition, from our experiment, we show that SIR-GN:&nbsp;K-Means is often better or, in the worst-case comparable than the existing structural graph representation learning methods present in the literature. Also, we empirically show its superior scalability and computational performance when compared to other existing approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {100},
numpages = {39},
keywords = {Datasets, neural networks, gaze detection, text tagging}
}

@inproceedings{10.1145/3586183.3606822,
author = {Pu, Kevin and Yang, Jim and Yuan, Angel and Ma, Minyi and Dong, Rui and Wang, Xinyu and Chen, Yan and Grossman, Tovi},
title = {DiLogics: Creating Web Automation Programs with Diverse Logics},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606822},
doi = {10.1145/3586183.3606822},
abstract = {Knowledge workers frequently encounter repetitive web data entry tasks, like updating records or placing orders. Web automation increases productivity, but translating tasks to web actions accurately and extending to new specifications is challenging. Existing tools can automate tasks that perform the same logical trace of UI actions (e.g., input text in each field in order), but do not support tasks requiring different executions based on varied input conditions. We present DiLogics, a programming-by-demonstration system that utilizes NLP to assist users in creating web automation programs that handle diverse specifications. DiLogics first semantically segments input data to structured task steps. By recording user demonstrations for each step, DiLogics generalizes the web macros to novel but semantically similar task requirements. Our evaluation showed that non-experts can effectively use DiLogics to create automation programs that fulfill diverse input instructions. DiLogics provides an efficient, intuitive, and expressive method for developing web automation programs satisfying diverse specifications.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {74},
numpages = {15},
keywords = {PBD, Web automation, neurosymbolic programming},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3555041.3589406,
author = {Chai, Chengliang and Tang, Nan and Fan, Ju and Luo, Yuyu},
title = {Demystifying Artificial Intelligence for Data Preparation},
year = {2023},
isbn = {9781450395076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555041.3589406},
doi = {10.1145/3555041.3589406},
abstract = {Data preparation -- the process of discovering, integrating, transforming, cleaning, and annotating data -- is one of the oldest, hardest, yet inevitable data management problems. Unfortunately, data preparation is known to be iterative, requires high human cost, and is error-prone. Recent advances in artificial intelligence (AI) have shown very promising results on many data preparation tasks. At a high level, AI for data preparation (AI4DP) should have the following abilities. First, the AI model should capture real-world knowledge so as to solve various tasks. Second, it is important to easily adapt to new datasets/tasks. Third, data preparation is a complicated pipeline with many operations, which results in a large number of candidates to select the optimum, and thus it is crucial to effectively and efficiently explore the large space of possible pipelines.In this tutorial, we will cover three important topics to address the above issues: demystifying foundation models to inject knowledge for data preparation, tuning and adapting pre-trained language models for data preparation, and orchestrating data preparation pipelines for different downstream applications.},
booktitle = {Companion of the 2023 International Conference on Management of Data},
pages = {13–20},
numpages = {8},
keywords = {artificial intelligence, data preparation, foundation models},
location = {Seattle, WA, USA},
series = {SIGMOD '23}
}

@proceedings{10.1145/3627377,
title = {ICBDT '23: Proceedings of the 2023 6th International Conference on Big Data Technologies},
year = {2023},
isbn = {9798400707667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Qingdao, China}
}

@proceedings{10.1145/3584376,
title = {RICAI '22: Proceedings of the 2022 4th International Conference on Robotics, Intelligent Control and Artificial Intelligence},
year = {2022},
isbn = {9781450398343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dongguan, China}
}

@inproceedings{10.1145/3447548.3469053,
author = {Wang, Yu and Li, Jinchao and Naumann, Tristan and Xiong, Chenyan and Cheng, Hao and Tinn, Robert and Wong, Cliff and Usuyama, Naoto and Rogahn, Richard and Shen, Zhihong and Qin, Yang and Horvitz, Eric and Bennett, Paul N. and Gao, Jianfeng and Poon, Hoifung},
title = {Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3469053},
doi = {10.1145/3447548.3469053},
abstract = {Information overload is a prevalent challenge in many high-value domains. A prominent case in point is the explosion of the biomedical literature on COVID-19, which swelled to hundreds of thousands of papers in a matter of months. In general, biomedical literature expands by two papers every minute, totalling over a million new papers every year. Search in the biomedical realm, and many other vertical domains is challenging due to the scarcity of direct supervision from click logs. Self-supervised learning has emerged as a promising direction to overcome the annotation bottleneck. We propose a general approach for vertical search based on domain-specific pretraining and present a case study for the biomedical domain. Despite being substantially simpler and not using any relevance labels for training or development, our method performs comparably or better than the best systems in the official TREC-COVID evaluation, a COVID-related biomedical search competition. Using distributed computing in modern cloud infrastructure, our system can scale to tens of millions of articles on PubMed and has been deployed as Microsoft Biomedical Search, a new search experience for biomedical literature: https://aka.ms/biomedsearch.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {3717–3725},
numpages = {9},
keywords = {COVID-19, NLP, biomedical, domain-specific pretraining, search},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@proceedings{10.1145/3570773,
title = {ISAIMS '22: Proceedings of the 3rd International Symposium on Artificial Intelligence for Medicine Sciences},
year = {2022},
isbn = {9781450398442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3539618.3591911,
author = {Kaur, Simerjot and Smiley, Charese and Gupta, Akshat and Sain, Joy and Wang, Dongsheng and Siddagangappa, Suchetha and Aguda, Toyin and Shah, Sameena},
title = {REFinD: Relation Extraction Financial Dataset},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591911},
doi = {10.1145/3539618.3591911},
abstract = {A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with ~29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity. To encourage further research in this direction, REFinD is available at https://www.jpmorgan.com/technology/artificial-intelligence/initiatives/refind-dataset/problem-motivation-outcome.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3054–3063},
numpages = {10},
keywords = {annotation datasets, benchmarking, finance, information retrieval, natural language processing, relation extraction},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@proceedings{10.1145/3594315,
title = {ICCAI '23: Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tianjin, China}
}

@inproceedings{10.1145/3563657.3596002,
author = {Cho, Hyungjun and Lee, Jiyeon and Ku, Bonhee and Jeong, Yunwoo and Yadgarova, Shakhnozakhon and Nam, Tek-Jin},
title = {ARECA: A Design Speculation on Everyday Products Having Minds},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596002},
doi = {10.1145/3563657.3596002},
abstract = {An increasing number of everyday products are being designed to possess qualities such as intelligence, consciousness, and emotion. However, there is a need for more understanding on how to design for these properties of mind. To address this, we present the design of Areca, an air purifier that keeps a diary. This paper outlines our design process, focusing on how the diary generation process gives Areca properties of mind and how its appearance and interaction design support this concept. Through exhibiting Areca in a design exhibition, we gathered people's initial reactions and perceptions to further evaluate the effectiveness of our design intentions. Finally, based on these experiences, we engage in discussions on the design of products having minds.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {31–44},
numpages = {14},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3459637.3482328,
author = {Wu, Junda and Zhao, Canzhe and Yu, Tong and Li, Jingyang and Li, Shuai},
title = {Clustering of Conversational Bandits for User Preference Learning and Elicitation},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482328},
doi = {10.1145/3459637.3482328},
abstract = {Conversational recommender systems elicit user preference via interactive conversational interactions. By introducing conversational key-terms, existing conversational recommenders can effectively reduce the need for extensive exploration in a traditional interactive recommender. However, there are still limitations of existing conversational recommender approaches eliciting user preference via key-terms. First, the key-term data of the items needs to be carefully labeled, which requires a lot of human efforts. Second, the number of the human labeled key-terms is limited and the granularity of the key-terms is fixed, while the elicited user preference is usually from coarse-grained to fine-grained during the conversations. In this paper, we propose a clustering of conversational bandits algorithm. To avoid the human labeling efforts and automatically learn the key-terms with the proper granularity, we online cluster the items and generate meaningful key-terms for the items during the conversational interactions. Our algorithm is general and can also be used in the user clustering when the feedback from multiple users is available, which further leads to more accurate learning and generations of conversational key-terms. We analyze the regret bound of our learning algorithm. In the empirical evaluations, without using any human labeled key-terms, our algorithm effectively generates meaningful coarse-to-fine grained key-terms and performs as well as or better than the state-of-the-art baseline.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {2129–2139},
numpages = {11},
keywords = {clustering of bandits, conversational recommender, online learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3510858.3511425,
author = {Liu, Nan},
title = {Intelligent English automatic translation system based on computer corpus},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3511425},
doi = {10.1145/3510858.3511425},
abstract = {Machine translation system, also known as automatic translation system, generally refers to a relatively complex process of automatically by computer, and generally refers to the translation of sentences or full texts in this process. At present, systems are generally divided into two categories: one is statistical-based machine translation systems, and the other is case-based machine translation systems. Corpus-based machine translation system involves many disciplines, among which linguistics, mathematics and computer science are the most basic. Multi-disciplinary cooperation constitutes a rapidly developing machine translation system based on corpus. The rapid development of machine translation system has accelerated the cultural, economic and other exchanges between different countries. Looking at the current development situation, we can see that the rapid development of corpus linguistics and computer linguistics has greatly improved the performance of today's machine translation system.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {925–929},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@proceedings{10.1145/3569192,
title = {ICBRA '22: Proceedings of the 9th International Conference on Bioinformatics Research and Applications},
year = {2022},
isbn = {9781450396868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Berlin, Germany}
}

@article{10.1145/3589786,
author = {Langenecker, Sven and Sturm, Christoph and Schalles, Christian Schalles and Binnig, Carsten},
title = {Steered Training Data Generation for Learned Semantic Type Detection},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589786},
doi = {10.1145/3589786},
abstract = {In this paper, we introduce STEER to adapt learned semantic type extraction approaches to a new, unseen data lake. STEER provides a data programming framework for semantic labeling which is used to generate new labeled training data with minimal overhead. At its core, STEER comes with a novel training data generation procedure called Steered-Labeling that can generate high quality training data not only for non-numeric but also for numerical columns. With this generated training data STEER is able to fine-tune existing learned semantic type extraction models. We evaluate our approach on four different data lakes and show that we can significantly improve the performance of two different types of learned models across all data lakes.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {201},
numpages = {25},
keywords = {data discovery, data lakes, semantic type detection}
}

@inproceedings{10.1145/3528588.3528658,
author = {Alchokr, Rand and Borkar, Manoj and Thotadarya, Sharanya and Saake, Gunter and Leich, Thomas},
title = {Supporting systematic literature reviews using deep-learning-based language models},
year = {2023},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528588.3528658},
doi = {10.1145/3528588.3528658},
abstract = {Background: Systematic Literature Reviews are an important research method for gathering and evaluating the available evidence regarding a specific research topic. However, the process of conducting a Systematic Literature Review manually can be difficult and time-consuming. For this reason, researchers aim to semi-automate this process or some of its phases. Aim: We aimed at using a deep-learning based contextualized embeddings clustering technique involving transformer-based language models and a weighted scheme to accelerate the conduction phase of Systematic Literature Reviews for efficiently scanning the initial set of retrieved publications. Method: We performed an experiment using two manually conducted SLRs to evaluate the performance of two deep-learning-based clustering models. These models build on transformer-based deep language models (i.e., BERT and S-BERT) to extract contextualized embeddings on different text levels along with a weighted scheme to cluster similar publications. Results: Our primary results show that clustering based on embedding at paragraph-level using S-BERT-paragraph represents the best performing model setting in terms of optimizing the required parameters such as correctly identifying primary studies, number of additional documents identified as part of the relevant cluster and the execution time of the experiments. Conclusions: The findings indicate that using natural-language-based deep-learning architectures for semi-automating the selection of primary studies can accelerate the scanning and identification process. While our results represent first insights only, such a technique seems to enhance SLR process, promising to help researchers identify the most relevant publications more quickly and efficiently.},
booktitle = {Proceedings of the 1st International Workshop on Natural Language-Based Software Engineering},
pages = {67–74},
numpages = {8},
keywords = {BERT, deep learning, language models, systematic literature review},
location = {Pittsburgh, Pennsylvania},
series = {NLBSE '22}
}

@inproceedings{10.1145/3437963.3441730,
author = {Zhang, Yu and Chen, Xiusi and Meng, Yu and Han, Jiawei},
title = {Hierarchical Metadata-Aware Document Categorization under Weak Supervision},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441730},
doi = {10.1145/3437963.3441730},
abstract = {Categorizing documents into a given label hierarchy is intuitively appealing due to the ubiquity of hierarchical topic structures in massive text corpora. Although related studies have achieved satisfying performance in fully supervised hierarchical document classification, they usually require massive human-annotated training data and only utilize text information. However, in many domains, (1) annotations are quite expensive where very few training samples can be acquired; (2) documents are accompanied by metadata information. Hence, this paper studies how to integrate the label hierarchy, metadata, and text signals for document categorization under weak supervision. We develop HiMeCat, an embedding-based generative framework for our task. Specifically, we propose a novel joint representation learning module that allows simultaneous modeling of category dependencies, metadata information and textual semantics, and we introduce a data augmentation module that hierarchically synthesizes training documents to complement the original, small-scale training set. Our experiments demonstrate a consistent improvement of HiMeCat over competitive baselines and validate the contribution of our representation learning and data augmentation modules.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {770–778},
numpages = {9},
keywords = {hierarchical text categorization, metadata-aware text categorization, weak supervision},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@inproceedings{10.1145/3491102.3517434,
author = {Hope, Tom and Tamari, Ronen and Hershcovich, Daniel and Kang, Hyeonsu B and Chan, Joel and Kittur, Aniket and Shahaf, Dafna},
title = {Scaling Creative Inspiration with Fine-Grained Functional Aspects of Ideas},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517434},
doi = {10.1145/3491102.3517434},
abstract = {Large repositories of products, patents and scientific papers offer an opportunity for building systems that scour millions of ideas and help users discover inspirations. However, idea descriptions are typically in the form of unstructured text, lacking key structure that is required for supporting creative innovation interactions. Prior work has explored idea representations that were either limited in expressivity, required significant manual effort from users, or dependent on curated knowledge bases with poor coverage. We explore a novel representation that automatically breaks up products into fine-grained functional aspects capturing the purposes and mechanisms of ideas, and use it to support important creative innovation interactions: functional search for ideas, and exploration of the design space around a focal problem by viewing related problem perspectives pooled from across many products. In user studies, our approach boosts the quality of creative search and inspirations, substantially outperforming strong baselines by 50-60%.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {12},
numpages = {15},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@proceedings{10.1145/3574318,
title = {FIRE '22: Proceedings of the 14th Annual Meeting of the Forum for Information Retrieval Evaluation},
year = {2022},
isbn = {9798400700231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kolkata, India}
}

@article{10.1109/TASLP.2022.3153254,
author = {Chen, Xiaofeng and Wang, Guohua and Ren, Haopeng and Cai, Yi and Leung, Ho-fung and Wang, Tao},
title = {Task-Adaptive Feature Fusion for Generalized Few-Shot Relation Classification in an Open World Environment},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153254},
doi = {10.1109/TASLP.2022.3153254},
abstract = {Relation Classification (RC) is an important task in information extraction. In most real-world scenarios, the frequency of relations often follows a long-tailed and open-ended distribution. However, current efforts mainly focus on the partial frequency distribution of relations, which is limited in real-world applications. Meanwhile, prototypical network achieves remarkable performance among fields of deep supervised learning, few-shot learning and open set learning. Nevertheless, in the open world environment, it still suffers from the incompatible feature embedding problem as the novel and unknown relations come in. To address these problems, we propose an Open Generalized Prototypical Network with task-adaptive feature fusion for the open generalized few-shot relation classification. Extensive experiments are conducted on public large-scale datasets and our proposed model obtains the better performances.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1003–1015},
numpages = {13}
}

@inproceedings{10.1145/3460231.3474272,
author = {Polignano, Marco and Musto, Cataldo and de Gemmis, Marco and Lops, Pasquale and Semeraro, Giovanni},
title = {Together is Better: Hybrid Recommendations Combining Graph Embeddings and Contextualized Word Representations},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474272},
doi = {10.1145/3460231.3474272},
abstract = {In this paper, we present a hybrid recommendation framework based on the combination of graph embeddings and contextual word representations. Our approach is based on the intuition that each of the above mentioned representation models heterogeneous (and equally important) information, that is worth to be taken into account to generate a recommendation. Accordingly, we propose a strategy to combine both the features, which is based on the following steps: first, we separately generate graph embeddings and contextual word representations by exploiting state-of-the-art techniques. Next, these embeddings are used to feed a deep architecture that learns a hybrid representation based on the combination of the single groups of features. Finally, we exploit the resulting embedding to identify suitable recommendations. In the experimental session, we evaluate the effectiveness of our strategy on two datasets and results show that the use of a hybrid representation leads to an improvement of the predictive accuracy. Moreover, our approach overcomes several competitive baselines, thus confirming the validity of this work.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {187–198},
numpages = {12},
keywords = {BERT embeddings, USE embeddings, deep learning, graph embeddings, recommender systems},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/3540250.3549123,
author = {Cao, Junming and Chen, Bihuan and Sun, Chao and Hu, Longjie and Wu, Shuaihong and Peng, Xin},
title = {Understanding performance problems in deep learning systems},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549123},
doi = {10.1145/3540250.3549123},
abstract = {Deep learning (DL) has been widely applied to many domains. Unique challenges in engineering DL systems are posed by the programming paradigm shift from traditional systems to DL systems, and performance is one of the challenges. Performance problems (PPs) in DL systems can cause severe consequences such as excessive resource consumption and financial loss. While bugs in DL systems have been extensively investigated, PPs in DL systems have hardly been explored. To bridge this gap, we present the first comprehensive study to i) characterize symptoms, root causes, and introducing and exposing stages of PPs in DL systems developed in TensorFLow and Keras, with 224 PPs collected from 210 StackOverflow posts, and to ii) assess the capability of existing performance analysis approaches in tackling PPs, with a constructed benchmark of 58 PPs in DL systems. Our findings shed light on the implications on developing high-performance DL systems, and detecting and localizing PPs in DL systems. To demonstrate the usefulness of our findings, we develop a static checker DeepPerf to detect three types of PPs. It has detected 488 new PPs in 130 GitHub projects. 105 and 27 PPs have been confirmed and fixed.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {357–369},
numpages = {13},
keywords = {deep learning, performance analysis, performance problems},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3597926.3598085,
author = {Xu, Sihan and Gao, Ya and Fan, Lingling and Li, Linyu and Cai, Xiangrui and Liu, Zheli},
title = {LiResolver: License Incompatibility Resolution for Open Source Software},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598085},
doi = {10.1145/3597926.3598085},
abstract = {Open source software (OSS) licenses regulate the conditions under which OSS can be legally reused, distributed, and modified. However, a common issue arises when incorporating third-party OSS accompanied with licenses, i.e., license incompatibility, which occurs when multiple licenses exist in one project and there are conflicts between them. Despite being problematic, fixing license incompatibility issues requires substantial efforts due to the lack of license understanding and complex package dependency. In this paper, we propose LiResolver, a fine-grained, scalable, and flexible tool to resolve license incompatibility issues for open source software. Specifically, it first understands the semantics of licenses through fine-grained entity extraction and relation extraction. Then, it detects and resolves license incompatibility issues by recommending official licenses in priority. When no official licenses can satisfy the constraints, it generates a custom license as an alternative solution. Comprehensive experiments demonstrate the effectiveness of LiResolver, with 4.09% false positive (FP) rate and 0.02% false negative (FN) rate for incompatibility issue localization, and 62.61% of 230 real-world incompatible projects resolved by LiResolver. We discuss the feedback from OSS developers and the lessons learned from this work. All the datasets and the replication package of LiResolver have been made publicly available to facilitate follow-up research.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {652–663},
numpages = {12},
keywords = {License, License Incompatibility Resolution, Open Source Software},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@proceedings{10.1145/3608298,
title = {ICMHI '23: Proceedings of the 2023 7th International Conference on Medical and Health Informatics},
year = {2023},
isbn = {9798400700712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kyoto, Japan}
}

@article{10.1109/TASLP.2022.3161146,
author = {Su, Fangfang and Zhang, Yue and Li, Fei and Ji, Donghong},
title = {Balancing Precision and Recall for Neural Biomedical Event Extraction},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3161146},
doi = {10.1109/TASLP.2022.3161146},
abstract = {Biomedicalevent extraction is an essential task in the biomedical research. Existing models suffer from the issue of low recall due to the large proportion of unrecognized events and inflexible event argument combination. To address this issue, we propose an end-to-end multi-task approach for biomedical event extraction. Our model is able to achieve balanced precision and recall with several nichetargeting designs. First, neural encoders with rich lexical and syntactic features are used and shared by multiple subtasks such as event trigger recognition and argument relation extraction, in order to enhance the generalizability of the model. Second, a novel auxiliary subtask is added to identify the proteins that participate in the events, which helps decreasing the challenge of mining event-related proteins from the large candidate space. Third, event argument combination is performed using a strong neural network rather than inflexible rules or templates, to further increase the recall, especially for complex nested events. To demonstrate the effectiveness of our model, we evaluate it on two widely-used biomedical event extraction datasets used in the BioNLP 2011 and 2013 shared tasks. Our model achieves the state-of-the-art results (63.15% and 55.67% in F1 score) by significantly improving the recalls (compared with &lt;italic&gt;DeepEvnetMine&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$_{SciBERT}$&lt;/tex-math&gt;&lt;/inline-formula&gt;&lt;/italic&gt;, 4.65% and 5.0%) on the two datasets. Further experiments and analyses show the effectiveness of our proposed features and modules in the model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1637–1649},
numpages = {13}
}

@inproceedings{10.1145/3404835.3462977,
author = {Liao, Jinzhi and Zhao, Xiang and Li, Xinyi and Zhang, Lingling and Tang, Jiuyang},
title = {Learning Discriminative Neural Representations for Event Detection},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462977},
doi = {10.1145/3404835.3462977},
abstract = {Retrieving event instances from texts is pivotal to various natural language processing applications (e.g., automatic question answering and dialogue systems), and the first task to perform is event detection. There are two related sub-tasks therein-trigger identification and type classification, and the former is considered to play a dominant role. Nevertheless, it is notoriously challenging to predict event triggers right. To handle the task, existing work has made tremendous progress by incorporating manual features, data augmentation and neural networks, etc. Due to the scarcity of data and insufficient representation of trigger words, however, they still fail to precisely determine the spans of triggers (coined as trigger span detection problem). To address the challenge, we propose to learn discriminative neural representations (DNR) from texts. Specifically, our DNR model tackles the trigger span detection problem by exploiting two novel techniques: 1) a contrastive learning strategy, which enlarges the discrepancy between representations of words inside and outside triggers; and 2) a Mixspan strategy, which better trains the model to differentiate words nearby triggers' span boundaries. Extensive experiments on benchmarks-ACE2005 and TAC2015-demonstrate the superiority of our DNR model, leading to state-of-the-art performance.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {644–653},
numpages = {10},
keywords = {contrastive learning, event detection, mixed representation},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3404835.3462862,
author = {Wu, Jiancan and Wang, Xiang and Feng, Fuli and He, Xiangnan and Chen, Liang and Lian, Jianxun and Xie, Xing},
title = {Self-supervised Graph Learning for Recommendation},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462862},
doi = {10.1145/3404835.3462862},
abstract = {Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage and LightGCN. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.In this work, we explore self-supervised learning on user-item graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary self-supervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise three operators to generate the views --- node dropout, edge dropout, and random walk --- that change the graph structure in different manners. We term this new learning paradigm asSelf-supervised Graph Learning (SGL), implementing it on the state-of-the-art model LightGCN. Through theoretical analyses, we find that SGL has the ability of automatically mining hard negatives. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the recommendation accuracy, especially on long-tail items, and the robustness against interaction noises. Our implementations are available at urlhttps://github.com/wujcan/SGL.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {726–735},
numpages = {10},
keywords = {collaborative filtering, graph neural network, long-tail recommendation, self-supervised learning},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@article{10.1145/3604605,
author = {Ibrohim, Muhammad Okky and Bosco, Cristina and Basile, Valerio},
title = {Sentiment Analysis for the Natural Environment: A Systematic Review},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604605},
doi = {10.1145/3604605},
abstract = {In this systematic review, Kitchenham’s framework is used to explore what tasks, techniques, and benchmarks for Sentiment Analysis have been developed for addressing topics about the natural environment. We comprehensively analyze seven dimensions including contribution, topical focus, data source and query, annotation, language, detail of the task, and technology/algorithm used. By showing how this research area has grown during the last few years, our investigation provides important findings about the results achieved and the challenges that need to be still addressed for making this technology actually helpful for stakeholders such as policymakers and governments.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {88},
numpages = {37},
keywords = {Natural environment, data-driven policy, sentiment analysis, natural language processing (NLP), systematic review}
}

@inproceedings{10.1145/3442381.3450134,
author = {Liao, Lizi and Zhu, Tongyao and Long, Le Hong and Chua, Tat Seng},
title = {Multi-domain Dialogue State Tracking with Recursive Inference},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450134},
doi = {10.1145/3442381.3450134},
abstract = {Multi-domain dialogue state tracking (DST) is a critical component for monitoring user goals during the course of an interaction. Existing approaches have relied on dialogue history indiscriminately or updated on the most recent turns incrementally. However, in spite of modeling it based on fixed ontology or open vocabulary, the former setting violates the interactive and progressing nature of dialogue, while the later easily gets affected by the error accumulation conundrum. Here, we propose a Recursive Inference mechanism (ReInf) to resolve DST in multi-domain scenarios that call for more robust and accurate tracking capability. Specifically, our agent reversely reviews the dialogue history until the agent has pinpointed sufficient turns confidently for slot value prediction. It also recursively factors in potential dependencies among domains and slots to further solve the co-reference and value sharing problems. The quantitative and qualitative experimental results on the MultiWOZ 2.1 corpus demonstrate that the proposed ReInf not only outperforms the state-of-the-art methods, but also achieves reasonable turn reference and interpretable slot co-reference.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2568–2577},
numpages = {10},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@proceedings{10.1145/3599589,
title = {ICMIP '23: Proceedings of the 2023 8th International Conference on Multimedia and Image Processing},
year = {2023},
isbn = {9781450399586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tianjin, China}
}

@article{10.1109/TCBB.2022.3233856,
author = {Chen, Peng and Wang, Jian and Lin, Hongfei and Zhang, Yijia and Yang, Zhihao},
title = {Knowledge Adaptive Multi-Way Matching Network for Biomedical Named Entity Recognition via Machine Reading Comprehension},
year = {2023},
issue_date = {May-June 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3233856},
doi = {10.1109/TCBB.2022.3233856},
abstract = {Rapid and effective utilization of biomedical literature is paramount to combat diseases like COVID19. Biomedical named entity recognition (BioNER) is a fundamental task in text mining that can help physicians accelerate knowledge discovery to curb the spread of the COVID-19 epidemic. Recent approaches have shown that casting entity extraction as the machine reading comprehension task can significantly improve model performance. However, two major drawbacks impede higher success in identifying entities (1) ignoring the use of domain knowledge to capture the context beyond sentences and (2) lacking the ability to deeper understand the intent of questions. In this paper, to remedy this, we introduce and explore external domain knowledge which cannot be implicitly learned in text sequence. Previous works have focused more on text sequence and explored little of the domain knowledge. To better incorporate domain knowledge, a multi-way matching reader mechanism is devised to model representations of interaction between sequence, question and knowledge retrieved from Unified Medical Language System (UMLS). Benefiting from these, our model can better understand the intent of questions in complex contexts. Experimental results indicate that incorporating domain knowledge can help to obtain competitive results across 10 BioNER datasets, achieving absolute improvement of up to 2.02% in the f1 score.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {2101–2111},
numpages = {11}
}

@article{10.1109/TASLP.2022.3153256,
author = {Zhang, Mi and Qian, Tieyun and Liu, Bing},
title = {Exploit Feature and Relation Hierarchy for Relation Extraction},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153256},
doi = {10.1109/TASLP.2022.3153256},
abstract = {Existing methods in relation extraction have leveraged the lexical features in the word sequence and the syntactic features in the parse tree. Though effective, the lexical features extracted from the successive word sequence may introduce some noise that has little or no meaningful content. Meanwhile, the syntactic features are usually encoded via graph convolutional networks which have restricted receptive field. In addition, the relation between lexical and syntactic features in the representation space has been largely neglected. To address the above limitations, we propose a multi-scale representation and metric learning framework to exploit the feature and relation hierarchy for RE tasks. Methodologically, we &lt;italic&gt;build a lexical and syntactic feature and relation hierarchy&lt;/italic&gt; in text data. Technically, we first develop &lt;italic&gt;a multi-scale convolutional neural network&lt;/italic&gt; to aggregate the non-successive lexical patterns in the word sequence. We also design &lt;italic&gt;a multi-scale graph convolutional network&lt;/italic&gt; to increase the receptive field via the coarsened syntactic graph. Moreover, we present &lt;italic&gt;a multi-scale metric learning&lt;/italic&gt; paradigm to exploit both the feature-level relation between lexical and syntactic features and the sample-level relation between instances with the same or different classes. Extensive experiments on three public datasets for two RE tasks prove that our model achieves a new state-of-the-art performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {917–930},
numpages = {14}
}

@proceedings{10.1145/3573428,
title = {EITCE '22: Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
year = {2022},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@inproceedings{10.1109/ICSE43902.2021.00040,
author = {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane},
title = {Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00040},
doi = {10.1109/ICSE43902.2021.00040},
abstract = {Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31% measured using Mean Average Precision (MAP). RNN severely underper-formed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {324–335},
numpages = {12},
keywords = {Software traceability, deep learning, language models},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1109/TASLP.2023.3240661,
author = {Liu, Hong and Cai, Yucheng and Lin, Zhenru and Ou, Zhijian and Huang, Yi and Feng, Junlan},
title = {Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3240661},
doi = {10.1109/TASLP.2023.3240661},
abstract = {Recently, two approaches, fine-tuning large pre-trained language models and variational training, have attracted significant interests, separately, for semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper, we propose Variational Latent-State GPT model (VLS-GPT), which is the first to combine the strengths of the two approaches. Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner. Variational training of VLS-GPT is both statistically and computationally more challenging than previous variational learning works for sequential latent variable models, which use turn-level first-order Markovian. The inference model in VLS-GPT is non-Markovian due to the use of the Transformer architecture. In this work, we establish Recursive Monte Carlo Approximation (RMCA) to the variational objective with non-Markovian inference model and prove its unbiasedness. Further, we develop the computational strategy of sampling-then-forward-computation to realize RMCA, which successfully overcomes the memory explosion issue of using GPT in variational learning and speeds up training. Semi-supervised TOD experiments are conducted on two benchmark multi-domain datasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both supervised-only and semi-supervised self-training baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {970–984},
numpages = {15}
}

@inproceedings{10.1145/3459637.3482197,
author = {Zhou, Yiwei and Singh, Siffi and Christodoulopoulos, Christos},
title = {Tabular Data Concept Type Detection Using Star-Transformers},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482197},
doi = {10.1145/3459637.3482197},
abstract = {Tabular data is an invaluable information resource for search, in-formation extraction and question answering about the world. It is critical to understand the semantic concept types for table columns in order to fully exploit the information in tabular data. In this paper, we focus on learning-based approaches for column concept type detection without relying on any metadata or queries to existing knowledge bases. We propose a model that employs both statistical and semantic features of table columns, and use Star-Transformers to gather and scatter information across the whole table to boost the performance on individual columns. We apply distant supervision to construct a tabular dataset with columns annotated with DBpedia classes. Our experiment results show that our model achieves 93.57 accuracy on the dataset, exceeding that of the state-of-the-art baselines.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3677–3681},
numpages = {5},
keywords = {column classification, concept type detection, neural networks, tabular data},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1613/jair.1.12228,
author = {Burkart, Nadia and Huber, Marco F.},
title = {A Survey on the Explainability of Supervised Machine Learning},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12228},
doi = {10.1613/jair.1.12228},
abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {245–317},
numpages = {73}
}

@inproceedings{10.1145/3549737.3549752,
author = {Zoupanos, Spyros and Kolovos, Stratis and Kanavos, Athanasios and Papadimitriou, Orestis and Maragoudakis, Manolis},
title = {Efficient comparison of sentence embeddings},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549752},
doi = {10.1145/3549737.3549752},
abstract = {The evolution of natural language processing (NLP) has drastically improved numerous applications in terms of quality of results and speed, like the use of semantic search in modern search engines. NLP has highly benefited from the recent developments in word and sentence embeddings which enable the transformation of complex NLP tasks, such as semantic similarity or Question and Answering (Q&amp;A), into much simpler to perform vector comparisons. However, the new problems resulting from such transformations have also challenging tasks to address like the efficient comparison of embeddings and their manipulation. In this work, we will discuss about various word and sentence embeddings algorithms, we will select a sentence embedding algorithm, BERT, as our algorithm of choice and we will evaluate the performance of two vector comparison approaches, FAISS and Elasticsearch, in the specific problem of sentence embeddings. According to the results, FAISS outperforms Elasticsearch when used in a centralized environment with only one node, especially when big datasets are included.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {11},
numpages = {6},
keywords = {Elasticsearch, FAISS, sentence embeddings, vector performance comparison},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3485447.3512074,
author = {Manotumruksa, Jarana and Dalton, Jeffrey and Meij, Edgar and Yilmaz, Emine},
title = {Similarity-based Multi-Domain Dialogue State Tracking with Copy Mechanisms for Task-based Virtual Personal Assistants},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512074},
doi = {10.1145/3485447.3512074},
abstract = {Task-based Virtual Personal Assistants (VPAs) rely on multi-domain Dialogue State Tracking (DST) models to monitor goals throughout a conversation. Previously proposed models show promising results on established benchmarks, but they have difficulty adapting to unseen domains due to domain-specific parameters in their model architectures. We propose a new Similarity-based Multi-domain Dialogue State Tracking model (SM-DST) that uses retrieval-inspired and fine-grained contextual token-level similarity approaches to efficiently and effectively track dialogue state. The key difference with state-of-the-art DST models is that SM-DST has a single model with shared parameters across domains and slots. Because we base SM-DST on similarity it allows the transfer of tracking information between semantically related domains as well as to unseen domains without retraining. Furthermore, we leverage copy mechanisms that consider the system’s response and the dialogue state from previous turn predictions, allowing it to more effectively track dialogue state for complex conversations. We evaluate SM-DST on three variants of the MultiWOZ DST benchmark datasets. The results demonstrate that SM-DST significantly and consistently outperforms state-of-the-art models across all datasets by absolute 5-18% and 3-25% in the few- and zero-shot settings, respectively.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2006–2014},
numpages = {9},
keywords = {copy mechanisms, dialogue state tracking, task-based virtual personal assistants},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3626686,
title = {ICDTE '23: Proceedings of the 7th International Conference on Digital Technology in Education},
year = {2023},
isbn = {9798400708527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@article{10.1145/3603109,
author = {Zhang, Jingxuan and Luo, Junpeng and Liang, Jiahui and Gong, Lina and Huang, Zhiqiu},
title = {An Accurate Identifier Renaming Prediction and Suggestion Approach},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3603109},
doi = {10.1145/3603109},
abstract = {Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {148},
numpages = {51},
keywords = {Identifier renaming, source code analysis, code refactoring, mining code repository}
}

@inproceedings{10.1145/3487553.3524703,
author = {Saeidi, Mozhgan and Milios, Evangelos and Zeh, Norbert},
title = {Biomedical Word Sense Disambiguation with Contextualized Representation Learning},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524703},
doi = {10.1145/3487553.3524703},
abstract = {Representation learning is an important component in solving most Natural Language Processing&nbsp;(NLP) problems, including Word Sense Disambiguation&nbsp;(WSD). The WSD task tries to find the best meaning in a knowledge base for a word with multiple meanings&nbsp;(ambiguous word). WSD methods choose this best meaning based on the context, i.e., the words around the ambiguous word in the input text document. Thus, word representations may improve the effectiveness of the disambiguation models if they carry useful information from the context and the knowledge base. Most of the current representation learning approaches are that they are mostly trained on the general English text and are not domain specified. In this paper, we present a novel contextual-knowledge base aware sense representation method in the biomedical domain. The novelty in our representation is the integration of the knowledge base and the context. This representation lies in a space comparable to that of contextualized word vectors, thus allowing a word occurrence to be easily linked to its meaning by applying a simple nearest neighbor approach. Comparing our approach with state-of-the-art methods shows the effectiveness of our method in terms of text coherence.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {843–848},
numpages = {6},
keywords = {Biomedical Text, Neural Networks, Representation Learning, Transformers, Word Sense Disambiguation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3604951,
title = {HIP '23: Proceedings of the 7th International Workshop on Historical Document Imaging and Processing},
year = {2023},
isbn = {9798400708411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Jose, CA, USA}
}

@proceedings{10.1145/3617023,
title = {WebMedia '23: Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
year = {2023},
isbn = {9798400709081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ribeir\~{a}o Preto, Brazil}
}

@proceedings{10.1145/3587716,
title = {ICMLC '23: Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Zhuhai, China}
}

@article{10.1145/3611008,
author = {Sun, Yatong and Yang, Xiaochun and Sun, Zhu and Wang, Bin},
title = {BERD+: A Generic Sequential Recommendation Framework by Eliminating Unreliable Data with Item- and Attribute-level Signals},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3611008},
doi = {10.1145/3611008},
abstract = {Most sequential recommendation systems (SRSs) predict the next item as the target for users given its preceding items as input, assuming the target is definitely related to its input. However, users may unintentionally click items that are inconsistent with their preference due to external factors, causing unreliable instances whose target mismatches the input. We, for the first time, verify SRSs can be misguided by such unreliable instances and design a generic SRS framework By Eliminating unReliable Data (BERD+), which can be flexibly plugged into existing SRSs. Specifically, BRED+ is guided with observations on the training process of instances: Unreliable instances generally have high training loss; high-loss instances are not necessarily unreliable but uncertain ones caused by blurry sequential patterns; and item attributes help rectify instance loss and uncertainty, but may also introduce disturbance. Accordingly, BERD+ models both the loss and uncertainty of each instance via a Gaussian distribution, whereby a heterogeneous uncertainty-aware graph convolution network is designed to learn accurate embeddings for different entities while reducing the disturbance caused by uncertain attribute values. Thereafter, an explicit preference extractor rectifies instance loss and uncertainty and reduces the disturbance caused by less-focused attribute types. Finally, instances with high loss and low uncertainty are eliminated as unreliable data. Extensive experiments verify the efficacy of BERD+.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {41},
numpages = {33},
keywords = {Sequential recommender systems, unreliable instances, heterogeneous graph, graph convolution network}
}

@inproceedings{10.1145/3468264.3468618,
author = {Liu, Mingwei and Peng, Xin and Marcus, Andrian and Treude, Christoph and Bai, Xuefang and Lyu, Gang and Xie, Jiazhan and Zhang, Xiaoxin},
title = {Learning-based extraction of first-order logic representations of API directives},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468618},
doi = {10.1145/3468264.3468618},
abstract = {Developers often rely on API documentation to learn API directives, i.e., constraints and guidelines related to API usage. Failing to follow API directives may cause defects or improper implementations. Since there are no industry-wide standards on how to document API directives, they take many forms and are often hard to understand by developers or challenging to parse with tools.  In this paper, we propose a learning based approach for extracting first-order logic representations of API directives (FOL directives for short). The approach, called LEADFOL, uses a joint learning method to extract atomic formulas by identifying the predicates and arguments involved in directive sentences, and recognizes the logical relations between atomic formulas, by parsing the sentence structures. It then parses the arguments and uses a learning based method to link API references to their corresponding API elements. Finally, it groups the formulas of the same class or method together and transforms them into conjunctive normal form. Our evaluation shows that LEADFOL can accurately extract more FOL directives than a state-of-the-art approach and that the extracted FOL directives are useful in supporting code reviews.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {491–502},
numpages = {12},
keywords = {API Documentation, Directive, First Order Logic},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1109/TCBB.2021.3089195,
author = {Sun, Sunny and Chen, Yi-Ping Phoebe},
title = {Editorial},
year = {2022},
issue_date = {Jan.-Feb. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3089195},
doi = {10.1109/TCBB.2021.3089195},
abstract = {Presents the introductory editorial for this issue of the publication.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {1–2},
numpages = {2}
}

@article{10.1145/3564156,
author = {Alqahtani, Fatimah and Dohler, Mischa},
title = {Survey of Authorship Identification Tasks on Arabic Texts},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564156},
doi = {10.1145/3564156},
abstract = {Authorship identification is the process of extracting and analysing the writing styles of authors to identify the authorship. From the writing style, the author and his/her different characteristics can be recognised, which is very useful in digital forensics and cyber investigations. In the literature, authorship identification tasks were addressed on both long and short documents and performed on different languages, such as English, Arabic, Chinese, and Greek. This survey has reviewed the authorship identification tasks for the Arabic language to contribute to this area of research by exploring Arabic language performance and challenges. A total of 27 prominent Arabic studies of each authorship identification domain were reviewed considering the used data, selected features, utilised methods, and results. After a review of the various studies, it was concluded that the results of authorship identification tasks vary based on mostly the selected features and used dataset. Furthermore, the effective features differ from one dataset to another based on the various types of the&nbsp;Arabic language. However, all authorship identification tasks involving the Arabic language face considerable challenges with data pre-processing due to the challenging Arabic concatenative morphology.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {93},
numpages = {24},
keywords = {Authorship identification, authorship attribution, authorship verification, Arabic texts, stylometry}
}

@proceedings{10.1145/3551349,
title = {ASE '22: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
year = {2022},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rochester, MI, USA}
}

@inproceedings{10.1145/3585967.3585994,
author = {Jiang, Haoyu},
title = {Short-Text Semantic Similarity Model of BERT-Based Siamese Network},
year = {2023},
isbn = {9781450398466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585967.3585994},
doi = {10.1145/3585967.3585994},
abstract = {People convey their emotions and thoughts through words, the medium of human thoughts. Up against the vigorous development of streaming media, the calculation of text similarity is imperative in the field of natural language processing. Any text-related field is inseparable from text semantic similarity. The calculation of text semantic similarity plays a key role in document management, document classification, and document relevance. Besides, popular natural language processing tasks in some trendy fields, such as artificial intelligence, human-machine translation, problem system, intelligent chat system, and nomenclature recognition, are intertwined with text semantic similarity calculation. In recent years, many excellent researchers have studied the algorithms and models of text semantic similarity from different dimensions. In this paper, a new short-text cosine similarity calculation model of the BERT-based Siamese network is proposed.},
booktitle = {Proceedings of the 2023 10th International Conference on Wireless Communication and Sensor Networks},
pages = {145–149},
numpages = {5},
keywords = {BERT, Cosine Similarity, Short-Text Semantic Similarity, Siamese Network},
location = {Chengdu, China},
series = {icWCSN '23}
}

@proceedings{10.1145/3603719,
title = {SSDBM '23: Proceedings of the 35th International Conference on Scientific and Statistical Database Management},
year = {2023},
isbn = {9798400707469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Los Angeles, CA, USA}
}

@inproceedings{10.1145/3511808.3557068,
author = {Li, Sen and Lv, Fuyu and Jin, Taiwei and Li, Guiyang and Zheng, Yukun and Zhuang, Tao and Liu, Qingwen and Zeng, Xiaoyi and Kwok, James and Ma, Qianli},
title = {Query Rewriting in TaoBao Search},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557068},
doi = {10.1145/3511808.3557068},
abstract = {In e-commerce search engines, query rewriting (QR) is a crucial technique that improves shopping experience by reducing the vocabulary gap between user queries and product catalog. Recent works have mainly adopted the generative paradigm. However, they hardly ensure high-quality generated rewrites and do not consider personalization, which leads to degraded search relevance. In this work, we present Contrastive Learning Enhanced Query Rewriting (CLE-QR), the solution used in Taobao product search. It uses a novel contrastive learning enhanced architecture based on "query retrieval-semantic relevance ranking-online ranking". It finds the rewrites from hundreds of millions of historical queries while considering relevance and personalization. Specifically, we first alleviate the representation degeneration problem during the query retrieval stage by using an unsupervised contrastive loss, and then further propose an interaction-aware matching method to find the beneficial and incremental candidates, thus improving the quality and relevance of candidate queries. We then present a relevance-oriented contrastive pre-training paradigm on the noisy user feedback data to improve semantic ranking performance. Finally, we rank these candidates online with the user profile to model personalization for the retrieval of more relevant products. We evaluate CLE-QR on Taobao Product Search, one of the largest e-commerce platforms in China. Significant metrics gains are observed in online A/B tests. CLE-QR has been deployed to our large-scale commercial retrieval system and serviced hundreds of millions of users since December 2021. We also introduce its online deployment scheme, and share practical lessons and optimization tricks of our lexical match system.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {3262–3271},
numpages = {10},
keywords = {e-commerce search, lexical match, query rewriting},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@proceedings{10.1145/3597512,
title = {TAS '23: Proceedings of the First International Symposium on Trustworthy Autonomous Systems},
year = {2023},
isbn = {9798400707346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Edinburgh, United Kingdom}
}

@article{10.1145/3462207,
author = {Xu, Ruijian and Tao, Chongyang and Feng, Jiazhan and Wu, Wei and Yan, Rui and Zhao, Dongyan},
title = {Response Ranking with Multi-types of Deep Interactive Representations in Retrieval-based Dialogues},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3462207},
doi = {10.1145/3462207},
abstract = {Building an intelligent dialogue system with the ability to select a proper response according to a multi-turn context is challenging in three aspects: (1) the meaning of a context–response pair is built upon language units from multiple granularities (e.g., words, phrases, and sub-sentences, etc.); (2) local (e.g., a small window around a word) and long-range (e.g., words across the context and the response) dependencies may exist in dialogue data; and (3) the relationship between the context and the response candidate lies in multiple relevant semantic clues or relatively implicit semantic clues in some real cases. However, existing approaches usually encode the dialogue with mono-type representation and the interaction processes between the context and the response candidate are executed in a rather shallow manner, which may lead to an inadequate understanding of dialogue content and hinder the recognition of the semantic relevance between the context and response. To tackle these challenges, we propose a representation[K]-interaction[L]-matching framework that explores multiple types of deep interactive representations to build context-response matching models for response selection. Particularly, we construct different types of representations for utterance–response pairs and deepen them via alternate encoding and interaction. By this means, the model can handle the relation of neighboring elements, phrasal pattern, and long-range dependencies during the representation and make a more accurate prediction through multiple layers of interactions between the context–response pair. Experiment results on three public benchmarks indicate that the proposed model significantly outperforms previous conventional context-response matching models and achieve slightly better results than the BERT model for multi-turn response selection in retrieval-based dialogue systems.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {44},
numpages = {28},
keywords = {Retrieval-based dialogue systems, response selection, context-response matching}
}

@article{10.1145/3617371,
author = {Banerjee, Anasua and Kumar, Vinay and Shankar, Achyut and Jhaveri, Rutvij H. and Banik, Debajyoty},
title = {Automatic Resource Augmentation for Machine Translation in Low Resource Language: EnIndic Corpus},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3617371},
doi = {10.1145/3617371},
abstract = {Parallel corpus is the primary ingredient of machine translation. It is required to train the statistical machine translation (SMT) and neural machine translation (NMT) systems. There is a lack of good quality parallel corpus for Hindi to English. Comparable corpora for a given language pair are comparatively easy to find, but this cannot be used directly in SMT or NMT systems. As a result, we generate a parallel corpus from the comparable corpus. For this purpose, the sentences (which are translations of each other) are mined from the comparable corpus to prepare the parallel corpus. The proposed algorithm uses the length of the sentence and word translation model to align sentence pairs that are translations of each other. Then, the sentence pairs that are poor translations of each other (measured by a similarity score based on IBM model 1 translation probability) are filtered out. We apply this algorithm to comparable corpora, which are crawled from speeches of the President and Vice-President of India, and mined parallel corpora out of them. The prepared parallel corpus contains good quality aligned sentences (with 96.338% f-score). Subsequently, incorrect sentence pairs are filtered out manually to make the corpus in qualitative practical use. Finally, we gather various sentences from different sources to prepare the EnIndic corpus, which comprises 1,656,207 English-Hindi sentence pairs (miscellaneous domain). We have deployed this prepared largest English-Hindi parallel corpus at https://github.com/debajyoty/EnIndic.git
and the source code at https://github.com/debajyoty/EnIndicSourceCode.git.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
keywords = {Parallel Corpus, Comparable Corpus, Machine Translation, Linguistic Resources and Natural Language Processing}
}

@proceedings{10.1145/3572549,
title = {ICETC '22: Proceedings of the 14th International Conference on Education Technology and Computers},
year = {2022},
isbn = {9781450397766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3584371.3612998,
author = {Theodorou, Brandon Philip and Xiao, Cao and Sun, Jimeng},
title = {TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-based Memory Network},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612998},
doi = {10.1145/3584371.3612998},
abstract = {Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHRs) and eligibility criteria of trials. However, they either depend on trial-specific expert rules that cannot be generalized or perform matching more generally with a black-box model where the lack of interpretability makes the model results difficult to be adopted.To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network, TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to offer a granular level of alignment for improved performance and interpretability. We evaluate TREEMENT against existing models on real-world datasets and show that TREEMENT outperforms the top baseline by 7% in terms of error reduction in criteria-level matching and achieves state-of-the-art results at the trial-level too. Furthermore, we show TREEMENT offers good interpretability to make the model results easier for adoption.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {55},
numpages = {9},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1109/ASE51524.2021.9678670,
author = {Chen, Songqiang and Jin, Shuo and Xie, Xiaoyuan},
title = {Testing your question answering software via asking recursively},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678670},
doi = {10.1109/ASE51524.2021.9678670},
abstract = {Question Answering (QA) is an attractive and challenging area in NLP community. There are diverse algorithms being proposed and various benchmark datasets with different topics and task formats being constructed. QA software has also been widely used in daily human life now. However, current QA software is mainly tested in a reference-based paradigm, in which the expected outputs (labels) of test cases need to be annotated with much human effort before testing. As a result, neither the just-in-time test during usage nor the extensible test on massive unlabeled real-life data is feasible, which keeps the current testing of QA software from being flexible and sufficient. In this paper, we propose a method, QAAskeR, with three novel Metamorphic Relations for testing QA software. qaAskeR does not require the annotated labels but tests QA software by checking its behaviors on multiple recursively asked questions that are related to the same knowledge. Experimental results show that qaAskeR can reveal violations at over 80% of valid cases without using any pre-annotated labels. Diverse answering issues, especially the limited generalization on question types across datasets, are revealed on a state-of-the-art QA algorithm.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {104–116},
numpages = {13},
keywords = {natural language processing, question answering, recursive metamorphic testing, testing and validation},
location = {Melbourne, Australia},
series = {ASE '21}
}

@article{10.1109/TASLP.2023.3302232,
author = {Lim, Jungwoo and Whang, Taesun and Lee, Dongyub and Lim, Heuiseok},
title = {Adaptive Multi-Domain Dialogue State Tracking on Spoken Conversations},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3302232},
doi = {10.1109/TASLP.2023.3302232},
abstract = {The main objective of the task-oriented dialogue system is to identify the intent and needs of human dialogue. Many existing studies are conducted under the setting of written dialogue, but there always exists a difficulty in coping with real-world spoken dialogues. To this end, DSTC10 challenge organizers propose the task of building robust dialogue state tracking (DST) models on spoken dialogues. With the powerful existing DST model (i.e., MinTL), this article suggests integral components for building a dialogue state tracker; 1) Data augmentation effectively enhances the capability of the model to catch the entities that exist in the evaluation dataset. 2) Levenshtein post-processing aims to prevent the distortion in model prediction caused by automatic speech recognition errors. To validate the effectiveness of our methods, we evaluate our model on DSTC10 datasets and conduct qualitative analysis by ablating each component of the model. Experimental results show that our model significantly outperforms baselines in all evaluation metrics and took 3rd place in the challenge.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {727–732},
numpages = {6}
}

@inproceedings{10.5555/3507788.3507811,
author = {Kici, Derya and Bozanta, Aysun and Cevik, Mucahit and Parikh, Devang and Ba\c{s}ar, Ay\c{s}e},
title = {Text classification on software requirements specifications using transformer models},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Text classification in Software Requirements Specifications (SRS) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information, which all contribute to avoiding potential issues in the later stages of the software development life cycle. While a variety of machine learning approaches have been considered for text classification over SRS documents, many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task. Recent advances in deep learning methodology have significantly contributed to Natural Language Processing (NLP) and text classification. One of the main challenges in using deep learning models for various NLP tasks in the software engineering domain is the scarcity of labeled textual data. In addition, even with sufficient data, training from the scratch still requires significant training time and computational resources. Transfer learning is a novel approach that proposes a solution to such reservations by providing pre-trained models that enable fine-tuning with the customized data. In this research, we conduct an empirical analysis on multi-class text classification over SRS documents using different pre-trained transformer models including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and compare their performance. We test the performance of these models using three SRS datasets: DOORS, NFR-PROMISE, and PURE. Our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except Priority of the requirements. While all models provide a 80% or higher accuracy for other classification tasks, the accuracy of the models to classify the Priority does not exceed 60%.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {163–172},
numpages = {10},
keywords = {BERT, NLP, software requirement specifications, text classification, transfer learning},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/3482632.3482752,
author = {Chai, Jinlian},
title = {Design of English Translation Computer Intelligent Proofreading System Based on Fuzzy Decision Algorithm},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482752},
doi = {10.1145/3482632.3482752},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {568–571},
numpages = {4},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@proceedings{10.1145/3605390,
title = {CHItaly '23: Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Torino, Italy}
}

@article{10.1145/3468889,
author = {Zhang, Ruqing and Guo, Jiafeng and Chen, Lu and Fan, Yixing and Cheng, Xueqi},
title = {A Review on Question Generation from Natural Language Text},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3468889},
doi = {10.1145/3468889},
abstract = {Question generation is an important yet challenging problem in Artificial Intelligence (AI), which aims to generate natural and relevant questions from various input formats, e.g., natural language text, structure database, knowledge base, and image. In this article, we focus on question generation from natural language text, which has received tremendous interest in recent years due to the widespread applications such as data augmentation for question answering systems. During the past decades, many different question generation models have been proposed, from traditional rule-based methods to advanced neural network-based methods. Since there have been a large variety of research works proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we try to provide a more comprehensive taxonomy of question generation tasks from three different perspectives, i.e., the types of the input context text, the target answer, and the generated question. We take a deep look into existing models from different dimensions to analyze their underlying ideas, major design principles, and training strategies We compare these models through benchmark tasks to obtain an empirical understanding of the existing techniques. Moreover, we discuss what is missing in the current literature and what are the promising and desired future directions.},
journal = {ACM Trans. Inf. Syst.},
month = sep,
articleno = {14},
numpages = {43},
keywords = {Question generation, natural language generation, survey}
}

@inproceedings{10.1145/3575879.3575985,
author = {Kostis, Ioannis Aris and Sarafis, Dimitrios and Karamitsios, Konstantinos and Kotrotsios, Konstantinos and Kravari, Kalliopi and Badica, Costin and Chatzimisios, Periklis},
title = {Towards an Integrated Retrieval System to Semantically Match CVs, Job Descriptions and Curricula},
year = {2023},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575879.3575985},
doi = {10.1145/3575879.3575985},
abstract = {The job market is continuously evolving. The specific occupations, skills, competences and qualifications that people need change over time, as does their description. To deal with this, effective and intelligent communication and information exchange between the job market and the education and training sector is vital. On the other hand, and from the perspective of the individual (job seeker), especially the less privileged there is a need for approaches that combine practical tools with motivation and mentoring support since skill-matching it is not enough, skill-building is also needed. In this context, the current approach follows a bottom-up methodology investigating the problem of formalizing the lifelong learning process in a dynamic and flexible way. On the other hand, this proposal utilizes a parallel top-down approach in applying semantics and standards upon data in order to alleviate the gap among individuals, workplaces and educational contexts for the benefit of all in a transparent way. More specifically, this article reports towards an approach on tackling the complex task of interconnecting job seekers, employers and educational agents in the current European labor market. To perform this task, we implement an end-to-end service to parse resumes, job descriptions and open courses descriptions, retrieve information on the qualifications associated with the aforementioned, and semantically match them. The proposed implementation effectively detects the underlying information associated with those sources, and manages to interlink job seekers’ resumes to occupations and job vacancies, while being able to assign skill deficits to courses provided by educational agents. The performance of our implementation on CVs, job descriptions and course descriptions in English, Greek, Romanian and Bulgarian, indicate that our approach yields results on par with the state-of-the-art, however on a much larger scale: to the best of our knowledge, this is the first research work that engages with this task on three stakeholders (job seekers, employers, educational agents) and in four European languages.},
booktitle = {Proceedings of the 26th Pan-Hellenic Conference on Informatics},
pages = {151–157},
numpages = {7},
keywords = {ESCO, Information Retrieval, Linked Data, Semantic Web},
location = {Athens, Greece},
series = {PCI '22}
}

@article{10.1145/3445965,
author = {Nasar, Zara and Jaffry, Syed Waqar and Malik, Muhammad Kamran},
title = {Named Entity Recognition and Relation Extraction: State-of-the-Art},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3445965},
doi = {10.1145/3445965},
abstract = {With the advent of Web 2.0, there exist many online platforms that result in massive textual-data production. With ever-increasing textual data at hand, it is of immense importance to extract information nuggets from this data. One approach towards effective harnessing of this unstructured textual data could be its transformation into structured text. Hence, this study aims to present an overview of approaches that can be applied to extract key insights from textual data in a structured way. For this, Named Entity Recognition and Relation Extraction are being majorly addressed in this review study. The former deals with identification of named entities, and the latter deals with problem of extracting relation between set of entities. This study covers early approaches as well as the developments made up till now using machine learning models. Survey findings conclude that deep-learning-based hybrid and joint models are currently governing the state-of-the-art. It is also observed that annotated benchmark datasets for various textual-data generators such as Twitter and other social forums are not available. This scarcity of dataset has resulted into relatively less progress in these domains. Additionally, the majority of the state-of-the-art techniques are offline and computationally expensive. Last, with increasing focus on deep-learning frameworks, there is need to understand and explain the under-going processes in deep architectures.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {20},
numpages = {39},
keywords = {Information extraction, deep learning, joint modeling, named entity recognition, relation extraction}
}

@article{10.1145/3453185,
author = {Tan, Minghuan and Jiang, Jing and Dai, Bing Tian},
title = {A BERT-Based Two-Stage Model for Chinese Chengyu Recommendation},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3453185},
doi = {10.1145/3453185},
abstract = {In Chinese, Chengyu are fixed phrases consisting of four characters. As a type of idioms, their meanings usually cannot be derived from their component characters. In this article, we study the task of recommending a Chengyu given a textual context. Observing some of the limitations with existing work, we propose a two-stage model, where during the first stage we re-train a Chinese BERT model by masking out Chengyu from a large Chinese corpus with a wide coverage of Chengyu. During the second stage, we fine-tune the re-trained, Chengyu-oriented BERT on a specific Chengyu recommendation dataset. We evaluate this method on ChID and CCT datasets and find that it can achieve the state of the art on both datasets. Ablation studies show that both stages of training are critical for the performance gain.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {92},
numpages = {18},
keywords = {Question answering, Chengyu recommendation, idiom understanding}
}

@inproceedings{10.1145/3586183.3606806,
author = {Setlur, Vidya and Kanyuka, Andriy and Srinivasan, Arjun},
title = {Olio: A Semantic Search Interface for Data Repositories},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606806},
doi = {10.1145/3586183.3606806},
abstract = {Search and information retrieval systems are becoming more expressive in interpreting user queries beyond the traditional weighted bag-of-words model of document retrieval. For example, searching for a flight status or a game score returns a dynamically generated response along with supporting, pre-authored documents contextually relevant to the query. In this paper, we extend this hybrid search paradigm to data repositories that contain curated data sources and visualization content. We introduce a semantic search interface, Olio, that provides a hybrid set of results comprising both auto-generated visualization responses and pre-authored charts to blend analytical question-answering with content discovery search goals. We specifically explore three search scenarios - question-and-answering, exploratory search, and design search over data repositories. The interface also provides faceted search support for users to refine and filter the conventional best-first search results based on parameters such as author name, time, and chart type. A preliminary user evaluation of the system demonstrates that Olio’s interface and the hybrid search paradigm collectively afford greater expressivity in how users discover insights and visualization content in data repositories.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {95},
numpages = {16},
keywords = {Hybrid search, curated data sources., design search, dynamic and static content, exploratory search, federated querying, question and answering, visualizations},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3487553.3524935,
author = {Zhang, Dongyu and Zhang, Minghao and Peng, Ciyuan and Xia, Feng},
title = {Expressing Metaphorically, Writing Creatively: Metaphor Identification for Creativity Assessment in Writing},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524935},
doi = {10.1145/3487553.3524935},
abstract = {Metaphor, which can implicitly express profound meanings and emotions, is a unique writing technique frequently used in human language. In writing, meaningful metaphorical expressions can enhance the literariness and creativity of texts. Therefore, the usage of metaphor is a significant impact factor when assessing the creativity and literariness of writing. However, little to no automatic writing assessment system considers metaphorical expressions when giving the score of creativity. For improving the accuracy of automatic writing assessment, this paper proposes a novel creativity assessment model that imports a token-level metaphor identification method to extract metaphors as the indicators for creativity scoring. The experimental results show that our model can accurately assess the creativity of different texts with precise metaphor identification. To the best of our knowledge, we are the first to apply automatic metaphor identification to assess writing creativity. Moreover, identifying features (e.g., metaphors) that influence writing creativity using computational approaches can offer fair and reliable assessment methods for educational settings.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1198–1205},
numpages = {8},
keywords = {Metaphor identification, Metaphorical feature analytics., Textual data mining, Writing analytics, Writing creativity assessment},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3628797,
title = {SOICT '23: Proceedings of the 12th International Symposium on Information and Communication Technology},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ho Chi Minh, Vietnam}
}

@proceedings{10.1145/3543434,
title = {dg.o '22: Proceedings of the 23rd Annual International Conference on Digital Government Research},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Republic of Korea}
}

@inproceedings{10.1145/3617695.3617702,
author = {Le, Quang Ba Minh and Vo, Chau Thi Ngoc},
title = {Patient Information Retrieval Based on BERT Variants and Clinical Texts in Electronic Medical Records},
year = {2023},
isbn = {9798400708015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617695.3617702},
doi = {10.1145/3617695.3617702},
abstract = {Information retrieval is a task related to search a database for the most relevant similar objects to a given query object. In medicine, patient information retrieval is important to get the patients that are the most similar to a patient being considered. The resulting data can be further used for physicians to produce adaptive treatment plans as well as for other applications such as disease classification, re-admission prediction, and stay-length prediction. Due to its significance, several traditional information retrieval approaches were applied on medical databases. However, there is still a growing need for an effective solution to patient information retrieval in the context where more and more electronic medical records and their clinical texts are captured. In this paper, we focus on this task and propose to perform local learning on the BERT-based embeddings from clinical texts of patients to achieve an effective solution. The advanced properties of BERT variants help better represent each patient using the clinical texts instead of other data types like medication codes and demographic data. The experimental results on MIMIC III database have confirmed the effectiveness of our proposed solution. Above all, the better differences between our solution and the others in F-measure are statistically significant in all the cases.},
booktitle = {Proceedings of the 2023 7th International Conference on Big Data and Internet of Things},
pages = {188–194},
numpages = {7},
location = {Beijing, China},
series = {BDIOT '23}
}

@article{10.1145/3569576,
author = {Abdelrahman, Ghodai and Wang, Qing and Nunes, Bernardo},
title = {Knowledge Tracing: A Survey},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3569576},
doi = {10.1145/3569576},
abstract = {Humans’ ability to transfer knowledge through teaching is one of the essential aspects for human intelligence. A human teacher can track the knowledge of students to customize the teaching on students’ needs. With the rise of online education platforms, there is a similar need for machines to track the knowledge of students and tailor their learning experience. This is known as the Knowledge Tracing (KT) problem in the literature. Effectively solving the KT problem would unlock the potential of computer-aided education applications such as intelligent tutoring systems, curriculum learning, and learning materials’ recommendation. Moreover, from a more general viewpoint, a student may represent any kind of intelligent agents including both human and artificial agents. Thus, the potential of KT can be extended to any machine teaching application scenarios which seek for customizing the learning experience for a student agent (i.e., a machine learning model). In this paper, we provide a comprehensive survey for the KT literature. We cover a broad range of methods starting from the early attempts to the recent state-of-the-art methods using deep learning, while highlighting the theoretical aspects of models and the characteristics of benchmark datasets. Besides these, we shed light on key modelling differences between closely related methods and summarize them in an easy-to-understand format. Finally, we discuss current research gaps in the KT literature and possible future research and application directions.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {224},
numpages = {37},
keywords = {Knowledge tracing, memory networks, deep learning, sequence modelling, key-value memory, Bayesian knowledge tracing (BKT), intelligent education, factor analysis, survey}
}

@inproceedings{10.1145/3583780.3615109,
author = {Wang, Tianle and Wang, Zihan and Liu, Weitang and Shang, Jingbo},
title = {WOT-Class: Weakly Supervised Open-world Text Classification},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615109},
doi = {10.1145/3583780.3615109},
abstract = {State-of-the-art weakly supervised text classification methods, while significantly reduced the required human supervision, still requires the supervision to cover all the classes of interest. This is never easy to meet in practice when human explore new, large corpora without complete pictures. In this paper, we work on a novel yet important problem of weakly supervised open-world text classification, where supervision is only needed for a few examples from a few known classes and the machine should handle both known and unknown classes in test time. General open-world classification has been studied mostly using image classification; however, existing methods typically assume the availability of sufficient known-class supervision and strong unknown-class prior knowledge (e.g., the number and/or data distribution). We propose a novel framework \o{}ur that lifts those strong assumptions. Specifically, it follows an iterative process of (a) clustering text to new classes, (b) mining and ranking indicative words for each class, and (c) merging redundant classes by using the overlapped indicative words as a bridge. Extensive experiments on 7 popular text classification datasets demonstrate that \o{}ur outperforms strong baselines consistently with a large margin, attaining 23.33% greater average absolute macro-F1 over existing approaches across all datasets. Such competent accuracy illuminates the practical potential of further reducing human effort for text classification.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2666–2675},
numpages = {10},
keywords = {open-world learning, text classification, weak supervision},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3617233.3617242,
author = {Neptune, Nathalie and Mothe, Josiane},
title = {Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection},
year = {2023},
isbn = {9798400709128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617233.3617242},
doi = {10.1145/3617233.3617242},
abstract = {The Amazon rain forest is a vital ecosystem that plays a crucial role in regulating the Earth’s climate and providing habitat for countless species. Deforestation in the Amazon is a major concern as it has a significant impact on global carbon emissions and biodiversity. In this paper, we present a method for detecting deforestation in the Amazon using image pairs from Earth observation satellites. Our method leverages deep learning techniques to compare the images of the same area at different dates and identify changes in the forest cover. We also propose a visual semantic model that automatically annotates the detected changes with relevant keywords. The candidate annotation for images are extracted from scientific documents related to the Amazon region. We evaluate our approach on a dataset of Amazon image pairs and demonstrate its effectiveness in detecting deforestation and generating relevant annotations. Our method provides a useful tool for monitoring and studying the impact of deforestation in the Amazon. While we focus on environment applications of our work by using images of deforestation in the Amazon rain forest to demonstrate the effectiveness of our proposed approach, it is generic enough to be applied to other domains.},
booktitle = {Proceedings of the 20th International Conference on Content-Based Multimedia Indexing},
pages = {14–20},
numpages = {7},
keywords = {Image annotation, change detection, deforestation detection.},
location = {Orleans, France},
series = {CBMI '23}
}

@inproceedings{10.1145/3534678.3539449,
author = {Shen, Wei and Yang, Yang and Liu, Yinan},
title = {Multi-View Clustering for Open Knowledge Base Canonicalization},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539449},
doi = {10.1145/3534678.3539449},
abstract = {Open information extraction (OIE) methods extract plenty of OIE triples &lt;noun phrase, relation phrase, noun phrase&gt; from unstructured text, which compose large open knowledge bases (OKBs). Noun phrases and relation phrases in such OKBs are not canonicalized, which leads to scattered and redundant facts. It is found that two views of knowledge (i.e., a fact view based on the fact triple and a context view based on the fact triple's source context) provide complementary information that is vital to the task of OKB canonicalization, which clusters synonymous noun phrases and relation phrases into the same group and assigns them unique identifiers. However, these two views of knowledge have so far been leveraged in isolation by existing works. In this paper, we propose CMVC, a novel unsupervised framework that leverages these two views of knowledge jointly for canonicalizing OKBs without the need of manually annotated labels. To achieve this goal, we pro- pose a multi-view CH K-Means clustering algorithm to mutually reinforce the clustering of view-specific embeddings learned from each view by considering their different clustering qualities. In order to further enhance the canonicalization performance, we propose a training data optimization strategy in terms of data quantity and data quality respectively in each particular view to refine the learned view-specific embeddings in an iterative manner. Additionally, we propose a Log-Jump algorithm to predict the optimal number of clusters in a data-driven way without requiring any labels. We demonstrate the superiority of our framework through extensive experiments on multiple real-world OKB data sets against state-of-the-art methods.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1578–1588},
numpages = {11},
keywords = {multi-view clustering, open knowledge base canonicalization, training data optimization},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3569927,
author = {Broy, Manfred and Rumpe, Bernhard},
title = {Development Use Cases for Semantics-Driven Modeling Languages},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3569927},
doi = {10.1145/3569927},
abstract = {Choosing underlying semantic theories and definition techniques must closely follow intended use cases for the modeling language.},
journal = {Commun. ACM},
month = apr,
pages = {62–71},
numpages = {10}
}

@inproceedings{10.1145/3476887.3476895,
author = {Nagy, George},
title = {Generalized Template Matching for Semi-structured Text},
year = {2021},
isbn = {9781450386906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3476887.3476895},
doi = {10.1145/3476887.3476895},
abstract = {Conventional template matching for named entity recognition on book-length text strings is generalized by allowing search phrases to capture distant tokens. Combined with word-type tagging and format variants (alternative name/date formats), a few initial templates (class—search-phrase—extract-phrase triples) can label most of the significant tokens. The program then uses its book-length statistics of tag-label associations to suggest candidate text for further template construction. The method serves as a preprocessor for error-free extraction of semantic relations from text obeying explicit semi-structure constraints. On three sample books of genealogical records, an F-measure of over 0.99 was achieved with less than 3 hours’ user time on each book.},
booktitle = {Proceedings of the 6th International Workshop on Historical Document Imaging and Processing},
pages = {55–60},
numpages = {6},
keywords = {mask matching, tokenization, tagging, data formats, semi-structure},
location = {Lausanne, Switzerland},
series = {HIP '21}
}

@inproceedings{10.1145/3464509.3464892,
author = {G\"{u}nther, Michael and Thiele, Maik and Gonsior, Julius and Lehner, Wolfgang},
title = {Pre-Trained Web Table Embeddings for Table Discovery},
year = {2021},
isbn = {9781450385350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464509.3464892},
doi = {10.1145/3464509.3464892},
abstract = {Pre-trained word embedding models have become the de-facto standard to model text in state-of-the-art analysis tools and frameworks. However, while there are massive amounts of textual data stored in tables, word embedding models are usually pre-trained on large documents. This mismatch can lead to narrowed performance on tasks where text values in tables are analyzed. To improve analysis and retrieval tasks working with tabular data, we propose a novel embedding technique to be pre-trained directly on a large Web table corpus. In an experimental evaluation, we employ our models for various data analysis tasks on different data sources. Our evaluation shows that models using pre-trained Web table embeddings outperform the same models when applied to embeddings pre-trained on text. Moreover, we show that by using Web table embeddings state-of-the-art models for the investigated tasks can be outperformed.},
booktitle = {Proceedings of the Fourth International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
pages = {24–31},
numpages = {8},
keywords = {Web tables, learned representations, table discovery},
location = {Virtual Event, China},
series = {aiDM '21}
}

@inproceedings{10.1145/3462757.3466081,
author = {Hamdani, Rajaa El and Mustapha, Majd and Amariles, David Restrepo and Troussel, Aurore and Mee\`{u}s, S\'{e}bastien and Krasnashchok, Katsiaryna},
title = {A combined rule-based and machine learning approach for automated GDPR compliance checking},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466081},
doi = {10.1145/3462757.3466081},
abstract = {The General Data Protection Regulation (GDPR) requires data controllers to implement end-to-end compliance. Controllers must therefore ensure that the terms agreed with the data subject and their own obligations under GDPR are respected in the data flows from data subject to controllers, processors and sub processors (i.e. data supply chain). This paper seeks to contribute to bridge both ends of compliance checking through a two-pronged study. First, we conceptualize a framework to implement a document-centric approach to compliance checking in the data supply chain. Second, we develop specific methods to automate compliance checking of privacy policies. We test a two-modules system, where the first module relies on NLP to extract data practices from privacy policies. The second module encodes GDPR rules to check the presence of mandatory information. The results show that the text-to-text approach outperforms local classifiers and enables the extraction of both coarse-grained and fine-grained information with only one model. We implement full evaluation of our system on a dataset of 30 privacy policies annotated by legal experts. We conclude that this approach could be generalized to other documents in the data supply as a means to improve end-to-end compliance.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {40–49},
numpages = {10},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.1145/3570991.3571008,
author = {Bar-Haim, Roy and Eden, Lilach and Kantor, Yoav and Agarwal, Vikas and Devereux, Mark and Gupta, Nisha and Kumar, Arun and Orbach, Matan and Zan, Michael},
title = {Towards Automated Assessment of Organizational Cybersecurity Posture in Cloud},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571008},
doi = {10.1145/3570991.3571008},
abstract = {In a world where reliance on digital services becomes more critical every year with billions of dollars in penalties being levied annually by regulators and the impacts from security control failures growing, the potential consequence of organizations being unable to determine the completeness of their cybersecurity strategy and control environment are worsening. Established standards such as NIST 800-53, Cloud Security Alliance Cloud Controls Matrix (CSA-CCM) and CIS 20 Security Controls offer baselines against which organizations can mandate compliance, in the support of managing their security control environment and meeting risk and regulatory expectations. While there is increased security and compliance automation, it is hampered by the fact that control requirements are expressed in natural language text. With large organizations often needing to comply with several thousand security requirements across their IT enterprise, it becomes humanly impossible to assess coverage and identify potential gaps. In this paper, we present a system that enables performing a coarse-grained assessment of an organization’s security posture, against a standard control framework. We propose an AI-based model for performing the mapping automatically and evaluate its performance empirically. We further develop the idea and employ a novel domain-specific taxonomy that enhances the granularity of the coverage assessment while providing explainability. We also describe how this system is being used in production.},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {167–175},
numpages = {9},
keywords = {cloud security, mapping, regulations, security and compliance},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@article{10.1145/3624013,
author = {Kumari, Namrata and Singh, Pardeep},
title = {Hindi Text Summarization Using Sequence to Sequence Neural Network},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {2375-4699},
url = {https://doi.org/10.1145/3624013},
doi = {10.1145/3624013},
abstract = {Text summarizing reduces a large block of text data to a precise, short, and intelligible text that conveys the whole meaning of the actual text in a few words while maintaining the original context. Due to a lack of relevant summaries, it is hard to understand the main idea of the document. Text summarization using the abstractive technique is well-studied in English, although it is still in its infancy in Indian regional languages. In this study, we investigate the effectiveness of using a sequence-to-sequence (Seq2Seq) neural network based on attention and its optimization for text summarization for the Hindi language (HiATS), explicitly comparing the Adam and RMSprop optimizers. Our method allows the model to take the Hindi language dataset and, as output, provides a concise summary that accurately reflects the gist of the original text. The performance of the models will be evaluated using Rouge-1 and Rouge-2 metrics.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {239},
numpages = {18},
keywords = {Abstractive text summarization, optimizers, word embedding, neural network}
}

@inproceedings{10.1145/3404835.3462970,
author = {Liao, Lizi and Long, Le Hong and Zhang, Zheng and Huang, Minlie and Chua, Tat-Seng},
title = {MMConv: An Environment for Multimodal Conversational Search across Multiple Domains},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462970},
doi = {10.1145/3404835.3462970},
abstract = {Although conversational search has become a hot topic in both dialogue research and IR community, the real breakthrough has been limited by the scale and quality of datasets available. To address this fundamental obstacle, we introduce the Multimodal Multi-domain Conversational dataset (MMConv), a fully annotated collection of human-to-human role-playing dialogues spanning over multiple domains and tasks. The contribution is two-fold. First, beyond the task-oriented multimodal dialogues among user and agent pairs, dialogues are fully annotated with dialogue belief states and dialogue acts. More importantly, we create a relatively comprehensive environment for conducting multimodal conversational search with real user settings, structured venue database, annotated image repository as well as crowd-sourced knowledge database. A detailed description of the data collection procedure along with a summary of data structure and analysis is provided. Second, a set of benchmark results for dialogue state tracking, conversational recommendation, response generation as well as a unified model for multiple tasks are reported. We adopt the state-of-the-art methods for these tasks respectively to demonstrate the usability of the data, discuss limitations of current methods and set baselines for future studies.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {675–684},
numpages = {10},
keywords = {conversational search, datasets, multimodal dialogue},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@proceedings{10.1145/3582935,
title = {ICITEE '22: Proceedings of the 5th International Conference on Information Technologies and Electrical Engineering},
year = {2022},
isbn = {9781450396806},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Changsha, China}
}

@inproceedings{10.1145/3594536.3595124,
author = {van Drie, Romy A. N. and de Boer, Maaike H. T. and Bakker, Roos M. and Tolios, Ioannis and Vos, Daan},
title = {The Dutch Law as a Semantic Role Labeling Dataset},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594536.3595124},
doi = {10.1145/3594536.3595124},
abstract = {Legal documents, and specifically law texts, are not easy to understand by humans. The specific terminology and sentence constructions are particular, which also makes it a difficult machine understanding task. In this paper, we present a publicly available benchmark dataset containing Dutch law texts which can be used to train AI models that assist humans equipped with the task of interpreting legal texts. However, the dataset can be used in a broader context, such as semantic role labeling of Dutch (legal) texts. Our dataset contains 4463 annotated sentences from 55 different Dutch laws, in which four roles are annotated by human annotators: action, actor, object and recipient. The inter-annotator agreement is substantial (κ=0.75). In experiments with a rule-based and a transformer-based method, results show that the transformer-based method performs quite well on the dataset (accuracy &gt; 0.8). These results indicate that we can reliably predict actions, actors, objects and recipients in legal texts. This can help people equipped with the task of formal interpretation of legal texts.},
booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
pages = {316–322},
numpages = {7},
keywords = {datasets, legal interpretation, legal text, natural language processing, norms, semantic role labeling},
location = {Braga, Portugal},
series = {ICAIL '23}
}

@proceedings{10.1145/3573381,
title = {IMX '23: Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nantes, France}
}

@proceedings{10.1145/3607199,
title = {RAID '23: Proceedings of the 26th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2023},
isbn = {9798400707650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, China}
}

@proceedings{10.5555/3606010,
title = {ICSE '23: Proceedings of the 45th International Conference on Software Engineering},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
abstract = {ICSE is the leading and by far the largest conference in Software Engineering, attracting researchers, practitioners and students from around the world. ICSE2023 is co-located with 10 conferences and symposia this year, many long-established and prestigious venues in their own right.},
location = {Melbourne, Victoria, Australia}
}

@inproceedings{10.1145/3404835.3463058,
author = {Song, Liqiang and Yao, Mengqiu and Bi, Ye and Wu, Zhenyu and Wang, Jianming and Xiao, Jing and Wen, Juan and Yu, Xin},
title = {LS-DST: Long and Sparse Dialogue State Tracking with Smart History Collector in Insurance Marketing},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463058},
doi = {10.1145/3404835.3463058},
abstract = {Different from traditional task-oriented and open-domain dialogue systems, insurance agents aim to engage customers for helping them satisfy specific demands and emotional companionship. As a result, customer-to-agent dialogues are usually very long, and many turns of them are pure chit-chat without any useful marketing clues. This brings challenges to dialogue state tracking task in insurance marketing. To deal with these long and sparse dialogues, we propose a new dialogue state tracking architecture containing three components: dialogue encoder, Smart History Collector (SHC) and dialogue state classifier. SHC, a deliberately designed memory network, effectively selects relevant dialogue history via slot-attention, and then updates dialogue history memory. With SHC, our model is able to keep track of the vital information and filter out pure chit-chat. Experimental results demonstrate that our proposed LS-DST significantly outperforms the state-of-the-art baselines on real insurance dialogue dataset.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1960–1964},
numpages = {5},
keywords = {dialogue state tracking, insurance marketing, memory network, smart history collector},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3442381.3449827,
author = {Nguyen, Tuan-Phong and Razniewski, Simon and Weikum, Gerhard},
title = {Advanced Semantics for Commonsense Knowledge Extraction},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449827},
doi = {10.1145/3442381.3449827},
abstract = {Commonsense knowledge (CSK) about concepts and their properties is useful for AI applications such as robust chatbots. Prior works like ConceptNet, TupleKB and others compiled large CSK collections, but are restricted in their expressiveness to subject-predicate-object (SPO) triples with simple concepts for S and monolithic strings for P and O. Also, these projects have either prioritized precision or recall, but hardly reconcile these complementary goals. This paper presents a methodology, called Ascent, to automatically build a large-scale knowledge base (KB) of CSK assertions, with advanced expressiveness and both better precision and recall than prior works. Ascent goes beyond triples by capturing composite concepts with subgroups and aspects, and by refining assertions with semantic facets. The latter are important to express temporal and spatial validity of assertions and further qualifiers. Ascent combines open information extraction with judicious cleaning using language models. Intrinsic evaluation shows the superior size and quality of the Ascent KB, and an extrinsic evaluation for QA-support tasks underlines the benefits of Ascent. A web interface, data and code can be found at https://www.mpi-inf.mpg.de/ascent.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2636–2647},
numpages = {12},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@proceedings{10.1145/3624062,
title = {SC-W '23: Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Denver, CO, USA}
}

@article{10.1109/TCBB.2023.3247634,
author = {Dhanuka, Richa and Singh, Jyoti Prakash and Tripathi, Anushree},
title = {A Comprehensive Survey of Deep Learning Techniques in Protein Function Prediction},
year = {2023},
issue_date = {May-June 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2023.3247634},
doi = {10.1109/TCBB.2023.3247634},
abstract = {Protein function prediction is a major challenge in the field of bioinformatics which aims at predicting the functions performed by a known protein. Many protein data forms like protein sequences, protein structures, protein-protein interaction networks, and micro-array data representations are being used to predict functions. During the past few decades, abundant protein sequence data has been generated using high throughput techniques making them a suitable candidate for predicting protein functions using deep learning techniques. Many such advanced techniques have been proposed so far. It becomes necessary to comprehend all these works in a survey to provide a systematic view of all the techniques along with the chronology in which the techniques have advanced. This survey provides comprehensive details of the latest methodologies, their pros and cons as well as predictive accuracy, and a new direction in terms of interpretability of the predictive models needed to be ventured by protein function prediction systems.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {2291–2301},
numpages = {11}
}

@inproceedings{10.1145/3503823.3503903,
author = {Tsimpiris, Alkiviadis and Varsamis, Dimitrios and Strouthopoulos, Charalampos and Pavlidis, George and Chairi, Kiourt},
title = {Open-source OCR Engine Integration with Greek Dictionary},
year = {2022},
isbn = {9781450395557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503823.3503903},
doi = {10.1145/3503823.3503903},
booktitle = {Proceedings of the 25th Pan-Hellenic Conference on Informatics},
pages = {436–441},
numpages = {6},
keywords = {Greek, OCR, Tesseract, dictionary},
location = {Volos, Greece},
series = {PCI '21}
}

@inproceedings{10.1145/3527188.3561941,
author = {Saund, Carolyn and Matuszak, Haley and Weinstein, Anna and Marsella, Stacy},
title = {Motion and Meaning: Data-Driven Analyses of The Relationship Between Gesture and Communicative Semantics},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527188.3561941},
doi = {10.1145/3527188.3561941},
abstract = {Gestures convey critical information within social interactions. As such, the success of virtual agents (VA) in both building social relationships and achieving their goals is heavily dependent on the information conveyed within their gestures. Because of the precision required for effective gesture behavior, it is prudent to retain some designer control over these conversational gestures. However, in order to exercise that control practically we must first understand how gestural motion conveys meaning. One consideration in this relationship between motion and meaning is the notion of Ideational Units, meaning that only parts of a gesture’s motion at a point in time may convey meaning, while other parts may be held from the previous gesture. In this paper, we develop, demonstrate, and release a set of tools that help quantify the relationship between the semantics conveyed in a gesture’s co-speech utterance and the fine-grained motion of that gesture. This allows us to explore insights into the complex relationship between motion and meaning. In particular, we use spectral motion clustering to discern patterns of motion that tend to be associated with semantic concepts, on both an aggregate and individual-speaker level. We then discuss the potential for these tools to serve as a framework for both automated gesture generation and interpretation in virtual agents. These tools can ideally be used within approaches to automating VA gesture performances as well as serve as an analysis framework for fundamental gesture research.},
booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
pages = {227–235},
numpages = {9},
keywords = {Analysis Techniques, Animation, Clustering, Gesture, Human-Agent Interaction, Motion Capture, Virtual Humans},
location = {Christchurch, New Zealand},
series = {HAI '22}
}

@proceedings{10.1145/3631991,
title = {WSSE '23: Proceedings of the 2023 5th World Symposium on Software Engineering},
year = {2023},
isbn = {9798400708053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3485447.3511933,
author = {Hamzei, Ehsan and Tomko, Martin and Winter, Stephan},
title = {Translating Place-Related Questions to GeoSPARQL Queries},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511933},
doi = {10.1145/3485447.3511933},
abstract = {Many place-related questions can only be answered by complex spatial reasoning, a task poorly supported by factoid question retrieval. Such reasoning using combinations of spatial and non-spatial criteria pertinent to place-related questions is increasingly possible on linked data knowledge bases. Yet, to enable question answering based on linked knowledge bases, natural language questions must first be re-formulated as formal queries. Here, we first present an enhanced version of YAGO2geo, the geospatially-enabled variant of the YAGO2 knowledge base, by linking and adding more than one million places from OpenStreetMap data to YAGO2. We then propose a novel approach to translate the place-related questions into logical representations, theoretically grounded in the core concepts of spatial information. Next, we use a dynamic template-based approach to generate fully executable GeoSPARQL queries from the logical representations. We test our approach using the Geospatial Gold Standard dataset and report substantial improvements over existing methods.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {902–911},
numpages = {10},
keywords = {geographic question answering, geospatial knowledge bases, place-based search, query generation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3556223,
title = {ICCCM '22: Proceedings of the 10th International Conference on Computer and Communications Management},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Okayama, Japan}
}

@article{10.1109/TASLP.2023.3313415,
author = {Wang, Ante and Song, Linfeng and Jin, Lifeng and Yao, Junfeng and Mi, Haitao and Lin, Chen and Su, Jinsong and Yu, Dong},
title = {D&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$^{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt;PSG: Multi-Party Dialogue Discourse Parsing as Sequence Generation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3313415},
doi = {10.1109/TASLP.2023.3313415},
abstract = {Conversational discourse analysis aims to extract the interactions between dialogue turns, which is crucial for modeling complex multi-party dialogues. As the benchmarks are still limited in size and human annotations are costly, the current standard approaches apply pretrained language models, but they still require randomly initialized classifiers to make predictions. These classifiers usually require massive data to work smoothly with the pretrained encoder, causing severe data hunger issue. We propose two convenient strategies to formulate this task as a sequence generation problem, where classifier decisions are carefully converted into sequence of tokens. We then adopt a pretrained T5 [C. Raffel et al., 2020] model to solve this task so that no parameters are randomly initialized. We also leverage the descriptions of the discourse relations to help model understand their meanings. Experiments on two popular benchmarks show that our approach outperforms previous state-of-the-art models by a large margin, and it is also more robust in zero-shot and few-shot settings.&lt;sup&gt;1&lt;/sup&gt;},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4004–4013},
numpages = {10}
}

@proceedings{10.1145/3623462,
title = {KUI '23: Proceedings of the 20th International Conference on Culture and Computer Science: Code and Materiality},
year = {2023},
isbn = {9798400708367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3539618.3591748,
author = {Pang, Ning and Zhao, Xiang and Zeng, Weixin and Wang, Ji and Xiao, Weidong},
title = {Personalized Federated Relation Classification over Heterogeneous Texts},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591748},
doi = {10.1145/3539618.3591748},
abstract = {Relation classification detects the semantic relation between two annotated entities from a piece of text, which is a useful tool for structurization of knowledge. Recently, federated learning has been introduced to train relation classification models in decentralized settings. Current methods strive for a strong server model by decoupling the model training at server from direct access to texts at clients while taking advantage of them. Nevertheless, they overlook the fact that clients have heterogeneous texts (i.e., texts with diversely skewed distribution of relations), which renders existing methods less practical. In this paper, we propose to investigate personalized federated relation classification, in which strong client models adapted to their own data are desired. To further meet the challenges brought by heterogeneous texts, we present a novel framework, namely pf-RC, with several optimized designs. It features a knowledge aggregation method that exploits a relation-wise weighting mechanism, and a feature augmentation method that leverages prototypes to adaptively enhance the representations of instances of long-tail relations. We experimentally validate the superiority of pf-RC against competing baselines in various settings, and the results suggest that the tailored techniques mitigate the challenges.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {973–982},
numpages = {10},
keywords = {federated learning, heterogeneous texts, relation classification},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3564769,
author = {Kumar Attar, Rakesh and Goyal, Vishal and Goyal, Lalit},
title = {State of the Art of Automation in Sign Language: A Systematic Review},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564769},
doi = {10.1145/3564769},
abstract = {Sign language is the fundamental communication language of deaf people. Efforts to develop sign language generation systems can make the life of these people smooth and effortless. Despite the importance of sign language generation systems, there is a paucity of a systematic literature review. This is the foremost recognizable scholastic literature review of sign language generation systems. It presents a scholastic database of the literature between 1998 and 2020 and suggests classification criteria to systematize research studies. Four hundred fourteen research studies were recognized and reviewed for their direct pertinence to sign language generation systems. One hundred sixty-two research studies were subsequently chosen, examined, and classified. Each of the 162 chosen research papers was categorized based on 30 sign languages and was further comparatively analyzed based on seven comparison parameters (input form, translation technologies, application domain, use of parsers/grammars, manual/non-manual features, accuracy, and output form). It is evident from our research findings that the majority of research on sign language generation was carried out using data-driven approaches in the absence of proper grammar rules and generated only manual signs. This research study may provide researchers a roadmap toward future research directions and facilitate the compilation of information in the field of sign language generation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {94},
numpages = {80},
keywords = {Machine translation, Interlingua, virtual avatar, SiGML, HamNoSys}
}

@article{10.1145/3453156,
author = {Alhussain, Arwa I. and Azmi, Aqil M.},
title = {Automatic Story Generation: A Survey of Approaches},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453156},
doi = {10.1145/3453156},
abstract = {Computational generation of stories is a subfield of computational creativity where artificial intelligence and psychology intersect to teach computers how to mimic humans’ creativity. It helps generate many stories with minimum effort and customize the stories for the users’ education and entertainment needs. Although the automatic generation of stories started to receive attention many decades ago, advances in this field to date are less than expected and suffer from many limitations. This survey presents an extensive study of research in the area of non-interactive textual story generation, as well as covering resources, corpora, and evaluation methods that have been used in those studies. It also shed light on factors of story interestingness.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {103},
numpages = {38},
keywords = {Text generation, datasets, evaluation, story generation, survey}
}

@article{10.14778/3494124.3494131,
author = {Jin, Di and Sisman, Bunyamin and Wei, Hao and Dong, Xin Luna and Koutra, Danai},
title = {Deep transfer learning for multi-source entity linkage via domain adaptation},
year = {2021},
issue_date = {November 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3494124.3494131},
doi = {10.14778/3494124.3494131},
abstract = {Multi-source entity linkage focuses on integrating knowledge from multiple sources by linking the records that represent the same real world entity. This is critical in high-impact applications such as data cleaning and user stitching. The state-of-the-art entity linkage pipelines mainly depend on supervised learning that requires abundant amounts of training data. However, collecting well-labeled training data becomes expensive when the data from many sources arrives incrementally over time. Moreover, the trained models can easily overfit to specific data sources, and thus fail to generalize to new sources due to significant differences in data and label distributions. To address these challenges, we present AdaMEL, a deep transfer learning framework that learns generic high-level knowledge to perform multi-source entity linkage. AdaMEL models the attribute importance that is used to match entities through an attribute-level self-attention mechanism, and leverages the massive unlabeled data from new data sources through domain adaptation to make it generic and data-source agnostic. In addition, AdaMEL is capable of incorporating an additional set of labeled data to more accurately integrate data sources with different attribute importance. Extensive experiments show that our framework achieves state-of-the-art results with 8.21% improvement on average over methods based on supervised learning. Besides, it is more stable in handling different sets of data sources in less runtime.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {465–477},
numpages = {13}
}

@proceedings{10.1145/3542637,
title = {APNet '22: Proceedings of the 6th Asia-Pacific Workshop on Networking},
year = {2022},
isbn = {9781450397483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fuzhou, China}
}

@inproceedings{10.1145/3447548.3467308,
author = {Zeng, Qingkai and Lin, Jinfeng and Yu, Wenhao and Cleland-Huang, Jane and Jiang, Meng},
title = {Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467308},
doi = {10.1145/3447548.3467308},
abstract = {Automatic construction of a taxonomy supports many applications in e-commerce, web search, and question answering. Existing taxonomy expansion or completion methods assume that new concepts have been accurately extracted and their embedding vectors learned from the text corpus. However, one critical and fundamental challenge in fixing the incompleteness of taxonomies is the incompleteness of the extracted concepts, especially for those whose names have multiple words and consequently low frequency in the corpus. To resolve the limitations of extraction-based methods, we propose GenTaxo to enhance taxonomy completion by identifying positions in existing taxonomies that need new concepts and then generating appropriate concept names. Instead of relying on the corpus for concept embeddings, GenTaxo learns the contextual embeddings from their surrounding graph-based and language-based relational information, and leverages the corpus for pre-training a concept name generator. Experimental results demonstrate that GenTaxo improves the completeness of taxonomies over existing methods.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining},
pages = {2104–2113},
numpages = {10},
keywords = {concept generation, taxonomy completion},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@inproceedings{10.1145/3539618.3591897,
author = {Kartchner, David and Al-Hussaini, Irfan and Turner, Haydn and Deng, Jennifer and Lohiya, Shubham and Bathala, Prasanth and Mitchell, Cassie},
title = {BioSift: A Dataset for Filtering Biomedical Abstracts for Drug Repurposing and Clinical Meta-Analysis},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591897},
doi = {10.1145/3539618.3591897},
abstract = {This work presents a new, original document classification dataset, BioSift, to expedite the initial selection and labeling of studies for drug repurposing. The dataset consists of 10,000 human-annotated abstracts from scientific articles in PubMed. Each abstract is labeled with up to eight attributes necessary to perform meta-analysis utilizing the popular patient-intervention-comparator-outcome (PICO) method: has human subjects, is clinical trial/cohort, has population size, has target disease, has study drug, has comparator group, has a quantitative outcome, and an "aggregate" label. Each abstract was annotated by 3 different annotators (i.e., biomedical students) and randomly sampled abstracts were reviewed by senior annotators to ensure quality. Data statistics such as reviewer agreement, label co-occurrence, and confidence are shown. Robust benchmark results illustrate neither PubMed advanced filters nor state-of-the-art document classification schemes (e.g., active learning, weak supervision, full supervision) can efficiently replace human annotation. In short, BioSift is a pivotal but challenging document classification task to expedite drug repurposing. The full annotated dataset is publicly available and enables research development of algorithms for document classification that enhance drug repurposing.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2913–2923},
numpages = {11},
keywords = {document classification, document filtering, drug repurposing, meta analysis, natural language processing},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3606370,
author = {Chan, Chia-Pang and Yang, Jun-He},
title = {Instagram Text Sentiment Analysis Combining Machine Learning and NLP},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3606370},
doi = {10.1145/3606370},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
keywords = {Instagram, natural language processing, machine learning, deep learning, word embedding technology}
}

@article{10.1145/3418208,
author = {Ni, Pin and Li, Yuming and Li, Gangmin and Chang, Victor},
title = {A Hybrid Siamese Neural Network for Natural Language Inference in Cyber-Physical Systems},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3418208},
doi = {10.1145/3418208},
abstract = {Cyber-Physical Systems (CPS), as a multi-dimensional complex system that connects the physical world and the cyber world, has a strong demand for processing large amounts of heterogeneous data. These tasks also include Natural Language Inference (NLI) tasks based on text from different sources. However, the current research on natural language processing in CPS does not involve exploration in this field. Therefore, this study proposes a Siamese Network structure that combines Stacked Residual Long Short-Term Memory (bidirectional) with the Attention mechanism and Capsule Network for the NLI module in CPS, which is used to infer the relationship between text/language data from different sources. This model is mainly used to implement NLI tasks and conduct a detailed evaluation in three main NLI benchmarks as the basic semantic understanding module in CPS. Comparative experiments prove that the proposed method achieves competitive performance, has a certain generalization ability, and can balance the performance and the number of trained parameters.},
journal = {ACM Trans. Internet Technol.},
month = mar,
articleno = {33},
numpages = {25},
keywords = {Cyber-physical systems, Natural language inference, Siamese neural networks}
}

@inproceedings{10.1145/3511616.3513115,
author = {Thapa, Nischay Bikram and Seifollahi, Sattar and Taheri, Sona},
title = {Hospital Readmission Prediction Using Clinical Admission Notes},
year = {2022},
isbn = {9781450396066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511616.3513115},
doi = {10.1145/3511616.3513115},
abstract = {Clinical notes contain contextualised information beyond structured data relating to patients’ past and current health conditions. Despite the richness, their unstructured, long, and high dimensional nature presents challenges to traditional text representation techniques. The advancement of deep contextual representation techniques in natural language processing (NLP) has shown remarkable performance in the biomedical and clinical domains for various information extraction and predictive tasks, including hospital readmission. However, most previous works have proposed discharge summary models where on-site medical intervention is impossible, and readmission could still occur. This paper utilises clinical notes recorded during admissions to study the risk of 30-day hospital readmissions. We employ clinical notes from MIMIC-III and consider competing baselines for clinical text representation, where a set of machine learning and deep learning algorithms are used to classify hospital readmission. The study demonstrates that notes captured during admissions play a crucial role to recognise potential readmission risk supporting healthcare practitioners for practical therapeutic intervention and discharge planning.},
booktitle = {Proceedings of the 2022 Australasian Computer Science Week},
pages = {193–199},
numpages = {7},
keywords = {Electronic health records, Embedding techniques, Hospital readmission, Natural language processing},
location = {Brisbane, Australia},
series = {ACSW '22}
}

@inproceedings{10.1145/3544548.3581252,
author = {Xiao, Ziang and Li, Tiffany Wenting and Karahalios, Karrie and Sundaram, Hari},
title = {Inform the Uninformed: Improving Online Informed Consent Reading with an AI-Powered Chatbot},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581252},
doi = {10.1145/3544548.3581252},
abstract = {Informed consent is a core cornerstone of ethics in human subject research. Through the informed consent process, participants learn about the study procedure, benefits, risks, and more to make an informed decision. However, recent studies showed that current practices might lead to uninformed decisions and expose participants to unknown risks, especially in online studies. Without the researcher’s presence and guidance, online participants must read a lengthy form on their own with no answers to their questions. In this paper, we examined the role of an AI-powered chatbot in improving informed consent online. By comparing the chatbot with form-based interaction, we found the chatbot improved consent form reading, promoted participants’ feelings of agency, and closed the power gap between the participant and the researcher. Our exploratory analysis further revealed the altered power dynamic might eventually benefit study response quality. We discussed design implications for creating AI-powered chatbots to offer effective informed consent in broader settings.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {112},
numpages = {17},
keywords = {AI-powered chatbot, conversational agents, human-AI interaction, informed consent, power dynamic},
location = {Hamburg, Germany},
series = {CHI '23}
}

@proceedings{10.1145/3536221,
title = {ICMI '22: Proceedings of the 2022 International Conference on Multimodal Interaction},
year = {2022},
isbn = {9781450393904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bengaluru, India}
}

@inproceedings{10.1145/3485447.3512002,
author = {Lee, Dongha and Shen, Jiaming and Kang, Seongku and Yoon, Susik and Han, Jiawei and Yu, Hwanjo},
title = {TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512002},
doi = {10.1145/3485447.3512002},
abstract = {Topic taxonomies, which represent the latent topic (or category) structure of document collections, provide valuable knowledge of contents in many applications such as web search and information filtering. Recently, several unsupervised methods have been developed to automatically construct the topic taxonomy from a text corpus, but it is challenging to generate the desired taxonomy without any prior knowledge. In this paper, we study how to leverage the partial (or incomplete) information about the topic structure as guidance to find out the complete topic taxonomy. We propose a novel framework for topic taxonomy completion, named TaxoCom, which recursively expands the topic taxonomy by discovering novel sub-topic clusters of terms and documents. To effectively identify novel topics within a hierarchical topic structure, TaxoCom devises its embedding and clustering techniques to be closely-linked with each other: (i) locally discriminative embedding optimizes the text embedding space to be discriminative among known (i.e., given) sub-topics, and (ii) novelty adaptive clustering assigns terms into either one of the known sub-topics or novel sub-topics. Our comprehensive experiments on two real-world datasets demonstrate that TaxoCom not only generates the high-quality topic taxonomy in terms of term coherency and topic coverage but also outperforms all other baselines for a downstream task.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2819–2829},
numpages = {11},
keywords = {Hierarchical topic discovery, Novelty detection, Text clustering, Text embedding, Topic taxonomy completion},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@proceedings{10.1145/3528588,
title = {NLBSE '22: Proceedings of the 1st International Workshop on Natural Language-based Software Engineering},
year = {2022},
isbn = {9781450393430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 1st edition of the International Workshop on Natural Language-Based Software Engineering (NLBSE). The potential of Natural Language Processing (NLP) and Natural Language Generation (NLG) to support developers and engineers in a wide number of software engineering-related tasks (e.g., requirements engineering, extraction of knowledge and patterns from the software artifacts, summarization and prioritization of development and maintenance activities, etc.) is increasingly evident. Furthermore, the current availability of libraries (e.g., NLTK, CoreNLP, and fasttext) and models (e.g., BERT) that allow efficiently and easily dealing with low-level aspects of natural language processing and representation, pushed more and more researchers to closely work with industry to attempt to solve software engineers' real-world problems.},
location = {Pittsburgh, Pennsylvania}
}

@inproceedings{10.1145/3534678.3539077,
author = {Yang, Jiuding and Guo, Weidong and Liu, Bang and Yu, Yakun and Wang, Chaoyue and Luo, Jinwen and Kong, Linglong and Niu, Di and Wen, Zhen},
title = {TAG: Toward Accurate Social Media Content Tagging with a Concept Graph},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539077},
doi = {10.1145/3534678.3539077},
abstract = {Although conceptualization has been widely studied in semantics and knowledge representation, it is still challenging to find the most accurate concept terms to tag fast-growing social media content. This is partly attributed to the fact that most traditional knowledge bases contain general terms of the world, such as trees and cars, which are not interesting to users, and do not have the defining power for social media content. Another reason is that the intricate use of tense, negation and grammar in social media content may change the logic or emphasis of the content, thus focusing on different main ideas. In this paper, we present TAG, a high-quality concept matching dataset consisting of 10,000 labeled pairs of fine-grained concepts and web-styled natural language sentences, mined from open-domain social media content. The concepts we provide are the trending terms on social media and have the right granularity to define user interests, e.g., highly educated actors instead of just actors. In the meantime, TAG offers a concept graph which interconnects these fine-grained concepts and entities to provide contextual information. We evaluate a wide range of neural text matching models as well as pre-trained language models for the concept matching task on TAG, and point out their insufficiency to tag social media content to characterize its main idea. We further propose a novel graph-graph matching framework that demonstrates superior abstraction and generalization performance by better utilizing both the structural information in the concept graph and logic interactions between semantic units in the natural language sentence via syntactic dependency parsing.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4332–4341},
numpages = {10},
keywords = {concept-sentence matching, datasets, graph-graph matching, heterogeneous graph},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3412841.3441957,
author = {Meijer, Lisa and Frasincar, Flavius and Tru\c{s}c\u{a}, Maria Mihaela},
title = {Explaining a neural attention model for aspect-based sentiment classification using diagnostic classification},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441957},
doi = {10.1145/3412841.3441957},
abstract = {Many high performance machine learning models for Aspect-Based Sentiment Classification (ABSC) produce black box models, and therefore barely explain how they classify a certain sentiment value towards an aspect. In this paper, we propose explanation models, that inspect the internal dynamics of a state-of-the-art neural attention model, the LCR-Rot-hop, by using a technique called Diagnostic Classification. Our diagnostic classifier is a simple neural network, which evaluates whether the internal layers of the LCR-Rot-hop model encode useful word information for classification, i.e., the part of speech, the sentiment value, the presence of aspect relation, and the aspect-related sentiment value of words. We conclude that the lower layers in the LCR-Rot-hop model encode the part of speech and the sentiment value, whereas the higher layers represent the presence of a relation with the aspect and the aspect-related sentiment value of words.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {821–827},
numpages = {7},
keywords = {aspect-based sentiment classification, diagnostic classification, neural rotatory attention model},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3477314.3507256,
author = {Kanwal, Neel and Rizzo, Giuseppe},
title = {Attention-based clinical note summarization},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507256},
doi = {10.1145/3477314.3507256},
abstract = {In recent years, the trend of deploying digital systems in numerous industries has hiked. The health sector has observed an extensive adoption of digital systems and services that generate significant medical records. Electronic health records contain valuable information for prospective and retrospective analysis that is often not entirely exploited because of the complicated dense information storage. The crude purpose of condensing health records is to select the information that holds most characteristics of the original documents based on a reported disease. These summaries may boost diagnosis and save a doctor's time during a saturated workload situation like the COVID-19 pandemic. In this paper, we are applying a multi-head attention-based mechanism to perform extractive summarization of meaningful phrases on clinical notes. Our method finds major sentences for a summary by correlating tokens, segments, and positional embeddings of sentences in a clinical note. The model outputs attention scores that are statistically transformed to extract critical phrases for visualization on the heat-mapping tool and for human use.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {813–820},
numpages = {8},
keywords = {ICD-9, MIMIC-III, clinical notes, deep learning, electronic health records, extractive summarization, information extraction, medical records, multi-head attention, natural language processing, transformer models},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3460231.3474260,
author = {Jiang, Jyun-Yu and Lee, Chia-Jung and Yang, Longqi and Sarrafzadeh, Bahareh and Hecht, Brent and Teevan, Jaime},
title = {Learning to Represent Human Motives for Goal-directed Web Browsing},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3474260},
doi = {10.1145/3460231.3474260},
abstract = {Motives or goals are recognized in psychology literature as the most fundamental drive that explains and predicts why people do what they do, including when they browse the web. Although providing enormous value, these higher-ordered goals are often unobserved, and little is known about how to leverage such goals to assist people’s browsing activities. This paper proposes to take a new approach to address this problem, which is fulfilled through a novel neural framework, Goal-directed Web Browsing &nbsp;(GoWeB). We adopt a psychologically-sound taxonomy of higher-ordered goals and learn to build their representations in a structure-preserving manner. Then we incorporate the resulting representations for enhancing the experiences of common activities people perform on the web. Experiments on large-scale data from Microsoft Edge web browser show that GoWeB significantly outperforms competitive baselines for in-session web page recommendation, re-visitation classification, and goal-based web page grouping. A follow-up analysis further characterizes how the variety of human motives can affect the difference observed in human behavioral patterns.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {361–371},
numpages = {11},
keywords = {Goal Representation Learning, User Behavior, User Goals, Web Browser Session Modeling},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@proceedings{10.1145/3607505,
title = {CSET '23: Proceedings of the 16th Cyber Security Experimentation and Test Workshop},
year = {2023},
isbn = {9798400707889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Marina del Rey, CA, USA}
}

@proceedings{10.1145/3564746,
title = {ACMSE '23: Proceedings of the 2023 ACM Southeast Conference},
year = {2023},
isbn = {9781450399210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, USA}
}

@inproceedings{10.1145/3583780.3614984,
author = {Wang, Siyuan and Zheng, Jianming and Chen, Wanyu and Cai, Fei and Luo, Xueshan},
title = {MultiPLe: Multilingual Prompt Learning for Relieving Semantic Confusions in Few-shot Event Detection},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614984},
doi = {10.1145/3583780.3614984},
abstract = {Event detection (ED) is a challenging task in the field of information extraction. Due to the monolingual text and rampant confusing triggers, traditional ED models suffer from semantic confusions in terms of polysemy and synonym, leading to severe detection mistakes. Such semantic confusions can be further exacerbated in a practical situation where scarce labeled data cannot provide sufficient semantic clues. To mitigate such bottleneck, we propose a multilingual prompt learning (MultiPLe) framework for few-shot event detection (FSED), including three components, i.e., a multilingual prompt, a hierarchical prototype and a quadruplet contrastive learning module. In detail, to ease the polysemy confusion, the multilingual prompt module develops the in-context semantics of triggers via the multilingual disambiguation and prior knowledge in pretrained language models. Then, the hierarchical prototype module is adopted to diminish the synonym confusion by connecting the captured inmost semantics of fuzzy triggers with labels at a fine granularity. Finally, we employ the quadruplet contrastive learning module to tackle the insufficient label representation and potential noise. Experiments on two public datasets show that MultiPLe outperforms the state-of-the-art baselines in weighted F1-score, presenting a maximum improvement of 13.63% for FSED.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2676–2685},
numpages = {10},
keywords = {few-shot event detection, prompt learning, semantic confusions},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3450273,
author = {Qi, Shanshan and Zheng, Limin and Shang, Feiyu},
title = {Dependency Parsing-based Entity Relation Extraction over Chinese Complex Text},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3450273},
doi = {10.1145/3450273},
abstract = {Open Relation Extraction (ORE) plays a significant role in the field of Information Extraction. It breaks the limitation that traditional relation extraction must pre-define relational types in the annotated corpus and specific domains restrictions, to realize the goal of extracting entities and the relation between entities in the open domain. However, with the increase of sentence complexity, the precision and recall of Entity Relation Extraction will be significantly reduced. To solve this problem, we present an unsupervised Clause_CORE method based on Chinese grammar and dependency parsing features. Clause_CORE is used for complex sentences processing, including decomposing complex sentence and dynamically complementing sentence components, which can reduce sentences complexity and maintain the integrity of sentences at the same time. Then, we perform dependency parsing for complete sentences and implement open entity relation extraction based on the model constructed by Chinese grammar rules. The experimental results show that the performance of Clause_CORE method is better than that of other advanced Chinese ORE systems on Wikipedia and Sina news datasets, which proves the correctness and effectiveness of the method. The results on mixed datasets of news data and encyclopedia data prove the generalization and portability of the method.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {67},
numpages = {34},
keywords = {Open entity relation extraction, dependency parsing, complex sentences processed, Chinese grammar rules, unsupervised}
}

@proceedings{10.1145/3565472,
title = {UMAP '23: Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
year = {2023},
isbn = {9781450399326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@proceedings{10.1145/3584748,
title = {EBIMCS '22: Proceedings of the 2022 5th International Conference on E-Business, Information Management and Computer Science},
year = {2022},
isbn = {9781450397827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hong Kong, Hong Kong}
}

@inproceedings{10.1145/3487664.3487706,
author = {Odakura, Fumimaro and Kobayashi, Koga and Wakabayashi, Kei},
title = {Active Learning for Extracting Technical Terms Covering Multiword Phrases},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487706},
doi = {10.1145/3487664.3487706},
abstract = {Automatic extraction of technical terms is an important task for organizing a set of documents. While the sequence labeling formulation is the major approach, we explore a method that takes examples of terms as input and outputs phrases in the same category as the given terms to avoid the heavy cost for building a training dataset. The existing methods in this direction are template-based, which the user cannot give any feedback to the system even if some of the extracted terms are not intended. This paper proposes a framework for extracting technical terms that considers the user’s feedback by adopting active learning approach. The proposed method can extract terms consisting of multiple words by dynamically accessing an inverted index created in advance. We empirically show the effectiveness of the proposed method in comparison to the straightforward application of active learning to an existing method.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {311–318},
numpages = {8},
keywords = {active learning, inverted index, multiword phrases, part of speech, terminology extraction},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3511095.3531279,
author = {Vitiugin, Fedor and Castillo, Carlos},
title = {Cross-Lingual Query-Based Summarization of Crisis-Related Social Media: An Abstractive Approach Using Transformers},
year = {2022},
isbn = {9781450392334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511095.3531279},
doi = {10.1145/3511095.3531279},
abstract = {Relevant and timely information collected from social media during crises can be an invaluable resource for emergency management. However, extracting this information remains a challenging task, particularly when dealing with social media postings in multiple languages. This work proposes a cross-lingual method for retrieving and summarizing crisis-relevant information from social media postings. We describe a uniform way of expressing various information needs through structured queries and a way of creating summaries answering those information needs. The method is based on multilingual transformers embeddings. Queries are written in one of the languages supported by the embeddings, and the extracted sentences can be in any of the other languages supported. Abstractive summaries are created by transformers. The evaluation, done by crowdsourcing evaluators and emergency management experts, and carried out on collections extracted from Twitter during five large-scale disasters spanning ten languages, shows the flexibility of our approach. The generated summaries are regarded as more focused, structured, and coherent than existing state-of-the-art methods, and experts compare them favorably against summaries created by existing, state-of-the-art methods.},
booktitle = {Proceedings of the 33rd ACM Conference on Hypertext and Social Media},
pages = {21–31},
numpages = {11},
keywords = {abstractive summarization, emergency management, multilingual retrieval, social media},
location = {Barcelona, Spain},
series = {HT '22}
}

@inproceedings{10.1145/3460120.3484536,
author = {Bui, Duc and Yao, Yuan and Shin, Kang G. and Choi, Jong-Min and Shin, Junbum},
title = {Consistency Analysis of Data-Usage Purposes in Mobile Apps},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484536},
doi = {10.1145/3460120.3484536},
abstract = {While privacy laws and regulations require apps and services to disclose the purposes of their data collection to the users (i.e., why do they collect my data?), the data usage in an app's actual behavior does not always comply with the purposes stated in its privacy policy. Automated techniques have been proposed to analyze apps' privacy policies and their execution behavior, but they often overlooked the purposes of the apps' data collection, use and sharing. To mitigate this oversight, we propose PurPliance, an automated system that detects the inconsistencies between the data-usage purposes stated in a natural language privacy policy and those of the actual execution behavior of an Android app. PurPliance analyzes the predicate-argument structure of policy sentences and classifies the extracted purpose clauses into a taxonomy of data purposes. Purposes of actual data usage are inferred from network data traffic. We propose a formal model to represent and verify the data usage purposes in the extracted privacy statements and data flows to detect policy contradictions in a privacy policy and flow-to-policy inconsistencies between network data flows and privacy statements. Our evaluation results of end-to-end contradiction detection have shown PurPliance to improve detection precision from 19% to 95% and recall from 10% to 50% compared to a state-of-the-art method. Our analysis of 23.1k Android apps has also shown PurPliance to detect contradictions in 18.14% of privacy policies and flow-to-policy inconsistencies in 69.66% of apps, indicating the prevalence of inconsistencies of data practices in mobile apps.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2824–2843},
numpages = {20},
keywords = {consistency analysis, data flow, data-usage purposes, mobile apps, privacy policies},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@article{10.1145/3483424,
author = {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
title = {A Survey of AIOps Methods for Failure Management},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3483424},
doi = {10.1145/3483424},
abstract = {Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {81},
numpages = {45},
keywords = {AIOps, IT operations and maintenance, failure management, artificial intelligence}
}

@article{10.1145/3584861,
author = {Das, Ringki and Singh, Thoudam Doren},
title = {Image–Text Multimodal Sentiment Analysis Framework of Assamese News Articles Using Late Fusion},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3584861},
doi = {10.1145/3584861},
abstract = {Before the arrival of the web as a corpus, people detected positive and negative news based on the understanding of the textual content from physical newspaper rather than an automatic identification approach from readily available e-newspapers. Thus, the earlier sentiment analysis approach is based on unimodal data, and less effort is paid to the multimodal data. However, the presence of multimodal information helps us to get a clearer understanding of the sentiment. To the best of our knowledge, less work has been introduced on the image–text multimodal sentiment analysis framework of Assamese, a low-resource Indian language mostly spoken in the northeast part of India. We built an Assamese news articles dataset consisting of news text and associated images and one image caption to conduct an experimental study. Focusing on important words and discriminative regions of the images mostly related to sentiment, two individual unimodal such as textual and visual models are proposed. The visual model is developed using an encoder-decoder–based image caption generation system. An image–text multimodal approach is proposed to explore the internal correlation between textual and visual features for joint sentiment classification. Finally, we propose the multimodal sentiment analysis framework, i.e., Textual Visual Multimodal Fusion, by employing a late fusion scheme to merge the three different modalities for the final sentiment prediction. Experimental results conducted on the Assamese dataset built in-house demonstrate that the contextual integration of multimodal features delivers better performance than unimodal features.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {161},
numpages = {30},
keywords = {Multimodal sentiment analysis, low resource language, caption generation, machine learning classifier, late fusion}
}

@proceedings{10.1145/3568562,
title = {SoICT '22: Proceedings of the 11th International Symposium on Information and Communication Technology},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hanoi, Vietnam}
}

@proceedings{10.1145/3580219,
title = {CCEAI '23: Proceedings of the 7th International Conference on Control Engineering and Artificial Intelligence},
year = {2023},
isbn = {9781450397513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@article{10.1109/TASLP.2022.3155281,
author = {Wu, Junshuang and Zhang, Richong and Mao, Yongyi and Huai, Jinpeng},
title = {Dealing With Hierarchical Types and Label Noise in Fine-Grained Entity Typing},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3155281},
doi = {10.1109/TASLP.2022.3155281},
abstract = {Fine-Grained entity typing is complicated by the fact that type labels form a hierarchical structure, and those training examples usually contain noisy type labels. This paper addresses these two issues by proposing a novel framework that simultaneously models the correlation among hierarchical types and the noise within the training data. Additionally, the framework contains an innovative training approach during which the noise in the training data is progressively removed. Experiments on standard benchmarking datasets validate the proposed framework and establish it as a new state of the art for this problem.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1305–1318},
numpages = {14}
}

@inproceedings{10.1145/3531146.3533161,
author = {Lu, Christina and Kay, Jackie and McKee, Kevin},
title = {Subverting machines, fluctuating identities: Re-learning human categorization},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533161},
doi = {10.1145/3531146.3533161},
abstract = {Most machine learning systems that interact with humans construct some notion of a person’s “identity,” yet the default paradigm in AI research envisions identity with essential attributes that are discrete and static. In stark contrast, strands of thought within critical theory present a conception of identity as malleable and constructed entirely through interaction; a doing rather than a being. In this work, we distill some of these ideas for machine learning practitioners and introduce a theory of identity as autopoiesis, circular processes of formation and function. We argue that the default paradigm of identity used by the field immobilizes existing identity categories and the power differentials that co-occur, due to the absence of iterative feedback to our models. This includes a critique of emergent AI fairness practices that continue to impose the default paradigm. Finally, we apply our theory to sketch approaches to autopoietic identity through multilevel optimization and relational learning. While these ideas raise many open questions, we imagine the possibilities of machines that are capable of expressing human identity as a relationship perpetually in flux.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1005–1015},
numpages = {11},
keywords = {algorithmic fairness, identity systems, social construction, theories of identity},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@proceedings{10.1145/3614321,
title = {ICEGOV '23: Proceedings of the 16th International Conference on Theory and Practice of Electronic Governance},
year = {2023},
isbn = {9798400707421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Belo Horizonte, Brazil}
}

@article{10.1145/3590152,
author = {Wagner, Isabel},
title = {Privacy Policies across the Ages: Content of Privacy Policies 1996–2021},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {2471-2566},
url = {https://doi.org/10.1145/3590152},
doi = {10.1145/3590152},
abstract = {It is well known that most users do not read privacy policies but almost always tick the box to agree with them. While the length and readability of privacy policies have been well studied and many approaches for policy analysis based on natural language processing have been proposed, existing studies are limited in their depth and scope, often focusing on a small number of data practices at single point in time. In this article, we fill this gap by analyzing the 25-year history of privacy policies using machine learning and natural language processing and presenting a comprehensive analysis of policy contents. Specifically, we collect a large-scale longitudinal corpus of privacy policies from 1996 to 2021 and analyze their content in terms of the data practices they describe, the rights they grant to users, and the rights they reserve for their organizations. We pay particular attention to changes in response to recent privacy regulations such as the GDPR and CCPA. We observe some positive changes, such as reductions in data collection post-GDPR, but also a range of concerning data practices, such as widespread implicit data collection for which users have no meaningful choices or access rights. Our work is an important step toward making privacy policies machine readable on the user side, which would help users match their privacy preferences against the policies offered by web services.},
journal = {ACM Trans. Priv. Secur.},
month = may,
articleno = {32},
numpages = {32},
keywords = {Privacy policy, longitudinal study, natural language processing, machine learning, neural networks, BERT}
}

@proceedings{10.1145/3523227,
title = {RecSys '22: Proceedings of the 16th ACM Conference on Recommender Systems},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1145/3612921,
author = {Bensalem, Raja and Haddar, Kais and Blache, Philippe},
title = {An Arabic Probabilistic Parser Based on a Property Grammar},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {2375-4699},
url = {https://doi.org/10.1145/3612921},
doi = {10.1145/3612921},
abstract = {The specificities of Arabic parsing, such as agglutination, vocalization, and the relatively order-free words in Arabic sentences, remain major issues to consider. To promote its robustness, such parseing should define different types of constraints. Property Grammar (PG) formalism verifies the satisfiability of the constraints directly on the units of the structure, thanks to its properties (or relations). In this context, we propose to build a probabilistic parser with syntactic properties, using a PG, and we measure the production rules in terms of different implicit information and in particular the syntactic properties. We experimented with our parser on the treebank ATB, using the parsing algorithm CYK, and we obtained encouraging results. Our method is also automatic for implementation of most property types. Its generalization for other languages or corpus domains (using treebanks) could be a good perspective. Its combination with pre-trained models of BERT may also make our parser faster.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {237},
numpages = {25},
keywords = {Probabilistic parser, property grammar formalism, Arabic language, lexicalized grammar}
}

@proceedings{10.1145/3626705,
title = {MUM '23: Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3557915,
title = {SIGSPATIAL '22: Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The conference started as a series of workshops and symposia back in 1993 with the aim of promoting interdisciplinary discussions among researchers, developers, users, and practitioners and fostering research in all aspects of geographic information systems, especially in relation to novel systems based on geospatial data and knowledge. It continues to provide a forum for original research contributions covering all conceptual, design and implementation aspects of geospatial data ranging from applications, user interfaces and visualization, to data storage, query processing, indexing, machine learning and data mining. The conference is the premier annual event of the ACM Special Interest Group on Spatial Information (ACM SIGSPATIAL).},
location = {Seattle, Washington}
}

@proceedings{10.1145/3560470,
title = {ICHMI '22: Proceedings of the 2022 International Conference on Human Machine Interaction},
year = {2022},
isbn = {9781450396615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@article{10.14778/3611540.3611634,
author = {Halevy, Alon and Choi, Yejin and Floratou, Avrilia and Franklin, Michael J. and Noy, Natasha and Wang, Haixun},
title = {Will LLMs Reshape, Supercharge, or Kill Data Science? (VLDB 2023 Panel)},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611634},
doi = {10.14778/3611540.3611634},
abstract = {Large language models (LLMs) have recently taken the world by storm, promising potentially game changing opportunities in multiple fields. Naturally, there is significant promise in applying LLMs to the management of structured data, or more generally, to the processes involved in data science. At the very least, LLMs have the potential to provide substantial advancements in long-standing challenges that our community has been tackling for decades. On the other hand, they may introduce completely new capabilities that we have only dreamed of thus far. This panel will bring together a few leading experts who have been thinking about these opportunities from various perspectives and fielding them in research prototypes and even in commercial applications.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4114–4115},
numpages = {2}
}

@article{10.1145/3473337,
author = {Pan, Yaoxin and Liang, Shangsong and Ren, Jiaxin and Meng, Zaiqiao and Zhang, Qiang},
title = {Personalized, Sequential, Attentive, Metric-Aware Product Search},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3473337},
doi = {10.1145/3473337},
abstract = {The task of personalized product search aims at retrieving a ranked list of products given a user’s input query and his/her purchase history. To address this task, we propose the PSAM model, a Personalized, Sequential, Attentive and Metric-aware (PSAM) model, that learns the semantic representations of three different categories of entities, i.e., users, queries, and products, based on user sequential purchase historical data and the corresponding sequential queries. Specifically, a query-based attentive LSTM (QA-LSTM) model and an attention mechanism are designed to infer users dynamic embeddings, which is able to capture their short-term and long-term preferences. To obtain more fine-grained embeddings of the three categories of entities, a metric-aware objective is deployed in our model to force the inferred embeddings subject to the triangle inequality, which is a more realistic distance measurement for product search. Experiments conducted on four benchmark datasets show that our PSAM model significantly outperforms the state-of-the-art product search baselines in terms of effectiveness by up to 50.9% improvement under NDCG@20. Our visualization experiments further illustrate that the learned product embeddings are able to distinguish different types of products.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {36},
numpages = {29},
keywords = {Product search, personalized web search, neural networks, LSTM, metric learning}
}

@inproceedings{10.1145/3581641.3584049,
author = {Prakash, Yash and Sunkara, Mohan and Lee, Hae-Na and Jayarathna, Sampath and Ashok, Vikas},
title = {AutoDesc: Facilitating Convenient Perusal of Web Data Items for Blind Users},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584049},
doi = {10.1145/3581641.3584049},
abstract = {Web data items such as shopping products, classifieds, and job listings are indispensable components of most e-commerce websites. The information on the data items are typically distributed over two or more webpages, e.g., a ‘Query-Results’ page showing the summaries of the items, and ‘Details’ pages containing full information about the items. While this organization of data mitigates information overload and visual cluttering for sighted users, it however increases the interaction overhead and effort for blind users, as back-and-forth navigation between webpages using screen reader assistive technology is tedious and cumbersome. Existing usability-enhancing solutions are unable to provide adequate support in this regard as they predominantly focus on enabling efficient content access within a single webpage, and as such are not tailored for content distributed across multiple webpages. As an initial step towards addressing this issue, we developed AutoDesc, a browser extension that leverages a custom extraction model to automatically detect and pull out additional item descriptions from the ‘details’ pages, and then proactively inject the extracted information into the ‘Query-Results’ page, thereby reducing the amount of back-and-forth screen reader navigation between the two webpages. In a study with 16 blind users, we observed that within the same time duration, the participants were able to peruse significantly more data items on average with AutoDesc, compared to that with their preferred screen readers as well as with a state-of-the-art solution.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {32–45},
numpages = {14},
keywords = {Blind, Screen reader, Visual impairment, Web accessibility},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@article{10.1145/3505243,
author = {Yang, Yanming and Xia, Xin and Lo, David and Grundy, John},
title = {A Survey on Deep Learning for Software Engineering},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3505243},
doi = {10.1145/3505243},
abstract = {In 2006, Geoffrey Hinton proposed the concept of training “Deep Neural Networks (DNNs)” and an improved model training method to break the bottleneck of neural network development. More recently, the introduction of AlphaGo in 2016 demonstrated the powerful learning ability of deep learning and its enormous potential. Deep learning has been increasingly used to develop state-of-the-art software engineering (SE) research tools due to its ability to boost performance for various SE tasks. There are many factors, e.g., deep learning model selection, internal structure differences, and model optimization techniques, that may have an impact on the performance of DNNs applied in SE. Few works to date focus on summarizing, classifying, and analyzing the application of deep learning techniques in SE. To fill this gap, we performed a survey to analyze the relevant studies published since 2006. We first provide an example to illustrate how deep learning techniques are used in SE. We then conduct a background analysis (BA) of primary studies and present four research questions to describe the trend of DNNs used in SE (BA), summarize and classify different deep learning techniques (RQ1), and analyze the data processing including data collection, data classification, data pre-processing, and data representation (RQ2). In RQ3, we depicted a range of key research topics using DNNs and investigated the relationships between DL-based model adoption and multiple factors (i.e., DL architectures, task types, problem types, and data types). We also summarized commonly used datasets for different SE tasks. In RQ4, we summarized the widely used optimization algorithms and provided important evaluation metrics for different problem types, including regression, classification, recommendation, and generation. Based on our findings, we present a set of current challenges remaining to be investigated and outline a proposed research road map highlighting key opportunities for future work.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {206},
numpages = {73},
keywords = {Deep learning, neural network, machine learning, software engineering, survey}
}

@inproceedings{10.1145/3485447.3512135,
author = {Chen, Zhendong and Hui, Siu Cheung and Zhuang, Fuzhen and Liao, Lejian and Li, Fei and Jia, Meihuizi and Li, Jiaqi},
title = {EvidenceNet: Evidence Fusion Network for Fact Verification},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512135},
doi = {10.1145/3485447.3512135},
abstract = {Fact verification is a challenging task that requires the retrieval of multiple pieces of evidence from a reliable corpus for verifying the truthfulness of a claim. Although the current methods have achieved satisfactory performance, they still suffer from one or more of the following three problems: (1) unable to extract sufficient contextual information from the evidence sentences; (2) containing redundant evidence information and (3) incapable of capturing the interaction between claim and evidence. To tackle the problems, we propose an evidence fusion network called EvidenceNet. The proposed EvidenceNet model captures global contextual information from various levels of evidence information for deep understanding. Moreover, a gating mechanism is designed to filter out redundant information in evidence. In addition, a symmetrical interaction attention mechanism is also proposed for identifying the interaction between claim and evidence. We conduct extensive experiments based on the FEVER dataset. The experimental results have shown that the proposed EvidenceNet model outperforms the current fact verification methods and achieves the state-of-the-art performance.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2636–2645},
numpages = {10},
keywords = {fact verification, gating mechanism, symmetrical interaction attention mechanism},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3439800,
author = {Bi, Mingwen and Zhang, Qingchuan and Zuo, Min and Xu, Zelong and Jin, Qingyu},
title = {Bi-directional Long Short-Term Memory Model with Semantic Positional Attention for the Question Answering System},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3439800},
doi = {10.1145/3439800},
abstract = {The intelligent question answering system aims to provide quick and concise feedback on the questions of users. Although the performance of phrase-level and numerous attention models have been improved, the sentence components and position information are not emphasized enough. This article combines Ci-Lin and word2vec to divide all of the words in the question-answer pairs into groups according to the semantics and select one kernel word in each group. The remaining words are common words and realize the semantic mapping mechanism between kernel words and common words. With this Chinese semantic mapping mechanism, the common words in all questions and answers are replaced by the semantic kernel words to realize the normalization of the semantic representation. Meanwhile, based on the bi-directional LSTM model, this article introduces a method of the combination of semantic role labeling and positional context, dividing the sentence into multiple semantic segments according to semantic logic. The weight is given to the neighboring words in the same semantic segment and propose semantic role labeling position attention based on the bi-directional LSTM model (BLSTM-SRLP). The good performance of the BLSTM-SRLP model has been demonstrated in comparative experiments on the food safety field dataset (FS-QA).},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {77},
numpages = {13},
keywords = {Question answering, BLSTM model, semantic positional-based attention, Chinese semantic mapping mechanism}
}

@proceedings{10.1145/3570945,
title = {IVA '23: Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains the papers presented at the 23nd International Conference on Intelligent Virtual Agents (IVA 2023) located in W\"{u}rzburg, Germany, from 19. to 22.09.2023.},
location = {W\"{u}rzburg, Germany}
}

@proceedings{10.1145/3594806,
title = {PETRA '23: Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@inproceedings{10.1145/3483207.3483231,
author = {Lin, Yiquan and Xie, Hongtu},
title = {Learning Dense Entity-Aware Dialogue Intentions with Rewritten Utterance for External Knowledge Documents Retrieval},
year = {2021},
isbn = {9781450390170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483207.3483231},
doi = {10.1145/3483207.3483231},
abstract = {External knowledge-enhanced task-oriented dialogue systems aim to cover user requests beyond pre-defined DBs/APIs. Recently, existing dialogue systems have focused more on retrieving external knowledge sources relevant to dialogue contexts, achieving competitive results. However, due to the lack of modeling entity-aware dialogue intention, such dialogue systems are hard to accurately and efficiently link the out-of-API functions in real-world scenarios. To tackle this problem, this paper investigates learning dense entity-aware dialogue intentions for external knowledge documents retrieval in task-oriented dialogues. To this end, we propose an intention-guided two-stage training approach that includes intention-guided training and knowledge transfer stages. This approach, which leverages rewritten utterances that explicitly convey entity-aware user intentions, can improve the performance of existing Bi-Encoder retrievers such as DPR (Deep Passage Retriever). In intention-guided training stage, a posterior history encoder is initialized and guided by inputting rewritten utterances for learning discriminative dense representations. In knowledge transfer stage, these representations are transferred to a newly initialized prior encoder for inference via an extra intent consistency loss. In addition, negative sampling in test knowledge documents is used to learn more discriminative dense representations of the unseen domain. The advantages of our approach are no need for response annotations and extra response generator, additionally, it provides great scalability. The experimental results on augmented MultiWOZ 2.1 dataset show that our approach outperforms baseline models except for relevance classifiers in retrieval accuracy and has reasonably high efficiency.},
booktitle = {Proceedings of the 2021 4th International Conference on Signal Processing and Machine Learning},
pages = {142–151},
numpages = {10},
keywords = {Bi-Encoder retrievers, External knowledge documents retrieval, Intention-guided two-stage training, Task-oriented dialogue systems},
location = {Beijing, China},
series = {SPML '21}
}

@inproceedings{10.1145/3477495.3531729,
author = {Lin, Tengteng and Chen, Qiaosheng and Cheng, Gong and Soylu, Ahmet and Ell, Basil and Zhao, Ruoqi and Shi, Qing and Wang, Xiaxia and Gu, Yu and Kharlamov, Evgeny},
title = {ACORDAR: A Test Collection for Ad Hoc Content-Based (RDF) Dataset Retrieval},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531729},
doi = {10.1145/3477495.3531729},
abstract = {Ad hoc dataset retrieval is a trending topic in IR research. Methods and systems are evolving from metadata-based to content-based ones which exploit the data itself for improving retrieval accuracy but thus far lack a specialized test collection. In this paper, we build and release the first test collection for ad hoc content-based dataset retrieval, where content-oriented dataset queries and content-based relevance judgments are annotated by human experts who are assisted with a dashboard designed specifically for comprehensively and conveniently browsing both the metadata and data of a dataset. We conduct extensive experiments on the test collection to analyze its difficulty and provide insights into the underlying task.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2981–2991},
numpages = {11},
keywords = {ad hoc dataset retrieval, dataset browsing, dataset search, rdf, test collection},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@article{10.1145/3572905,
author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
title = {Machine Learning for Software Engineering: A Tertiary Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3572905},
doi = {10.1145/3572905},
abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {256},
numpages = {39},
keywords = {Tertiary study, machine learning, software engineering, systematic literature review}
}

@article{10.5555/3586589.3586815,
author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
title = {Underspecification presents challenges for credibility in modern machine learning},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Machine learning (ML) systems often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification in ML pipelines as a key reason for these failures. An ML pipeline is the full procedure followed to train and validate a predictor. Such a pipeline is underspecified when it can return many distinct predictors with equivalently strong test performance. Underspecification is common in modern ML pipelines that primarily validate predictors on held-out data that follow the same distribution as the training data. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We provide evidence that underspecfication has substantive implications for practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {226},
numpages = {61},
keywords = {distribution shift, spurious correlation, fairness, identifiability, computer vision, natural language processing, medical imaging, electronic health records, genomics}
}

@proceedings{10.1145/3558489,
title = {PROMISE 2022: Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 18th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2022), to be held in hybrid mode (physically and virtually) on November 18th, 2022, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). PROMISE is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in the construction and/or application of predictive models and data analytics in software engineering. Such models and analyses could be targeted at planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. This year PROMISE received a total of 18 paper submissions. The review process was double blind and each paper was reviewed by at least three members of the program committee. An online discussion was also held for 8 days. Based on this procedure, we accepted a total of 10 full papers, which will be presented in 3 technical sessions. The acceptance criteria were entirely based on the quality of the papers, without imposing any constraint on the number of papers to be accepted.  

We are delighted to announce an outstanding keynote: Release Engineering in the AI World: How can Analytics Help? By Prof. Bram Adams, Queen’s University, Canada  

We would like to thank all authors for submitting high quality papers, and program committee members for their timely and accurate reviewing activity. Last, but not least, we would like to thank the FSE 2022 organizers for hosting PROMISE 2022 as a co-located event and for their logistic support in the organization of the conference.  

We hope you will enjoy PROMISE 2022.  
We certainly will!  

Many thanks from  
Shane McIntosh (General Chair),  
Gema Rodriguez-Perez and Weiyi Shang (Program Chairs).},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3459637.3482440,
author = {Sheng, Qiang and Zhang, Xueyao and Cao, Juan and Zhong, Lei},
title = {Integrating Pattern- and Fact-based Fake News Detection via Model Preference Learning},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482440},
doi = {10.1145/3459637.3482440},
abstract = {To defend against fake news, researchers have developed various methods based on texts. These methods can be grouped as 1) pattern-based methods, which focus on shared patterns among fake news posts rather than the claim itself; and 2) fact-based methods, which retrieve from external sources to verify the claim's veracity without considering patterns. The two groups of methods, which have different preferences of textual clues, actually play complementary roles in detecting fake news. However, few works consider their integration. In this paper, we study the problem of integrating pattern- and fact-based models into one framework via modeling their preference differences, i.e., making the pattern- and fact-based models focus on respective preferred parts in a post and mitigate interference from non-preferred parts as possible. To this end, we build a Preference-aware Fake News Detection Framework (Pref-FEND), which learns the respective preferences of pattern- and fact-based models for joint detection. We first design a heterogeneous dynamic graph convolutional network to generate the respective preference maps, and then use these maps to guide the joint learning of pattern- and fact-based models for final prediction. Experiments on two real-world datasets show that Pref-FEND effectively captures model preferences and improves the performance of models based on patterns, facts, or both.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1640–1650},
numpages = {11},
keywords = {fact-checking, fake news detection, graph neural networks, pattern mining, preference learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3477495.3531737,
author = {Alexander, Daria and Kusa, Wojciech and P. de Vries, Arjen},
title = {ORCAS-I: Queries Annotated with Intent using Weak Supervision},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531737},
doi = {10.1145/3477495.3531737},
abstract = {User intent classification is an important task in information retrieval. In this work, we introduce a revised taxonomy of user intent. We take the widely used differentiation between navigational, transactional and informational queries as a starting point, and identify three different sub-classes for the informational queries: instrumental, factual and abstain. The resulting classification of user queries is more fine-grained, reaches a high level of consistency between annotators, and can serve as the basis for an effective automatic classification process. The newly introduced categories help distinguish between types of queries that a retrieval system could act upon, for example by prioritizing different types of results in the ranking.  We have used a weak supervision approach based on Snorkel to annotate the ORCAS dataset according to our new user intent taxonomy, utilising established heuristics and keywords to construct rules for the prediction of the intent category. We then present a series of experiments with a variety of machine learning models, using the labels from the weak supervision stage as training data, but find that the results produced by Snorkel are not outperformed by these competing approaches and can be considered state-of-the-art. The advantage of a rule-based approach like Snorkel's is its efficient deployment in an actual system, where intent classification would be executed for every query issued.  The resource released with this paper is the ORCAS-I dataset: a labelled version of the ORCAS click-based dataset of Web queries, which provides 18 million connections to 10 million distinct queries. We anticipate the usage of this resource in a scenario where the retrieval system would change its internal workings and search user interface to match the type of information request. For example, a navigational query could trigger just a short result list; and, for instrumental intent the system could rank tutorials and instructions higher than for other types of queries.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3057–3066},
numpages = {10},
keywords = {click data, intent labelling, snorkel, weak supervision, web search},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3558489.3559074,
author = {Mohamad, Mazen and Stegh\"{o}fer, Jan-Philipp and \r{A}str\"{o}m, Alexander and Scandariato, Riccardo},
title = {Identifying security-related requirements in regulatory documents based on cross-project classification},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558489.3559074},
doi = {10.1145/3558489.3559074},
abstract = {Security is getting substantial focus in many industries, especially safety-critical ones. When new regulations and standards which can run to hundreds of pages are introduced, it is necessary to identify the requirements in those documents which have an impact on security. Additionally, it is necessary to revisit the requirements of existing systems and identify the security related ones.  
We investigate the feasibility of using a classifier for security-related requirements trained on requirement specifications available online.  
We base our investigation on 15 requirement documents, randomly selected and partially pre-labelled, with a total of 3,880 requirements.  
To validate the model, we run a cross-project prediction on the data where each specification constitutes a group. We also test the model on three different United Nations (UN) regulations from the automotive domain with different magnitudes of security relevance.  
Our results indicate the feasibility of training a model from a heterogeneous data set including specifications from multiple domains and in different styles.  
Additionally, we show the ability of such a classifier to identify security requirements in real-life regulations and discuss scenarios in which such a classification becomes useful to practitioners.},
booktitle = {Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {82–91},
numpages = {10},
keywords = {Automated Requirements Engineering, Machine Learning, Requirements Classification, Security Requirements},
location = {Singapore, Singapore},
series = {PROMISE 2022}
}

@inproceedings{10.1145/3459637.3481909,
author = {Liu, Peiyang and Wang, Xi and Wang, Lin and Ye, Wei and Xi, Xiangyu and Zhang, Shikun},
title = {Distilling Knowledge from BERT into Simple Fully Connected Neural Networks for Efficient Vertical Retrieval},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481909},
doi = {10.1145/3459637.3481909},
abstract = {Distilled BERT models are more suitable for efficient vertical retrieval in online sponsored vertical search with low-latency requirements than BERT due to fewer parameters and faster inference. Unfortunately, most of these models are still far from ideal inference speed. This paper presents a novel and effective method to distill knowledge from BERT into simple fully connected neural networks (FNN). Results of extensive experiments on English and Chinese datasets demonstrate that our method achieves comparable results with existing distilled BERT models while the inference is accelerated by more than ten times. We have successfully applied our method on our online sponsored vertical search engine and get remarkable improvements.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3965–3975},
numpages = {11},
keywords = {bert, knowledge distillation, sponsored search, vertical retrieval},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@proceedings{10.1145/3623809,
title = {HAI '23: Proceedings of the 11th International Conference on Human-Agent Interaction},
year = {2023},
isbn = {9798400708244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Gothenburg, Sweden}
}

@proceedings{10.1145/3613372,
title = {SBES '23: Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Campo Grande, Brazil}
}

@article{10.1145/3476106,
author = {Zhao, Jiashu and Huang, Jimmy Xiangji and Deng, Hongbo and Chang, Yi and Xia, Long},
title = {Are Topics Interesting or Not? An LDA-based Topic-graph Probabilistic Model for Web Search Personalization},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3476106},
doi = {10.1145/3476106},
abstract = {In this article, we propose a Latent Dirichlet Allocation– (LDA) based topic-graph probabilistic personalization model for Web search. This model represents a user graph in a latent topic graph and simultaneously estimates the probabilities that the user is interested in the topics, as well as the probabilities that the user is not interested in the topics. For a given query issued by the user, the webpages that have higher relevancy to the interested topics are promoted, and the webpages more relevant to the non-interesting topics are penalized. In particular, we simulate a user’s search intent by building two profiles: A positive user profile for the probabilities of the user is interested in the topics and a corresponding negative user profile for the probabilities of being not interested in the the topics. The profiles are estimated based on the user’s search logs. A clicked webpage is assumed to include interesting topics. A skipped (viewed but not clicked) webpage is assumed to cover some non-interesting topics to the user. Such estimations are performed in the latent topic space generated by LDA. Moreover, a new approach is proposed to estimate the correlation between a given query and the user’s search history so as to determine how much personalization should be considered for the query. We compare our proposed models with several strong baselines including state-of-the-art personalization approaches. Experiments conducted on a large-scale real user search log collection illustrate the effectiveness of the proposed models.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {51},
numpages = {24},
keywords = {Personalization, probabilistic model, Web search, Latent Dirichlet Allocation (LDA), topic-graph}
}

@inproceedings{10.1145/3534678.3539187,
author = {Srivastava, Aseem and Suresh, Tharun and Lord, Sarah P. and Akhtar, Md Shad and Chakraborty, Tanmoy},
title = {Counseling Summarization Using Mental Health Knowledge Guided Utterance Filtering},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539187},
doi = {10.1145/3534678.3539187},
abstract = {The psychotherapy intervention technique is a multifaceted conversation between a therapist and a patient. Unlike general clinical discussions, psychotherapy's core components (viz. symptoms) are hard to distinguish, thus becoming a complex problem to summarize later. A structured counseling conversation may contain discussions about symptoms, history of mental health issues, or the discovery of the patient's behavior. It may also contain discussion filler words irrelevant to a clinical summary. We refer to these elements of structured psychotherapy as counseling components. In this paper, the aim is mental health counseling summarization to build upon domain knowledge and to help clinicians quickly glean meaning. We create a new dataset after annotating 12.9K utterances of counseling components and reference summaries for each dialogue. Further, we propose ConSum, a novel counseling-component guided summarization model. ConSum undergoes three independent modules. First, to assess the presence of depressive symptoms, it filters utterances utilizing the Patient Health Questionnaire (PHQ-9), while the second and third modules aim to classify counseling components. At last, we propose a problem-specific Mental Health Information Capture (MHIC) evaluation metric for counseling summaries. Our comparative study shows that we improve on performance and generate cohesive, semantic, and coherent summaries. We comprehensively analyze the generated summaries to investigate the capturing of psychotherapy elements. Human and clinical evaluations on the summary show that ConSum generates quality summary. Further, mental health experts validate the clinical acceptability of the ConSum. Lastly, we discuss the uniqueness in mental health counseling summarization in the real world and show evidences of its deployment on an online application with the support of mpathic.ai},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3920–3930},
numpages = {11},
keywords = {dialogue summarization, natural language processing},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3512768,
author = {Sworna, Zarrin Tasnim and Islam, Chadni and Babar, Muhammad Ali},
title = {APIRO: A Framework for Automated Security Tools API Recommendation},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3512768},
doi = {10.1145/3512768},
abstract = {Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC team’s ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API Recommendation for security Orchestration, automation, and response, APIRO. To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93%, 23.03%, and 20.87% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7% in terms of Mean Reciprocal Rank (MRR).},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {24},
numpages = {42},
keywords = {Security Orchestration, Incident Response Plan, security tool API, Security Operation Center, API Recommendation, SOAR}
}

@inproceedings{10.1145/3524481.3527229,
author = {Liu, Yu and Yandrapally, Rahulkrishna and Kalia, Anup K. and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah, Ali},
title = {CrawLabel: computing natural-language labels for UI test cases},
year = {2022},
isbn = {9781450392860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524481.3527229},
doi = {10.1145/3524481.3527229},
abstract = {End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently, diagnosing failures in these tests and maintaining them can be tedious. Techniques for computing natural-language descriptions of test cases can help increase test readability. However, so far, such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases.In this paper, we focus on the problem of computing natural-language labels for the steps of end-to-end UI test cases for web applications. We present two techniques that apply natural-language processing to information available in the browser document object model (DOM). The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation. However, supervised approach requires a training dataset. So we propose the second technique, which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically. We implemented these techniques, along with two simpler baseline techniques, in a tool called CrawLabel (available as a plugin to Crawljax, a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications. Our results indicate that the supervised approach can achieve precision, recall, and Fl-score of 83.38, 60.64, and 66.40, respectively. The unsupervised approach, although less effective, is competitive, achieving scores of 72.37, 58.12, and 59.77. We highlight key results and discuss the implications of our findings.},
booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test},
pages = {103–114},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {AST '22}
}

@proceedings{10.1145/3568364,
title = {WSSE '22: Proceedings of the 4th World Symposium on Software Engineering},
year = {2022},
isbn = {9781450396950},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3582197,
title = {ICIT '22: Proceedings of the 2022 10th International Conference on Information Technology: IoT and Smart City},
year = {2022},
isbn = {9781450397438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@article{10.1109/TASLP.2022.3210442,
author = {Liu, Jian and Chen, Yufeng and Xu, Jinan},
title = {MRCAug: Data Augmentation via Machine Reading Comprehension for Document-Level Event Argument Extraction},
year = {2023},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3210442},
doi = {10.1109/TASLP.2022.3210442},
abstract = {Document-level event argument extraction (EAE) is a critical event semantic understanding task that requires a model to identify an event's global arguments beyond the sentence level. Existing approaches to this problem are based on supervised learning, which require a large amount of labeled data for model training. However, due to the complicated structure of an event, human annotation for this task is costly, and the issue of inadequacy of training data has long hampered the study. In this study, we propose a novel approach to mitigating the data sparsity problem faced by document-level EAE, by linking the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including an implicit knowledge transfer method, which enables knowledge transfer from other tasks to the document-level EAE task, and an explicit data generation method, which can explicitly generate new training examples by treating a pre-trained MRC model as an annotator. Furthermore, we propose a self-training based noise reduction strategy that can effectively addresses the out-of-domain noise introduced by the data augmentation methods. The extensive assessments on three benchmarks have validated the effectiveness of our approach — it not only achieves state-of-the-art performance but also demonstrates superior results in the data-low scenario.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {3160–3172},
numpages = {13}
}

@article{10.1109/TASLP.2021.3065234,
author = {Xie, Huang and Virtanen, Tuomas},
title = {Zero-Shot Audio Classification Via Semantic Embeddings},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3065234},
doi = {10.1109/TASLP.2021.3065234},
abstract = {In this paper, we study zero-shot learning in audio classification via semantic embeddings extracted from textual labels and sentence descriptions of sound classes. Our goal is to obtain a classifier that is capable of recognizing audio instances of sound classes that have no available training samples, but only semantic side information. We employ a bilinear compatibility framework to learn an acoustic-semantic projection between intermediate-level representations of audio instances and sound classes, i.e., acoustic embeddings and semantic embeddings. We use VGGish to extract deep acoustic embeddings from audio clips, and pre-trained language models (Word2Vec, GloVe, BERT) to generate either label embeddings from textual labels or sentence embeddings from sentence descriptions of sound classes. Audio classification is performed by a linear compatibility function that measures how compatible an acoustic embedding and a semantic embedding are. We evaluate the proposed method on a small balanced dataset ESC-50 and a large-scale unbalanced audio subset of AudioSet. The experimental results show that classification performance is significantly improved by involving sound classes that are semantically close to the test classes in training. Meanwhile, we demonstrate that both label embeddings and sentence embeddings are useful for zero-shot learning. Classification performance is improved by concatenating label/sentence embeddings generated with different language models. With their hybrid concatenations, the results are improved further.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1233–1242},
numpages = {10}
}

@article{10.1145/3616017,
author = {Smith, Ronnie and Dragone, Mauro},
title = {Generalisable Dialogue-based Approach for Active Learning of Activities of Daily Living},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3616017},
doi = {10.1145/3616017},
abstract = {While Human Activity Recognition systems may benefit from Active Learning by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in activity recognition systems, which utilises semantic similarity measures and a dataset of natural language descriptions of common activities (which we make publicly available). Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply this approach to two active learning scenarios: (i) using an existing CASAS dataset, demonstrating long-term usage; and (ii) using an online activity recognition system, which tackles the issue of online segmentation and labelling. We demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) quantitatively, as an efficient way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to compare our approach and an established method. Results show the potential of our approach as a hands-free interface for annotation of sensor data as part of an active learning system. We provide insights into the challenges of active learning for activity recognition under real-world conditions and identify potential ways to address them.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = sep,
articleno = {18},
numpages = {37},
keywords = {Human-in-the-Loop (HITL) annotation, Active Learning (AL), natural language, semantic similarity, Human Activity Recognition (HAR) labelling}
}

@proceedings{10.1145/3589462,
title = {IDEAS '23: Proceedings of the 27th International Database Engineered Applications Symposium},
year = {2023},
isbn = {9798400707445},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Heraklion, Crete, Greece}
}

@inproceedings{10.1145/3626705.3627775,
author = {Gallo, Simone and Paterno, Fabio and Malizia, Alessio},
title = {Conversational Interfaces in IoT Ecosystems: Where We Are, What Is Still Missing},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3627775},
doi = {10.1145/3626705.3627775},
abstract = {In the last few years, text and voice-based conversational agents have become more and more popular all over the world as virtual assistants for a variety of tasks. In addition, the deployment on the market of many smart objects connected with these agents has introduced the possibility of controlling and personalising the behaviour of several connected objects using natural language. This has the potential to allow people, also those without a technical background, to effectively control and use the wide variety of connected objects and services. In this paper, we present an analysis of how conversational agents have been used to interact with smart environments (such as smart homes). For this purpose, we have carried out a systematic literature review considering publications selected from the ACM and IEEE digital libraries to investigate the technologies used to design and develop conversational agents for IoT settings, including Artificial Intelligence techniques, the purpose that they have been used for, and the level of user involvement in such studies. The resulting analysis is useful to better understand how this field is evolving and indicate the challenges still open in this area that should be addressed in future research work to allow people to completely benefit from this type of solution.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {279–293},
numpages = {15},
keywords = {Conversational Agents, Internet of Things, User Experience},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1145/3490099.3511130,
author = {Smith, Ronnie and Dragone, Mauro},
title = {A Dialogue-Based Interface for Active Learning of Activities of Daily Living},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511130},
doi = {10.1145/3490099.3511130},
abstract = {While Human Activity Recognition (HAR) systems may benefit from Active Learning (AL) by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in HAR systems, which utilises a dataset of natural language descriptions of common activities (which we make publicly available) and semantic similarity measures. Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply our work to an existing CASAS dataset in an active learning scenario, to demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) technically, as an effective way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to use our approach and an established method, and to subsequently compare the two. Results show the potential of our approach as a user-friendly mechanism for annotation of sensor data as part of an active learning system.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {820–831},
numpages = {12},
keywords = {Active Learning (AL), Human Activity Recognition (HAR) labelling, Human-in-the-Loop (HITL) annotation, natural language, semantic similarity},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1145/3533020,
author = {Sun, Kai and Zhang, Richong and Mensah, Samuel and Mao, Yongyi and Liu, Xudong},
title = {Learning Implicit and Explicit Multi-task Interactions for Information Extraction},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3533020},
doi = {10.1145/3533020},
abstract = {Information extraction aims at extracting entities, relations, and so on, in text to support information retrieval systems. To extract information, researchers have considered multitask learning (ML) approaches. The conventional ML approach learns shared features across tasks, with the assumption that these features capture sufficient task interactions to learn expressive shared representations for task classification. However, such an assumption is flawed in different perspectives. First, the shared representation may contain noise introduced by another task; tasks coupled for multitask learning may have different complexities but this approach treats all tasks equally; the conventional approach has a flat structure that hinders the learning of explicit interactions. This approach, however, learns implicit interactions across tasks and often has a generalization ability that has benefited the learning of multitasks. In this article, we take advantage of implicit interactions learned by conventional approaches while alleviating the issues mentioned above by developing a Recurrent Interaction Network with an effective Early Prediction Integration (RIN-EPI) for multitask learning. Specifically, RIN-EPI learns implicit and explicit interactions across two different but related tasks. To effectively learn explicit interactions across tasks, we consider the correlations among the outputs of related tasks. It is, however, obvious that task outputs are unobservable during training, so we leverage the predictions at intermediate layers (referred to as early predictions) as proxies as well as shared features across tasks to learn explicit interactions through attention mechanisms and sequence learning models. By recurrently learning explicit interactions, we gradually improve predictions for the individual tasks in the multitask learning. We demonstrate the effectiveness of RIN-EPI on the learning of two mainstream multitasks for information extraction: (1) entity recognition and relation classification and (2) aspect and opinion term co-extraction. Extensive experiments demonstrate the effectiveness of the RIN-EPI architecture, where we achieve state-of-the-art results on several benchmark datasets.},
journal = {ACM Trans. Inf. Syst.},
month = apr,
articleno = {27},
numpages = {29},
keywords = {Multitask learning, information extraction}
}

@proceedings{10.1145/3540250,
title = {ESEC/FSE 2022: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of all members of the organizing committee, we are delighted to welcome everyone to the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2022. The event continues the long, distinguished ESEC/FSE tradition of presenting the most innovative research, and facilitating interactions between scientists and engineers who are passionate about advancing the theory and practice of software engineering.},
location = {Singapore, Singapore}
}

@article{10.1109/TASLP.2021.3138670,
author = {Li, Qian and Peng, Hao and Li, Jianxin and Wu, Jia and Ning, Yuanxing and Wang, Lihong and Yu, Philip S. and Wang, Zheng},
title = {Reinforcement Learning-Based Dialogue Guided Event Extraction to Exploit Argument Relations},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3138670},
doi = {10.1109/TASLP.2021.3138670},
abstract = {Event extraction is a ftask for natural language processing. Finding the roles of event arguments like event participants is essential for event extraction. However, doing so for real-life event descriptions is challenging because an argument’s role often varies in different contexts. While the relationship and interactions between multiple arguments are useful for settling the argument roles, such information is largely ignored by existing approaches. This paper presents a better approach for event extraction by explicitly utilizing the relationships of event arguments. We achieve this through a carefully designed task-oriented dialogue system. To model the argument relation, we employ reinforcement learning and incremental learning to extract multiple arguments via a multi-turned, iterative process. Our approach leverages knowledge of the already extracted arguments of the same sentence to determine the role of arguments that would be difficult to decide individually. It then uses the newly obtained information to improve the decisions of previously extracted arguments. This two-way feedback process allows us to exploit the argument relations to effectively settle argument roles, leading to better sentence understanding and event extraction. Experimental results show that our approach consistently outperforms seven state-of-the-art event extraction methods for the classification of events and argument role and argument identification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {520–533},
numpages = {14}
}

@article{10.1145/3588722,
author = {Genossar, Bar and Shraga, Roee and Gal, Avigdor},
title = {FlexER: Flexible Entity Resolution for Multiple Intents},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588722},
doi = {10.1145/3588722},
abstract = {Entity resolution, a longstanding problem of data cleaning and integration, aims at identifying data records that represent the same real-world entity. Existing approaches treat entity resolution as a universal task, assuming the existence of a single interpretation of a real-world entity and focusing only on finding matched records, separating corresponding from non-corresponding ones, with respect to this single interpretation. However, in real-world scenarios, where entity resolution is part of a more general data project, downstream applications may have varying interpretations of real-world entities relating, for example, to various user needs. In what follows, we introduce the problem of multiple intents entity resolution (MIER), an extension to the universal (single intent) entity resolution task. As a solution, we propose FlexER, utilizing contemporary solutions to universal entity resolution tasks to solve MIER. FlexER addresses the problem as a multi-label classification problem. It combines intent-based representations of tuple pairs using a multiplex graph representation that serves as an input to a graph neural network (GNN). FlexER learns intent representations and improves the outcome to multiple resolution problems. A large-scale empirical evaluation introduces a new benchmark and, using also two well-known benchmarks, shows that FlexER effectively solves the MIER problem and outperforms the state-of-the-art for a universal entity resolution.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {42},
numpages = {27},
keywords = {entity matching, entity resolution, graph neural networks, supervised learning}
}

@proceedings{10.1145/3549555,
title = {CBMI '22: Proceedings of the 19th International Conference on Content-based Multimedia Indexing},
year = {2022},
isbn = {9781450397209},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Graz, Austria}
}

@article{10.1145/3440755,
author = {Chandrasekaran, Dhivya and Mago, Vijay},
title = {Evolution of Semantic Similarity—A Survey},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3440755},
doi = {10.1145/3440755},
abstract = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network–based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {41},
numpages = {37},
keywords = {Semantic similarity, corpus-based methods, knowledge-based methods, linguistics, supervised and unsupervised methods, word embeddings}
}

@article{10.1109/TCBB.2019.2937771,
author = {Gao, Jianliang and Tian, Ling and Lv, Tengfei and Wang, Jianxin and Song, Bo and Hu, Xiaohua},
title = {Protein2Vec: Aligning Multiple PPI Networks with Representation Learning},
year = {2021},
issue_date = {Jan.-Feb. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2937771},
doi = {10.1109/TCBB.2019.2937771},
abstract = {Research of Protein-Protein Interaction (PPI) Network Alignment is playing an important role in understanding the crucial underlying biological knowledge such as functionally homologous proteins and conserved evolutionary pathways across different species. Existing methods of PPI network alignment often try to improve the coverage ratio of the alignment result by aligning all proteins from different species. However, there is a fundamental biological premise that needs to be considered carefully: not every protein in a species can, nor should, find its homologous proteins in other species. In this work, we propose a novel alignment method to map only those proteins with the most similarity throughout the PPI networks of multiple species. For the similarity features of the protein in the networks, we integrate both topological features with biological characteristics to provide enhanced supports for the alignment procedures. For topological features, we apply a representation learning method on the networks and generate a low dimensional vector embedding with its surrounding structural features for each protein. The topological similarity of proteins from different PPI networks can thus be transferred as the similarity of their corresponding vector representations, which provides a new way to comprehensively quantify the topological similarities between proteins. We also propose a new measure for the topological evaluation of the alignment results which better uncover the structural quality of the alignment across multiple networks. Both biological and topological evaluations on the alignment results of real datasets demonstrate our approach is promising and preferable against previous multiple alignment methods.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {240–249},
numpages = {10}
}

@inproceedings{10.1145/3628454.3629551,
author = {Nimpattanavong, Chollakorn and Taveekitworachai, Pittawat and Khan, Ibrahim and Nguyen, Thai Van and Thawonmas, Ruck and Choensawat, Worawat and Sookhanaphibarn, Kingkarn},
title = {Am I Fighting Well? Fighting Game Commentary Generation With ChatGPT},
year = {2023},
isbn = {9798400708497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628454.3629551},
doi = {10.1145/3628454.3629551},
abstract = {This paper presents a new approach for leveraging ChatGPT in fighting game commentary generation task. Commentary generation often relies on deep learning techniques, which typically demand extensive data to achieve effectiveness. Large language models (LLMs) have become essential due to their remarkable ability to process data efficiently, thanks to their extensive training on vast datasets. Our proposed approach integrates the use of LLMs, specifically the GPT-3.5 model, for generating commentaries through the utilization of various prompts with data from the open-source fighting game, DareFightingICE. Four prompt variants are employed to assess the effectiveness of each prompt components. Objective evaluation using natural language metrics reveals that different prompt components significantly affect the generated commentaries. Additionally, subjective evaluation through a questionnaire reveals that prompts without parameter definitions received the highest preference from human evaluators. These results suggest that LLMs exhibit versatility in generating fighting game commentaries and hold promise for broader applications.},
booktitle = {Proceedings of the 13th International Conference on Advances in Information Technology},
articleno = {14},
numpages = {7},
keywords = {ChatGPT, Commentary Generation, DareFightingICE, Fighting Game, Prompt Engineering},
location = {Bangkok, Thailand},
series = {IAIT '23}
}

@inproceedings{10.1109/ASE51524.2021.9678894,
author = {Shi, Lin and Jiang, Ziyou and Yang, Ye and Chen, Xiao and Zhang, Yumin and Mu, Fangwen and Jiang, Hanzhi and Wang, Qing},
title = {ISPY: automatic issue-solution pair extraction from community live chats},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678894},
doi = {10.1109/ASE51524.2021.9678894},
abstract = {Collaborative live chats are gaining popularity as a development communication tool. In community live chatting, developers are likely to post issues they encountered (e.g., setup issues and compile issues), and other developers respond with possible solutions. Therefore, community live chats contain rich sets of information for reported issues and their corresponding solutions, which can be quite useful for knowledge sharing and future reuse if extracted and restored in time. However, it remains challenging to accurately mine such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the problem of issue-solution pair extraction from developer live chat data, and propose an automated approach, named ISPY, based on natural language processing and deep learning techniques with customized enhancements, to address the problem. Specifically, ISPY automates three tasks: 1) Disentangle live chat logs, employing a feedforward neural network to disentangle a conversation history into separate dialogs automatically; 2) Detect dialogs discussing issues, using a novel convolutional neural network (CNN), which consists of a BERT-based utterance embedding layer, a context-aware dialog embedding layer, and an output layer; 3) Extract appropriate utterances and combine them as corresponding solutions, based on the same CNN structure but with different feeding inputs. To evaluate ISPY, we compare it with six baselines, utilizing a dataset with 750 dialogs including 171 issue-solution pairs and evaluate ISPY from eight open source communities. The results show that, for issue-detection, our approach achieves the F1 of 76%, and outperforms all baselines by 30%. Our approach achieves the F1 of 63% for solution-extraction and outperforms the baselines by 20%. Furthermore, we apply ISPY on three new communities to extensively evaluate ISPY's practical usage. Moreover, we publish over 30K issue-solution pairs extracted from 11 communities. We believe that ISPY can facilitate community-based software development by promoting knowledge sharing and shortening the issue-resolving process.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {142–154},
numpages = {13},
location = {Melbourne, Australia},
series = {ASE '21}
}

@proceedings{10.1145/3600006,
title = {SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Proceedings of the 29th ACM Symposium on Operating Systems Principles (SOSP 2023). This year's program includes 43 papers that reflect today's broad range of topics that comprise modern computer systems research. The program committee carefully reviewed submitted papers and worked closely with the authors of selected papers to produce the collection of high-quality, readable papers presented here. We hope that you enjoy the program!},
location = {Koblenz, Germany}
}

@proceedings{10.1145/3584871,
title = {ICSIM '23: Proceedings of the 2023 6th International Conference on Software Engineering and Information Management},
year = {2023},
isbn = {9781450398237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Palmerston North, New Zealand}
}

@inproceedings{10.1145/3397481.3450697,
author = {Karimi, Pegah and Plebani, Emanuele and Bolchini, Davide},
title = {Textflow: Screenless Access to Non-Visual Smart Messaging},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450697},
doi = {10.1145/3397481.3450697},
abstract = {Texting relies on screen-centric prompts designed for sighted users, still posing significant barriers to people who are blind and visually impaired (BVI). Can we re-imagine texting untethered from a visual display? In an interview study, 20 BVI adults shared situations surrounding their texting practices, recurrent topics of conversations, and challenges. Informed by these insights, we introduce TextFlow: a mixed-initiative context-aware system that generates entirely auditory message options relevant to the users’ location, activity, and time of the day. Users can browse and select suggested aural messages using finger-taps supported by an off-the-shelf finger-worn device, without having to hold or attend to a mobile screen. In an evaluative study, 10 BVI participants successfully interacted with TextFlow to browse and send messages in screen-free mode. The experiential response of the users shed light on the importance of bypassing the phone and accessing rapidly controllable messages at their fingertips while preserving privacy and accuracy with respect to speech or screen-based input. We discuss how non-visual access to proactive, contextual messaging can support the blind in a variety of daily scenarios.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {186–196},
numpages = {11},
keywords = {Assistive technologies, Aural navigation, Intelligent wearable and mobile interfaces, Text entry, Ubiquitous smart environments},
location = {College Station, TX, USA},
series = {IUI '21}
}

@proceedings{10.1145/3549737,
title = {SETN '22: Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Corfu, Greece}
}

@proceedings{10.1145/3594739,
title = {UbiComp/ISWC '23 Adjunct: Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cancun, Quintana Roo, Mexico}
}

@article{10.1145/3577925,
author = {Schiappa, Madeline C. and Rawat, Yogesh S. and Shah, Mubarak},
title = {Self-Supervised Learning for Videos: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3577925},
doi = {10.1145/3577925},
abstract = {The remarkable success of deep learning in various domains relies on the availability of large-scale annotated datasets. However, obtaining annotations is expensive and requires great effort, which is especially challenging for videos. Moreover, the use of human-generated annotations leads to models with biased learning and poor domain generalization and robustness. As an alternative, self-supervised learning provides a way for representation learning that does not require annotations and has shown promise in both image and video domains. In contrast to the image domain, learning video representations are more challenging due to the temporal dimension, bringing in motion and other environmental dynamics. This also provides opportunities for video-exclusive ideas that advance self-supervised learning in the video and multimodal domains. In this survey, we provide a review of existing approaches on self-supervised learning focusing on the video domain. We summarize these methods into four different categories based on their learning objectives: (1) pretext tasks, (2) generative learning, (3) contrastive learning, and (4) cross-modal agreement. We further introduce the commonly used datasets, downstream evaluation tasks, insights into the limitations of existing works, and the potential future directions in this area.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {288},
numpages = {37},
keywords = {Self-supervised learning, deep learning, video understanding, zero-shot learning, representation learning, multimodal learning, visual-language models}
}

@article{10.1145/3603499,
author = {Sangsavate, Suntarin and Sinthupinyo, Sukree and Chandrachai, Achara},
title = {Experiments of Supervised Learning and Semi-Supervised Learning in Thai Financial News Sentiment: A Comparative Study},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3603499},
doi = {10.1145/3603499},
abstract = {Sentiment classification is an instrument of natural language processing tasks in text analysis to measure customer feedback from given documents such as product reviews, news, and texts. This research aims to experiment with Thai financial news sentiment classification and evaluate sentiment classification performance. In this research, we show financial news sentiment classification experimental results when comparing supervised and semi-supervised methods. In the research methodology, we use PyThaiNLP to tokenize and remove stopwords and split datasets into 85% of the training set and 15% of the testing set. Next, we classify sentiment using machine learning and deep learning approaches with feature extraction such as bag-of-words, term frequency–inverse document frequency, and word embedding (Word2Vec and Bidirectional Encoder Representations from Transformers (BERT)) in given texts. The results show that support vector machine with the BERT model yields the best performance at 83.38%; in contrast, the random forest classifier with bag-of-words yields the worst performance at 54.10% in the machine learning approach. Another experiment reveals that long short-term memory with the BERT model yields the best performance at 84.07% in contrast to the convolutional neural network with bag-of-words, which yields the worst performance at 69.80% in the deep learning approach. The results imply that support vector machine, convolutional neural network, and long short-term memory are suitable for classifying sentiment in complex structure language. From this study, we observe the importance of sentiment classification tools between supervised and semi-supervised learning, and we look forward to furthering this work.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {197},
numpages = {36},
keywords = {Natural language processing, semi-supervised learning, sentiment classification, supervised learning, Thai language}
}

@inproceedings{10.1145/3324884.3416668,
author = {Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and Grunske, Lars},
title = {MoFuzz: a fuzzer suite for testing model-driven software engineering tools},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416668},
doi = {10.1145/3324884.3416668},
abstract = {Fuzzing or fuzz testing is an established technique that aims to discover unexpected program behavior (e.g., bugs, security vulnerabilities, or crashes) by feeding automatically generated data into a program under test. However, the application of fuzzing to test Model-Driven Software Engineering (MDSE) tools is still limited because of the difficulty of existing fuzzers to provide structured, well-typed inputs, namely models that conform to typing and consistency constraints induced by a given meta-model and underlying modeling framework. By drawing from recent advances on both fuzz testing and automated model generation, we present three different approaches for fuzzing MDSE tools: A graph grammar-based fuzzer and two variants of a coverage-guided mutation-based fuzzer working with different sets of model mutation operators. Our evaluation on a set of real-world MDSE tools shows that our approaches can outperform both standard fuzzers and model generators w.r.t. their fuzzing capabilities. Moreover, we found that each of our approaches comes with its own strengths and weaknesses in terms of fault finding capabilities and the ability to cover different aspects of the system under test. Thus the approaches complement each other, forming a fuzzer suite for testing MDSE tools.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1103–1115},
numpages = {13},
keywords = {automated model generation, eclipse modeling framework, fuzzing, model-driven software engineering, modeling tools},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@proceedings{10.1145/3544548,
title = {CHI '23: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hamburg, Germany}
}

@proceedings{10.1145/3579375,
title = {ACSW '23: Proceedings of the 2023 Australasian Computer Science Week},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@article{10.1145/3519263,
author = {Karimi, Pegah and Plebani, Emanuele and Martin-Hammond, Aqueasha and Bolchini, Davide},
title = {Textflow: Toward Supporting Screen-free Manipulation of Situation-Relevant Smart Messages},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3519263},
doi = {10.1145/3519263},
abstract = {Texting relies on screen-centric prompts designed for sighted users, still posing significant barriers to people who are blind and visually impaired (BVI). Can we re-imagine texting untethered from a visual display? In an interview study, 20 BVI adults shared situations surrounding their texting practices, recurrent topics of conversations, and challenges. Informed by these insights, we introduce TextFlow, a mixed-initiative context-aware system that generates entirely auditory message options relevant to the users’ location, activity, and time of the day. Users can browse and select suggested aural messages using finger-taps supported by an off-the-shelf finger-worn device without having to hold or attend to a mobile screen. In an evaluative study, 10 BVI participants successfully interacted with TextFlow to browse and send messages in screen-free mode. The experiential response of the users shed light on the importance of bypassing the phone and accessing rapidly controllable messages at their fingertips while preserving privacy and accuracy with respect to speech or screen-based input. We discuss how non-visual access to proactive, contextual messaging can support the blind in a variety of daily scenarios.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = nov,
articleno = {31},
numpages = {29},
keywords = {Text entry, assistive technologies, intelligent wearable and mobile interfaces, aural navigation, ubiquitous smart environments}
}

@inproceedings{10.1145/3607541.3616821,
author = {Zou, Jialing and Mei, Jiahao and Ye, Guangze and Huai, Tianyu and Shen, Qiwei and Dong, Daoguo},
title = {EMID: An Emotional Aligned Dataset in Audio-Visual Modality},
year = {2023},
isbn = {9798400702785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607541.3616821},
doi = {10.1145/3607541.3616821},
abstract = {In this paper, we propose Emotionally paired Music and Image Dataset (EMID), a novel dataset designed for the emotional matching of music and images, to facilitate auditory-visual cross-modal tasks such as generation and retrieval. Unlike existing approaches that primarily focus on semantic correlations or roughly divided emotional relations, EMID emphasizes the significance of emotional consistency between music and images using an advanced 13-dimension emotional model. By incorporating emotional alignment into the dataset, it aims to establish pairs that closely align with human perceptual understanding, thereby raising the performance of auditory-visual cross-modal tasks. We also design a supplemental module named EMI-Adapter to optimize existing cross-modal alignment methods. To validate the effectiveness of the EMID, we conduct a psychological experiment, which has demonstrated that considering the emotional relationship between the two modalities effectively improves the accuracy of matching in abstract perspective. This research lays the foundation for future cross-modal research in domains such as psychotherapy and contributes to advancing the understanding and utilization of emotions in cross-modal alignment. The EMID dataset is available at https://github.com/ecnu-aigc/EMID.},
booktitle = {Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice},
pages = {41–48},
numpages = {8},
keywords = {music-image dataset, emotional matching, cross-modal alignment},
location = {Ottawa ON, Canada},
series = {McGE '23}
}

@article{10.1145/3604550,
author = {Liu, Yaochen and Li, Qiuchi and Wang, Benyou and Zhang, Yazhou and Song, Dawei},
title = {A Survey of Quantum-cognitively Inspired Sentiment Analysis Models},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604550},
doi = {10.1145/3604550},
abstract = {Quantum theory, originally proposed as a physical theory to describe the motions of microscopic particles, has been applied to various non-physics domains involving human cognition and decision-making that are inherently uncertain and exhibit certain non-classical, quantum-like characteristics. Sentiment analysis is a typical example of such domains. In the last few years, by leveraging the modeling power of quantum probability (a non-classical probability stemming from quantum mechanics methodology) and deep neural networks, a range of novel quantum-cognitively inspired models for sentiment analysis have emerged and performed well. This survey presents a timely overview of the latest developments in this fascinating cross-disciplinary area. We first provide a background of quantum probability and quantum cognition at a theoretical level, analyzing their advantages over classical theories in modeling the cognitive aspects of sentiment analysis. Then, recent quantum-cognitively inspired models are introduced and discussed in detail, focusing on how they approach the key challenges of the sentiment analysis task. Finally, we discuss the limitations of the current research and highlight future research directions.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {15},
numpages = {37},
keywords = {emotion recognition, sarcasm detection, sentiment analysis, non-classical probability from quantum mechanics methodology, Quantum-cognitively inspired models}
}

@proceedings{10.1145/3579370,
title = {SYSTOR '23: Proceedings of the 16th ACM International Conference on Systems and Storage},
year = {2023},
isbn = {9781450399623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Haifa, Israel}
}

@proceedings{10.1145/3533271,
title = {ICAIF '22: Proceedings of the Third ACM International Conference on AI in Finance},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3583678,
title = {DEBS '23: Proceedings of the 17th ACM International Conference on Distributed and Event-based Systems},
year = {2023},
isbn = {9798400701221},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {DEBS 2023 is the seventeenth in a series that spans more than 20 years of history, with 16 past editions as a conference and five editions as a workshop co-located with major conferences.The objectives of DEBS have been to provide a forum dedicated to the dissemination of original research, the discussion of practical insights, and the reporting of experiences relevant to distributed systems and event-based computing. The conference provides a forum for academia and industry to exchange ideas through its tutorials, research papers, and the grand challenge. Recently, the ACM International Conference on Distributed and Event-Based Systems, including DEBS 2022, has become the premier venue for cutting-edge research in the integration of distributed and event-based systems in relevant domains such as Big Data, AI, ML, IoT, and Blockchain.},
location = {Neuchatel, Switzerland}
}

@proceedings{10.1145/3598469,
title = {dg.o '23: Proceedings of the 24th Annual International Conference on Digital Government Research},
year = {2023},
isbn = {9798400708374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Gda?sk, Poland}
}

@proceedings{10.1145/3597638,
title = {ASSETS '23: Proceedings of the 25th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2023},
isbn = {9798400702204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@proceedings{10.1145/3611450,
title = {AI2A '23: Proceedings of the 2023 3rd International Conference on Artificial Intelligence, Automation and Algorithms},
year = {2023},
isbn = {9798400707605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@article{10.1145/3494560,
author = {Abulaish, Muhammad and Fazil, Mohd and Zaki, Mohammed J.},
title = {Domain-Specific Keyword Extraction Using Joint Modeling of Local and Global Contextual Semantics},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3494560},
doi = {10.1145/3494560},
abstract = {Domain-specific keyword extraction is a vital task in the field of text mining. There are various research tasks, such as spam e-mail classification, abusive language detection, sentiment analysis, and emotion mining, where a set of domain-specific keywords (aka lexicon) is highly effective. Existing works for keyword extraction list all keywords rather than domain-specific keywords from a document corpus. Moreover, most of the existing approaches perform well on formal document corpuses but fail on noisy and informal user-generated content in online social media. In this article, we present a hybrid approach by jointly modeling the local and global contextual semantics of words, utilizing the strength of distributional word representation and contrasting-domain corpus for domain-specific keyword extraction. Starting with a seed set of a few domain-specific keywords, we model the text corpus as a weighted word-graph. In this graph, the initial weight of a node (word) represents its semantic association with the target domain calculated as a linear combination of three semantic association metrics, and the weight of an edge connecting a pair of nodes represents the co-occurrence count of the respective words. Thereafter, a modified PageRank method is applied to the word-graph to identify the most relevant words for expanding the initial set of domain-specific keywords. We evaluate our method over both formal and informal text corpuses (comprising six datasets), and show that it performs significantly better in comparison to state-of-the-art methods. Furthermore, we generalize our approach to handle the language-agnostic case, and show that it outperforms existing language-agnostic approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {70},
numpages = {30},
keywords = {language-agnostic keyword extraction, domain-specific keyword extraction, information extraction, Text mining}
}

@proceedings{10.1145/3538969,
title = {ARES '22: Proceedings of the 17th International Conference on Availability, Reliability and Security},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@inproceedings{10.1145/3587259.3627564,
author = {Jia, Yue-Bo and Johnson, Gavin and Arnold, Alex and Heflin, Jeff},
title = {An Evaluation of Strategies to Train More Efficient Backward-Chaining Reasoners},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627564},
doi = {10.1145/3587259.3627564},
abstract = {Knowledge bases traditionally require manual optimization to ensure reasonable performance when answering queries. We build on previous work on training a deep learning model to learn heuristics for answering queries by comparing different representations of the sentences contained in knowledge bases. We decompose the problem into issues of representation, training, and control and propose solutions for each subproblem. We evaluate different configurations on three synthetic knowledge bases. In particular we compare a novel representation approach based on learning to maximize similarity of logical atoms that unify and minimize similarity of atoms that do not unify, to two vectorization strategies taken from the automated theorem proving literature: a chain-based and a 3-term-walk strategy. We also evaluate the efficacy of pruning the search by ignoring rules with scores below a threshold.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {206–213},
numpages = {8},
keywords = {backward chaining, efficient queries, knowledge bases, machine learning, meta-reasoning, neurosymbolic AI},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@proceedings{10.1145/3617694,
title = {EAAMO '23: Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
year = {2023},
isbn = {9798400703812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Boston, MA, USA}
}

@proceedings{10.1145/3552326,
title = {EuroSys '23: Proceedings of the Eighteenth European Conference on Computer Systems},
year = {2023},
isbn = {9781450394871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rome, Italy}
}

@proceedings{10.1145/3581784,
title = {SC '23: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Started in 1988, the SC Conference has become the annual nexus for researchers and practitioners from academia, industry and government to share information and foster collaborations to advance the state of the art in High Performance Computing (HPC), Networking, Storage, and Analysis.},
location = {Denver, CO, USA}
}

@proceedings{10.1145/3582437,
title = {FDG '23: Proceedings of the 18th International Conference on the Foundations of Digital Games},
year = {2023},
isbn = {9781450398558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3576842,
title = {IoTDI '23: Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {San Antonio, TX, USA}
}

@proceedings{10.1145/3543712,
title = {ICCTA '22: Proceedings of the 2022 8th International Conference on Computer Technology Applications},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vienna, Austria}
}

@proceedings{10.1145/3597926,
title = {ISSTA 2023: Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ISSTA 2023, the 32nd edition of the International Symposium on Software Testing and Analysis, to be held on July 18–20, 2023 in Seattle, USA. The symposium has become a premier scientific event in the expanding area of software testing and analysis, with a strong appeal to researchers from all continents.},
location = {Seattle, WA, USA}
}

@proceedings{10.1145/3579142,
title = {BiDEDE '23: Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},
year = {2023},
isbn = {9798400700934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@proceedings{10.1145/3600160,
title = {ARES '23: Proceedings of the 18th International Conference on Availability, Reliability and Security},
year = {2023},
isbn = {9798400707728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Benevento, Italy}
}

@proceedings{10.1145/3569951,
title = {PEARC '23: Practice and Experience in Advanced Research Computing 2023: Computing for the Common Good},
year = {2023},
isbn = {9781450399852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Portland, OR, USA}
}

@proceedings{10.1145/3625403,
title = {ADMIT '23: Proceedings of the 2023 2nd International Conference on Algorithms, Data Mining, and Information Technology},
year = {2023},
isbn = {9798400707629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chengdu, China}
}

@proceedings{10.1145/3569219,
title = {Academic Mindtrek '22: Proceedings of the 25th International Academic Mindtrek Conference},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tampere, Finland}
}

@proceedings{10.1145/3558100,
title = {DocEng '22: Proceedings of the 22nd ACM Symposium on Document Engineering},
year = {2022},
isbn = {9781450395441},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The symposium brings together experts in all areas of document engineering, across academia and industry, with the intention of presenting and discussing the most recent advances in the field of Document Engineering.},
location = {San Jose, California}
}

@proceedings{10.1145/3617233,
title = {CBMI '23: Proceedings of the 20th International Conference on Content-based Multimedia Indexing},
year = {2023},
isbn = {9798400709128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orleans, France}
}

@proceedings{10.1145/3577190,
title = {ICMI '23: Proceedings of the 25th International Conference on Multimodal Interaction},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Paris, France}
}

@proceedings{10.1145/3568739,
title = {ICDTE '22: Proceedings of the 6th International Conference on Digital Technology in Education},
year = {2022},
isbn = {9781450398091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3615522,
title = {VINCI '23: Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guangzhou, China}
}

@proceedings{10.1145/3606843,
title = {ITCC '23: Proceedings of the 2023 5th International Conference on Information Technology and Computer Communications},
year = {2023},
isbn = {9798400700583},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tianjin, China}
}

@proceedings{10.1145/3603555,
title = {MuC '23: Proceedings of Mensch und Computer 2023},
year = {2023},
isbn = {9798400707711},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Rapperswil, Switzerland}
}

@proceedings{10.1145/3587281,
title = {W4A '23: Proceedings of the 20th International Web for All Conference},
year = {2023},
isbn = {9798400707483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3584318,
title = {NSPW '22: Proceedings of the 2022 New Security Paradigms Workshop},
year = {2022},
isbn = {9781450398664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {North Conway, NH, USA}
}

@proceedings{10.1145/3569966,
title = {CSSE '22: Proceedings of the 5th International Conference on Computer Science and Software Engineering},
year = {2022},
isbn = {9781450397780},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Guilin, China}
}

@proceedings{10.1145/3625135,
title = {DLfM '23: Proceedings of the 10th International Conference on Digital Libraries for Musicology},
year = {2023},
isbn = {9798400708336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Milan, Italy}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3563357,
title = {BuildSys '22: Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the past thirteen years, BuildSys has been an interdisciplinary conference that brings together various stakeholders, including researchers, practitioners, and policymakers from different disciplines, including civil engineering, mechanical engineering, environmental science, electrical and computer engineering, computer science, system management and control, and many others. This year is no exception, with papers and attendees from all these disciplines and regions worldwide. The conference's focus extends beyond building systems to the built environment more generally.},
location = {Boston, Massachusetts}
}

@proceedings{10.1145/3543829,
title = {CUI '22: Proceedings of the 4th Conference on Conversational User Interfaces},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Glasgow, United Kingdom}
}

@proceedings{10.1145/3544902,
title = {ESEM '22: Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Helsinki, Finland}
}

@proceedings{10.1145/3517428,
title = {ASSETS '22: Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility},
year = {2022},
isbn = {9781450392587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3583131,
title = {GECCO '23: Proceedings of the Genetic and Evolutionary Computation Conference},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3583133,
title = {GECCO '23 Companion: Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {GECCO is the largest peer-reviewed conference in the field of Evolutionary Computation, and the main conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO) of the Association for Computing Machinery (ACM).},
location = {Lisbon, Portugal}
}

@proceedings{10.1145/3524458,
title = {GoodIT '22: Proceedings of the 2022 ACM Conference on Information Technology for Social Good},
year = {2022},
isbn = {9781450392846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@proceedings{10.1145/3545008,
title = {ICPP '22: Proceedings of the 51st International Conference on Parallel Processing},
year = {2022},
isbn = {9781450397339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Bordeaux, France}
}

@proceedings{10.1145/3592813,
title = {SBSI '23: Proceedings of the XIX Brazilian Symposium on Information Systems},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macei\'{o}, Brazil}
}

@proceedings{10.1145/3593663,
title = {ECSEE '23: Proceedings of the 5th European Conference on Software Engineering Education},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seeon/Bavaria, Germany}
}

@proceedings{10.1145/3624007,
title = {GPCE 2023: Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts &amp; Experiences (GPCE’23). GPCE is the premiere venue for researchers and practitioners interested in techniques that use program generation to increase programmer productivity, improve software quality, and shorten the time-to-market of software products. In addition to exploring cutting-edge techniques of generative software, GPCE seeks to foster cross-fertilization between the programming languages research communities.},
location = {Cascais, Portugal}
}

@proceedings{10.1145/3555858,
title = {FDG '22: Proceedings of the 17th International Conference on the Foundations of Digital Games},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3560905,
title = {SenSys '22: Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM SenSys 2022, the 20th ACM Conference on Embedded Networked Sensor Systems, the premier computer systems conference focused on networked sensing systems and applications.},
location = {Boston, Massachusetts}
}

@proceedings{10.1145/3564625,
title = {ACSAC '22: Proceedings of the 38th Annual Computer Security Applications Conference},
year = {2022},
isbn = {9781450397599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Austin, TX, USA}
}

@proceedings{10.1145/3575879,
title = {PCI '22: Proceedings of the 26th Pan-Hellenic Conference on Informatics},
year = {2022},
isbn = {9781450398541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Athens, Greece}
}

@proceedings{10.1145/3548785,
title = {IDEAS '22: Proceedings of the 26th International Database Engineered Applications Symposium},
year = {2022},
isbn = {9781450397094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Budapest, Hungary}
}

@proceedings{10.1145/3573834,
title = {AISS '22: Proceedings of the 4th International Conference on Advanced Information Science and System},
year = {2022},
isbn = {9781450397933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sanya, China}
}

@proceedings{10.1145/3556384,
title = {SPML '22: Proceedings of the 2022 5th International Conference on Signal Processing and Machine Learning},
year = {2022},
isbn = {9781450396912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@proceedings{10.5555/3571885,
title = {SC '22: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {This volume, containing the accepted technical papers and ACM Gordon Bell prize finalists, captures the best current research in all aspects of High Performance Computing (HPC). The SC22 Archive at the conference web site sc22.supercomputing.org complements this volume by collecting other high quality, peer-reviewed material including research posters, the visualization &amp; data analytics showcase, panels, birds of a feather, workshops, and tutorials.},
location = {Dallas, Texas}
}

@proceedings{10.1145/3527188,
title = {HAI '22: Proceedings of the 10th International Conference on Human-Agent Interaction},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Christchurch, New Zealand}
}

@proceedings{10.1145/3500868,
title = {CSCW'22 Companion: Companion Publication of the 2022 Conference on Computer Supported Cooperative Work and Social Computing},
year = {2022},
isbn = {9781450391900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Virtual Event, Taiwan}
}

@proceedings{10.1145/3577530,
title = {CSAI '22: Proceedings of the 2022 6th International Conference on Computer Science and Artificial Intelligence},
year = {2022},
isbn = {9781450397773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@book{10.1145/3563659,
editor = {Lugrin, Birgit and Pelachaud, Catherine and Traum, David},
title = {The Handbook on Socially Interactive Agents: 20 years of Research on Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics Volume 2: Interactivity, Platforms, Application},
year = {2022},
isbn = {9781450398961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {48},
abstract = {The Handbook on Socially Interactive Agents provides a comprehensive overview of the research fields of Embodied Conversational Agents, Intelligent Virtual Agents, and Social Robotics. Socially Interactive Agents (SIAs), whether virtually or physically embodied, are autonomous agents that are able to perceive an environment including people or other agents, reason, and decide how to interact, and express attitudes such as emotions, engagement, or empathy. They are capable of interacting with people and each other in a socially intelligent manner using multimodal communicative behaviors with the goal to support humans in various domains.Written by international experts in their respective fields, the book summarizes research in the many important research communities pertinent for SIAs, while discussing current challenges and future directions. The handbook provides easy access to modeling and studying SIAs for researchers and students and aims at further bridging the gap between the research communities involved.In two volumes, the book clearly structures the vast body of research. The first volume starts by introducing what is involved in SIAs research, in particular research methodologies and ethical implications of developing SIAs. It further examines research on appearance and behavior, focusing on multimodality. Finally, social cognition for SIAs is investigated by different theoretical models and phenomena such as theory of mind or pro-sociality. The second volume starts with perspectives on interaction, examined from different angles such as interaction in social space, group interaction, or long-term interaction. It also includes an extensive overview summarizing research and systems of human-agent platforms and of some of the major application areas of SIAs such as education, aging support, autism or games.}
}

@proceedings{10.5555/3581644,
title = {CNSM '22: Proceedings of the 18th International Conference on Network and Service Management},
year = {2022},
isbn = {9783903176515},
publisher = {International Federation for Information Processing},
address = {Laxenburg, AUT},
abstract = {CNSM 2022 focuses on the theme "Intelligent Management of Disruptive Network Technologies and Services", that aims at capturing emerging approaches and intelligent solutions for dealing with disruptive network technologies, as well as associated services and applications.},
location = {Thessaloniki, Greece}
}

@proceedings{10.1145/3561613,
title = {ICCCV '22: Proceedings of the 5th International Conference on Control and Computer Vision},
year = {2022},
isbn = {9781450397315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Xiamen, China}
}

@proceedings{10.1145/3545948,
title = {RAID '22: Proceedings of the 25th International Symposium on Research in Attacks, Intrusions and Defenses},
year = {2022},
isbn = {9781450397049},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3582580,
title = {ICETM '22: Proceedings of the 2022 5th International Conference on Education Technology Management},
year = {2022},
isbn = {9781450398015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lincoln, United Kingdom}
}

@proceedings{10.1145/3585967,
title = {icWCSN '23: Proceedings of the 2023 10th International Conference on Wireless Communication and Sensor Networks},
year = {2023},
isbn = {9781450398466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chengdu, China}
}

@proceedings{10.1145/3582099,
title = {AICCC '22: Proceedings of the 2022 5th Artificial Intelligence and Cloud Computing Conference},
year = {2022},
isbn = {9781450398749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Osaka, Japan}
}

@proceedings{10.1145/3539637,
title = {WebMedia '22: Proceedings of the Brazilian Symposium on Multimedia and the Web},
year = {2022},
isbn = {9781450394093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Curitiba, Brazil}
}

@proceedings{10.1145/3590837,
title = {ICIMMI '22: Proceedings of the 4th International Conference on Information Management &amp; Machine Intelligence},
year = {2022},
isbn = {9781450399937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Jaipur, India}
}

@proceedings{10.1145/3616712,
title = {ICEME '23: Proceedings of the 2023 14th International Conference on E-business, Management and Economics},
year = {2023},
isbn = {9798400708022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@book{10.1145/3477355,
editor = {Jones, Cliff B. and Misra, Jayadev},
title = {Theories of Programming: The Life and Works of Tony Hoare},
year = {2021},
isbn = {9781450387286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {39},
abstract = {Sir Tony Hoare has had an enormous influence on computer science, from the Quicksort algorithm to the science of software development, concurrency and program verification. His contributions have been widely recognised: He was awarded the ACM’s Turing Award in 1980, the Kyoto Prize from the Inamori Foundation in 2000, and was knighted for “services to education and computer science” by Queen Elizabeth II of England in 2000.This book presents the essence of his various works—the quest for effective abstractions—both in his own words as well as chapters written by leading experts in the field, including many of his research collaborators. In addition, this volume contains biographical material, his Turing award lecture, the transcript of an interview and some of his seminal papers.Hoare’s foundational paper “An Axiomatic Basis for Computer Programming”, presented his approach, commonly known as Hoare Logic, for proving the correctness of programs by using logical assertions. Hoare Logic and subsequent developments have formed the basis of a wide variety of software verification efforts. Hoare was instrumental in proposing the Verified Software Initiative, a cooperative international project directed at the scientific challenges of large-scale software verification, encompassing theories, tools and experiments.Tony Hoare’s contributions to the theory and practice of concurrent software systems are equally impressive. The process algebra called Communicating Sequential Processes (CSP) has been one of the fundamental paradigms, both as a mathematical theory to reason about concurrent computation as well as the basis for the programming language occam. CSP served as a framework for exploring several ideas in denotational semantics such as powerdomains, as well as notions of abstraction and refinement. It is the basis for a series of industrial-strength tools which have been employed in a wide range of applications.This book also presents Hoare’s work in the last few decades. These works include a rigorous approach to specifications in software engineering practice, including procedural and data abstractions, data refinement, and a modular theory of designs. More recently, he has worked with collaborators to develop Unifying Theories of Programming (UTP). Their goal is to identify the common algebraic theories that lie at the core of sequential, concurrent, reactive and cyber-physical computations. Theories of Programming: The Life and Works of Tony Hoare’ is available as a printed book (DOI: ) and an on-line version. In addition to the book itself, a number of on-line resources might be of interest to readers:
A bibliography of Tony Hoare’s papers with clickable DOIs/URLs where available (ACM: INSERT URL)Appendix E of the book provides links to talks and interviews featuring Tony Hoare ()The Oxford archive of Hoare’s manuscripts:  
Supplementary Material: Tony Hoare’ is a PDF of additional material (not included in the book) containing the following:
Stories from a Life in Interesting Times (A transcription by Jayadev Misra of Tony Hoare’s acceptance speech for the 2000 Kyoto prize)Tony Hoare’s Heidelberg comments: (A transcription by Margaret Gray of Tony Hoare’s part in the 2020 Heidelberg event)Milestones in Tony’s Life and Work: A ‘cv’ of Tony Hoare prepared by Margaret GrayExtended version - ’Bernard Sufrin: Teaching at Belfast and Oxford’}
}

@proceedings{10.1145/3613424,
title = {MICRO '23: Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Toronto, ON, Canada}
}

@proceedings{10.1145/3593743,
title = {C&amp;T '23: Proceedings of the 11th International Conference on Communities and Technologies},
year = {2023},
isbn = {9798400707582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Lahti, Finland}
}

@proceedings{10.1145/3568231,
title = {SIET '22: Proceedings of the 7th International Conference on Sustainable Information Engineering and Technology},
year = {2022},
isbn = {9781450397117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Malang, Indonesia}
}

